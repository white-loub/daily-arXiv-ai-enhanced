<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 183]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.LG](#cs.LG) [Total: 184]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.MA](#cs.MA) [Total: 10]
- [cs.RO](#cs.RO) [Total: 62]
- [cs.AI](#cs.AI) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 本文提出了一种基于单人动作捕捉数据的交互式AI舞蹈生成模型，结合扩散模型、动作补全与风格迁移，实现与人类舞者的创造性互动。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型缺乏具身化的人机交互体验，而舞蹈作为一种原始的人类表达形式，有望弥补这一缺陷，增强人机互动的创造性与自然性。

Method: 构建一个基于单人动作捕捉（MoCap）数据的交互式生成模型，利用高阶特征而非低层次人际互动数据，结合两种扩散模型、动作补全和动作风格迁移技术，生成在时间上连贯且对输入动作有响应的舞蹈动作序列。

Result: 通过量化生成样本与测试集特征分布的收敛性验证了模型的有效性，生成的舞蹈动作既多样化又逼真，表现出与人类舞者不同的创造性变体，同时保持现实感。

Conclusion: 该模型是迈向创造性人机共舞的重要第一步，能够在无需双人互动数据的情况下实现对人类舞蹈的模仿与创造性增强。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 本研究提出了一种基于机器学习的珊瑚白化分类系统，使用全球多样化数据集训练并比较了ResNet、ViT和CNN三种模型，结果显示CNN在准确率上表现最佳（88%），为珊瑚礁的自动监测提供了有效方案。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、海洋酸化和海水温度异常等日益严重的威胁，亟需高效的保护与监测手段。

Method: 基于包含健康与白化珊瑚样本的全球多样化数据集，采用ResNet、ViT和CNN三种先进的计算机视觉模型进行珊瑚白化分类，并通过全面的超参数调优进行性能比较。

Result: 经过调优后，CNN模型达到了88%的最高准确率，优于现有基准，表现出最优性能。

Conclusion: 该研究为自主珊瑚监测提供了可行的技术路径，并对主流视觉模型在珊瑚分类中的应用进行了系统评估，具有实际应用价值。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 本研究评估了基于YOLOv8的深度学习流程，用于自动化识别肯尼亚和坦桑尼亚海域视频样带中的鱼类科级分类，首次为西印度洋珊瑚礁鱼类监测提供了区域特定基准。


<details>
  <summary>Details</summary>
Motivation: 由于水下视觉普查劳动强度大，西印度洋珊瑚礁监测受限，亟需自动化方法提升效率。

Method: 采用YOLOv8深度学习模型，对来自肯尼亚和坦桑尼亚的视频样带数据进行鱼类科级识别，测试了24个鱼类科在不同配置下的表现。

Result: 最佳模型在mAP@0.5指标上达到0.52，对常见鱼类科识别准确率高，但对稀有或形态复杂类群检测效果较弱。

Conclusion: 深度学习有望作为传统监测方法的可扩展补充手段，提升西印度洋珊瑚礁鱼类监测效率。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 本文提出一种基于互信息的数据增强方法，通过从真实分布中计算样本间的互信息来选择训练数据，从而提升对比学习中特征表示的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习中的数据增强依赖人工设计，可能次优；希望利用互信息自动选择更具信息量的正样本以提升模型在开放环境中的泛化性能。

Method: 利用自然扰动（如颜色变化和运动）下场景中图像块之间的高互信息作为正样本，结合对比损失进行表示学习。

Result: 在多个主流表示学习框架和基准上验证了该方法的有效性，相比传统数据增强策略能带来性能提升。

Conclusion: 基于互信息的数据选择是一种有前景的方法，可减少对人工增强策略的依赖，并推动更鲁棒的自监督学习发展。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯表示和运动扩散先验的零样本4D人体运动生成框架MSDI，通过结合大语言模型和运动扩散分数蒸馏采样，生成符合物体和语义约束的自然且物理合理的4D人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成高质量视频时仍存在不真实的变形、语义冲突和物理不一致问题，主要源于缺乏3D物理先验。因此需要一种能够融合3D空间信息与语义理解的生成框架来提升人体运动的真实性和合理性。

Method: 提出Motion Score Distilled Interaction (MSDI) 框架，利用预生成的3D人体与物体，结合大语言模型（LLMs）的空间与提示语义信息，以及提出的Motion Diffusion Score Distillation Sampling (MSDS) 方法，从预训练的运动扩散模型中蒸馏得分梯度，实现空间感知的运动优化，无需联合训练即可生成符合上下文约束的人体动作。

Result: 实验表明，该方法能生成自然且物理上合理的人体运动，更好地尊重3D空间上下文和语义约束，在未见过的物体交互场景中也具备良好泛化能力，优于依赖有限数据集训练的先前方法。

Conclusion: MSDI为4D人体运动生成提供了一个可扩展、无需重训练的零样本解决方案，有效结合了3D几何表示、运动先验与语义理解，显著提升了生成动作的真实性与物理合理性。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [6] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本研究评估了三种主流联邦学习框架（NVIDIA FLARE、Flower 和 Owkin Substra）在医学影像应用中的适用性，发现各框架在性能、可扩展性、隐私保护和开发体验方面各有优势，适用于不同医疗场景。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗AI中具有巨大潜力，但不同框架在实际医疗环境中的表现尚不明确，需系统评估以指导实际部署。

Method: 使用PathMNIST数据集，对NVIDIA FLARE、Flower和Owkin Substra三个联邦学习框架进行基准测试，评估指标包括模型性能、收敛效率、通信开销、可扩展性和开发者体验。

Result: NVIDIA FLARE在生产环境可扩展性方面表现最佳，Flower适合原型设计和学术研究，Owkin Substra在隐私保护和合规性方面表现出色。

Conclusion: 三种联邦学习框架各有优势，应根据具体医疗应用场景选择合适的框架，以实现高效、安全的协作建模。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [7] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Oitijjo-3D的免费生成式AI框架，利用公开的Google街景图像，通过多模态视觉推理和神经图像到3D生成技术，快速重建孟加拉国文化遗产的逼真3D模型，显著降低了传统3D数字化对硬件和专家的依赖。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国在文化遗产修复方面面临资源有限和技术专家稀缺的双重挑战，传统的3D数字化方法成本高、操作复杂，难以广泛应用。

Method: 采用两阶段流程：首先使用Gemini 2.5 Flash Image进行结构-纹理合成的多模态视觉推理，然后通过Hexagen实现几何恢复的神经图像到3D生成，基于Google街景图像重建3D模型。

Result: 系统能在几秒内生成具有照片真实感且度量一致的3D重建结果，在Ahsan Manzil、Choto Sona清真寺和Paharpur等遗址上验证了其在视觉和结构保真度方面的有效性，并显著优于传统运动恢复结构（SfM）方法的速度。

Conclusion: Oitijjo-3D通过将开放图像转化为数字遗产，为资源有限国家提供了一种社区驱动、AI辅助的文化延续性保护新范式。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [8] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对常用的图像去噪方法结合CLAHE（限制对比度自适应直方图均衡化）在水稻叶片图像去噪中的效果进行了广泛的比较研究，旨在提升图像质量以支持病害检测和生长分析等农业应用。


<details>
  <summary>Details</summary>
Motivation: 图像增强是图像处理链中的关键预处理步骤，对于提高水稻叶片图像质量、辅助疾病检测和营养缺乏评估具有重要意义。需要系统评估不同去噪方法的性能。

Method: 采用多种经典图像去噪方法与CLAHE相结合，在水稻叶片图像数据集上进行实验，并使用多种评价指标对结果进行综合评估。

Result: 通过多种指标验证了不同去噪方法结合CLAHE的增强效果，识别出若干高效的方法组合，显著提升了图像质量和后续分析的可靠性。

Conclusion: 该研究为数字图像处理方法的有效性提供了坚实基础，揭示了适用于农业研究及其他领域的图像增强策略，具有广泛的应用前景。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [9] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: 提出了一种基于异构图神经网络的多智能体辩论框架GraphGeo，用于视觉地理定位，通过建模不同类型的辩论关系显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多样地理区域和复杂场景时表现有限，且多智能体系统缺乏有效处理预测冲突的机制。

Method: 构建了一个包含支持性协作、竞争性论证和知识迁移的多智能体辩论框架，采用双层辩论机制和跨层次拓扑优化策略，结合节点级精炼与边级论证建模。

Result: 在多个基准测试上实验表明，GraphGeo显著优于当前最先进的方法。

Conclusion: GraphGeo能将智能体间的认知冲突转化为更准确的地理定位结果，提升了视觉地理定位的性能。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [10] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 本文提出了一个名为“InfraLiDARs' Benchmark”的新数据集，用于系统研究不同LiDAR扫描模式对路侧感知性能的影响，发现非重复扫描LiDAR与128线重复扫描LiDAR在检测性能上相当，且更具成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注不同LiDAR扫描模式对基础设施感知性能的影响，尤其是重复与非重复扫描模式的差异，亟需系统性评估以指导实际部署。

Method: 在CARLA仿真环境中构建包含多种扫描模式的基础设施LiDAR数据集，并进行统计分析和多种3D目标检测算法的性能评估。

Result: 非重复扫描LiDAR与128线重复扫描LiDAR在各类场景下检测性能相近，尽管前者感知范围较短，但因其低成本而具备优势。

Conclusion: 研究为路侧感知系统的LiDAR选型和算法匹配提供了实践指导，并公开数据集以促进后续研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [11] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文指出了语音驱动3D手势生成领域中人类评估方法缺乏标准化和存在缺陷的问题，提出了一种针对BEAT2数据集的详细人类评估协议，并通过大规模众包实验评估了六种最新模型在动作真实感和语音-手势对齐方面的表现。结果表明，新模型并未持续优于旧方法，已有研究中的性能声明在严格评估下可能不成立，强调需分离评估动作质量和多模态对齐以推动领域发展。作者还将公开合成动作、视频刺激、渲染脚本及1.6万条人类偏好投票，促进标准化和后续研究。


<details>
  <summary>Details</summary>
Motivation: 当前语音驱动3D手势生成领域的用户评估缺乏统一标准，常采用有缺陷的实验设计，导致无法准确比较不同方法的性能，阻碍了领域进展。因此，亟需建立标准化的人类评估协议以实现可靠、可比的基准测试。

Method: 提出了一种详细的标准化人类评估协议，基于广泛使用的BEAT2动作捕捉数据集，采用大规模众包方式进行双维度评估：动作真实感和语音-手势对齐。所有六种被测模型均由原作者训练，确保公平比较。同时，公开大量生成内容和评估数据以支持复现与进一步研究。

Result: 1) 更新的手势生成模型并未在所有指标上 consistently 超过早期方法；2) 许多论文中声称的高真实感或良好对齐性在严格的人类评估下未能得到验证；3) 成功实施了标准化评估流程，并收集了16,000条人类偏好数据，为未来研究提供了可靠基准。

Conclusion: 该研究揭示了当前语音驱动手势生成领域评估方法的不足，证明了标准化人类评估的重要性。未来应采用解耦的评估方式分别衡量动作质量和多模态对齐，并呼吁社区采纳统一协议以实现可比性和可重复性，从而真正推动技术进步。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [12] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5 是一种基于流架构的物理AI世界基础模型，统一了文本、图像和视频到世界的生成，并通过强化学习优化，在视频质量和指令对齐方面显著优于前代模型；配套的 Cosmos-Transfer2.5 框架实现了高保真、长时域的仿真到真实世界转换，且模型更小更高效。


<details>
  <summary>Details</summary>
Motivation: 为了提升物理AI中世界模拟的准确性与可控性，支持机器人和自主系统所需的可靠合成数据生成与闭环仿真，推动具身智能的发展。

Method: 采用流-based架构，结合 Cosmos-Reason1 视觉语言模型增强文本理解与控制能力；在2亿个视频片段上训练，并使用基于强化学习的后训练优化；同时推出控制网风格的 Cosmos-Transfer2.5 框架用于Sim2Real和Real2Real转换。

Result: Cosmos-Predict2.5 在视频质量与指令对齐上显著优于前代模型，支持2B和14B两种规模；Cosmos-Transfer2.5 虽然比前代小3.5倍，但生成视频质量更高、更稳定，具备更强的长时域建模能力。

Conclusion: Cosmos-Predict2.5 和 Cosmos-Transfer2.5 构成了一个高效、可控且可扩展的物理AI模拟框架，为具身智能的研究与应用提供了强大工具，且通过开源促进社区发展。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [13] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于MoE LoRA策略的统一框架HMVLM，用于融合3D人体运动与文本模态，通过零专家机制缓解灾难性遗忘，并采用身体部位特定的分词方法提升姿态表示的空间分辨率，在多个人体运动下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决3D人体运动与文本模态融合中的模态差距问题，防止在指令调优过程中发生灾难性遗忘，并开发适用于自回归模型且具有跨任务泛化能力的姿态表示方法。

Method: 提出HMVLM框架，采用Mixture of Expert Low-Rank Adaptation（MoE LoRA）策略，利用门控网络动态分配LoRA专家权重；引入零专家以保留预训练语言参数；并通过身体部位分组进行特定分词以增强姿态表示。

Result: 实验表明该方法有效缓解了指令调优过程中的知识遗忘问题，并在多种人体运动相关下游任务中实现了卓越性能。

Conclusion: HMVLM通过动态专家分配和零专家机制，在保持语言能力的同时成功融合了人体运动与文本模态，为多模态理解与生成提供了有效的解决方案。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [14] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 本研究利用深度学习对奥地利Gesäuse国家公园的高山栖息地进行变化检测，比较了后分类与直接变化检测方法，并评估了地理空间基础模型（如Clay v1.0）在复杂自然环境中的表现，结果显示Clay模型在多时相和多模态数据下具有更强的鲁棒性和更高的精度。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统面临快速气候变化和干扰，需要高频次栖息地监测，但传统人工制图成本高、效率低，且现有地理空间基础模型在模糊类别边界和类别不平衡的复杂自然环境中应用不足。

Method: 采用深度学习方法，比较后分类变化检测（使用Prithvi-EO-2.0、Clay v1.0和U-Net CNN）与直接变化检测（使用ChangeViT和U-Net基线），利用高分辨率多模态数据（RGB、NIR、LiDAR、地形属性）进行实验。

Result: Clay v1.0在多类栖息地变化检测中达到51%整体准确率，优于U-Net的41%；二分类检测中两者均达67%准确率；直接变化检测在二分类中IoU为0.53（优于后分类的0.35），但多类检测仅28%准确率；加入LiDAR使语义分割准确率从30%提升至50%；跨时相验证显示Clay在2020年数据上保持33%准确率，优于U-Net的23%。

Conclusion: 地理空间基础模型（尤其是Clay v1.0）在复杂高山环境中表现出优于传统CNN的性能和更强的时间泛化能力，结合LiDAR可显著提升精度，尽管整体精度低于均质景观，但仍反映了真实生态场景下的实用潜力，未来可通过引入对象级后处理和物理约束进一步优化。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [15] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 提出了一种基于学习的无监督方法，用于生成具有局部特征（如污渍、撕裂、孔洞等）的可控纹理合成与编辑系统。


<details>
  <summary>Details</summary>
Motivation: 为了更真实地合成纹理，需要包含自然界中普遍存在的表面外观变化（如瑕疵、磨损等），而现有方法通常依赖人工标注，缺乏自动化和可控性。

Method: 采用无监督异常检测技术从无标签样本中自动识别外观改变的特征，并将这些特征聚类为语义一致的组，用于指导条件图像生成；结合扩散模型实现纹理编辑和无限静态纹理生成。

Result: 实现了从少量图像构建多功能生成模型，支持用户交互式地在任意大小的纹理上创建和绘制局部特征，且算法具有通用性，可用于其他场景。

Conclusion: 该方法有效实现了无需人工标注的高质量、可控纹理合成与编辑，提升了纹理生成的真实感和实用性。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [16] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一种无需训练且高效的扩散视频生成加速框架，通过将缓存调度建模为带误差权重的有向图，并引入字典序极小极大路径优化策略，有效控制全局误差累积，显著提升生成视频的全局内容与风格一致性。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略多关注局部启发式误差的减少，忽视了全局误差的累积，导致加速视频与原始视频之间出现明显的内容退化问题。

Method: 将缓存调度建模为带有误差权重的有向图，提出字典序极小极大路径优化（Lexicographic Minimax Path Optimization）策略，显式约束最坏情况下的路径误差，从而优化全局生成一致性。

Result: 在多个文本到视频生成基准上实验表明，LeMiCa在推理速度和生成质量上均有提升，例如在Latte模型上实现2.9倍加速，在Open-Sora上LPIPS得分达到0.05，优于先前缓存方法，且感知质量损失极小。

Conclusion: LeMiCa是一种鲁棒且可泛化的扩散视频生成加速范式，能有效平衡速度与质量，为高效可靠的视频合成研究提供了坚实基础。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [17] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为Probe, Learn, Distill (PLD) 的三阶段框架，通过残差强化学习和分布感知数据收集来提升视觉-语言-动作模型（VLA），在多个任务上实现了接近饱和的成功率。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）依赖昂贵的人类示范，限制了VLA模型的可扩展性和泛化能力，因此需要一种更高效、可扩展的自我改进方法。

Method: PLD分为三个阶段：1）训练轻量级残差actor探测VLA的失败区域；2）采用混合rollout策略收集与部署分布对齐且包含恢复行为的数据；3）通过标准SFT将精选轨迹蒸馏回通用VLA模型。

Result: 在LIBERO上达到99%的任务成功率，在SimplerEnv上提升超过50%，在真实世界的Franka和YAM机械臂操作任务中实现100%成功率。消融实验表明残差探测和分布感知回放对提升新旧任务性能至关重要。

Conclusion: PLD提供了一条可扩展的路径，使VLA模型能够通过自我改进不断优化性能，减少对人工标注数据的依赖。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [18] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 本文提出了一种用于脊柱CT图像分割的多模态视觉-语言交互系统SpinalSAM-R1，结合了微调的SAM模型与DeepSeek-R1，通过解剖学引导注意力机制和语义驱动交互协议提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型在脊柱CT图像上受限于高标注成本和较差的领域适应性，且因低对比度和复杂边界导致分割困难。

Method: 提出SpinalSAM-R1，融合细调的SAM与大语言模型DeepSeek-R1；引入解剖学引导注意力机制和基于自然语言的交互协议，并采用LoRA进行高效微调。

Result: 在脊柱CT数据上验证，分割性能优越；开发了基于PyQt5的交互软件，支持点、框和文本提示，支持11项临床操作，解析准确率达94.3%，响应时间低于800ms。

Conclusion: SpinalSAM-R1显著提升了脊柱CT图像的分割精度与交互能力，具备良好的临床应用潜力，代码已开源。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [19] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 本文提出了一种针对共聚焦激光显微内镜（CLE）视频序列的滤波方法，以减少自监督学习（SSL）中的数据冗余，提升训练效率和模型性能。实验表明，使用滤波后数据预训练的模型在两个医学数据集上均取得最佳准确率，且训练时间减少了67%。


<details>
  <summary>Details</summary>
Motivation: CLE图像难以解读，且缺乏与组织病理学标注匹配的大规模数据集，导致机器学习模型易过拟合。自监督学习可缓解标注不足问题，但CLE视频帧间高度相关，造成数据分布不均衡，影响SSL训练效果。

Method: 提出一种针对CLE视频序列的滤波功能，以降低数据冗余；采用四种主流网络结构及基于视觉Transformer的师生网络，在鼻窦肿瘤和皮肤鳞状细胞癌数据集上评估下游任务性能。

Result: 过滤后的SSL预训练模型在两个数据集上分别达到67.48%和73.52%的测试准确率，显著优于非SSL基线模型，并使训练时间减少67%。

Conclusion: 自监督学习是有效的CLE预训练方法，而所提出的视频滤波策略能显著提升SSL训练效率和模型性能。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [20] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: 本文提出了FreeSliders，一种无需训练且适用于多模态的细粒度概念控制生成方法，通过推理时部分估计Concept Sliders公式实现跨图像、视频和音频的即插即用式语义编辑，并构建了首个支持多模态评估的基准，提出新的评估指标与两阶段自适应遍历策略以提升编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有的概念控制生成方法（如Concept Sliders）依赖于每概念训练和特定架构微调，难以扩展到新模态，缺乏通用性和可扩展性。

Method: FreeSliders在推理过程中部分估计Concept Sliders的公式，实现完全无需训练的概念控制；引入两阶段遍历策略自动检测饱和点并进行非线性重参数化，以实现感知一致且语义合理的编辑。

Result: 在图像、视频和音频多个模态上验证了FreeSliders的有效性，实现了优于现有基线的训练-free概念控制效果，同时发布了首个支持多模态细粒度控制评估的基准及交互式展示平台。

Conclusion: FreeSliders是一种通用、无需训练的多模态概念控制方法，推动了细粒度可控生成的研究，为未来跨模态可控生成提供了新工具和评估体系。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [21] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI提出了一种新的分层框架，用于高质量的文本到视频生成，通过组合场景理解与时间感知扩散模型，在保持时空一致性的同时实现细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在时间一致性、组合理解及对视觉叙事的精细控制方面存在不足。

Method: 引入三个关键组件：组合场景解析器（CSP）将文本分解为带时间标注的层次化场景图；时空注意力机制（TSAM）确保帧间运动连贯性并保留空间细节；渐进式视频优化模块（PVR）通过多尺度时间推理迭代提升视频质量。

Result: 在标准基准上的实验表明，MOVAI在LPIPS、FVD和用户偏好研究中分别提升了15.3%、12.7%和18.9%，尤其擅长生成具有真实时间动态和细粒度语义控制的复杂多对象场景。

Conclusion: MOVAI在文本到视频生成任务中实现了最先进的性能，有效解决了时间一致性与语义控制难题。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [22] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出了一种受认知启发的“时间链”方法，用于改进和解释视觉-语言模型中的物理模拟，无需微调即可在推理时使用，并在合成和真实世界场景中显著提升生成模型性能。


<details>
  <summary>Details</summary>
Motivation: 受机器学习中的上下文推理和人类心智模拟的启发，旨在提升视觉-语言模型对物理过程的理解与生成能力。

Method: 通过在推理过程中生成一系列中间图像，构建“时间链”进行物理模拟，无需额外训练或微调。

Result: 在2D图形模拟和3D自然视频等任务中显著提升生成模型性能，揭示了模型在速度、重力、碰撞等动态物理属性上的模拟能力，并发现了其在从输入图像推断某些物理参数方面的局限性。

Conclusion: Chain of Time方法不仅提升了物理模拟效果，还为理解生成模型内部动态提供了可解释性工具。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [23] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 提出首个结合生成式AI与深度强化学习（DRL）的端到端框架，实现可重复、自主的心脏超声扫描。


<details>
  <summary>Details</summary>
Motivation: 现有DRL方法在心脏超声扫描中缺乏可重复性、依赖专有数据且模型简化，同时临床操作受限于人为因素和专业人员短缺，亟需自动化解决方案。

Method: 框架包含两部分：一是结合GAN与VAE的条件生成模拟器，生成逼真的动作相关超声图像；二是利用该模拟器训练DRL模块以学习自主扫描策略，并通过公开真实数据集确保可重复性。

Result: 所提VAE-GAN在定性和定量评估中优于现有GAN变体，DRL系统在不同配置下均表现出有效扫描能力，框架支持图像类型分类、质量评估及条件图像生成。

Conclusion: 该框架为心脏超声提供了可重复、可扩展的自动化解决方案，有望提升医疗可及性并推动AI在医学影像中的应用。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [24] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D提出了一种双流架构，结合视觉与几何信息，实现鲁棒且精确的6D姿态估计，在遮挡等复杂场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从合成数据泛化到真实场景时，面对光照变化、无纹理物体和严重遮挡时表现脆弱，需提升鲁棒性与泛化能力。

Method: 采用双流架构：使用自监督Vision Transformer（DINOv2）处理RGB图像以应对纹理和光照变化，同时用PointNet++处理深度生成的点云进行几何推理；两路特征融合后送入多任务预测头。

Result: 在Occluded-LineMOD数据集上取得SOTA性能，验证了方法在遮挡、光照变化等挑战下的优越精度与鲁棒性。

Conclusion: VLM6D通过融合视觉与几何双流特征，显著提升了6D姿态估计在复杂真实场景中的性能，具备强泛化能力和应用潜力。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [25] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: 本文提出了Vote-in-Context（ViC），一种无需训练的通用框架，将视觉语言模型（VLM）用于跨模态视频检索中的列表重排序与融合，通过在提示中序列化内容证据和检索器元数据，显著提升了零样本下的检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统检索融合方法仅依赖排名或分数信号，忽略候选内容的表示，难以有效处理复杂的多模态视频数据，因此需要一种能结合内容信息与检索器元数据的更优融合策略。

Method: 提出Vote-in-Context（ViC）框架，利用视觉语言模型进行零样本推理，引入S-Grid将视频表示为图像网格并结合字幕，在提示中序列化候选内容与检索器信息，实现自适应的列表级重排序与融合。

Result: 在MSR-VTT和VATEX等视频检索基准上，ViC实现了新的零样本性能纪录，例如在VATEX上v2t Recall@1达到99.6%，相比先前最优方法提升高达+40。

Conclusion: ViC是一种简单、可复现且高效的框架，能够将现代视觉语言模型转化为强大的零样本重排序器和融合器，适用于复杂多模态检索任务。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [26] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 本文提出了一种结合ConvNeXt和Vision Transformers的混合架构，用于面部图像年龄估计，在多个基准数据集上取得了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升年龄估计的准确性，利用CNN的局部特征提取能力和Transformer的全局注意力机制的互补优势。

Method: 提出一种ConvNeXt-ViT混合模型，使用预训练模型、线性层和高级正则化技术进行优化，并通过消融实验验证各组件的作用。

Result: 在MORPH II、CACD和AFAD数据集上实现了更低的平均绝对误差（MAE），证明了模型的有效性。

Conclusion: 混合架构在年龄估计任务中表现出色，为CNN与Transformer的融合提供了有前景的方向。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [27] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: 本文提出了一种基于设施选址函数的高效视觉令牌压缩框架FLoC，用于解决长视频理解中视觉令牌过多导致的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 由于长视频生成的视觉令牌数量庞大，现有视频大模型在处理长视频时面临可扩展性受限的问题。

Method: 采用设施选址函数和懒惰贪心算法，在预定义的令牌数量预算内快速选择紧凑、有代表性且多样化的视觉令牌子集，实现无需训练、模型无关且查询无关的压缩。

Result: 在Video-MME、MLVU和LongVideoBench等多个大规模基准上，FLoC显著优于现有的压缩方法，在保持近似最优性能的同时大幅减少视觉令牌数量，并提升处理速度。

Conclusion: FLoC是一种高效、通用且无需训练的视觉令牌压缩方案，能有效支持多种视频大模型，显著提升长视频理解任务的效率与性能。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [28] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种增强图像保护中对抗性噪声鲁棒性的新方法，通过自适应区域高斯模糊调整噪声的频率谱，提升了对多种噪声逆转技术的防御能力，并减少了感知质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性噪声保护方法容易被简单技术（如JPEG压缩）逆转，缺乏实际安全性，因此需要更不可逆且难以检测的保护机制。

Method: 提出一种简单的自适应区域高斯模糊方法，针对不同区域调整对抗噪声的频率分布，使其更难被分离或逆转。

Result: 实验表明该方法在多种图像编辑场景和逆转技术下， consistently 提升了现有方法的最坏情况保护性能，同时降低了噪声引起的感知质量下降。

Conclusion: 通过引入频率域上的自适应模糊，可以使对抗性噪声更不可逆且更实用，为图像防编辑提供了更可靠的保护方案。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [29] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: 本文提出了CompAgent，首个用于视觉合规性验证的代理框架，通过结合多模态大语言模型与多种视觉工具（如目标检测、面部分析、NSFW检测等），实现对复杂政策规则的细粒度视觉内容审查。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于特定任务的深度学习模型和人工标注数据，成本高且泛化能力有限；而现有的多模态大模型虽具备广泛的知识和政策理解能力，但在细粒度视觉推理和结构化规则应用上表现不足。

Method: 提出CompAgent框架，包含一个规划代理和一个验证代理：规划代理根据合规策略动态选择合适的视觉工具（如检测器、分析器等）；验证代理则融合图像、工具输出和策略上下文进行多模态推理。

Result: 在公开基准测试中，CompAgent优于专用分类器、直接提示MLLM和定制路由基线，在UnsafeBench数据集上达到最高76%的F1分数，比现有最优方法提升10%。

Conclusion: 实验证明，基于代理的规划与工具增强推理能够有效提升视觉合规验证的准确性、可扩展性和适应性，为内容审核提供了更具前景的解决方案。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [30] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为AIFo的基于多智能体协作的图像取证框架，通过结合多种取证工具和结构化辩论机制，在跨源证据推理的基础上实现了对AI生成图像的高精度、可解释检测。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法在可解释性、泛化性和多源信息整合方面存在局限，传统分类器缺乏解释能力，视觉语言模型受限于单次分析和像素级推理。

Method: 设计了一个无需训练的多智能体框架AIFo，利用LLM驱动的智能体协调反向图像搜索、元数据提取、预训练分类器和视觉语言模型等多种取证工具，通过多智能体辩论机制解决证据冲突，并引入记忆增强推理模块从历史案例中学习以提升性能。

Result: 在6000张图像上进行评估，涵盖实验室环境和真实场景，AIFo达到97.05%的准确率，显著优于传统分类器和最先进的视觉语言模型。

Conclusion: 基于智能体的程序化推理为AI生成图像检测提供了更鲁棒、可解释且适应性强的新范式。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [31] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于能量的多提示学习方法（EMPL），用于视觉-语言预训练模型中的多提示学习，通过从由VLM隐式定义的能量分布中采样生成多个提示嵌入，在参数高效的同时实现了领域内与领域外开放词汇泛化的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于单提示范式，缺乏对多提示学习潜力的深入探索。本文旨在系统回顾并挖掘视觉-语言多提示学习的技术潜力。

Method: 提出能量-based多提示学习（EMPL），利用VLM隐式定义的能量分布生成多个可学习的提示嵌入，实现多提示增强的视觉-语言迁移。

Result: 实验证明EMPL在多个下游任务上优于单提示方法，并在开放词汇识别中展现出更好的领域内外泛化能力。

Conclusion: 多提示学习优于单提示范式，EMPL提供了一种参数高效且理论严谨的方法，推动了VLM在有限资源下的适应性发展。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [32] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的迁移学习方法，用于低地球轨道卫星地面终端组件对天气状况（如雪、湿等）的细粒度检测，提升了卫星互联网在恶劣天气下的可靠性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气严重影响卫星互联网地面终端的性能和可靠性，现有方法缺乏有效的细粒度天气条件检测能力，难以支持故障诊断与应对。

Method: 采用高效的迁移学习（TL）方法，使地面终端能够本地化检测代表性的天气相关状态，如积雪、湿润等，并与YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等主流深度学习模型进行对比。

Result: 所提迁移学习方法在检测精度上优于YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法，且具备更强的泛化能力，适用于多种场景。

Conclusion: 该迁移学习方法能有效提升卫星互联网地面终端对天气干扰的感知能力，具有良好的实用性、高效性和跨场景推广潜力，有助于增强系统可靠性和运维效率。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [33] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: 提出DM-QPMNet，一种用于单次定量相位显微成像中细胞分割的双编码器网络，通过多头注意力融合偏振强度图像和相位图的模态特异性特征，实现更鲁棒的分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统阈值方法对噪声和细胞密度敏感，现有深度学习方法未能充分利用偏振强度图像与相位图之间的互补性。

Method: 设计双编码器网络DM-QPMNet，分别处理偏振强度和相位图；通过中间层的多头注意力机制进行内容感知的特征融合，并引入双源跳跃连接和单模态归一化以提升稳定性与融合效果。

Result: 在多种基准上显著优于单一流水线拼接和单模态基线方法，验证了模态特异性编码与可学习融合策略的有效性。

Conclusion: DM-QPMNet通过有原则的多模态融合策略，有效利用ssQPM中光照与相位信息的互补性，实现了鲁棒的细胞分割。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [34] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 提出基于VQ-VAE的压缩框架，支持16x到1024x压缩比，实现按需解码和ROI驱动的高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 应对EB级电子显微镜数据在存储、传输和分析上的极限挑战。

Method: 采用向量量化变分自编码器（VQ-VAE）结合Transformer先验模型，通过FiLM和拼接恢复纹理，并设计ROI驱动的工作流实现局部高分辨率重建。

Result: 实现了高达1024倍的压缩比，支持极端压缩下的顶部解码，并可通过Transformer预测底层token恢复细节。

Conclusion: 该框架显著降低大数据量电子显微图像的存储与传输成本，同时保留关键区域的高分辨率重构能力。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [35] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 本文提出了一种用于计算双曲空间中最优传输映射的新算法，通过将欧几里得和球面几何的方法扩展到双曲情形，并利用几何变分技术实现高效求解，实验验证了方法在合成数据和多孔曲面模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输方法主要针对欧几里得空间和球面，而在涉及层次结构数据、网络和多孔黎曼曲面等场景中，双曲空间更为自然，因此需要发展适用于双曲空间的最优传输计算方法。

Method: 采用几何变分技术，将适用于欧几里得和球面的最优传输方法推广到双曲空间，提出一种新颖且高效的算法。

Result: 所提方法在合成数据和多孔曲面模型上进行了实验，结果表明该方法能有效计算双曲空间中的最优传输映射。

Conclusion: 本文成功地将最优传输计算扩展至双曲空间，为处理具有层次结构或复杂拓扑的数据提供了新的工具，具有广泛的应用潜力。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [36] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 本文介绍了L48数据集，一个源自鸟类声音记录的真实细粒度多标签数据集，用于单正好多标签（SPML）学习。与以往在合成数据上评估的方法不同，L48提供了一个更贴近现实且更具挑战性的基准，揭示了现有SPML方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的SPML方法主要在由完全标注数据集生成的合成数据上进行评估，这无法反映真实场景中的复杂性和细粒度差异，因此需要一个更真实、更具挑战性的基准来评估和改进SPML方法。

Method: 提出L48数据集，该数据集基于真实的鸟类声音 recordings，自然地符合SPML设定，并引入了利用领域先验知识获取额外负标签的扩展设置；在此数据集上对现有SPML方法进行了基准测试。

Result: 在L48上的实验表明，现有SPML方法相比在合成数据上的表现有显著差异，暴露出其在真实复杂场景下的弱点。

Conclusion: L48作为一个真实世界、细粒度的SPML基准，凸显了开发更强大SPML算法的需求，并推动研究向更贴近实际应用的方向发展。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [37] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 提出了一种用于处理大规模甲虫图像数据的三阶段自动化管道，结合了基于Transformer的检测与分割模型，显著提高了昆虫学研究的效率。


<details>
  <summary>Details</summary>
Motivation: 生物学家在生态学研究中需要整理大量甲虫图像，传统手工方式效率低下，亟需自动化方法处理海量托盘图像数据。

Method: 设计了一个三阶段管道：1）利用基于Transformer的开放词汇目标检测器和视觉-语言模型迭代检测托盘中的甲虫；2）对检测到的甲虫进行排序并裁剪；3）使用人工标注的670张图像微调两种Transformer-based分割模型，实现精细形态分割。

Result: 该管道成功实现了甲虫的自动检测、裁剪和高精度细粒度形态分割，整合多种深度学习方法，专门针对甲虫图像处理优化。

Conclusion: 所提出的自动化管道能显著提升大规模甲虫图像数据的处理效率，有助于加速生物学研究。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [38] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出MambaNetLK，一种用于内窥镜导航的无对应点3D配准框架，并发布大规模临床数据集C3VD-Raycasting-10k，在配准精度和鲁棒性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决生物组织纹理重复、几何局部同质化及术前术后域偏移导致的点云配准不稳定问题，提升图像引导结肠镜手术中的病灶定位与导航安全性。

Method: 基于PointNetLK架构，引入Mamba状态空间模型（SSM）作为跨模态特征提取器，结合光线投射生成大规模对齐点云数据，并采用Lucas-Kanade算法迭代实现配准。

Result: 在C3VD-Raycasting-10k数据集上，相比次优方法中位旋转误差降低56.04%，RMSE平移误差减少26.19%，并在ModelNet40上表现出良好泛化性和对初始姿态扰动的强鲁棒性。

Conclusion: MambaNetLK结合全局表达能力强的SSM特征提取器与大规模临床数据集，为微创手术中的3D配准提供了更准确、可靠的导航基础。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [39] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 本文提出了一个名为Spot The Ball的基准，用于评估视觉语言模型在体育图像中通过社会线索推断被移除球的位置的能力，发现人类的表现显著优于现有模型，突显了当前模型在视觉社会推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 为了提升AI代理的人类类比推理能力，研究者希望探索和评估视觉语言模型在理解人类行为细微线索（如视线、姿态）方面的表现。

Method: 构建了一个名为Spot The Ball的挑战性基准，使用足球、篮球和排球图像测试模型对被移除球的定位能力，并设计了可扩展的测试项目生成流程，评估了四种最先进的视觉语言模型及三种提示策略。

Result: 人类在任务中的准确率是模型的两到三倍（20-34% vs ≤17%），且模型倾向于依赖表层空间启发式（如靠近图像中心或球员附近猜测），而人类则利用视线方向和身体姿态等社会线索进行推理。

Conclusion: 当前视觉语言模型在视觉社会推理方面与人类存在显著差距，需设计能显式编码结构化行为线索的新架构以实现更鲁棒、类人的推理能力。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [40] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出一种结合冻结的CLIP视觉Transformer与轻量级分类器的联邦学习框架，用于解决农业智能中的隐私、非IID数据和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统集中训练存在隐私风险，标准联邦学习在非独立同分布数据下性能差且通信成本高，需更高效、隐私保护的农业图像分类方案。

Method: 采用预训练的冻结CLIP ViT提取特征，仅对轻量级Transformer分类器进行联邦训练，并共享1%的CLIP特征表示以缓解非IID问题，特征不可逆以保障隐私。

Result: 在农业分类任务中达到86.6%的准确率，性能超过基线联邦学习方法4倍以上。

Conclusion: 该方法有效结合视觉-语言模型与联邦学习，显著提升准确率并降低通信开销，适用于隐私敏感、资源受限的智慧农业场景。

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [41] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: 提出PersonalView方法，通过轻量级适配使现有模型仅用100个样本即可实现多视角一致的人像生成。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型难以控制生成图像的视角，也无法生成一致的多视角人像。

Method: 设计条件架构利用预训练扩散Transformer的上下文学习能力，并引入语义对应对齐损失以保持原有生成能力。

Result: 在多视角一致性、文本对齐、身份相似性和视觉质量方面显著优于使用大量多视角数据训练的基线方法。

Conclusion: PersonalView仅需100个训练样本即可有效赋予现有模型多视角生成能力，具有高效性和实用性。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [42] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 本文提出了一个名为LITHOS的大规模公开数据集，用于自动化岩石薄片偏光图像分析，包含21万多个高分辨率图像块和10万多个专家标注的矿物颗粒，涵盖25类矿物。作者评估了多种深度学习方法，并提出一种双编码器Transformer架构，融合两种偏振模态信息，显著提升了矿物分类性能，为自动岩相学提供了强基线和开放研究平台。


<details>
  <summary>Details</summary>
Motivation: 传统岩相学依赖专家通过偏光显微镜进行费时费力的人工分析，难以规模化，亟需自动化方法来提升效率与可扩展性。

Method: 构建了大规模公开数据集LITHOS，包含多模态偏振图像与精细标注；采用多种深度学习模型进行矿物分类实验，并提出一种融合双偏振模态的双编码器Transformer架构。

Result: 所提出的双编码器Transformer在矿物分类任务上 consistently 优于单偏振模型，验证了偏振信息融合的有效性；LITHOS成为当前最大且最多样化的公开岩相学数据集。

Conclusion: LITHOS为自动化岩相分析提供了重要基准，所提出的模型展示了多模态深度学习在该领域的潜力，推动了可重复研究和后续技术发展。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [43] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 本文对11种轻量级视觉模型在7个不同数据集上进行了系统性评估，提出了跨数据集评分（xScore）作为衡量模型跨域鲁棒性的统一指标，发现ImageNet上的准确率不能可靠预测其他领域（如细粒度或医学图像）的性能，且某些结构组件（如各向同性卷积和通道注意力）更有利于泛化。


<details>
  <summary>Details</summary>
Motivation: 轻量级模型通常以ImageNet表现作为基准，但其在其他领域的泛化能力尚不明确，缺乏系统性的跨数据集评估方法和指标。

Method: 在固定训练条件下对11种轻量级模型（约250万参数）在7个多样化数据集上进行评估，并提出xScore指标来量化跨数据集性能一致性。

Result: 1) ImageNet准确率无法可靠预测在细粒度或医学数据集上的表现；2) xScore可仅用四个数据集有效预测模型跨域性能；3) 各向同性卷积、高空间分辨率和通道注意力有助于泛化，而Transformer模块增益有限但参数开销更高。

Conclusion: 研究提供了超越ImageNet的轻量级模型评估框架，揭示了面向移动端的架构设计原则，并指导未来在多样化应用场景中具备更好泛化能力的模型开发。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [44] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出了一种结合DeepONet与神经正切核（NTK）的混合方法，用于求解由Navier-Stokes方程控制的源定位和图像重建等复杂逆问题。


<details>
  <summary>Details</summary>
Motivation: 解决非线性、稀疏性和噪声数据带来的挑战，提升物理场逆问题的求解精度与鲁棒性。

Method: 将DeepONet与NTK结合，并在损失函数中引入物理信息约束和任务特定正则化。

Result: 在多种合成与真实数据集上验证了该方法的鲁棒性、可扩展性和高精度。

Conclusion: 该框架能够有效求解复杂逆问题，在计算物理与成像科学中具有广泛应用前景。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [45] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出了一种新的联邦学习框架FedDISC，用于解决多模态对话情绪识别中模态缺失的问题，通过扩散模型和语义一致性机制实现高效模态恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法在面对真实场景中不可预测的模态缺失时性能下降严重，且传统恢复方法在极端数据分布下易产生语义失真。

Method: 提出FedDISC框架，结合联邦学习与对话引导的语义一致扩散模型；使用对话图网络建模上下文依赖，语义条件网络保证恢复模态的语义一致性，并采用交替冻结聚合策略优化训练过程。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上实验表明，FedDISC在多种模态缺失模式下均优于现有方法，显著提升情绪分类性能。

Conclusion: FedDISC有效解决了多模态情绪识别中的模态缺失问题，具备良好的泛化能力和应用前景。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [46] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen是一个从OpenStreetMap原始数据生成逼真卫星图像的生成框架，能创建前后一致的图像对，支持城市规划和自动化地图更新。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏特定城市特征及其变化的标注数据集，城市监测自动化面临挑战。

Method: 利用OSM的JSON数据（包括矢量几何、语义标签、位置和时间）生成卫星图像，并通过修改OSM输入生成对应的视觉变化图像对。

Result: 能够生成高质量、成对的（JSON, 图像）数据，用于训练模型并支持城市规划中的干预预览。

Conclusion: OSMGen为解决城市监测中数据稀缺和类别不平衡问题提供了有效方案，并推动了卫星影像驱动OSM自动更新的闭环系统发展。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [47] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型重建动态的取证框架（diffusion snap-back），通过分析不同噪声强度下的重建指标变化来区分真实与合成图像，在4000张图像上达到0.993 AUROC，具有强鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在面对Stable Diffusion和DALL-E等生成高质量、无明显伪影图像的模型时失效，难以有效区分真实与合成视觉内容。

Method: 利用多强度图像重建动态（diffusion snap-back），分析LPIPS、SSIM和PSNR等重建指标随噪声强度变化的演化过程，提取基于流形的可解释特征进行分类。

Result: 在包含4000张图像的平衡数据集上，交叉验证下达到0.993 AUROC，对压缩和噪声等常见失真具有鲁棒性，并展现出良好的泛化能力和可解释性。

Conclusion: 该方法仅使用有限数据和单一扩散模型（Stable Diffusion v1.5）即可实现高效、可解释的AI生成图像检测，为可扩展、模型无关的合成媒体取证提供了可行基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [48] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 本研究提出了一种基于迁移学习的轻量级热红外云分割方法，适用于硬件和数据受限的CubeSat任务。


<details>
  <summary>Details</summary>
Motivation: CubeSat任务受限于硬件资源和光谱信息，传统云掩膜方法难以适用，且缺乏足够的标注数据。

Method: 采用UNet结合MobileNet轻量级编码器，利用Landsat-7公开数据集进行预训练，并在FOREST-2任务的小样本上进行联合微调，模型转换为TensorRT引擎以提升推理速度。

Result: 在FOREST-2数据上宏F1分数从0.850提升至0.877，NVIDIA Jetson Nano上实现5秒内全图推理。

Conclusion: 结合公开数据集与轻量模型可通过迁移学习实现星载热红外云分割，支持数据受限遥感任务的实时决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [49] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的多智能体视频 moment 检索框架，通过证据学习解决模型间的定位冲突，并能识别查询在视频中无对应 moment 的情况。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑不同模型在定位结果上的冲突，导致多模型难以有效融合，且无法处理查询在视频中无对应 moment 的情况。

Method: 引入基于强化学习的视频 moment 检索模型，单次扫描视频即可定位 moment 边界并生成位置证据；提出多智能体系统框架，利用证据学习解决各智能体输出的冲突。

Result: 在基准数据集上优于现有方法，验证了建模多智能体竞争与冲突的有效性，并揭示了证据学习在多智能体框架中的新作用。

Conclusion: 通过建模多智能体冲突和引入证据学习，不仅提升了 moment 检索性能，还能自然识别 out-of-scope 查询，具有良好的实际应用潜力。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [50] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的放射学辅助框架VisionCAD，通过摄像头直接从显示器捕获医学图像，无需集成到医院现有IT基础设施，实现了与传统CAD系统相当的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助诊断（CAD）系统因难以集成到现有医院IT基础设施而限制临床广泛应用的问题。

Method: 开发了一个自动化管道，通过摄像头检测、恢复和分析屏幕上的医学图像，将捕获的视觉数据转换为适合自动分析和报告生成的诊断级图像。

Result: 在多个医学影像数据集上验证，该系统在分类任务中F1分数下降通常小于2%，自动生成报告的自然语言指标与原始图像相比差异在1%以内。

Conclusion: VisionCAD仅需摄像头和标准计算资源，可在不改变现有基础设施的情况下实现AI辅助诊断，具有广泛的临床部署潜力。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [51] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了FERBench，一个系统性的基准测试，用于评估20种最先进的多模态大语言模型在面部表情识别（FER）任务上的表现，并引入了UniFER-7B模型，通过后训练策略提升了模型的推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域表现出色，但其在面部表情识别任务中的性能尚未充分探索，现有方法在推理和可解释性方面存在局限。

Method: 将传统FER数据集转换为视觉问答（VQA）格式，构建FERBench基准；提出两种后训练策略，基于两个大规模高质量数据集（UniFER-CoT-230K和UniFER-RLVR-360K），采用冷启动初始化和强化学习与可验证奖励（RLVR）优化模型。

Result: 实验表明，MLLMs在分类任务上表现良好，但在推理和可解释性方面仍有不足；UniFER-7B在多个开源和闭源模型中表现出优越性能，优于Gemini-2.5-Pro和Qwen2.5-VL-72B等模型。

Conclusion: 通过系统评估和提出的后训练方法，显著提升了MLLM在FER任务中的推理能力，UniFER-7B成为一个统一且可解释的FER基础模型，推动了多模态模型在情感计算中的应用。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [52] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了VinciCoder，一个通过两阶段训练框架解决多模态代码生成中视觉语言模型泛化能力不足问题的统一模型，结合大规模监督微调和基于粗到细奖励机制的视觉强化学习，在多个基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在多模态代码生成任务中依赖单任务训练，缺乏泛化能力，限制了视觉代码智能的发展。

Method: 提出两阶段训练框架：首先构建包含160万图像-代码对的大规模监督微调语料库；然后引入一种粗到细奖励机制的视觉强化学习（ViRL）策略，通过计算局部和全局图像块的视觉相似性来提升生成代码的视觉保真度。

Result: 在多个多模态代码生成基准上的实验表明，VinciCoder实现了最先进的性能，验证了所提ViRL策略的有效性。

Conclusion: VinciCoder通过统一的训练框架和创新的ViRL策略，显著提升了多模态代码生成的泛化能力和生成质量，推动了通用视觉代码智能的发展。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [53] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出了首个统一框架，通过将SOD、CoSOD和SIS三种显著性任务建模为视觉-语言模型中的思维链（CoT）推理过程，实现了联合处理。该框架采用两阶段CoT训练：监督微调（SFT）和强化学习（RL），并提出了一种轻量级的单样本算法CGPO，利用奖励与模型置信度之间的差异作为每样本优势信号，提升了RL中的CoT质量。此外，还引入了“输出到推理”策略以构建高保真SFT数据。实验表明，该方法在使用更少训练数据的情况下，在多个任务上达到或超越现有最先进方法和闭源VLM的表现，尤其在CoSOD的CoCA数据集上S-measure达到0.899，超越先前最佳结果8.0个百分点。


<details>
  <summary>Details</summary>
Motivation: 由于SOD、CoSOD和SIS三类显著性任务在操作上存在异质性，现有方法通常独立设计，难以共享知识与统一建模。因此，亟需一个能够统一处理这三类任务的框架，以克服任务异构带来的挑战，并提升模型泛化能力与效率。

Method: 将三类显著性任务统一建模为视觉-语言模型中的Chain-of-Thought（CoT）推理过程；采用两阶段CoT训练：监督微调（SFT）和基于强化学习（RL）的优化；提出Confidence-Guided Policy Optimization（CGPO），利用奖励与模型置信度的差异作为每样本优势信号，提升RL训练效率与CoT生成质量；设计“输出到推理”策略，构建逻辑一致的高保真SFT训练数据。

Result: 在多个显著性检测任务上达到或超越现有最先进方法的表现，尤其是在CoSOD的CoCA数据集上S-measure达到0.899，比之前最优结果提升8.0个百分点；同时使用的训练数据远少于对比方法；在计算开销更低的情况下克服了GRPO存在的置信度无关学习、信号稀释和高计算成本等问题。

Conclusion: 本文提出的统一CoT框架成功地将SOD、CoSOD和SIS三类异构显著性任务整合到同一视觉-语言模型中，通过CGPO和“输出到推理”策略有效提升了CoT推理质量与训练效率，在更少数据和更低计算成本下实现了对多种任务的优越性能，展示了思维链在统一视觉任务建模中的巨大潜力。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [54] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为Localized-Globalized Cross-Alignment (LGCA)的框架，通过捕捉图像的局部特征并选择最显著区域进行扩展，有效提升零样本图像分类性能，同时保持时间复杂度不变。


<details>
  <summary>Details</summary>
Motivation: 由于CLIP对随机图像裁剪敏感，可能导致小尺度特征相似性引入错误信息和偏差，因此需要一种更鲁棒的方法来结合局部与全局特征。

Method: LGCA首先提取图像局部特征，迭代选择最显著区域并扩展，设计相似度评分机制融合原始与扩展图像信息，以同时捕获局部和全局特征。

Result: 实验表明，LGCA在多个数据集上显著提升了零样本分类性能，优于现有最先进基线方法，并证明其时间复杂度与原模型相当。

Conclusion: LGCA能有效减少因局部裁剪带来的误导信息，在保持高效计算的同时提升视觉-语言模型的零样本分类能力。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [55] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像-文本不匹配的多模态方法ITEM，用于检测生成的假图像，在CLIP空间中利用视觉-语言联合信息提升检测器对未见生成模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖视觉线索进行假图像检测，容易过拟合特定图像模式，难以泛化到未知生成模型。因此，需要一种更具泛化性的检测方法。

Method: 从多模态角度出发，利用预训练CLIP模型衡量图像与对应文本描述之间的不匹配程度，并设计一个MLP分类头进行检测；进一步提出分层不匹配策略，同时捕捉全局和细粒度的语义不一致信息。

Result: 实验表明，该方法在多种最新生成模型上均表现出优于现有最先进方法的泛化性和鲁棒性。

Conclusion: 通过引入图像-文本不匹配作为判别线索，ITEM有效提升了假图像检测的泛化能力，为未来检测技术提供了新的多模态思路。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [56] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出了一种基于频率伪造线索增强（F^2C）的通用扩散图像检测方法，通过频域加权滤波提升检测的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有检测器难以捕捉不同扩散模型和设置下的判别特征，泛化性和抗扰动能力有限。

Method: 提出频率选择性函数，对傅里叶频谱进行加权过滤，抑制判别性弱的频段，增强信息丰富的频段，从而强化各频带的伪造线索。

Result: 在多个扩散生成图像数据集上实验表明，该方法优于现有最先进检测器，具有更强的泛化性和鲁棒性。

Conclusion: 所提F^2C方法有效提升了对未知扩散模型生成图像的检测性能，并在多种扰动下保持稳定，验证了频域分析在图像取证中的优势。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [57] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: 本文提出了ToxicTextCLIP，一种针对CLIP模型预训练阶段的文本模态后门攻击框架，通过背景感知选择和增强生成高质量对抗性文本，在分类与检索任务中实现高攻击成功率并绕过现有防御方法。


<details>
  <summary>Details</summary>
Motivation: CLIP模型依赖大规模网络数据进行图文对齐，但未经过滤的数据使其面临数据投毒和后门风险；现有研究多关注图像模态攻击，而忽视了同样关键的文本模态安全问题。

Method: 提出ToxicTextCLIP框架，包含两个核心组件：1）背景感知选择器，筛选与目标类别背景一致的文本；2）背景驱动增强器，生成语义连贯且多样化的中毒样本，并在预训练阶段注入。

Result: 实验表明，ToxicTextCLIP在分类任务上最高达到95.83%的投毒成功率，在检索任务中Hit@1达98.68%，并能有效绕过RoCLIP、CleanCLIP和SafeCLIP等防御机制。

Conclusion: ToxicTextCLIP揭示了CLIP等视觉-语言模型在文本模态上的安全漏洞，强调了对文本输入进行审查和防护的重要性。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [58] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出一种基于Grad-CAM解释的弱监督深度学习框架，用于胸部X光片中肺炎的分类与定位，仅使用图像级标签即可生成具有临床意义的热图，实验表明ResNet-18和EfficientNet-B0在准确率和AUC上表现最佳，MobileNet-V2在精度与计算成本间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 避免依赖昂贵且耗时的像素级标注，利用弱监督方法实现肺炎的自动分类与病灶定位，提升模型可解释性与临床可信度。

Method: 采用七种ImageNet预训练模型（如ResNet、DenseNet、EfficientNet等），在相同训练条件下使用焦点损失和患者级数据划分，通过Grad-CAM生成可视化热图以定位肺炎区域。

Result: 在Kermany CXR数据集上，ResNet-18和EfficientNet-B0达到98%准确率、ROC-AUC为0.997、F1为0.987；Grad-CAM结果显示模型关注肺部临床相关区域，具备良好可解释性。

Conclusion: 弱监督可解释AI模型可在无需像素级标注的情况下有效支持肺炎筛查，增强AI辅助医学影像诊断的透明性与临床信任。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [59] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter是一个统一框架，通过结合几何先验和自监督语义先验，实现从单张图像进行高质量的3D人体重建与人体部位分割。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在3D人体重建中虽具高保真性，但在特定任务（如3D人体分割）中应用受限，且缺乏标注的3D人体数据。

Method: 引入HumanCrafter框架，在重建阶段融合人体几何先验，在分割阶段采用自监督语义先验；设计交互式标注流程生成高质量标签数据；通过像素对齐聚合实现跨任务协同，并以多任务目标联合优化纹理与语义一致性。

Result: 实验表明，HumanCrafter在单图像3D人体部件分割与3D人体重建任务上均优于现有最先进方法。

Conclusion: HumanCrafter通过多任务联合建模和新型数据生成策略，有效提升了3D人体重建与语义分割的性能。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [60] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的迭代分割与质量优化框架，结合多中心数据和专家共识，实现了前庭神经鞘瘤（VS）在MRI上的高效、准确自动分割，并公开了包含534次T1增强扫描的大规模标注数据集。


<details>
  <summary>Details</summary>
Motivation: 前庭神经鞘瘤（VS）的精确分割对临床管理至关重要，但传统手动标注耗时且依赖专家资源；现有深度学习方法在不同数据分布和复杂病例中泛化能力有限，亟需一种高效且鲁棒的自动化解决方案。

Method: 采用自举式深度学习框架进行迭代分割与质量精炼，融合多个医疗中心的MRI数据，并通过专家共识确保标注可靠性；构建包含纵向T1加权增强和T2加权序列的大型数据集，评估模型在内部验证集和外部数据集上的性能表现。

Result: 在目标内部验证集上，分割精度的Dice相似性系数（DSC）从0.9125提升至0.9670，在外部数据集上保持稳定；专家评估143次扫描发现部分复杂病例仍需人工干预；相比传统手动标注，效率预计提升37.4%。

Conclusion: 该人机协同的模型训练方法实现了高精度、可推广的VS自动分割，具有良好的临床适应潜力，所发布的公开数据集为后续研究提供了重要资源。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [61] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: 本文提出了FedMGP，一种用于视觉-语言模型的个性化联邦提示学习新范式，通过多组文本-视觉提示和基于相似度的动态聚合策略，在保持低通信开销的同时实现优异的个性化和领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 为了在联邦学习中更好地平衡全局共享知识与客户端个性化特征，提升视觉-语言模型的细粒度语义捕捉能力和领域泛化性。

Method: 为每个客户端配备多组配对的文本和视觉提示，引入多样性损失使各提示组专注于不同的语义方面；采用基于余弦相似度的softmax加权采样进行动态提示聚合，实现软选择机制。

Result: FedMGP在多个联邦视觉-语言基准上均优于先前方法，实现了最先进的性能，并具有最低的通信参数量，同时理论分析表明其能增强共享语义并抑制客户端噪声。

Conclusion: FedMGP通过多组提示与动态聚合策略，有效平衡了个性化与共性学习，在参数效率、模型性能和泛化能力方面表现优越，是联邦提示学习的一种高效解决方案。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [62] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat是一种从单张图像生成可控显式4D场景的前馈方法，结合视频扩散模型与4D数据集中的几何和运动约束，在30秒内合成高质量4D场景。


<details>
  <summary>Details</summary>
Motivation: 现有的动态场景建模方法通常依赖测试时优化或后处理，效率较低。希望实现快速、无需优化的高质量4D场景生成。

Method: 提出Diff4Splat，利用视频潜在Transformer增强视频扩散模型，统一生成先验与几何运动约束，直接预测可变形的3D高斯场，编码外观、几何和运动信息。

Result: 在视频生成、新视角合成和几何提取任务中，Diff4Splat在质量上达到或超过基于优化的方法，且推理时间仅需30秒，显著更高效。

Conclusion: Diff4Splat实现了高效、高质量的单图到4D场景生成，为动态场景建模提供了一种快速且无需优化的新方案。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [63] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA是一个大规模带空间标注和临床解释的胸部X光医学视觉问答数据集，包含17,597个问答对，旨在提升可解释性和可靠性的医学VQA研究。


<details>
  <summary>Details</summary>
Motivation: 为了推动可重复且临床可信的医学视觉问答（Med-VQA）研究，需要一个具有空间定位和临床解释的大规模高质量数据集。

Method: 构建了一个包含4,394张图像和17,597个问答对的数据集，每个样本均配有经放射科医生验证的边界框和临床推理解释，并设计了涵盖六类诊断问题的分类体系，同时平衡正负样本分布以减少模型幻觉。

Result: 在MedGemma-4B模型上进行基准测试，F1分数达到0.624，比基线提升11.8%，并支持病灶定位。

Conclusion: VinDr-CXR-VQA通过引入空间标注、临床解释和均衡样本分布，显著提升了医学VQA模型的性能和可解释性，为未来临床应用提供了可靠基础。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [64] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出OmniTrack++，一种用于全景图像多目标跟踪（MOT）的反馈驱动框架，通过动态特征稳定、轨迹反馈关联、专家记忆设计和自适应tracklet管理，在360°视场下显著提升跟踪性能，并发布EmboTrack基准数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 传统MOT方法在窄视场相机上表现良好，但在全景图像的360°视场、分辨率稀释和严重几何畸变下表现不佳，亟需专门针对全景视觉特性设计的跟踪框架。

Method: OmniTrack++采用反馈驱动框架：DynamicSSM模块稳定全景特征以缓解畸变；FlexiTrack Instances利用轨迹反馈实现灵活定位与短期关联；ExpertTrack Memory通过混合专家结构整合外观特征以增强长期鲁棒性；Tracklet Management模块根据场景动态自适应切换端到端与检测跟踪模式。

Result: 在JRDB和新提出的EmboTrack（含QuadTrack和BipTrack）数据集上实验表明，OmniTrack++相较原始OmniTrack在HOTA指标上分别提升25.5%和43.07%，达到当前最优性能。

Conclusion: OmniTrack++有效解决了全景MOT中的畸变、搜索空间大和身份模糊问题，结合新发布的EmboTrack基准，为真实场景下的全景感知提供了有效方案和评估平台。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [65] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: 本文提出了ID-Composer，一种结合文本提示和参考图像生成多主体视频的新框架，通过分层身份保持注意力机制、视觉语言模型语义理解和在线强化学习优化，在身份保持、时序一致性和视频质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型通常仅依赖文本或单张图像作为条件输入，缺乏对多主体身份的精确控制和跨模态语义整合能力，限制了其可控性和应用范围。

Method: 提出ID-Composer框架，包含三个核心组件：1）分层身份保持注意力机制，用于聚合主体内和跨主体、跨模态的特征；2）利用预训练视觉语言模型（VLM）增强语义理解与用户意图对齐；3）引入在线强化学习阶段（RLVR）优化训练目标，弥补扩散模型在关键概念对齐上的不足。

Result: 实验表明，ID-Composer在多个指标上优于现有方法，显著提升了生成视频中的身份一致性、时序连贯性和整体视觉质量。

Conclusion: ID-Composer有效解决了多主体视频生成中的身份保持与语义控制难题，通过融合注意力机制、语义理解和强化学习策略，实现了更高可控性和更高质量的视频生成。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [66] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 提出一种无需额外训练或偏见标注假设的测试时去偏方法，利用预训练分割模型隔离目标视觉属性，并调整非目标区域的嵌入以消除混淆区域的偏见信号。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常需要训练数据和显式分组标签，限制了在现实场景中的实用性；测试时方法虽避免此问题，但仍依赖特定数据集偏见的先验知识，泛化能力受限。

Method: 使用预训练分割模型分离目标视觉属性，调整非目标区域的嵌入使其对所有类别文本提示均匀相似，从而去除混淆区域带来的偏见信号，同时保留目标属性。

Result: 在Waterbirds和CelebA数据集上，该方法在组鲁棒性指标和注意力IoU方面均优于现有的测试时去偏方法。

Conclusion: 基于分割引导的干预策略可有效实现可扩展且无需标注的视觉语言模型去偏，具有良好的应用前景。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [67] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 本文提出了一种基于大视觉-语言模型的细粒度视频异常检测框架T-VAD，通过引入异常热图解码器和区域感知编码器，实现了对视频中异常事件的精确定位与文本描述，显著提升了检测的细粒度和交互性，并在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法多为半自动化且输出粗糙，仅能判断是否异常，缺乏细粒度定位与语义描述能力，难以满足实际应用需求。

Method: 提出T-VAD框架，结合大型视觉-语言模型，设计异常热图解码器（AHD）进行像素级视觉-文本特征对齐生成异常热图，并通过区域感知异常编码器（RAE）将热图转化为可学习的文本嵌入以引导模型进行异常识别与定位。

Result: 在UBnormal数据集上取得94.8% AUC、67.8%/76.7%热图准确率，在ShanghaiTech和UBnormal数据集上生成的文本描述具有更高的BLEU-4分数和Yes/No准确率，主观评估结果更优。

Conclusion: T-VAD通过视觉-语言对齐与热图到文本的编码机制，实现了细粒度、可解释的视频异常检测，在定量和定性指标上均表现出色，推动了异常检测向更高交互性和语义理解方向发展。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [68] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了Real-IAD Variety，目前最大且最多样化的工业异常检测基准，包含160类共198,960张高分辨率图像，涵盖28个行业、24种材料和22种颜色变化。实验表明，现有无监督多类异常检测方法在类别扩展时性能显著下降，而视觉-语言模型表现出更强的鲁棒性和泛化能力。该基准旨在推动通用型异常检测系统的研究，并将在未来公开以促进领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测（IAD）数据集类别多样性不足、规模有限，导致评估结果易出现指标饱和，且模型难以迁移到实际场景。因此，亟需一个更大规模、更多样化的基准来推动通用IAD算法的发展。

Method: 构建了一个名为Real-IAD Variety的大规模IAD基准数据集，包含160个对象类别和198,960张高分辨率图像，覆盖28个行业、24种材料和22种颜色变化。设计了多类无监督、多视角以及零样本/少样本等多种评估协议，并对现有先进方法及视觉-语言模型进行了系统性实验分析。

Result: 实验表明，当类别数量从30增加到160时，最先进的多类无监督异常检测方法性能显著下降；而视觉-语言模型在不同类别数量下表现稳定，展现出优异的鲁棒性和跨类别泛化能力。Real-IAD Variety因其规模和复杂性成为评估和训练下一代异常检测基础模型的重要资源。

Conclusion: Real-IAD Variety填补了当前IAD领域缺乏大规模多样化基准的空白，为评估模型在真实工业环境中的可扩展性和泛化能力提供了新标准。研究证明视觉-语言模型在处理大规模类别IAD任务中具有显著优势，未来将通过公开该数据集推动通用异常检测系统的发展。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [69] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种从单张图像中精确学习和合成多实例语义的方法，通过基于惩罚的注意力优化和注意力层中的框控制优化，实现了语义解耦和高质量生成。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据有限，且实例间语义或外观相似，多实例语义的学习与合成面临挑战。

Method: 提出基于惩罚的注意力优化以解耦相似语义，并在合成阶段引入并优化注意力层中的框控制，以减少语义泄漏并精确控制输出布局。

Result: 实验结果表明，该方法在语义解耦、生成质量、可编辑性和实例一致性之间取得了良好平衡，对语义或视觉上相似的实例及罕见物体具有鲁棒性。

Conclusion: 所提出的方法能有效实现从单幅图像中学习并合成多实例语义，显著提升了复杂场景下的生成控制能力与稳定性。

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [70] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出4D Neural Voxel Splatting (4D-NVS)，结合体素表示与神经高斯点阵，高效建模动态场景，显著降低内存消耗并加速训练，同时保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点阵在处理动态场景时因每帧复制高斯导致内存开销大，需更高效的动态建模方法。

Method: 采用紧凑的神经体素集合和学习到的形变场来建模时间动态，取代为每个时间戳生成独立的高斯集合，并引入新颖视图细化阶段优化困难视角。

Result: 实验表明该方法在显著减少内存使用和加快训练速度的同时，优于现有最先进方法，实现高质量实时渲染。

Conclusion: 4D-NVS通过神经体素与变形场的有效结合，在效率和渲染质量之间取得了良好平衡，适用于动态场景的新视角合成。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [71] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一种面向领域偏移的广义类别发现（DS_GCD）新框架FREE，利用频域信息提升模型在分布偏移下发现已知和未知类别的能力。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现方法在标准条件下表现良好，但在存在分布偏移时性能下降，难以应对包含未知领域样本的实际场景。

Method: 提出频率引导的FREE框架，包括基于频域幅度差异的领域分离策略、跨域与域内频域扰动策略、扩展的自监督对比学习和语义聚类损失，以及聚类难度感知的重采样技术。

Result: 在多个基准数据集上实验表明，该方法有效缓解了分布偏移的影响，在发现已知和未知类别方面均优于现有方法。

Conclusion: FREE框架通过引入频域分析和多种增强策略，显著提升了模型在领域偏移下的广义类别发现性能，具有更强的鲁棒性和实用性。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [72] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 本文提出了一种基于记忆增强的上下文感知零样本视频异常检测方法，通过跨注意力机制融合时间信号与视觉嵌入，并利用文本记忆线索实现实时异常分类，在UCF-Crime和XD-Violence数据集上达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法通常忽略上下文信息，导致在新场景中的泛化能力受限，本文旨在解决上下文感知的零样本异常检测问题。

Method: 提出一种记忆增强的管道，使用跨注意力机制关联时间信号与视觉特征，并通过上下文相似性评分实现零样本异常分类。

Result: 在UCF-Crime上达到90.4% AUC，在XD-Violence上达到83.67% AP，优于现有零样本方法，且支持实时推理与高可解释性。

Conclusion: 融合跨注意力时序建模与上下文记忆可有效提升零样本异常检测性能，推动其在真实监控与基础设施监测中的应用。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [73] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: 本文提出了CueBench，首个面向上下文感知视频异常理解（VAU）的统一评估基准，构建了事件中心的分层分类体系，并引入了Cue-R1模型，通过R1风格的强化微调在多个任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常理解方法仅停留在表面检测或解释层面，缺乏对复杂情境和细微上下文差异的深入理解，难以区分真实世界中的条件性异常（如是否佩戴安全装备攀岩），因此需要更全面、统一的评估框架。

Method: 提出CueBench基准，建立包含14种条件性和18种绝对异常事件的分层分类体系，涵盖174个场景和198个属性；设计涵盖识别、定位、检测和预测的多任务统一评估框架；并基于R1风格的强化学习微调，开发具备可验证、任务对齐、层次化奖励的生成式模型Cue-R1。

Result: 在CueBench上的实验表明，现有视觉语言模型在真实异常理解上表现不佳，而Cue-R1平均超越最先进方法超过24%。

Conclusion: 当前深度模型距离真正的现实世界视频异常理解仍有较大差距，CueBench为上下文感知VAU提供了严谨评估平台，Cue-R1展示了强化学习在提升模型理解能力上的潜力。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [74] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 本文提出了手术动作三元组的实例分割任务（triplet segmentation），通过引入CholecTriplet-Seg数据集和TargetFusionNet网络架构，实现了对手术场景中器械-动作-组织三元组的精确空间定位。


<details>
  <summary>Details</summary>
Motivation: 现有手术动作识别方法局限于帧级分类，难以准确关联动作与特定器械实例，且缺乏精细的空间定位能力。

Method: 提出Triplet Segmentation任务，构建CholecTriplet-Seg数据集，并设计TargetFusionNet模型，结合Mask2Former与目标感知融合机制，融合弱解剖先验与器械实例查询。

Result: 在识别、检测和三元组分割指标上，TargetFusionNet均优于现有基线方法，验证了强实例监督与弱目标先验结合的有效性。

Conclusion: Triplet Segmentation为手术动作理解提供了统一的空间接地框架，推动了可解释性更强的手术场景理解。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [75] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 本文研究了四种训练方法在将PilotNet模型从右舵驾驶环境（美国）适应到左舵驾驶环境（澳大利亚）中的表现，发现预训练时使用翻转数据并结合微调能显著提升模型对左侧行驶线索的关注和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要适应不同道路条件，而直接在目标域数据上训练成本高且数据有限，因此探索如何利用源域数据有效进行领域自适应具有重要意义。

Method: 采用四种训练策略：(1) 仅用美国右舵数据训练的基线模型；(2) 使用水平翻转后的美国数据训练；(3) 先在美国数据上预训练，再在澳大利亚数据上微调；(4) 先在翻转的美国数据上预训练，再在澳大利亚数据上微调。通过注意力热图分析模型关注区域的变化，并比较转向预测误差。

Result: 单独使用翻转数据预训练会因特征表示错位而降低预测稳定性，但若后续进行微调，则能显著降低预测误差，增强模型对左侧道路线索的关注。该结论在PilotNet和ResNet上均得到验证。

Conclusion: 翻转数据预训练结合微调是一种高效、低重训成本的领域自适应方法，有助于提升模型在左舵驾驶环境下的泛化能力。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [76] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 本研究提出了FGI-EMIT，首个用于单木分割的大规模多光谱LiDAR基准数据集，并系统评估了传统无监督算法与深度学习方法的性能，发现深度学习模型（尤其是ForestFormer3D）显著优于传统方法，尤其在林下小树分割方面。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模基准数据集和多光谱LiDAR数据的支持，单木分割方法的发展受到限制，本文旨在通过构建新数据集并评估现有方法，推动该领域的技术进步。

Method: 提出FGI-EMIT多光谱LiDAR数据集，包含1,561棵人工标注树木；采用贝叶斯优化调参的四种无监督几何算法和从零训练的四种深度学习模型进行系统性基准测试，并开展消融实验分析多光谱反射率的影响。

Result: 在测试集上，最佳深度学习模型ForestFormer3D达到73.3%的F1分数，显著优于最佳无监督方法Treeiso（52.7%）；在林下小树分割中，前者比后者高出25.9个百分点；消融实验表明当前深度学习模型难以有效利用多光谱反射率信息，但单通道反射率可轻微提升精度；即使在低至10点/m²的情况下，深度学习方法仍持续优于无监督方法。

Conclusion: 深度学习方法在单木分割任务中显著优于传统无监督方法，尤其在复杂林下环境中；尽管多光谱数据潜力未被充分挖掘，FGI-EMIT数据集为未来研究提供了重要基础。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [77] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一种基于元数据引导的框架，通过将MRI图像与其DICOM采集参数对齐，学习统一的MRI对比度表示，支持少样本分类和无监督质量控制。


<details>
  <summary>Details</summary>
Motivation: MRI数据在不同扫描仪、协议和机构间存在显著异质性，且缺乏标准化的对比度标签，限制了大规模自动化分析。需要一种无需人工标注的统一表示方法。

Method: 提出MR-CLIP框架，利用DICOM采集参数作为监督信号，将3D MRI体积图像与其元数据对齐，学习对比度感知的嵌入表示。

Result: 学习到的嵌入能清晰聚类不同MRI序列，在少样本序列分类任务上优于监督基线，并可通过图像-元数据嵌入距离实现无监督数据质量控制。

Conclusion: MR-CLIP通过利用常规可用的采集元数据作为监督信号，为多样化临床数据集中的高效标签MRI分析提供了可扩展的基础。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [78] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成安装在自主水下航行器（AUV）上的事件相机在水下环境中拍摄的逼真实验数据的流程，以应对传统视觉技术在恶劣光照和高动态范围场景中表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 水下环境中的不良光照条件和高动态范围场景使得传统视觉技术难以适应，性能不佳。事件相机通过逐帧跟踪画面变化，为解决这一问题提供了有前景的方案。

Method: 开发了一个生成水下事件相机合成数据的管道，模拟安装在AUV上的事件相机在水下环境中的工作情况，用于训练视觉模型。

Result: 通过岩石检测任务验证了该方法在低能见度和悬浮颗粒物干扰下的有效性，表明该流程具有良好的适用性。

Conclusion: 所提出的合成数据生成流程能够有效支持水下事件相机视觉模型的训练，且可推广至其他水下视觉任务。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [79] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 提出了一种针对图像超分辨率网络的后训练量化方法，采用双区域量化策略和敏感性感知微调，有效处理激活中的异常值并提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法忽略了激活中异常值的影响，导致性能不佳，且直接去除异常值会显著降低性能。

Method: 提出双区域量化策略，将激活分为异常值区域和密集区域分别进行均匀量化；引入敏感性感知微调，针对不同层的量化敏感性进行优化。

Result: 在多种SR网络和数据集上优于现有的PTQ方法，性能接近QAT方法，并实现至少75倍加速。

Conclusion: 所提方法有效解决了SR模型中激活异常值带来的量化问题，在无需重训练的情况下实现了高效且高性能的后训练量化。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [80] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 提出了一种基于SE(3)流形上流匹配的新型概率框架，用于估计6D物体姿态分布，能够处理对称性和遮挡等导致的位姿模糊问题，并在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有确定性深度网络难以捕捉位姿分布的多模态性，在对称、遮挡等情况下容易产生过度自信的错误预测，因此需要一种能建模不确定性的概率方法。

Method: 提出一种基于流匹配（flow matching）在SE(3)流形上的概率框架，使用基于样本的方式建模完整的6D位姿分布，以捕捉观测中的多假设和不确定性。

Result: 在Real275、YCB-V和LM-O数据集上实现了最先进的性能，并展示了该方法在主动感知和不确定性感知抓取规划等机器人操作任务中的应用潜力。

Conclusion: 所提方法能有效建模位姿不确定性，在存在对称性和严重遮挡的情况下优于确定性方法，且具备在实际机器人任务中进行不确定性推理的能力。

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [81] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: 本文提出了WANDER，一种基于新颖性搜索的文本到图像生成方法，通过大语言模型和CLIP嵌入提升生成图像的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型虽然生成质量高，但输出多样性有限，难以满足创意探索需求；现有提示优化技术多关注美学适配，不适用于创造性视觉领域。

Method: WANDER直接在自然语言提示上操作，利用大语言模型进行语义演化，并使用CLIP嵌入量化新颖性，结合发射器引导搜索进入不同的提示空间区域。

Result: 实验表明，使用FLUX-DEV生成和GPT-4o-mini进行变异时，WANDER在多样性指标上显著优于现有进化式提示优化基线，消融研究验证了发射器的有效性。

Conclusion: WANDER能有效提升单一提示下生成图像的多样性，适用于创意生成任务。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [82] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 本文提出DiMoDE框架，通过区分相机运动分量并利用几何规律，提升无监督深度与自运动估计的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法将自运动作为辅助任务，缺乏对不同运动分量的区分，限制了几何约束的使用，导致在复杂条件下性能不足。

Method: 引入运动分量的判别式处理，通过对齐光学轴和成像平面，分别对各运动分量施加几何约束，并利用闭式几何关系实现深度与平移分量的相互推导。

Result: DiMoDE在多个公开数据集及新采集的真实世界数据集上达到最优性能，尤其在挑战性场景下表现突出。

Conclusion: 通过精细化建模运动分量间的几何关系，DiMoDE有效提升了无监督深度与自运动联合学习的可靠性与鲁棒性。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [83] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 本文研究了深度学习中不同损失函数在低剂量CT图像质量增强中的表现，发现现有损失函数与图像质量指标（如PSNR、SSIM）之间存在不一致性，强调在设计新损失函数时应考虑感知质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT图像虽能减少辐射，但存在噪声和伪影，影响诊断；尽管深度学习模型在提升图像质量方面取得进展，但常用评价指标无法准确反映人眼感知质量，因此需深入分析损失函数与图像质量指标的一致性。

Method: 系统分析多种损失函数（如MSE、对抗损失及自定义损失）在LDCT图像增强中的表现，并评估其与PSNR、SSIM等图像质量指标的相关性和一致性。

Result: 发现当前损失函数与图像质量指标之间存在明显不一致，这些指标不能充分反映图像的视觉感知质量，尤其是在医学图像场景下。

Conclusion: 在设计用于图像质量增强的深度学习损失函数时，必须结合更符合人类视觉感知的图像质量评估指标，以提高模型输出的临床可用性。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [84] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: 本文提出了PixelVLA，首个支持像素级推理和多模态（文本与视觉）提示的视觉-语言-动作模型，通过新的视觉运动指令调优框架和大规模像素级标注数据集Pixel-160K，在多个基准上显著提升了操作成功率，同时大幅降低预训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在像素级场景理解和对文本提示的过度依赖方面存在局限，限制了其在真实场景中的灵活性和性能。

Method: 提出PixelVLA模型，结合多尺度像素感知编码器和视觉提示编码器，并设计两阶段自动标注 pipeline 构建包含像素级标注的大规模数据集Pixel-160K，用于有效训练。

Result: 在三个标准VLA基准和两种模型变体上，PixelVLA相比OpenVLA将操作成功率提高了10.1%-17.8%，且仅需其1.5%的预训练成本。

Conclusion: PixelVLA能够提升现有视觉-语言-动作模型的准确性、效率和通用性，适用于复杂环境中的机器人控制，具备良好的实用潜力和扩展性。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [85] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 该研究评估了基于ADNI数据集训练的深度学习模型在拉丁美洲临床队列（FLENI）上的泛化能力，发现尽管模型在ADNI上表现良好（AUC高达0.97），但在FLENI上性能显著下降（低至0.80），揭示了明显的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有阿尔茨海默病（AD）诊断模型多基于北美人群训练，其在代表性不足人群中的泛化能力尚不清楚，亟需评估和改进模型在不同人群中的适用性。

Method: 研究比较了卷积网络和Transformer模型在ADNI数据集上的表现，并在FLENI拉丁美洲队列上测试其泛化性能，通过消融实验分析归一化方法和采样策略的影响，并使用遮挡敏感性分析探究模型关注区域。

Result: 所有模型在ADNI上AUC达0.96–0.97，但在FLENI上降至0.80–0.82；两种架构性能相似，未体现Transformer优势；归一化和采样是影响泛化的关键因素；ADNI训练模型主要关注AD典型低代谢区，但对其他类别和FLENI图像的关注区域不明确。

Conclusion: 诊断AI模型需在多样化人群中进行验证，当前模型存在域偏移问题，未来应推动域适应方法和训练队列的多样化。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [86] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了3EED，一个大规模、多平台、多模态的3D视觉定位基准，包含超过128,000个物体和22,000个验证的指代表达，支持车辆、无人机和四足机器人平台的RGB与LiDAR数据，推动开放环境中语言驱动的3D感知研究。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位基准受限于室内场景、单一平台和小规模数据，难以支持开放世界中跨平台、多模态的具身智能研究需求。

Method: 提出3EED基准，结合视觉-语言模型提示与人工验证构建高质量标注流程，并设计平台感知归一化与跨模态对齐技术以支持跨平台学习。

Result: 提供了比现有数据集大10倍的数据规模，在多样化户外场景中建立了面向域内与跨平台评估的基准协议，实验揭示了显著的性能差距。

Conclusion: 3EED为语言驱动的3D具身感知提供了更具挑战性和实用性的基准，揭示了可泛化3D视觉定位的挑战与机遇，促进未来跨平台多模态感知研究。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [87] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 提出一种将地点识别作为多分类问题的方法，使用LiDAR扫描数据直接分类位置，在NuScenes数据集上表现良好且训练更高效稳定。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖对比学习，作者希望探索更高效稳定的替代方案。

Method: 将地点识别视为多分类任务，为LiDAR扫描分配离散位置标签，并使用编码器-解码器模型直接分类。

Result: 在NuScenes数据集上达到与对比学习方法相当的性能，同时训练效率更高、更稳定。

Conclusion: 该分类方法是实现地点识别的有效替代路径，具有实用潜力。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [88] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 该研究探讨了生成式AI模型如何编码“美”并消除“丑”，揭示了这些模型中存在的种族、性别和年龄等社会偏见，尤其是对非二元性别群体的过度性化和年轻化倾向。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方审美标准的传播，导致负面自我形象问题，而生成式AI可能进一步放大这些偏见，因此需要研究其内在审美偏见及其社会影响。

Method: 构建了一个结构化的 beauty 分类体系，结合文本到图像和文本到语言再到图像的生成管道，利用三个语言模型和两个文本到图像模型生成5984张图像，并通过包含1200张图像的李克特量表实验由女性和非二元性别用户进行评估。

Result: 86.5%的生成图像为浅肤色人物，74%呈现年轻化特征，22%包含非安全内容（尽管经过SFW训练），且带有'负面'或'丑'特征的提示反而产生更高NSFW评分；非二元个体图像被评价为更年轻且更具性化。

Conclusion: 生成式AI模型内嵌了严重的审美偏见，这些偏见通过开发者使用的负向提示等方式被持续强化，导致不符合主流审美的特征被系统性抹除，污染数据流并对社会多样性造成伤害。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [89] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 本研究提出了一种基于物联网的榴莲园动物入侵检测系统，结合YOLOv5和SSD算法提升检测精度，并通过Telegram通知和声音威慑实现自动预警与驱赶。


<details>
  <summary>Details</summary>
Motivation: 传统农业方法在无人监控下难以有效防止动物入侵，导致作物受损和经济损失，亟需自动化解决方案。

Method: 系统融合YOLOv5与SSD目标检测算法，利用物联网实现实时监控，检测到动物后通过Telegram发送警报并触发如虎啸声等声音驱赶机制。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别为90%、85%和70%，白天效果最佳，夜间性能下降，静态图像与视频均适用。

Conclusion: 该系统构建了一个集检测、通知与威慑于一体的实用框架，为智慧农业中的自动化防控提供了可行路径。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [90] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出了一种粒度一致的自动2D掩码跟踪方法，结合三阶段课程学习框架，实现从碎片化单视图数据到全局一致场景监督的渐进训练，显著提升3D实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将2D掩码转移至3D时因独立处理视频帧导致分割粒度不一致和伪标签冲突，影响3D实例分割精度。

Method: 引入粒度一致的自动2D掩码跟踪方法，保持帧间时序对应，并结合三阶段课程学习框架，逐步训练模型。

Result: 实验结果表明该方法能有效生成一致且准确的3D分割，在标准基准上达到最先进性能，并具备开放词汇能力。

Conclusion: 所提方法通过时序一致性和渐进式学习，成功从碎片化和矛盾的2D先验中提炼出鲁棒的3D表示，显著提升了无监督3D实例分割效果。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [91] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench是一个用于隐私感知联邦学习的可复现基准平台，基于合成肿瘤CT扫描数据，评估多种FL方法在医学图像分割中的隐私与性能权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私方面具有潜力，但在实际应用中仍面临成员推断攻击和数据异构性问题，亟需标准化基准来评估隐私与性能的权衡。

Method: 构建了一个名为FedOnco-Bench的开源基准平台，使用带肿瘤标注的合成CT图像，对比评估FedAvg、FedProx、FedBN和结合DP-SGD的FedAvg在分割性能和隐私泄露方面的表现。

Result: 实验表明，FedAvg性能高但隐私泄露较大（AUC≈0.72），DP-SGD显著提升隐私性（AUC≈0.25）但牺牲精度（Dice≈0.79），FedProx和FedBN在非独立同分布数据下表现出更好的平衡性。

Conclusion: FedOnco-Bench为医学图像分割中的隐私保护联邦学习提供了一个标准化、可复现的评估与开发平台，有助于推动隐私与性能兼顾的FL方法研究。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [92] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: Med-Banana-50K是一个包含5万个医学图像的大规模指令型医学图像编辑数据集，涵盖三种模态和23种疾病类型，通过Gemini模型生成双向编辑，并采用基于LLM的医学质量评估与迭代优化确保解剖与临床合理性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像编辑研究受限于缺乏大规模、高质量且公开可用的数据集，尤其是满足严格解剖和临床约束的数据集。

Method: 利用Gemini-2.5-Flash-Image模型对真实医学图像进行病灶添加和删除的双向编辑生成数据，并采用LLM-as-Judge结合医学评分标准（指令遵循、结构合理、真实感、保真度）进行多轮迭代质量控制。

Result: 构建了50K图像的Med-Banana-50K数据集，覆盖三种医学影像模态和23类疾病，包含37K次失败编辑记录与完整对话日志，支持偏好学习与模型对齐研究。所有数据经过医学质量验证。

Conclusion: Med-Banana-50K为医学图像编辑模型的训练与评估提供了高质量、可公开访问的基础资源，推动该领域的进一步发展。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [93] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制且无需坐标输出的GUI定位框架GUI-AIMA，通过在多模态大模型中对齐视觉图像块与自然语言指令，实现高效、数据友好的屏幕区域定位。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLM）的GUI定位方法直接生成精确坐标存在挑战，计算开销大且精度有限，因此需要一种更高效、准确的替代方案。

Method: 提出GUI-AIMA，利用MLLM内部的注意力机制提取与指令相关的视觉图像块，并通过多头注意力聚合简化后的查询-视觉注意力矩阵自适应生成定位信号，采用无坐标监督微调框架，并支持即插即用的放大机制以精确定位。

Result: 仅使用85k截图训练的GUI-AIMA-3B在ScreenSpot-Pro和OSWorld-G上分别达到58.6%和62.2%的平均准确率，性能优于同类3B模型，展现出卓越的数据效率和强大的原生定位能力。

Conclusion: GUI-AIMA通过挖掘和对齐MLLM内在的多模态注意力，实现了高效、轻量化的GUI定位，验证了轻量训练即可激发模型原生定位能力，为实际应用提供了高性价比解决方案。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [94] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: 提出了一种结合拓扑感知扩散概率模型与水平集能量的新型胰腺分割方法TA-LSDiff，在多个公开数据集上实现了最先进的精度。


<details>
  <summary>Details</summary>
Motivation: 胰腺分割因器官小、对比度低和拓扑变化大而具有挑战性，传统方法在几何演化和细节保持方面存在局限。

Method: 提出TA-LSDiff模型，融合拓扑感知扩散模型与水平集能量函数，并引入像素自适应精修模块，通过亲和加权局部调节能量函数。

Result: 在四个公开胰腺数据集上验证了方法的有效性，消融实验表明各组件贡献显著，整体性能优于现有方法。

Conclusion: TA-LSDiff是一种实用且精确的胰腺分割解决方案，有效结合了深度学习语义特征与水平集的结构建模能力。

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [95] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: 本文提出OMEGA，一种新的视觉-语言模型位置编码框架，通过模态特定的位置编码（MSPE）和全局自适应编码步长缩放（GAESS）提升多模态模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型采用统一的1D或2D位置编码策略，未能区分文本和视觉模态的结构特性，忽视了文本的序列连续性和视觉的空间一致性。

Method: 提出OMEGA框架，包含模态特定位置编码（MSPE），在独立坐标维度上保留各模态固有结构；引入全局自适应编码步长缩放（GAESS），根据两模态嵌入熵自适应调整视觉令牌的位置编码步长。

Result: 实验表明，OMEGA在多种架构和VQA基准上均显著提升性能，在Qwen2.5-VL-3B上最高提升3.43%，并在更大模型如Qwen2.5-VL-7B和LLaVA-v1.5-7B上保持一致增益。

Conclusion: OMEGA通过模态特定且自适应的位置编码策略，有效提升了视觉-语言模型在多模态任务中的表现，尤其在视觉密集型任务中效果显著。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [96] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 提出了一种新的攻击方法LSSA，通过局部图像块的随机打乱和采样来增强多模态对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态对抗攻击方法因过度依赖单一模态信息而导致输入多样性不足，容易过拟合，需提升对抗样本的迁移能力。

Method: 提出Local Shuffle and Sample-based Attack（LSSA），随机打乱图像的局部块以扩充图像-文本对，并在生成对抗图像后进行采样，结合原始和采样图像生成对抗文本。

Result: 实验表明，LSSA在多个模型和数据集上显著提升了多模态对抗样本的迁移性，并在大型视觉语言模型上优于其他先进攻击方法。

Conclusion: LSSA通过增加输入多样性有效缓解了过拟合问题，显著增强了跨VLP模型和下游任务的对抗迁移性能。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [97] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 提出视觉对比注意力（VCA），作为多头自注意力的替代方案，通过引入判别性机制并降低计算复杂度，在图像识别和生成任务中显著提升ViT性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViT中的多头自注意力（MHSA）在所有token对上进行二次查询-键交互，计算资源浪费在视觉上弱或冗余的相关性上，效率低下。

Method: 提出视觉对比注意力（VCA），将每个注意力头的密集查询场蒸馏为空间池化的视觉对比token，并分为正负两个流，通过差异性交互突出区域间的区别；理论复杂度从O(N²C)降至O(NnC)，且无需额外FLOPs。

Result: 在DeiT-Tiny上ImageNet-1K准确率从72.2%提升至75.6%（+3.4%），在多个分层ViT上最高提升3.1%；在图像生成任务中FID-50K降低2.1~5.2点；消融实验验证了空间池化、双位置编码及两阶段结合的有效性。

Conclusion: VCA是一种即插即用、参数轻量、架构无关的注意力模块，能有效提升ViT的效率与性能，为更快更清晰的视觉Transformer提供了简单可行路径。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [98] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 提出了一种新的对抗训练框架PIAT，通过插值相邻epoch的模型参数来缓解对抗训练中的振荡和过拟合问题，并结合NMSE损失进一步提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法在训练过程中存在模型鲁棒性振荡和过拟合问题，影响防御效果。

Method: 提出参数插值对抗训练（PIAT），在每轮训练间插值前后epoch的模型参数，并引入归一化均方误差（NMSE）对齐干净样本与对抗样本logits的相对幅度。

Result: 在多个基准数据集上实验表明，PIAT显著提升了CNN和Vision Transformer的模型鲁棒性。

Conclusion: PIAT有效缓解了对抗训练中的过拟合和振荡问题，使模型决策边界更平缓，收敛更好，显著增强了对对抗攻击的防御能力。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [99] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了OmniBrainBench，首个全面的多模态视觉问答基准，用于评估多模态大语言模型在脑成像分析中的表现，涵盖15种成像模态和31,706张图像，揭示了现有模型在临床推理任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的脑部视觉问答基准覆盖的成像模态有限，且仅限于粗粒度病理描述，难以全面评估多模态大语言模型在完整临床流程中的能力。

Method: 构建了一个包含15种不同脑成像模态、9,527个验证的VQA对和31,706张图像的综合性基准OmniBrainBench，涵盖15个多阶段临床任务，并由专业放射科医生进行严格验证。

Result: 对24种前沿模型的评估表明：专有模型（如GPT-5）优于开源和医学专用模型但仍不及医生；医学MLLM性能差异大；开源模型整体落后但在特定任务中表现优异；MLLM在复杂术前任务中显著欠佳，暴露出视觉到临床推理的差距。

Conclusion: OmniBrainBench为评估和推进脑成像分析中的多模态大语言模型设立了新标准，揭示了当前模型与专家临床推理能力之间的差距。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [100] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种遮挡感知扩散模型（ODM），用于在遮挡情况下预测行人穿越意图，通过重建被遮挡的运动模式并结合遮挡掩码引导反向过程，提升了预测鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的行人意图预测模型在处理遮挡导致的观测不完整问题上考虑不足，影响了预测性能。

Method: 提出Occlusion-Aware Diffusion Model（ODM），采用遮挡感知扩散Transformer架构估计被遮挡区域的噪声特征，并引入遮挡掩码引导的反向扩散过程，以更好利用观测信息并减少误差累积。

Result: 在PIE和JAAD基准上进行了广泛实验，结果表明ODM在不同遮挡场景下相比现有方法具有更优的鲁棒性和更高的运动特征重建与意图预测精度。

Conclusion: 所提出的ODM有效解决了遮挡下的行人意图预测问题，通过扩散模型与遮挡感知机制的结合，显著提升了复杂场景下的预测性能。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [101] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为Layer-Wise Modality Decomposition (LMD)的后验、模型无关的解释方法，用于分解自动驾驶中多传感器融合模型各层的模态特异性信息，首次实现了对感知模型预测结果在不同输入模态上的归因分析。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，感知模型决策的透明性至关重要，但由于多传感器信息在融合网络中纠缠，难以判断各模态对预测的贡献。

Method: 提出LMD方法，通过逐层分解技术，在预训练的融合模型上解耦各层中的模态特异性信息，适用于相机-雷达、相机-LiDAR及三者融合等多种设置。

Result: 在多种传感器组合下验证了LMD的有效性，通过结构化扰动指标和模态可视化分解展示了其对高容量多模态架构的实用解释能力。

Conclusion: LMD是首个能将自动驾驶感知模型预测归因于各个输入模态的通用解释方法，具有良好的可解释性和应用前景。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [102] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: 本文提出了一种名为Fleming-VL的统一端到端框架，用于跨多种医学模态（如2D图像、3D体数据和视频序列）的医学视觉理解。该方法通过扩大预训练、补充稀有医学数据微调以及扩展评估基准来提升性能，在多个医学视觉问答和3D/视频理解任务上达到SOTA，并公开发布模型以促进医学AI的可重复研究。


<details>
  <summary>Details</summary>
Motivation: 医学数据具有多模态异质性（如2D、3D、视频），存在显著的领域差异和格式不一致，阻碍了统一医学多模态大模型的发展，因此需要一个能统一处理这些模态的框架。

Method: 从数据驱动角度出发：1）结合自然场景与医学领域的长上下文数据进行大规模预训练；2）使用稀有医学数据（如超声、皮肤镜图像和视频）进行微调；3）扩展评估框架，包含3D和视频理解任务；并采用SFT和GRPO优化策略训练多尺度模型。

Result: Fleming-VL在多个医学视觉理解基准上实现SOTA，包括医学VQA、视频问答和3D医学图像理解任务。

Conclusion: Fleming-VL是一个能够有效统一处理多种医学模态的端到端框架，通过数据-centric 策略显著提升了跨模态医学理解能力，且已公开发布以推动医学AI的透明与可复现发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [103] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 提出动态多层级加权对齐网络，用于零样本草图图像检索，通过三模块设计提升跨模态对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练中使用模态不平衡样本和低质量信息，导致零样本草图图像检索性能不佳。

Method: 设计包含单模态特征提取、跨模态多层级加权和加权四元组损失的三模块网络，利用CLIP文本编码器和ViT提取特征，通过局部与全局聚合生成对齐权重，优化域间平衡。

Result: 在Sketchy、TU-Berlin和QuickDraw三个基准数据集上实验表明，该方法优于当前最先进的ZS-SBIR方法。

Conclusion: 所提方法有效提升了零样本草图图像检索的性能，解决了模态不平衡和低质量对齐问题。

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [104] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 提出EVTAR，一种端到端虚拟试穿模型，利用额外参考图像在仅需源图像和目标服装输入的情况下实现高保真实时试穿效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖复杂输入（如姿态、密集姿态、分割掩码等），导致标注成本高、实用性差，限制了其在现实场景中的应用。

Method: 采用两阶段训练策略，直接将目标服装拟合到人物图像上；引入额外参考图像（不同人穿着同一服装）以保留纹理和细节；训练中使用补充参考和无配对人物图像增强性能；推理时无需掩码、密集姿态或分割图。

Result: 在两个广泛使用的基准和多种任务上进行评估，结果表明EVTAR在保持服装细节和整体视觉质量方面优于现有方法，生成结果更真实自然。

Conclusion: EVTAR通过引入参考图像和简化输入要求，实现了高效、高质量的虚拟试穿，在减少人工标注依赖的同时提升了实际应用可行性。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [105] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出一种统一的零样本视频异常分析框架，通过任务链式推理实现时间检测、空间定位和文本解释的联合处理。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常分析方法多局限于帧级检测，缺乏对异常原因的解释，且多数方法依赖特定数据和任务，难以泛化。

Method: 基于基础模型设计任务链式推理流程，利用 intra-task 和 inter-task 推理机制，在不进行额外训练的情况下实现时间检测优化、空间定位与语义理解。

Result: 在多个视频异常检测、定位和解释基准上达到最先进的零样本性能，验证了提示设计与任务链结合的有效性。

Conclusion: 通过精心设计的提示和任务链式推理，可释放基础模型的推理能力，实现可解释、通用的零样本视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [106] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 本文提出了VesSAM，一种专为2D血管分割设计的高效框架，通过卷积适配器、多提示编码器和轻量解码器显著提升分割精度，并在多种模态和数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于血管结构细小、分支复杂且纹理对比度低，现有基础模型（如SAM）在血管分割任务中表现不佳，因此需要专门针对血管结构优化的分割框架。

Method: VesSAM引入了三个关键组件：卷积适配器以增强局部纹理特征，多提示编码器通过分层交叉注意力融合骨骼、分叉点和段中点等解剖提示，以及轻量级掩码解码器以减少锯齿伪影；同时构建自动化流程生成结构化多提示标注，并整理了一个涵盖5种成像模态共8个数据集的基准数据集。

Result: 实验表明，VesSAM在Dice和IoU指标上分别比现有的PEFT-based SAM变体高出10%和13%，性能媲美全微调方法但参数更少，并在外分布（OoD）场景中表现出更强的泛化能力，平均OoD Dice和IoU均超越所有基线模型。

Conclusion: VesSAM是一种高效且通用的2D血管分割框架，在跨模态和跨数据集场景下均表现出优异性能，具备临床应用潜力。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [107] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出一种自监督的多模态迭代去噪（MID）框架，通过建模噪声累积过程并利用神经网络估计和减去噪声增量，无需配对数据即可有效去除复杂非线性噪声，在多个领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的去噪方法难以应对现实世界中复杂的非线性噪声，且通常依赖成对的干净-噪声数据，限制了其应用。

Method: 将观测到的噪声数据视为非线性噪声累积过程的一个状态，通过迭代添加噪声并使用两个神经网络分别估计当前噪声步长和预测/减去相应的噪声增量；采用一阶泰勒展开局部线性化非线性噪声过程。

Result: 在四个经典计算机视觉任务以及生物医学和生物信息学任务中，MID均展现出强大的鲁棒性、适应性和持续领先的性能。

Conclusion: MID是一种无需配对训练数据、能有效处理复杂非线性噪声的通用去噪框架，在多种模态和任务中表现优异，具有广泛的应用前景。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [108] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 提出了一种用于戈雅画作鉴定的新型多模态机器学习框架，通过对视觉和X射线图像应用相同的特征提取方法，在认证准确性上达到97.8%，显著优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 由于戈雅风格演变复杂且存在大量伪造作品，传统艺术鉴定方法面临挑战，因此需要一种更可靠、自动化的计算方法来提高鉴定准确性。

Method: 采用统一的特征提取流程（包括灰度共生矩阵、局部二值模式、熵、能量和颜色分布分析），对视觉和X射线图像进行处理，并使用经过超参数优化的单类支持向量机进行分类。

Result: 在24幅 authenticated 戈雅画作的数据集上，系统取得了97.8%的分类准确率和0.022的假阳性率；对《Un Gigante》的案例研究显示其认证置信度达92.3%。

Conclusion: 应用相同的计算方法于多模态图像（视觉与X射线）能显著提升艺术鉴定性能，验证了该多模态框架的有效性与实用性。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [109] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: 提出HyFormer-Net，一种具有内在可解释性的混合CNN-Transformer模型，用于乳腺超声图像的同步分割与分类，在BUSI数据集上表现优异，并通过渐进式微调实现跨数据集良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在乳腺B超诊断中存在单任务学习、网络结构局限（CNN缺乏全局上下文，Transformer缺乏局部特征）和决策黑箱问题，限制了临床应用。

Method: 设计双分支编码器（EfficientNet-B3与Swin Transformer结合），通过多尺度层次融合模块和注意力门控解码器实现同步分割与分类；引入双管道可解释性分析（注意力验证与Grad-CAM）。

Result: 在BUSI数据集上，Dice得分为0.761±0.072，准确率93.2%，恶性肿瘤召回率92.1%；集成模型达到Dice 90.2%、准确率99.5%、恶性召回率100%；消融实验显示多尺度融合提升+16.8% Dice，注意力门控提升+5.9%；仅用10%目标域数据微调即可恢复92.5%性能，50%数据时Dice达77.3%，超过源域性能。

Conclusion: HyFormer-Net在乳腺超声图像分割与分类任务中性能优越且具备可解释性，通过少量目标域数据微调可实现良好跨域泛化，具有临床应用潜力。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [110] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost提出了一种参数高效的神经网络架构，通过动态缩放渐进注意力（DSPA）机制，在CIFAR基准上实现了最先进的性能，显著减少了参数量并提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的边缘设备上实现高性能和高效率的平衡，需要设计一种既轻量又准确的神经网络架构。

Method: 引入了动态缩放渐进注意力（DSPA）机制，包含自适应融合、相位缩放和残差自适应三个创新点，并结合增强型MBConv模块构建整体架构。

Result: 在CIFAR-10上达到95.57%准确率（仅0.85M参数），比MobileNetV3减少2.1倍参数且精度提升3.2个百分点；同时具备低FLOPs和高梯度流特性。

Conclusion: DSPA与高效卷积操作的协同优化实现了前所未有的参数-精度权衡，为边缘设备部署提供了高性能、低资源消耗的解决方案。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [111] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出了一种针对神经图像压缩（NIC）的新型多尺度对数-指数攻击框架T-MLA，通过在小波域中构造对抗性扰动，显著降低重建图像质量，同时保持扰动视觉上不可察觉。


<details>
  <summary>Details</summary>
Motivation: 现有对NIC的对抗攻击多为像素空间方法的简单迁移，忽略了压缩流程的独特结构特性，缺乏针对性和有效性。

Method: 在小波域中设计对抗扰动，直接针对图像质量和重建过程，将扰动限制在特定小波子带，实现离线、有策略的攻击。

Result: 在多个先进NIC模型和标准数据集上验证了T-MLA的有效性，导致重建质量大幅下降但扰动仍视觉不可见。

Conclusion: 揭示了神经图像压缩系统中存在的核心安全漏洞，对生成式模型和内容分发管道的安全性提出了严峻挑战。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [112] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 本文提出了一种基于层次化序列预测的图像地理定位方法，利用S2网格系统逐级细化位置预测，并引入语言模型中的推理策略（如束搜索和多样本推断）提升性能，在多个数据集上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 图像地理定位面临视觉相似性和搜索空间大的挑战，现有方法难以准确预测地理位置，因此需要一种能逐步缩小范围并有效处理不确定性的新方法。

Method: 采用S2细胞构建全球多层次网格系统，将地理定位建模为层次化序列预测任务，结合视觉输入与先前预测结果自回归地生成更精细的位置；在推理时引入束搜索和多样本推断等策略优化输出。

Result: 在Im2GPS3k和YFCC4k数据集上评估显示，不使用MLLM时性能超越大多数基线方法，最高精度提升达13.9%；结合MLLM后在所有指标上均达到最先进水平。

Conclusion: 所提出的层次化预测框架有效解决了图像地理定位中的大搜索空间和模糊性问题，通过借鉴语言模型的训练与推理机制，显著提升了定位精度，具有良好的可扩展性和应用前景。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [113] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I是一个用于下一代网络切片中特征可视化的合成数据集，包含四种编码方法生成的12万样本，每样本含原始KPI向量和对应低分辨率RGB图像，适用于视觉学习、网络状态分类和异常检测等任务。


<details>
  <summary>Details</summary>
Motivation: 随着5G和6G网络的发展，网络切片成为未来服务导向架构的重要组成部分，亟需基于高质量数据集的精细化识别方法来应对复杂多变的网络环境。

Method: 通过物理启发映射、Perlin噪声、神经壁纸化和分形分支四种编码方式，将多变量KPI向量转化为低分辨率RGB图像，构建包含12万样本的合成数据集。

Result: 生成了SliceVision-F2I数据集，每个编码方法包含30,000个样本，每个样本包括原始KPI向量和对应的可视化图像，并模拟真实且含噪声的网络条件。

Conclusion: 该数据集可用于基于图像的机器学习技术在网络安全中的应用研究，支持视觉学习、异常检测、网络状态分类及模型基准测试，具有良好的可复用性和公开性。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [114] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 提出了一种基于Epanechnikov核密度估计（EKDE）和双模态逻辑回归分类器的呼吸系统疾病诊断新方法，使用胸部X光图像进行实验，准确率为70.14%，敏感性为59.26%，特异性为74.18%。


<details>
  <summary>Details</summary>
Motivation: 旨在提高医学图像中呼吸系统疾病的诊断准确性，利用非参数化方法避免对数据分布形状的假设，适应像素强度变化。

Method: 结合Epanechnikov非参数核密度估计（EKDE）与双模态逻辑回归分类器，构建基于统计模型的学习框架，用于从医学图像中提取关键特征并进行分类。

Result: 在包含13808张胸部X光片的数据集上测试，准确率达到70.14%，敏感性为59.26%，特异性为74.18%，表现出中等检测性能，敏感性仍有提升空间。

Conclusion: EKDE-based方法在医学影像诊断中具有潜力，可提升诊断的准确性与可靠性，但需结合临床专业知识进一步优化模型。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [115] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: 提出ViACT框架，通过整合解剖先验信息到视频Transformer中，专注于心肌区域的表征学习，提升超声心动图分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型容易从非诊断区域（如背景）学习到虚假相关性，影响诊断准确性。

Method: 将解剖结构表示为点集，编码其几何和图像块为Transformer令牌，采用掩码自编码策略仅重建解剖区域补丁。

Result: 在左室射血分数回归和心脏淀粉样变检测任务中表现优异，注意力图可解释且与病理区域一致，并能泛化至心肌点跟踪任务。

Conclusion: ViACT通过解剖约束有效聚焦于诊断相关区域，提升了模型性能与可解释性，具有良好的跨任务泛化能力。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [116] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 本文提出利用带有GPU的嵌入式设备来提升移动设备上计算机视觉应用在边缘计算环境中的性能，从而改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉和增强现实应用对资源需求高，而移动设备和传统边缘计算设备资源有限，影响用户体验。

Method: 采用带有GPU的嵌入式设备进行任务卸载，并通过实验比较GPU与仅使用CPU的性能差异。

Result: 实验结果表明，使用GPU相比仅使用CPU可获得显著的性能提升。

Conclusion: 使用带GPU的嵌入式设备能有效缓解边缘计算中资源受限的问题，提升用户使用计算机视觉应用时的体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [117] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 提出了一种无需显式监督或依赖语言模型的弱监督框架PCP，利用类别级概念先验和优化机制提升医学图像中概念预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释AI方法在医学图像中需要昂贵的概念标注，而零样本方法难以捕捉医学领域的特定特征，可靠性差。

Method: 提出Prior-guided Concept Predictor (PCP)，利用类别级概念先验作为弱监督信号，并引入KL散度和熵正则化的 refinement 机制，使预测更符合临床推理。

Result: 在PH2和WBCatt数据集上，PCP相较于零样本基线提升了33%以上的概念级F1分数，并在四个医学数据集上达到与全监督概念瓶颈模型相当的分类性能。

Conclusion: PCP是一种有效且实用的弱监督可解释AI框架，能够在无需概念标注的情况下提升医学图像分析中概念预测的准确性和可解释性。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [118] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: 提出了一种名为CatEquiv的类别等变神经网络，用于基于惯性传感器的人类活动识别，通过编码时间、幅度和结构对称性，在分布外扰动下表现出更强的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了提升人类活动识别模型在分布外扰动下的鲁棒性和泛化能力，需系统地建模数据中的时间、幅度和结构对称性。

Method: 提出CatEquiv模型，引入分类对称积，将循环时间平移、正增益和传感器层次结构偏序集结合，实现对分类对称积的等变性。

Result: 在UCI-HAR数据集上，CatEquiv在分布外扰动下显著优于带循环填充的CNN和普通CNN，展现出更高的鲁棒性。

Conclusion: 强制施加分类对称性能有效提升模型的不变性和泛化能力，且无需增加模型容量。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [119] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出了一种轻量级注意力分割网络MicroAUNet，结合深度可分离膨胀卷积和单路径参数共享的通道-空间注意力模块，用于实时结直肠息肉分割。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的息肉分割模型在分割结果中提供模糊的息肉边界或依赖计算复杂度高的重型架构，难以满足实时临床应用需求。

Method: 设计了MicroAUNet网络，采用深度可分离膨胀卷积和参数共享的通道-空间注意力模块增强多尺度边界特征，并引入两阶段渐进式知识蒸馏方法从高容量教师模型迁移语义和边界信息。

Result: 在多个基准数据集上实现了最先进的精度，同时模型复杂度极低，具备快速推理能力，适用于实时内窥镜息肉分割。

Conclusion: MicroAUNet在保持极低计算复杂度的同时实现了高精度息肉边界分割，适合临床实时应用，具有良好的实用性与推广价值。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [120] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文提出了一个包含1333个英文Rebus谜题的大规模基准测试集，并提出了一种模型无关的框架RebusDescProgICE，通过结合非结构化描述和基于代码的结构化推理，提升了视觉-语言模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题涉及图像识别、常识推理、多步推理等多种能力，对当前视觉-语言模型构成挑战，因此需要更有效的基准和推理框架来评估和提升模型性能。

Method: 构建了一个大规模、多样化的Rebus谜题基准集，并提出RebusDescProgICE框架，结合非结构化描述、基于代码的结构化推理以及改进的上下文示例选择方法。

Result: 相比思维链推理，所提方法在闭源模型上性能提升2.1-4.1%，在开源模型上提升20-30%。

Conclusion: RebusDescProgICE框架有效提升了视觉-语言模型在复杂Rebus谜题上的理解与推理能力，验证了结构化推理与合理上下文示例选择的重要性。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [121] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: 本文提出了ROVER，一个用于评估统一多模态模型中双向跨模态推理能力的人工标注基准，揭示了跨模态推理对视觉生成质量的关键作用，并发现现有模型在物理与符号推理之间存在分离现象。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型评估方法大多孤立地对待文本和图像能力，缺乏对双向跨模态推理的有效评测，而这种能力对于实现真正的统一多模态智能至关重要。

Method: 提出ROVER基准，包含1312个任务和1876张图像，涵盖两种互补设置：用于视觉生成的言语增强推理和用于语言生成的视觉增强推理，通过人工标注评估模型在跨模态引导、验证和优化方面的能力。

Result: 在17个统一模型上的实验表明：(i) 跨模态推理显著影响视觉生成质量，交错式模型明显优于非交错式模型；(ii) 模型在物理推理上表现良好，但在需要视觉抽象的符号任务上失败，显示出物理与符号推理的分离。

Conclusion: 双向跨模态推理是实现真正多模态生成的关键前沿，ROVER为评估这一能力提供了有效工具，并揭示了当前模型在构建视觉抽象和整合多模态信息方面的不足。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [122] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 本文提出了Viewpoint Learning任务和Viewpoint-100K数据集，用于评估和提升多模态大语言模型（MLLM）的空间推理能力，通过两阶段微调策略显著增强了模型在3D视觉理解中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM在2D视觉理解上取得进展，但在需要精细空间信息的3D推理任务中表现尚不明确，尤其是跨视角一致性这一关键问题。因此，亟需方法来评估并增强MLLM的空间推理能力。

Method: 提出Viewpoint-100K数据集（含10万组不同视角的物体图像对及问答对），采用两阶段微调：第一阶段在该数据集上进行监督微调（SFT）注入基础空间知识；第二阶段使用GRPO强化学习算法在更广泛问题上提升泛化能力，并引入混合冷启动初始化方法以同时学习视角表示并保持连贯推理。

Result: 实验表明，所提方法显著提升了MLLM在域内和域外空间推理任务上的性能，有效激活了其空间推理能力。

Conclusion: 发展MLLM的基础空间技能至关重要，有助于推动机器人、自动驾驶和3D场景理解等领域的进步。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [123] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 提出了一种从YouTube视频中自动挖掘和处理动物视频的管道，构建了大规模数据集和Animal-in-Motion（AiM）基准，用于推动无标记、野外环境下的4D动物重建研究。


<details>
  <summary>Details</summary>
Motivation: 现有动物视频数据集规模小、标注不足，难以支持动物中心的3D/4D任务，且依赖受控采集环境，缺乏真实自然场景的数据。

Method: 设计自动化管道从YouTube挖掘单视角视频，提取对象中心的片段，并添加辅助标注；构建包含30K视频（2M帧）的大规模数据集和AiM基准（230个序列，11K帧）；改进基于序列优化的无模型方法，建立首个4D动物重建基线。

Result: 数据集规模超过以往工作一个数量级；在AiM基准上发现基于模型的方法在2D指标上表现好但3D形状不真实，无模型方法重建更自然但评分较低，揭示当前评估的不足；提出的基线方法提升了4D重建质量。

Conclusion: 所提出的管道、基准和基线有助于推进基于野外视频的大规模、无标记4D动物重建及相关任务的发展。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [124] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于离散小波变换和扩散Transformer的图像超分辨率方法DTWSR，通过捕捉多尺度频带间的相关性，提升了重建图像的一致性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有基于DWT的超分辨率方法忽略了多尺度频带之间的相互关系，导致重建图像出现不一致和不自然的伪影。

Method: 采用多级离散小波变换（MDWT）分解图像为小波谱，提出金字塔式令牌化方法将频谱嵌入序列供Transformer处理，并设计双解码器分别处理低频和高频子带，保持生成过程中的对齐性。

Result: 在多个基准数据集上的实验表明，该方法在感知质量和保真度方面均表现出色。

Conclusion: DTWSR有效建模了多尺度频带间的依赖关系，显著提升了图像超分辨率的视觉质量和一致性。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [125] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 提出了一种基于拓扑感知图卷积网络（GCN-PSN）的人体动作质量评估方法，通过将人体骨骼建模为图结构来学习具有判别性的姿态嵌入，在AQA-7和FineDiving基准上表现出竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估需要对人体运动进行细粒度理解并精确评估姿态相似性，现有方法在利用骨骼拓扑结构方面存在不足。

Method: 提出一种拓扑感知的图卷积网络框架GCN-PSN，将人体骨架建模为图结构，并采用Siamese架构结合对比回归目标进行训练，以学习对姿态相似性敏感的嵌入表示。

Result: 在AQA-7和FineDiving两个基准上取得了优于基于坐标的方法的性能，消融实验验证了利用骨骼拓扑结构的有效性。

Conclusion: 利用骨骼拓扑信息有助于提升姿态相似性和动作质量评估的准确性，所提方法在主流基准上表现优异。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [126] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一种新的层次化运动生成框架，用于文本驱动的3D人体运动生成，通过多尺度令牌保持策略和可扩展自回归建模，在生成质量和效率上均达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 提升文本驱动3D人体运动生成的质量与效率，解决现有方法在推理步数、重建质量及多尺度信息保留方面的局限性。

Method: 提出MoSa框架，结合多尺度令牌保持策略（MTPS）与分层残差向量量化变分自编码器（RQ-VAE），并设计可扩展自回归（SAR）生成机制；引入轻量化的CAQ-VAE以增强全局依赖建模并减少插值带来的重建退化。

Result: 在Motion-X数据集上实现0.06的FID（优于MoMask的0.20），推理时间减少27%；仅需10步推理即可完成生成，并在运动编辑等下游任务中展现出良好泛化能力。

Conclusion: MoSa通过层次化、多尺度的生成机制显著提升了文本到3D运动生成的性能，在生成质量、速度和通用性方面均表现优异。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [127] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一种多模态视觉-语言-动作模型，通过引入红外相机、毫米波雷达和麦克风阵列等新型传感器模态，扩展了传统仅依赖RGB图像的感知能力。其核心是传感器掩码图像，将物理上有意义的掩码叠加到RGB图像上，实现多模态信息的统一表示。该方法在保持与RGB统计特性兼容的同时，提升了操作任务的成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型主要依赖RGB图像，感知能力有限，难以应对需要更丰富物理感知的复杂操作任务。因此，需要引入更多样化的传感模态以增强模型的空间与物理理解能力。

Method: 提出传感器掩码图像作为统一表示，将来自红外、毫米波雷达和麦克风阵列的感知信息以空间对齐的方式融合到RGB图像中，并采用轻量级的每传感器投影器进行数据高效学习。基于预训练的RGB VLA模型构建多感官VLA架构。

Result: 在真实世界任务中，OmniVLA平均任务成功率达到84%，相比仅使用RGB和原始传感器输入的基线模型分别提升59%和28%，同时表现出更高的学习效率和更强的泛化能力。

Conclusion: OmniVLA通过融合多种物理传感器模态，显著增强了视觉-语言-动作模型的感知与操作能力，验证了多模态感知在机器人操作中的有效性与潜力。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [128] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 本文提出了一种基于推理链的多步推理方法，用于提升印度食品视觉问答（VQA）系统的性能，通过微调小型语言和视觉模型并结合强化学习，在基准上实现了平均10个百分点的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统主要偏向西方食物，对印度多样化的饮食文化支持不足，且现有印度食品VQA数据集采用两步式方法，缺乏对复杂烹饪语境的深入理解。

Method: 构建自动验证的推理链，结合知识图谱进行多步推理，并利用强化学习对小型LLM和VLM进行微调与训练。

Result: 在印度食品VQA任务中，引入推理链后模型准确率平均提升了10个百分点。

Conclusion: 多步推理结合推理链增强能有效提升印度食品VQA的准确性，为跨文化食品理解提供了可行的技术路径。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [129] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出了一种深度感知且注视引导的物体检测框架Eyes on Target，用于以自我为中心的视频分析，通过将注视特征注入Vision Transformer的注意力机制中，提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 利用人类注视提供的丰富监督信号，改善复杂视觉环境中对视觉注意力的理解，特别是在以自我为中心的视频中提升物体检测性能。

Method: 将基于注视的特征引入Vision Transformer的注意力机制，使空间特征选择偏向于人类关注区域，并设计了注视感知的注意力头重要性度量来解释模型行为。

Result: 在自建模拟器数据集和公开基准（如Ego4D Ego-Motion和Ego-CH-Gaze）上，相比无注视基线模型，检测准确率持续提升。

Conclusion: 该方法有效利用注视信息增强以自我为中心视频中的物体检测，展示了其在评估人类在模拟场景中表现的潜力。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [130] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于二阶信息的新型黑盒梯度可迁移攻击方法，引入对抗平坦性（AF）解决欺骗性平坦问题，并设计了对抗平坦攻击（AFA）和蒙特卡洛对抗采样（MCAS）来提升迁移性与攻击效率。


<details>
  <summary>Details</summary>
Motivation: 现有可迁移攻击方法易陷入平坦但局部尖锐的次优区域（即欺骗性平坦），导致迁移性不足，需更有效的优化目标提升跨模型攻击效果。

Method: 从二阶信息角度出发，提出对抗平坦性（AF）概念并理论分析其对迁移性的促进作用；通过高效近似构建AFA攻击算法，结合MCAS提升内循环采样效率以增强攻击能力。

Result: 在ImageNet兼容数据集上优于六种基线方法，生成的对抗样本位于更平坦区域，迁移性更高；在输入变换和百度云API测试中均表现出更强的攻击性能。

Conclusion: 所提AFA与MCAS有效缓解了欺骗性平坦问题，显著提升了黑盒环境下对抗样本的可迁移性与鲁棒性，验证了利用高阶信息优化对抗攻击的潜力。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [131] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: 提出CenterMamba-SAM，一种用于脑部病灶分割的端到端框架，通过冻结预训练主干网络并仅微调轻量适配器，结合新颖的扫描策略和记忆增强机制，在公共基准上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 脑部病灶分割因病灶小、对比度低、采样各向异性和切片间不连续而具有挑战性。

Method: 采用CenterMamba编码器，引入3x3角-轴-中心短序列扫描策略，优先聚合中心信息并强化轴向与对角补偿；结合记忆驱动的结构提示生成器和多尺度解码器，利用原型库自动生成提示并提升跨切片一致性。

Result: 在多个公开数据集上实验表明，该方法在病灶分割任务中达到最先进的性能，尤其在微小病灶和边界检测方面表现优异。

Conclusion: CenterMamba-SAM通过高效的微调策略和增强的上下文建模，显著提升了脑部病灶分割的精度与鲁棒性，适用于临床实际应用。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [132] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种轻量级几何感知适配器，通过水平圆形填充和局部窗口K近邻统计，在训练时稳定结构脆弱区域的预测，显著提升LiDAR语义分割在恶劣天气下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恶劣天气下因几何畸变导致性能下降，且忽视边界、角落和稀疏区域的结构性脆弱问题，缺乏针对几何结构的显式建模与正则化。

Method: 设计几何感知适配器：对方位角对齐并采用水平圆形填充以保持0~360度边界的邻域连续性；通过局部窗口KNN提取邻近点并计算局部统计特征，压缩为紧凑的几何感知线索，用于训练时的区域感知正则化。

Result: 在仅使用源域数据（SemanticKITTI）训练、无目标域标签（SemanticSTF）的情况下，相比基于数据增强的基线mIoU提升7.9个百分点，相比类感知正则化基线提升0.6个百分点。

Conclusion: 几何驱动的正则化是提升全天候LiDAR语义分割性能的关键方向，该适配器即插即用、训练专用、推理开销极低。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [133] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: 提出MotionStream，实现单GPU下亚秒级延迟、最高29FPS的实时视频流生成，支持无限时长视频的高质量、低延迟交互式生成。


<details>
  <summary>Details</summary>
Motivation: 现有运动条件视频生成方法存在高延迟和非因果处理问题，难以实现实时交互。

Method: 通过在文本到视频模型中引入运动控制，并利用带分布匹配蒸馏的自强制方法将双向教师模型蒸馏为因果学生模型；设计滑动窗口因果注意力与注意力sink机制，结合KV缓存滚动实现固定上下文窗口下的恒定速度生成。

Result: 实现了高达29FPS的实时生成，支持任意长度视频生成，在运动跟随和视频质量上达到SOTA，且速度比现有方法快两个数量级。

Conclusion: MotionStream解决了长时域视频生成中的延迟、误差累积和计算成本增长问题，首次实现了真正意义上的实时、无限长度交互式视频生成。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [134] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: 提出PRevivor，一种基于先验引导的色彩恢复Transformer模型，用于修复古代中国画的褪色问题。


<details>
  <summary>Details</summary>
Motivation: 古代中国画因复杂的化学机制导致颜色退化，且缺乏高质量数据集，使得数字化修复困难。

Method: 将色彩恢复分解为亮度增强和色调校正两个步骤；使用变分U-Net和多尺度映射模块进行亮度增强；设计双分支颜色查询模块，结合局部色调先验实现局部与全局的色调校正。

Result: 在多个先进着色方法的对比实验中，PRevivor在定量和定性评估上均表现出更优性能。

Conclusion: PRevivor能有效恢复古代绘画的色彩，为文化遗产的数字修复提供了可行方案。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [135] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 本文综述了基础模型在医学图像分析中的应用，探讨了其在临床实践中的适应策略及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决传统任务特定模型的局限性，利用大规模数据学习可迁移表示，提高医学图像分析的通用性和适用性。

Method: 评估监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法和多模态或跨模态框架等方法。

Result: 总结了各种方法的性能增益、临床适用性和局限性，并识别出先前综述常忽略的权衡和未解决的挑战。

Conclusion: 提出了持续学习、联邦学习和隐私保护方法、混合自监督学习、数据中心管道以及系统基准测试等新兴方向，为开发适应性强、可信且临床集成的基础模型提供了路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [136] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 提出一种基于自然图像与生成图像流形几何差异的检测框架，利用自监督模型和归一化流增强差异，实现高效生成图像检测。


<details>
  <summary>Details</summary>
Motivation: 随着生成图像真实感提升，其滥用风险增加，现有依赖二分类器的检测方法受限于生成图像的数量和质量，亟需更鲁棒的检测技术。

Method: 设计一对函数，使其对自然图像输出一致而对生成图像输出发散，利用它们梯度位于正交子空间的特性；通过沿数据流形变换并观察自监督模型损失变化来判断图像是否生成，并使用归一化流将生成图像从自然图像流形上推开以增强可检测性。

Result: 实验表明该方法在多种生成模型下均表现出优异的检测性能，尤其对先进生成模型仍有较强检测能力。

Conclusion: 该框架提供了一种不依赖大量生成样本训练的新范式，通过几何流形分析和流形增强显著提升了生成图像的检测鲁棒性和泛化能力。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [137] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了UniREditBench，一个用于基于推理的图像编辑评估的统一基准，涵盖真实和游戏场景下的多对象交互与复杂推理任务，并引入多模态双参考评估方法以提高评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑基准主要关注单个对象属性变换，忽略了多对象交互和具有人为规则的游戏场景，且仅依赖文本参考进行评估，易在复杂推理场景中产生误判。因此需要一个更全面、可靠的基准来系统评估生成模型在多样化推理任务中的表现。

Method: 构建了一个包含2700个样本的UniREditBench基准，覆盖8个主维度和18个子维度，涵盖真实与游戏世界场景；提出多模态双参考评估（文本+真实图像）；设计自动化多场景数据合成流程，生成含高质量思维链标注的大规模UniREdit-Data-100K数据集；基于该数据集微调Bagel模型，得到UniREdit-Bagel。

Result: 实验表明，UniREdit-Bagel在领域内和分布外设置下均显著优于现有模型；对多个开源与闭源图像编辑模型的评测揭示了它们在不同推理任务上的优缺点。

Conclusion: UniREditBench填补了复杂推理导向图像编辑任务评估的空白，通过多模态双参考和大规模合成数据提升了评估可靠性，为未来多模态模型的推理能力发展提供了重要支持。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [138] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为REASON的两阶段概率图引导双分支融合框架，用于胃内容物超声评估，显著提升了术前误吸风险自动评估的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工勾画和经验公式，存在效率低、准确性差的问题，难以满足临床对胃内容物评估的需求。

Method: 第一阶段使用分割模型生成抑制伪影并突出胃部解剖结构的概率图；第二阶段采用双分支分类器融合右侧卧位（RLD）和仰卧位（SUP）两种标准视图的信息，提升特征判别能力。

Result: 在自建数据集上的实验表明，该框架显著优于当前最先进的方法。

Conclusion: REASON框架在自动化术前误吸风险评估中表现出优越性能，为临床实践提供了更鲁棒、高效和准确的解决方案。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [139] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出一种基于潜在因子分组和聚类推理的实例解耦学习框架，用于全切片病理图像的可解释表示，有效缓解了多重实例学习中的空间、语义和决策纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 多重实例学习在全切片病理图像表示中存在空间、语义和决策上的纠缠，限制了其表征能力和可解释性。

Method: 采用正定潜在因子分组将实例映射到潜在子空间以缓解空间纠缠；通过基于聚类推理的实例概率反事实推断优化缓解语义纠缠；利用广义线性加权决策和实例效应重加权解决决策纠缠。

Result: 在多中心数据集上实验表明，该模型优于所有现有最先进模型，并实现了与病理学家判断一致的可解释性。

Conclusion: 所提出的解耦学习框架有效提升了全切片图像分析的性能与可解释性，具有临床应用潜力。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [140] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架APDM，用于防止扩散模型对特定主体的未经授权的个性化，通过将保护目标从图像转移到模型本身，并引入了新的损失函数DPO和优化策略L2P，在保持生成质量的同时有效阻止个性化。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性扰动方法在面对少量干净图像或简单图像变换时效果不佳，且依赖不切实际的假设，因此需要一种更鲁棒的方法来防止扩散模型的滥用。

Method: 提出了Anti-Personalized Diffusion Models (APDM) 框架，包括Direct Protective Optimization (DPO) 损失函数和Learning to Protect (L2P) 双路径优化策略，通过理论分析指导设计，直接在模型层面进行保护。

Result: 实验结果表明，APDM在防止未经授权的个性化方面优于现有方法，实现了最先进的性能，同时不影响模型的生成质量。

Conclusion: APDM通过DPO和L2P机制有效增强了扩散模型的隐私保护能力，能够在实际场景中抵御多种个性化攻击，为生成模型的安全应用提供了新思路。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [141] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 提出首个基于Mamba架构的多视图立体匹配网络MVSMamba，通过动态Mamba模块实现高效全局特征聚合，在DTU和Tanks-and-Temples数据集上实现了性能与效率的双重提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的MVS方法因二次复杂度在性能与效率间难以平衡，而Mamba架构具有线性复杂度和强大的全局建模能力，适合解决该问题。

Method: 设计了MVSMamba网络，提出基于参考中心动态扫描策略的动态Mamba模块（DM-module），实现视内和视间特征高效交互、全向多视图特征表示以及多尺度全局特征聚合。

Result: 在DTU和Tanks-and-Temples基准上，MVSMamba优于当前最先进的MVS方法，兼具更高性能和效率。

Conclusion: MVSMamba成功将Mamba引入多视图立体匹配，为高效高精度三维重建提供新思路。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [142] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 本文提出了一种基于CLIP模型的生成式对抗攻击方法，利用文本-图像对齐能力生成视觉上难以察觉但能有效欺骗多标签分类器的对抗扰动。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击可能严重削弱深度学习模型的可靠性，现有方法在保持扰动隐蔽性和攻击有效性方面存在局限。

Method: 结合CLIP模型的语义对齐能力与SSAE的集中扰动策略及GAMA的非相似文本嵌入，通过引导损失生成对抗样本。

Result: 在多种黑盒模型上实验表明，该方法在攻击成功率上具有竞争力，且生成的对抗样本保持更高的视觉保真度。

Conclusion: 所提方法能有效平衡攻击性能与视觉隐蔽性，适用于复杂多对象场景下的对抗攻击。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [143] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为RDTE-UNet的医学图像分割网络，结合局部建模与全局上下文，提升边界划分和细节保留能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像中存在显著的解剖结构变异性和边界模糊问题，导致精细结构的可靠分割困难。

Method: 采用混合ResBlock与细节感知Transformer骨干网络，设计ASBE（自适应边界增强）、HVDA（细粒度特征建模）和EulerFF（基于欧拉公式的融合加权）三个模块。

Result: 在Synapse和BUSI数据集上，RDTE-UNet在分割精度和边界质量方面达到了可比的高水平。

Conclusion: RDTE-UNet通过整合局部与全局信息，有效提升了医学图像分割中的结构一致性和边界准确性。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [144] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 提出MIQ-SAM3D，一种支持单点提示生成多实例分割的3D医学图像分割框架，结合CNN与Transformer优势，在LiTS17和KiTS21数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有SAM-based方法多采用单点对单目标范式，难以处理多病灶分割；同时ViT骨干网络易丢失局部细节信息。

Method: 设计prompt-conditioned实例查询生成器，将单个点提示转化为多个专用查询；采用混合CNN-Transformer编码器，通过空间门控注入CNN边界显著性信息；引入竞争性优化查询解码器，实现端到端并行多实例预测。

Result: 在LiTS17和KiTS21数据集上达到可比或更优性能，对提示点位置具有强鲁棒性，支持高效标注临床多病灶案例。

Conclusion: MIQ-SAM3D实现了从单点提示到多实例分割的范式转变，有效提升3D医学图像中多病灶的分割效率与鲁棒性。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [145] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 本文提出了一种通过内容-风格子空间融合和内容-风格平衡损失来扩展内容-风格前沿的新方法，显著提升了文本到图像生成中在不同风格强度下的内容相似性。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅在单一风格强度下评估内容相似性，而高风格强度会导致内容特征严重丢失，限制了内容与风格的权衡表现。

Method: 引入内容-风格子空间融合机制，并设计内容-风格平衡损失函数，以在增强风格化的同时保留更多内容特征。

Result: 实验表明，该方法在定性和定量评估中均优于现有技术，显著降低了IGD和GD分数，实现了更优的内容-风格权衡。

Conclusion: 所提方法有效扩展了内容-风格前沿，在多种风格强度下实现了更好的内容保持与风格化效果。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [146] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Cross-Mamba交互的多任务学习框架CMI-MTL，用于医学视觉问答（Med-VQA），在三个公开数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力方法难以有效处理视觉与语言之间的跨模态语义对齐，且分类方法受限于预定义答案集，无法适应自由形式答案的多样性。

Method: 设计了CMI-MTL框架，包含细粒度视觉-文本特征对齐（FVTA）、跨模态交错特征表示（CIFR）和自由形式答案增强的多任务学习（FFAE）三个模块，以提升跨模态表征与开放性问答能力。

Result: 在VQA-RAD、SLAKE和OVQA三个Med-VQA数据集上性能优于现有最先进方法，并通过可解释性实验验证了模块有效性。

Conclusion: CMI-MTL能有效提升医学视觉问答中的跨模态理解与自由形式回答生成能力，具有较强的泛化性和应用潜力。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [147] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: 本文提出了一种名为语义增强型Patch精简（SEPS）的框架，用于解决细粒度跨模态对齐中的patch冗余与歧义问题，通过融合密集与稀疏文本语义并引入相关性感知选择机制，在Flickr30K和MS-COCO数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的跨模态对齐方法在处理视觉与语言之间的信息密度差异时面临挑战，尤其是patch冗余和歧义问题，且难以准确衡量视觉patch与文本描述之间的语义相关性。

Method: 提出SEPS框架，采用两阶段机制融合来自密集和稀疏文本的统一语义，识别显著视觉patch，并利用相关性感知的选择与均值计算来突出关键的patch-词对应关系，提升跨模态相似性评估。

Result: 在Flickr30K和MS-COCO数据集上的实验表明，SEPS在不同模型架构下rSum指标上超越现有方法23%-86%，尤其在文本到图像检索任务中表现突出。

Conclusion: SEPS有效解决了跨模态对齐中的patch冗余与歧义问题，通过语义融合与相关性建模显著提升了细粒度视觉-语言对齐性能。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [148] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 本研究提出了Fire-ART数据集和基于全景图像的重建方法，用于将消防资产语义信息自动融入BIM模型，提升火灾安全设备的数字化管理精度。


<details>
  <summary>Details</summary>
Motivation: 传统消防资产管理方法在自动化识别与重建方面能力有限，难以满足应急响应和风险评估的需求，因此需要更高效、准确的技术手段。

Method: 构建了包含15类基本消防资产、2,626张图像和6,627个实例的Fire-ART数据集，并提出一种融合改进立方图转换与基于半径的球面相机投影的全景图像重建方法，以提高资产识别与定位精度。

Result: 通过两个真实案例验证，该方法分别实现了73%和88%的F1分数，定位误差分别为0.620米和0.428米。

Conclusion: Fire-ART数据集和所提出的重建方法为消防资产的自动识别与精确定位提供了有效技术支持，有助于提升建筑消防安全设备的数字化管理水平。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [149] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出一种无需训练的视觉模型解释方法，使用平滑可调轮廓替代密集扰动掩码，生成紧凑、连通且稳定的解释区域，在ImageNet上表现优于现有方法，尤其在DINO模型上显著提升相关性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于密集扰动掩码的解释方法常出现碎片化和过拟合问题，需要复杂后处理，缺乏稳定性和可解释性。

Method: 将星凸区域用截断傅里叶级数参数化，利用分类器梯度优化极值保留/删除目标，实现平滑、连通的轮廓生成，并扩展至多轮廓以定位多个对象。

Result: 在ImageNet上达到与密集掩码相当的保真度，但参数更少、运行一致性更高、边界更稳定；生成重要性轮廓图并实现显式面积控制；在DINO模型上相关性质量提升超15%，且保持正向保真相关性。

Conclusion: 该方法通过限制解空间为低维平滑轮廓，有效提升了视觉解释的紧凑性、稳定性与可解释性，尤其适用于自监督模型的高效可靠解释。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [150] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 提出一种基于蒸馏的高效因果视频生成框架，通过对抗自蒸馏和首帧增强策略，在极少数去噪步骤下实现高质量视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有混合视频生成模型因顺序迭代特性导致误差累积和推理时间长，需要更高效的生成方法。

Method: 基于分布匹配蒸馏（DMD）框架，提出对抗自蒸馏（ASD）策略，使学生模型的n步去噪输出与其n+1步版本在分布层面一致，并结合首帧增强（FFE）策略优化去噪步骤分配。

Result: 在VBench上实验表明，本方法在一步和两步视频生成中均优于现有最先进方法，且单一模型可灵活支持多种推理步数设置。

Conclusion: 所提框架显著提升极少数步数下的训练稳定性和生成质量，无需重复再蒸馏即可实现高效、高质量视频合成。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [151] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的单目标跟踪器UniSOT，能够同时处理三种参考模态和四种视频模态的任意组合，在18个基准上表现出优于模态专用方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器仅针对单一或少数几种模态设计，导致模型分离且限制了实际应用，亟需一个能适应多种参考和视频模态的统一跟踪器。

Method: 设计了一个具有统一参数的通用跟踪框架UniSOT，支持多种参考模态（如边界框、自然语言）与多种视频模态（如RGB、RGB+Depth等）的任意组合。

Result: 在18个视觉跟踪、视觉-语言跟踪和RGB+X跟踪基准上实验表明，UniSOT在所有参考模态下比先前方法在TNL2K上高出3.0%以上的AUC，并在所有RGB+X模态下超过Un-Track 2.0%以上的主要指标。

Conclusion: UniSOT是首个支持多种参考与视频模态组合的统一跟踪器，具备更强的泛化能力和实际应用价值。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [152] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出一种分辨率感知的token解码器，用于解决非结构化道路语义分割中的边界模糊、稀疏监督和标签噪声问题，在保持高效计算的同时提升边缘精度和局部一致性。


<details>
  <summary>Details</summary>
Motivation: 针对非结构化道路语义分割中存在的厚且不一致的边界、罕见类别标注稀疏以及普遍的标签噪声问题，现有方法在低分辨率融合时易丢失细节，而高分辨率处理则计算昂贵且对噪声敏感。

Method: 设计了一个分辨率感知的token解码器：大部分计算在低分辨率瓶颈层进行；通过门控交叉注意力注入细粒度特征；仅对不确定性选中的稀疏像素进行精细化。结合全局自注意力与轻量级空洞深度可分离卷积恢复局部一致性，引入边界带一致性正则化训练策略，增强边缘附近预测的连贯性。

Result: 该方法在多个基准上表现出竞争力的性能，尤其在边界质量和过渡区域稳定性方面优于现有方法，同时计算成本较低，对标签噪声具有更强鲁棒性。

Conclusion: 所提出的解码器在效率、精度和鲁棒性之间取得了良好平衡，特别适用于复杂真实场景下的语义分割任务。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [153] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 提出一种仅在训练时使用的多目标方法，通过增强类别间决策边界和利用RGB预训练教师模型的语义先验来提升热红外图像检测性能，保持单模态推理的同时实现最优效果。


<details>
  <summary>Details</summary>
Motivation: 热红外图像检测在夜间感知中面临低对比度、高频信息弱等问题，导致检测结果存在重复框、漏检小物体和类别混淆；现有方法依赖跨模态转换或融合，对颜色、结构伪影敏感且需要额外传感器和高计算成本。

Method: 保持单模态推理，在训练阶段引入两个目标：1）通过拉近同类特征、推远异类特征来锐化实例级决策边界；2）利用RGB预训练教师模型指导学生网络的多层特征金字塔，注入跨模态语义先验，增强热红外特征表示。

Result: 实验表明该方法优于先前方法，实现了热红外检测的最先进性能。

Conclusion: 所提训练策略有效改善了热红外检测中的特征表示与分类判别能力，无需测试时引入可见光输入或多传感器融合，兼具高效性与强鲁棒性。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [154] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出一种模型无关的序数元学习算法（MAOML），用于在零样本和少样本设置下提升开源视觉语言模型在水果新鲜度分类任务中的性能，解决了数据稀缺和标签序数性问题，达到行业标准92.71%的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于专家标注成本高，水果新鲜度数据稀缺，且闭源视觉语言模型存在数据隐私问题，现有开源模型性能不足，难以满足食品零售行业需求。

Method: 提出模型无关的序数元学习（MAOML）算法，结合元学习应对数据稀疏性，并利用标签的序数关系来提升小规模开源视觉语言模型的性能。

Result: 在零样本和少样本设置下，MAOML实现了最先进的水果新鲜度分类性能，平均准确率达到92.71%，满足工业标准。

Conclusion: MAOML有效提升了开源视觉语言模型在水果新鲜度预测任务中的表现，解决了数据稀缺和隐私问题，具有实际应用价值。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [155] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 提出GT-Pair自动构建高质量偏好对，并引入Reg-DPO结合SFT损失提升训练稳定性与生成质量，结合FSDP和内存优化技术显著提升训练容量，在I2V和T2V任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法多沿用图像领域范式且局限于小规模模型，难以应对视频生成中的数据构建成本高、训练不稳定和显存消耗大等挑战。

Method: 提出GT-Pair方法，利用真实视频作为正样本、模型生成视频作为负样本自动构建偏好对；引入Reg-DPO，在DPO目标中加入SFT损失作为正则项以提升训练稳定性和生成保真度；结合FSDP框架与多种内存优化技术提升训练效率。

Result: 在多个I2V和T2V任务及数据集上实验表明，该方法显著优于现有方法，生成视频质量更高，训练容量达到仅用FSDP的近三倍。

Conclusion: 所提方法有效解决了视频生成中偏好学习的数据、稳定性和效率问题，为大规模视频生成模型的优化提供了可行方案。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [156] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 本文提出了一种名为QA-SNNE的黑箱不确定性估计方法，通过结合问题语义与生成答案的语义熵来提升手术场景中视觉问答（VQA）系统的安全性。该方法在多种模型上验证了其在检测幻觉和提高AUROC方面的有效性，尤其在零样本大模型中表现突出，为实现自动故障检测（AFD）提供了可解释且实用的路径。


<details>
  <summary>Details</summary>
Motivation: 现有手术VQA研究多关注准确性和语言质量，忽视了对安全行为（如模糊性感知、转诊专家或触发第二意见）的支持。本文旨在通过不确定性估计提升系统安全性，防止因错误或模糊回答导致的患者伤害。

Method: 提出Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE)，通过在医学文本嵌入空间中比较生成答案与其最近邻的语义相似度，并结合问题上下文计算语义熵，作为不确定性估计指标。该方法不依赖模型内部结构，适用于黑箱模型。

Result: 在EndoVis18-VQA和PitVQA数据集上评估了五种模型（包括PEFT和零样本LVLM）。结果表明：PEFT模型对轻微改写敏感，而LVLM更具鲁棒性；QA-SNNE在多数模板内设置下提升了AUROC，零样本模型提升达15-38%，且在模板外压力测试下效果保持稳定；同时增强了幻觉检测能力。

Conclusion: QA-SNNE通过将语义不确定性与问题上下文关联，为手术VQA中的自动故障检测提供了实用且可解释的解决方案。结合LVLM骨干与问题对齐的不确定性估计，有助于提升系统安全性和临床医生信任度。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [157] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 提出了一种新的后训练量化框架，通过将激活和权重的量化误差建模为独立高斯噪声，主动预处理模型以获得平坦的全精度网络，从而提升视觉Transformer的低比特量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法忽略了全精度模型与量化模型之间的关系，导致量化误差较大，且缺乏针对预定义低比特精度进行优化的模型无关训练方法。

Method: 将激活量化误差（AQE）和权重量化误差（WQE）建模为独立高斯噪声，并通过噪声注入优化方法寻找平坦极小值，以提升量化鲁棒性。

Result: 实验结果表明所提方法在低比特量化下显著降低了量化误差，提升了模型性能，验证了方法的有效性。

Conclusion: 通过显式建模和解耦量化误差源，获得平坦的全精度网络是实现高性能低比特PTQ的关键，为ViT的高效量化提供了新思路。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [158] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SecDiff的扩散辅助解码框架，用于增强深度联合信源信道编码（JSCC）在对抗性无线环境下的安全性和鲁棒性。SecDiff通过伪逆引导采样和自适应引导加权实现高效语义重建，并结合功率子载波掩蔽策略与EM驱动的信道估计方法，有效应对干扰和导频欺骗攻击。实验表明，该方法在OFDM信道下优于现有安全和生成式JSCC基线。


<details>
  <summary>Details</summary>
Motivation: 现有JSCC框架在面对物理层对抗威胁（如导频欺骗和子载波干扰）时容易受损，影响语义保真度，因此需要更安全、鲁棒的解决方案。

Method: 提出SecDiff框架：采用伪逆引导采样和自适应引导加权以降低推理延迟；针对干扰攻击，使用基于功率的子载波掩蔽并将恢复问题转化为掩码修复问题；针对导频欺骗，将信道估计建模为盲逆问题，并设计EM驱动算法，结合重构损失和信道操作符进行联合优化；在扩散过程中交替执行导频恢复与信道估计。

Result: 在OFDM信道的对抗环境下，SecDiff在图像重建质量、计算成本和安全性方面均优于现有的安全JSCC和生成式JSCC方法，实现了重建质量与计算开销的良好平衡。

Conclusion: SecDiff为实现实际可用的低延迟、抗攻击语义通信提供了可行路径，推动了安全语义通信的发展。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [159] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: 本文提出了一种用于物联网环境下行人重识别的增强型行人对齐网络（EPAN），该网络采用双分支架构以应对不同视角和环境变化，实现了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在复杂多变的物联网监控环境中实现鲁棒的行人重识别，克服视角和环境变化带来的影响。

Method: 提出EPAN模型，采用双分支架构提取多尺度和多视角下的对齐特征，增强特征表达能力。

Result: 在Inspection-Personnel数据集上达到90.09%的Rank-1准确率和78.82%的mAP，表现出优越的性能。

Conclusion: EPAN在真实世界物联网应用中具有潜力，可有效支持跨摄像头的可靠行人重识别。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [160] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出了一种名为Luminance-Aware Statistical Quantification (LASQ)的低光照图像增强框架，将亮度转换建模为强度坐标空间中的幂律分布，通过分层抽样和扩散过程实现无监督的亮度分布模拟，在有无参考图像的情况下均表现出优异的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有低光增强方法多依赖成对数据进行确定性像素映射，忽略了真实环境中光照变化的连续物理过程，导致在缺乏正常光照参考时泛化性能下降。

Method: 提出LASQ框架，将亮度增强视为在分层亮度分布上的统计采样过程；利用幂函数逼近强度空间中的幂律分布，并设计扩散前向过程来自适应学习亮度层间的最优转换路径，实现无需正常光照参考的无监督分布模拟。

Result: 在有参考图像的特定领域数据集上表现优越，同时在无参考的跨场景数据集上展现出更强的泛化能力，显著提升了实际应用中的低光图像恢复效果。

Conclusion: LASQ通过统计建模和无监督学习重新定义了低光增强问题，有效平衡了重建保真度与跨场景泛化能力，为光照恢复提供了新的思路。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [161] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的新型框架NSYNC，通过生成负向合成数据来提升大模型在文本到图像生成中的风格化能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法难以准确捕捉特定艺术风格，仅靠微调效果有限。

Method: 利用合成数据生成负样本，在对比训练中使用正负样本梯度，通过去除正梯度在负梯度上的投影分量来更新模型参数，从而突出独特风格特征。

Result: 在多种画家和插画师风格上的实验表明，该方法在定量和定性指标上均优于基线方法。

Conclusion: NSYNC有效提升了文本到图像扩散模型的风格化生成能力，验证了利用负向合成数据进行对比训练的有效性。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [162] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 提出一种结构化的五层模型来改进罕见驾驶场景的评估与生成，利用基础大模型和数据增强策略生成新场景，并引入多样性与原创性评分评估合成数据集的相关性。


<details>
  <summary>Details</summary>
Motivation: 罕见且具有挑战性的驾驶场景对自动驾驶开发至关重要，但由于现实中难以遇到，需要通过生成模型或仿真来创建这些场景。

Method: 设计了一个包含子类和特征描述的五层结构化模型，结合大型基础模型进行场景生成；使用特定嵌入表示比较场景，并提出多样性分数和原创性分数两种评估指标。

Result: 在不同生成设置下验证了所提评估指标的有效性，并对从结构化描述生成的合成视频进行了定性评估，结果表明该方法能有效生成多样且贴近真实的罕见驾驶场景。

Conclusion: 所提出的五层结构化模型有助于提升罕见驾驶场景的生成与评估能力，为自动驾驶系统的测试提供了更可靠的数据增强方案。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [163] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的PCD-ReID算法，用于解决基站环境中遮挡行人重识别的问题，在真实巡逻监控数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统ResNet-based方法在处理遮挡问题时效果不佳，难以有效识别关键身体特征被遮挡的行人，因此需要更鲁棒的ReID方法。

Method: 设计了基于Transformer的PCD网络，提取如头盔、制服等共享部件特征，并收集了包含10,000人、50,000多张图像的真实巡逻监控数据集用于训练。

Result: 在实验中，该方法达到79.0%的mAP和82.7%的Rank-1准确率，相比ResNet50方法Rank-1提升15.9%。

Conclusion: PCD-ReID能有效应对塔检场景中的遮挡问题，具备在安防监控中实际部署的潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [164] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: 本文介绍了Napari Organoid Analyzer (NOA)，一个用于简化基于AI的类器官图像分析的通用图形用户界面，支持检测、分割、追踪、特征提取与预测等功能，并通过三个案例展示了其在形态学分析、光毒性评估和活力预测中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的类器官图像分析AI工具大多局限于特定任务且难以被无编程背景的生物学家使用，因此需要一个更通用、易用且可扩展的工具来降低使用门槛并提升分析效率。

Method: 开发了一个名为NOA的开源napari插件，集成了多种先进的AI算法模块，包括检测、分割、追踪、特征提取、自定义标注和机器学习预测功能，提供图形化界面以支持非编程用户。

Result: 通过三个实际案例验证了NOA的多功能性和适用性：能够量化类器官分化过程中的形态变化、评估光毒性影响，并预测类器官的存活率和分化状态。

Conclusion: NOA提供了一个易于访问、灵活且可扩展的平台，使生物学家能够全面利用AI进行类器官图像分析，显著提升了分析效率与自动化水平。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [165] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的DC-GAN方法生成合成MRI数据，并利用CNN分类器验证合成图像的质量和实用性，结果表明其在脑肿瘤分类任务中与真实数据具有相当的性能。


<details>
  <summary>Details</summary>
Motivation: 由于真实的MRI数据有限，生成逼真的医学图像是一个困难且具有挑战性的任务，因此需要合成数据来缓解数据不足的问题。

Method: 采用深度卷积生成对抗网络（DC-GAN）生成合成MRI数据，并使用卷积神经网络（CNN）分类器对真实和合成的MRI图像进行脑肿瘤分类，以评估合成图像的质量和效用。

Result: CNN分类器在真实和合成MRI图像上的分类性能相当，证明了GAN生成的图像具有较高的质量和可用于下游任务的能力。

Conclusion: 该研究表明，基于DC-GAN生成的合成MRI数据是有效的，能够在脑肿瘤分类等任务中替代或补充真实数据，缓解医学图像数据稀缺的问题。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [166] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 提出了一种连续-离散双视觉标记器（CDD-VT），通过自适应选择图像基元数量来统一多模态大模型中的理解与生成，兼顾连续与离散标记化的优点。


<details>
  <summary>Details</summary>
Motivation: 解决现有连续和离散视觉标记化方法在多模态大模型中理解与生成任务上的割裂问题，克服连续方法的工程复杂性和离散方法的信息损失。

Method: 受光的波粒二象性启发，设计CDD-VT，包含两个核心组件：鼓励基元正交性的多样量化基元和根据样本复杂度动态分配基元数量的动态基元分配器，实现对简单和复杂图像的灵活表征。

Result: 在重建、检索和分类任务上，CDD-VT均优于现有的专用连续和离散标记化方法，展现出更强的性能和可扩展性。

Conclusion: CDD-VT有效缓解了连续与离散视觉标记化之间的矛盾，为统一多模态理解与生成提供了一个简洁且高效的解决方案。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [167] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的Lite ENSAM模型，用于从CT扫描中高效进行肿瘤体积分割，仅使用RECIST标注训练，实现了较好的分割性能和较快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 由于手动进行肿瘤体积标注费时费力，限制了体积测量在临床中的应用，因此需要一种高效、自动化的体积分割方法。

Method: 提出Lite ENSAM，是ENSAM架构的轻量级改进版本，专为从仅含RECIST单径标注的CT图像中实现高效体积肿瘤分割而设计。

Result: 在MICCAI FLARE 2025 Task 1 Subtask 2中，Lite ENSAM在隐藏测试集上达到60.7%的DSC和63.6%的NSD，在公开验证集上平均内存消耗为50.6 GB·s，CPU推理时间平均为14.4秒。

Conclusion: Lite ENSAM在保持较低资源消耗的同时，能够有效利用RECIST标注进行体积肿瘤分割，有助于推动体积评估在临床实践中的应用。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [168] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的视觉基础模型训练框架，支持多种Transformer架构和自监督学习策略，兼顾高效性与跨域适用性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练流程常存在不灵活、领域特定或计算成本高的问题，限制了其在不同场景下的应用。

Method: 基于DINO系列核心思想构建统一的配置驱动系统，集成LoRA、层冻结、知识蒸馏等训练策略，支持DDP和FSDP分布式训练，并兼容Hugging Face生态。

Result: 在多个数据集上实现具有竞争力的性能，显著降低计算成本，同时提供可解释性工具和标签引导的数据增强方法，提升注意力定位能力。

Conclusion: DINO-MX为自监督视觉模型的开发、适配与基准测试提供了可复现、可扩展且高效的框架，适用于多种研究与实际应用场景。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [169] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 本文提出了一种用于解剖学3D形状分类的自监督图自编码方法，引入了预计算结构池化（PSPooling）以实现高效且保持结构的图粗化，并构建了标准化基准数据集MedShapeNet19，显著提升了低标签情况下的重建精度和分类性能。


<details>
  <summary>Details</summary>
Motivation: 由于3D网格数据的复杂性和缺乏标准化基准，解剖学3D形状分类进展受限，亟需鲁棒的学习方法和可复现的评估体系。

Method: 提出PSPooling——一种基于几何邻近性预计算节点对应关系的非学习型网格池化算子，集成到自监督图自编码器中，从无标签表面网格学习解剖感知表示，并在新构建的MedShapeNet19数据集上进行评估。

Result: PSPooling在重建保真度和分类准确率方面显著优于现有方法，尤其在低标签场景下表现突出，同时MedShapeNet19为医学3D形状分析提供了标准化基准。

Conclusion: PSPooling为高分辨率医学网格分析提供了一种高效、可逆且结构保持的池化方案，结合MedShapeNet19基准推动了解剖学形状分类的标准化与进一步发展。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [170] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 本文研究了如何将强化学习（RL）有效集成到基于扩散的图像恢复模型中，提出了一种基于图像质量评估（IQA）模型的奖励机制，并结合监督微调（SFT）动态调整策略，显著提升了多种图像恢复任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法直接应用于扩散型图像恢复模型效果不佳，因为图像恢复任务更强调保真度，与纯生成任务目标不同，需要专门设计的RL策略。

Method: 采用基于图像质量评估（IQA）模型的奖励函数，针对远离真实数据的难样本进行强化学习，并通过自动加权策略动态结合SFT，在训练过程中实现细粒度对齐。

Result: 在多个基准上的实验表明，该方法在多种图像恢复任务中均显著优于现有方法，具有良好的通用性和即插即用特性。

Conclusion: 所提出的框架能有效提升扩散模型在图像恢复任务中的表现，通过自适应融合RL与SFT，并利用MLLM-based IQA模型指导优化方向，为后续研究提供了可行路径。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [171] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: 提出UniLumos，一种结合RGB空间几何反馈与流匹配的统一图像和视频重打光框架，通过深度和法线图监督提升物理合理性，并引入路径一致性学习实现高效训练，配合六维标注协议和LumosBench评估基准，实现高质量、可控且快速的重打光。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的重打光方法在语义潜在空间优化，难以保证视觉空间中的物理正确性，常导致过曝、阴影错位等问题，缺乏精细控制和有效评估机制。

Method: 提出UniLumos框架，引入RGB空间的深度和法线图作为几何反馈，结合流匹配模型；采用路径一致性学习支持少步训练；设计六维光照属性标注协议，并构建基于视觉语言模型的LumosBench基准进行解耦评估。

Result: 在图像和视频重打光任务中实现了最先进的质量，显著提升物理一致性，同时训练和推理速度提升20倍。

Conclusion: UniLumos通过引入视觉空间几何监督和高效训练策略，有效提升了重打光的物理合理性和可控性，兼具高性能与实用性。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [172] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出一种渐进式网络架构，通过解耦结构、颜色和细胞边界生成过程，提升从H&E图像合成IHC图像的质量，在HER2和ER数据集上实现了更优的视觉效果和更精细的结构细节。


<details>
  <summary>Details</summary>
Motivation: 现有染色翻译方法多采用加权损失函数，忽视了各损失项之间的相互依赖，导致生成图像在结构真实性和色彩保真度方面表现不佳，且难以兼顾。

Method: 设计一种渐进式网络架构，分阶段优化结构、颜色和细胞边界；基于ASP框架引入DAB染色浓度和图像梯度损失函数，增强色彩保真度和细胞边界清晰度。

Result: 在HER2和ER数据集上的实验表明，该方法显著提升了生成IHC图像的视觉质量，恢复了更精细的结构细节。

Conclusion: 所提出的渐进式生成机制能有效解耦染色翻译中的关键视觉要素，优于传统端到端单一目标优化方法，为低成本蛋白定位提供了更可靠的计算染色方案。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [173] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出了一种名为LFRD2的混合框架，用于改善屏幕下ToF成像的深度感知质量，结合神经网络与物理建模优势，在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 屏幕下的ToF成像受TOLED层引起的信号衰减、多路径干扰和时域噪声严重影响，导致深度质量下降，需有效方法进行恢复。

Method: 提出Learnable Fractional Reaction-Diffusion Dynamics（LFRD2），采用时间分数阶反应-扩散模块实现迭代深度优化，并通过系数预测和重复微分引入高效的连续卷积算子。

Result: 在四个基准数据集上的实验表明，该方法显著提升了深度恢复质量，优于现有方法。

Conclusion: LFRD2有效解决了屏幕下ToF成像中的深度退化问题，兼具模型可解释性与高性能，具有广泛应用潜力。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [174] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了PRBench，首个专注于评估不同鲁棒性训练方法在概率鲁棒性（PR）上改进效果的基准。通过综合指标比较了常见对抗训练（AT）与PR专用方法，发现AT方法在提升对抗鲁棒性和PR方面更通用，而PR专用方法则具有更低的泛化误差和更高的干净准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管概率鲁棒性（PR）被视为对抗鲁棒性（AR）的实用补充，但针对PR的训练方法仍缺乏系统评估。现有研究存在评估协议不统一、与强基线对比不足及缺乏通用比较框架三大问题。

Method: 构建了PRBench基准，包含7个数据集、10种模型架构共222个训练模型，采用清洁准确率、PR与AR性能、训练效率和泛化误差等全面指标进行实证比较，并对不同方法的PR泛化误差进行了理论分析。

Result: 实验表明：对抗训练（AT）方法在多种超参数设置下对AR和PR均有更好表现，更具通用性；而PR专用训练方法虽在AR上较弱，但保持了更高的清洁准确率和更低的泛化误差。

Conclusion: PRBench为评估PR提供了标准化平台，揭示了AT方法在多方面优于现有PR专用方法，但也突出了后者在模型泛化和干净数据性能上的优势，推动未来更均衡的鲁棒性研究。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [175] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 提出了一种任务探索器流水线，利用聚类、因子分析和字符串编辑距离自动识别完成任务的关键全局和局部策略及子任务，并可编码用户的层级化子任务结构。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器对用户知识、技能和行为的理解，实现人机间的隐式协同。

Method: 开发了任务探索器流水线，结合聚类技术、因子分析和字符串编辑距离，识别任务中的全局策略（通用动作集）和局部策略（相似组合的动作序列），并识别不同长度的有意义子任务。

Result: 该流水线能自动识别关键策略并用层次化子任务结构编码用户操作，同时开发了Task Explorer应用以方便查看结果。

Conclusion: 该方法可推广到任何基于动作的时间序列数据，有助于人和机器更好地理解用户的知识、技能和行为。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [176] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种用于肺炎检测的实时检测Transformer模型CGF-DETR，通过改进骨干网络、特征聚合模块和颈部结构，在RSNA数据集上实现了优于RT-DETR的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的检测器在目标检测中表现良好，但在胸部X光片肺炎检测中的应用仍不足，需要更高效准确的自动化方法。

Method: 提出CGF-DETR模型，引入XFABlock增强多尺度特征提取，SPGA模块提升特征聚合效率，GCFC3模块加强特征表示并保持实时性。

Result: 在RSNA数据集上，CGF-DETR达到82.2% mAP@0.5，比RT-DETR-l高3.7%，推理速度为48.1 FPS；完整模型mAP@[0.5:0.95]为50.4%。

Conclusion: CGF-DETR在不牺牲推理速度的前提下显著提升了肺炎检测性能，各模块均对性能提升有贡献。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [177] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: 本文提出了一种名为HGFreNet的新型GraphFormer架构，用于解决2D到3D人体姿态提升中的深度模糊和时间抖动问题，通过在频域中建模全局时空相关性和轨迹一致性，在Human3.6M和MPI-INF-3DHP数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 2D姿态估计中的误差和深度模糊导致3D轨迹不连贯，且现有方法多局限于时间域的局部平滑，忽略了骨骼关节运动的全局时空相关性。

Method: 提出HGFreNet，包含hop-hybrid图注意力（HGA）模块和Transformer编码器，HGA聚合k-hop邻域形成混合组以扩大感受野，并在频域中约束轨迹一致性以增强时间连贯性，同时使用预网络估计3D姿态辅助深度推断。

Result: 在Human3.6M和MPI-INF-3DHP两个标准数据集上进行了大量实验，结果表明HGFreNet在位置精度和时间一致性方面均优于当前最先进的方法。

Conclusion: HGFreNet通过融合hop-hybrid特征聚合与频域轨迹一致性约束，有效提升了2D-to-3D姿态估计的精度与稳定性，为后续研究提供了新的建模思路。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [178] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: 本文提出Wonder3D++，一种从单视图图像高效生成高保真纹理网格的新方法，结合跨域扩散模型与多视角法线图和彩色图像生成，通过多视角跨域注意力机制保证生成一致性，并采用级联式3D网格提取算法在约3分钟内实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDS的方法存在优化耗时和几何不一致问题，而直接生成3D信息的方法则质量低、细节不足，因此需要兼顾质量、一致性和效率的单视图重建方法。

Method: 提出跨域扩散模型生成多视角法线图和对应彩色图像，引入多视角跨域注意力机制以增强视图与模态间的信息交互，并设计级联式3D网格提取算法实现从粗到精的高效高质量表面重建。

Result: 实验表明，该方法在重建质量、泛化能力和效率方面均优于先前方法，可在约3分钟内完成高质量3D网格生成。

Conclusion: Wonder3D++在单视图到3D重建任务中实现了质量、一致性和效率的平衡，为高效高保真3D内容生成提供了有效解决方案。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [179] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出了一种统一的自动驾驶模型UniLION，基于线性群RNN算子，高效处理大规模LiDAR点云、高分辨率多视角图像和时序数据，无需显式融合模块即可支持多种任务配置，并在3D感知、预测和规划等任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer的二次注意力机制在处理长序列数据时计算开销大，现有模型难以高效整合多模态和时序信息，因此需要一种更高效的统一架构来简化自动驾驶系统设计。

Method: 采用线性群RNN算子对分组特征进行处理，构建单一通用架构UniLION，支持LiDAR-only、多模态、时序等多种配置，无需专门的融合模块。

Result: UniLION在3D目标检测、跟踪、占据预测、BEV地图分割、运动预测和端到端规划等多个核心任务上达到具有竞争力甚至最先进的性能。

Conclusion: UniLION提供了一种简洁高效的统一范式，能够自然地支持多模态与多任务自动驾驶系统，在保持高性能的同时简化了模型设计，为自动驾驶中的3D基础模型发展提供了新视角。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [180] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: 本文提出了SurgVeo，首个针对手术视频生成模型的专家策划基准，以及外科合理性金字塔（SPP），用于评估生成视频从外观到手术策略的多层级合理性。通过对Veo-3模型的零样本测试，发现其虽在视觉上逼真，但在器械操作、环境反馈和手术意图等高阶层面存在“合理性鸿沟”。研究首次量化揭示了视觉模仿与因果理解之间的差距，为医疗AI的发展提供了基础与路线图。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成大模型虽能模拟物理世界，但在需要深度专业因果知识的高风险领域（如外科手术）中缺乏评估标准和系统验证，亟需专门的评估框架来检验其在专业场景中的合理性与可靠性。

Method: 提出SurgVeo基准和四层SPP评估框架，基于腹腔镜和神经外科手术视频，让先进模型Veo-3进行零样本生成，并由四位认证外科医生依据SPP对生成结果进行多层级评分。

Result: Veo-3在视觉感知层面表现优异，但在器械操作、环境反馈和手术意图等高阶合理性上显著失败，暴露出明显的“合理性鸿沟”，表明当前模型仅能模仿表层视觉模式而缺乏深层因果理解。

Conclusion: 视觉逼真性不等于外科合理性，当前视频生成模型在高风险专业领域应用仍面临根本性挑战；SurgVeo和SPP为未来具备因果理解能力的医疗AI模型提供了关键评估工具与发展路径。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [181] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 本文提出了一种基于提示的GraphRAG框架，强调提示设计在实体提取、事实选择和多跳问答中的重要性，通过构建符号知识图谱并结合LLM与Personalized PageRank实现高效检索与推理，在HotpotQA和2WikiMultiHopQA上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）已广泛用于增强大语言模型，但提示设计对图结构检索和复杂推理的影响尚未被充分研究。

Method: 提出Prompt-driven GraphRAG：从文本中提取实体和关系构建知识图谱（三元组），利用LLM进行语义过滤与答案生成，并采用基于实体引导的Personalized PageRank进行图遍历以实现高效检索。

Result: 在HotpotQA和2WikiMultiHopQA数据集上取得SOTA结果：F1分别为80.7%和78.9%，Recall@5分别为97.1%和98.1%。

Conclusion: 提示设计在提升检索准确性和回答质量方面起关键作用，所提方法为更高效、可解释的多跳问答系统奠定了基础，突出了提示感知的图推理的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [182] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: 本文提出了Scitextures数据集，包含来自科学、技术与艺术领域的1200多个模型和10万张纹理图像，旨在探索视觉模式与其生成机制之间的联系，并评估AI模型理解与重现这些模式的能力。


<details>
  <summary>Details</summary>
Motivation: 希望通过连接视觉模式与其形成机制，提升AI对物理系统的深层视觉理解能力。

Method: 构建了一个由AI驱动的自动化管道来收集并标准化实现各类科学模型，生成Scitextures数据集，并设计基准测试评估VLMs在识别、建模和重构视觉模式机制方面的能力。

Result: 实验表明，现有的视觉-语言模型能够理解并模拟产生视觉模式的物理系统，且能根据真实图像推断机制并生成相似的模拟图像。

Conclusion: Scitextures为研究视觉模式与生成机制的关系提供了有力工具，推动AI实现更深层次的视觉理解。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [183] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了TIR-Bench，一个包含13个多样化任务的基准，用于评估多模态大模型在链式思维中使用工具进行图像处理和操作的智能体式视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如Visual Search）仅测试基本图像操作，无法充分评估复杂、动态且依赖工具的视觉推理能力。

Method: 构建了一个涵盖13个需要新颖工具使用的任务的综合性基准TIR-Bench，并评估了22个多模态大语言模型（MLLMs），包括开源、闭源及具备显式工具使用增强的模型。

Result: 实验结果表明TIR-Bench具有普遍挑战性，只有具备真正‘以图像为媒介思考’能力的模型才能表现良好。此外，初步研究比较了直接微调与智能体式微调的效果。

Conclusion: TIR-Bench能有效评估高级视觉推理中工具使用的复杂性，推动多模态模型向更智能的‘思考-与-图像’方向发展。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [184] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了PlotCraft，一个包含1000个复杂可视化任务的新基准，并发布了SynthVis-30K数据集和PlotCraftor模型，显著提升了大语言模型在复杂数据可视化生成上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成方面表现出色，但在复杂、结构化数据的可视化生成能力上尚未充分评估和开发，缺乏系统性基准和高质量训练数据。

Method: 构建了PlotCraft基准，涵盖7类高层可视化任务和48种图表类型，支持单轮生成与多轮迭代评估；通过协作代理框架合成大规模数据集SynthVis-30K；基于该数据训练轻量级模型PlotCraftor。

Result: 在PlotCraft、VisEval和PandasPlotBench等多个基准上，PlotCraftor表现媲美领先的闭源模型，在困难任务上性能提升超过50%。

Conclusion: PlotCraft填补了复杂数据可视化评估的空白，SynthVis-30K和PlotCraftor为高效、高质量的可视化代码生成提供了有效解决方案。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [185] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于原型理论的MBTI人格识别框架ProtoMBTI，通过LLM引导的数据增强和轻量级模型微调，结合检索-复用-修正-保留机制，提升了文本中人格识别的准确性、可解释性和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的人格识别方法通常采用硬标签分类，忽略了人类人格判断的渐变性和原型特征，因此需要一种更符合认知机制的建模方式。

Method: 提出ProtoMBTI框架：首先利用LLM引导的多维数据增强构建高质量语料库；然后使用LoRA微调轻量级编码器以学习判别性嵌入并建立标准化人格原型库；推理时通过检索top-k原型，进行提示投票、不一致修正，并在预测正确时保留新样本以持续扩充原型库。

Result: 在Kaggle和Pandora基准上，ProtoMBTI在四个MBTI维度和16种人格类型分类任务中均优于基线模型，且表现出良好的跨数据集泛化能力。

Conclusion: 将推理过程与心理原型推理对齐，能有效提升基于文本的人格建模在准确性、可解释性和迁移性方面的表现。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [186] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 提出残差流解码器框架，用于探测语言模型中段落和文档级别的规划信息，能在小模型中解码出相当于5个以上未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在理解语言模型激活时局限于特定概念或token，难以捕捉长时程任务中的规划表示。

Method: 开发残差流解码器框架，通过探测模型激活来提取段落和文档尺度的计划信息。

Result: 在小模型中可解码出相当于5个以上未来token的上下文信息。

Conclusion: 该方法为监控语言模型和理解其长期规划信息编码机制提供了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [187] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出了一种通过选择信息丰富的目标词来优化大语言模型训练的新方法，相较于传统的下一个词预测，能更有效地提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型训练使用下一个词预测（NTP），效率较低，难以在保持计算成本的同时显著提升性能。本文旨在探索更优的目标词选择策略以提高训练效率和模型表现。

Method: 提出一种新的训练范式，优先预测信息量更大的目标词，而非简单的下一个词，并在算术、文本多标签分类和自然语言生成三类任务中验证其有效性。

Result: 该方法在多个任务上提升了模型性能，同时保持了计算成本的可控性，验证了信息丰富目标词选择的有效性。

Conclusion: 通过有原则地选择目标词进行训练，可以显著优化大语言模型的训练过程，为提升模型性能和理解训练机制提供了新方向。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [188] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 提出了一种评估和改进大语言模型在对话中角色一致性的统一框架，通过三种自动指标和多轮强化学习，显著减少了不一致性。


<details>
  <summary>Details</summary>
Motivation: 现成的大语言模型在模拟人类用户时容易偏离指定角色，导致行为不一致，影响其在治疗、教育等交互场景中的应用效果。

Method: 定义了三种自动评估指标（提示到语句、语句到语句、问答一致性），并使用这些指标作为奖励信号，通过多轮强化学习对大语言模型进行微调。

Result: 在患者、学生和社会聊天伙伴三个角色上，模型的不一致性减少了超过55%，生成的对话更加连贯和忠实于角色。

Conclusion: 该方法有效提升了大语言模型在模拟人类用户时的角色一致性，增强了其在交互式应用中的可靠性和实用性。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [189] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB 是一个基于浏览器的网络安全桌面演练系统，结合大语言模型和检索增强型辅助工具，提供可扩展、轻量化的训练体验。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全桌面演练（TTXs）通常脚本化、资源消耗大且难以扩展，因此需要一种更灵活、低成本的替代方案。

Method: 设计并实现 AgentBnB 系统，利用大语言模型作为队友，并通过 Bloom 分类对齐的检索增强型 copilot（C2D2）提供认知导向的提示；采用渐进式褪去的脚手架提示工程策略。

Result: 在四名研究生的单人试点中，参与者更倾向于使用基于代理的版本，认为其更具可扩展性，但在简单知识测验中出现天花板效应。

Conclusion: 大语言模型增强的 TTX 可提供轻量、可重复的训练，减轻传统演练的后勤负担，未来计划支持多人模式、数据驱动辅导及更大规模的对比研究。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [190] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了IL-PCR，一个用于印度法律先例和法规检索的统一语料库，支持同时开发两种相关检索任务的模型，并通过基于LLM的重排序方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将法规检索和先例检索视为独立任务，忽略了二者之间的内在关联（如相似案件常引用相似法规），本文旨在填补这一空白。

Method: 构建了一个名为IL-PCR的新型语料库，支持联合评估法规与先例检索；实验比较了词法模型、语义模型和基于图神经网络的集成模型，并提出一种基于大语言模型（LLM）的重排序方法以利用两任务间的依赖关系。

Result: 基于LLM的重排序方法在两个检索任务上均取得最佳性能，验证了利用任务间依赖关系的有效性。

Conclusion: IL-PCR为联合研究法律文本检索提供了新基础，表明整合法规与先例检索能提升整体效果，未来可推动多任务法律AI系统的发展。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [191] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH-SLT预训练方法，利用模板生成句子对，在低资源手语翻译中显著提升Transformer模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于大规模句对齐手语数据集稀缺，手语翻译任务面临挑战，现有方法多集中于特征提取和模型架构改进。

Method: 提出POSESTITCH-SLT，一种受语言模板句子生成启发的预训练方案，结合模板生成的合成句对进行训练，使用简单的Transformer编码器-解码器架构。

Result: 在How2Sign和iSign两个数据集上，BLEU-4分数分别从1.97提升至4.56，0.55提升至3.43，超越现有基于姿态的无gloss手语翻译方法。

Conclusion: 模板驱动的合成监督在低资源手语翻译设置中具有显著有效性，可有效提升翻译性能。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [192] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出了一种名为Factorization Memory的高效RNN架构，在短上下文任务中性能媲美Transformer，且在长上下文场景中表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 设计一种兼具高效训练、低推理复杂度和强长上下文建模能力的RNN架构。

Method: 基于Mamba-2构建Factorization Memory，引入稀疏化版本以提升效率和表征能力，仅在每一步更新部分循环状态。

Result: 模型在训练时可利用并行计算，推理时保持常数级计算与内存复杂度，并在短和长上下文语言建模任务中均表现优异。

Conclusion: 这是首个将稀疏记忆激活与跨长短上下文的竞争力性能成功结合的RNN架构。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [193] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文提出了因果语言模型的反向不变性问题，指出标准的预训练目标在处理文本方向上存在局限性，并呼吁未来研究关注显式建模语言时间不对称性的方法。


<details>
  <summary>Details</summary>
Motivation: 由于自然语言具有时间上的不对称性（如音韵、形态或因果依赖），而当前的因果语言模型预训练目标对文本反转具有不变性，可能导致无法捕捉这些方向性依赖。

Method: 通过形式化定义因果语言模型目标中的反向不变性，分析其在前向与反向文本上损失函数的一致性，并探讨该性质对模型学习方向性依赖的影响。

Result: 证明了标准的下一词预测损失在原始语料和其反转版本上具有相同的似然，解释了为何反向训练模型仍能取得相近性能，揭示了现有预训练目标的方向盲性。

Conclusion: 反向不变性是当前预训练目标的一个限制，未来应设计能够显式建模语言时间箭头的损失函数和架构，以更好捕捉语言中的方向性依赖。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [194] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: 本文介绍了LingGym，一个基于18种类型多样的参考语法书中提取的逐字对照文本（IGT）和语法描述的新基准，用于评估大语言模型在低资源语言和未见结构中的元语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于特定下游任务，缺乏对大语言模型在跨语言、跨结构的元语言推理泛化能力的系统评估。因此，本文旨在填补这一空白，探索模型在低资源语言环境下的语言推断能力。

Method: 提出Word-Gloss Inference任务，在该任务中模型需根据上下文及不同层次的语言信息（如词性标注、语法解释、翻译）推断缺失的词语及其词法标注。通过控制变量方式评估多种语言线索对推理性能的影响。

Result: 实验结果表明，引入结构化的语言线索能持续提升所有模型的推理表现，且模型在跨语言和未见结构上的泛化能力有限，仍存在较大改进空间。

Conclusion: LingGym为评估大语言模型的元语言推理能力提供了新基准，揭示了当前模型在低资源语言分析与记录中的潜力与局限。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [195] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文提出了“推理轨迹生成”任务，旨在通过苏格拉底式调试帮助学生发现并纠正由编程误解引起的错误。作者构建了带有推理轨迹标注的数据集，并基于大语言模型生成合理的推理路径和对话，评估显示前沿模型能生成91%正确的推理轨迹和98.7%有效的对话轮次。


<details>
  <summary>Details</summary>
Motivation: 由于初学者的多数编程错误源于对概念的误解，直接提供修复方案不利于根本性学习，因此需要一种引导式方法帮助学生自我发现和修正错误认知。

Method: 提出推理轨迹（RT）的概念，将苏格拉底式调试形式化为通向与错误信念相矛盾的程序行为陈述的引导过程；构建带标注的调试问题数据集，并利用大语言模型生成RT及基于RT的苏格拉底对话。

Result: 大规模的LLM-as-judge评估表明，前沿语言模型能够生成最多91%正确推理轨迹和98.7%有效对话轮次。

Conclusion: 推理轨迹生成是实现苏格拉底式调试的有效框架，基于大语言模型的方法能高度准确地生成引导性对话，有助于学生识别和修正编程误解。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [196] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 本文研究了迭代改写文本如何逃避AI生成文本检测器，并提出了PADBen基准来系统评估检测器对两类改写攻击（作者身份混淆和抄袭规避）的鲁棒性，发现现有检测方法在中间“清洗”区域表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的AI生成文本检测器在直接LLM输出上表现良好，但在面对经过迭代改写的文本时效果显著下降，因此需要探究其背后机制并提升检测器的鲁棒性。

Method: 通过内在机制分析，提出“中间清洗区域”的概念，并构建包含五类文本的PADBen基准，设计五个渐进式检测任务，评估11种最先进检测器的表现。

Result: 实验显示当前检测器能有效识别抄袭规避，但无法应对作者身份混淆，暴露出在语义位移与生成模式共存区域的检测盲区。

Conclusion: 现有AI生成文本检测方法难以应对迭代改写带来的语义位移与生成痕迹并存的挑战，需从根本上改进检测架构。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [197] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 本文介绍了MedRECT，首个跨语言医学错误处理基准，涵盖日语和英语，用于评估大语言模型在检测、定位和纠正临床文本错误方面的能力。研究发现推理模型表现更优，且经过LoRA微调后性能进一步提升，甚至超过人类专家。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗应用中潜力巨大，但其在非英语语境下检测和纠正临床文本错误的能力尚未充分评估。为确保安全部署，亟需一个跨语言的评估基准。

Method: 提出MedRECT基准，包含日语和英语数据集，通过自动化流程从日本医师执照考试及对应的英文数据构建。将医学错误处理分解为错误检测、定位（句子提取）和纠正三个子任务，并评估了9种主流大语言模型，包括专有、开源和推理类模型，同时采用LoRA进行微调。

Result: （1）推理模型显著优于标准架构，在错误检测上最高提升13.5%，在句子提取上提升达51.0%；（2）跨语言评估显示从英语到日语存在5-10%的性能差距，推理模型差距较小；（3）LoRA微调在错误纠正上带来不对称提升（日语+0.078，英语+0.168），且保持推理能力；（4）微调后的模型在结构化医学错误纠正任务上超过人类专家。

Conclusion: MedRECT是首个全面的跨语言医学错误纠正基准，提供了可复现的框架和资源，有助于推动多语言环境下更安全的医学大语言模型的发展。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [198] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出了一种无需训练的即插即用方法G2，通过双引导机制提升大语言模型输出多样性，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多次生成中输出内容高度相似，限制了其在需要多样性的任务中的应用。

Method: 提出Guide-to-Generation（G2）方法，利用基础生成器和两个引导模块，通过解码干预引导生成过程，提升输出多样性。

Result: 实验表明，G2在多种任务上有效提升了生成多样性，同时保持了生成质量。

Conclusion: G2是一种有效且无需训练的方法，能够在不牺牲质量的前提下显著增强大语言模型的输出多样性。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [199] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）记忆化对合作作者网络的影响，分析了DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三个主流模型在不同学科和地区的表现，发现普遍存在偏向高被引学者的偏差，但在临床医学和非洲部分地区呈现更均衡的表征。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于学术搜索与推荐，其记忆化可能导致合作网络中的公平性与偏见问题，影响科学计量的完整性，亟需系统评估其影响。

Method: 通过分析三个主流LLM在不同学术领域和地理区域生成的合作作者网络，评估模型记忆化对产出结果的影响，比较其在学科和区域层面的差异。

Result: 全球范围内LLM普遍偏向高被引研究人员，但临床医学和部分非洲地区表现出更平衡的代表性，表明训练数据在这些领域可能更具公平性。

Conclusion: LLM的记忆化可能放大科研社区中的现有偏见，但也存在实现更公平表征的机会，需优化训练数据以提升学术发现系统的公正性。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [200] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文介绍了首个大规模比哈尔语-印地语-英语平行语料库（BHEPC），包含11万句精心整理的句子，填补了印度部落语言机器翻译资源的空白，并基于该语料库建立了比哈尔语机器翻译基准，评估了多种多语言大模型的表现，验证了其在低资源场景下的潜力。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性高，但如比哈尔语等少数民族语言缺乏高质量语言资源，导致机器翻译研究受限，亟需构建相关语料库以推动低资源语言技术发展。

Method: 通过专业人工翻译构建BHEPC三语平行语料库，覆盖教育、行政和新闻等领域；在此基础上，评估多种开源与闭源多语言大模型在英/印地语与比哈尔语之间的双向翻译性能，并探究基于上下文学习的生成翻译能力及跨领域泛化表现。

Result: 微调后的NLLB-200蒸馏版6亿参数模型表现最优；实验证明多语言大模型在低资源条件下具有较强翻译能力，且语料库具备良好的跨领域泛化性与较低分布差异。

Conclusion: BHEPC填补了比哈尔语语言资源的空白，为低资源语言机器翻译提供了重要基准，展示了多语言大模型在边缘语言处理中的应用前景，有助于推动包容性自然语言处理技术的发展。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [201] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，通过端到端的架构搜索优化检索增强生成（RAG）系统，在多个领域中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统的质量依赖于多个组件的复杂交互，单独优化各模块效果有限且脆弱，因此需要一种整体性的优化方法。

Method: 提出RAGSmith框架，将RAG设计视为在九个技术类别和46,080种可行配置中的架构搜索问题，采用遗传算法联合优化检索与生成指标（如recall@k、LLM-Judge等）。

Result: 在六个维基百科衍生领域中，RAGSmith平均提升3.8%（范围+1.2%至+6.9%），最高检索提升12.5%，生成提升7.5%；搜索仅探索约0.2%的配置空间即发现稳健结构：向量检索加生成后反思/修订，并根据领域调整其他模块；段落压缩未被选中。

Conclusion: RAGSmith展示了进化搜索在全流水线优化中的有效性，提供了实用且领域感知的RAG系统构建指导，尤其在事实性和长答案问题上增益更大。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [202] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次研究了数据集规模对差分隐私文本重写机制在隐私-效用权衡中的影响，通过大规模动态数据集实验发现数据集大小显著影响评估结果，呼吁更严格的评估标准，并为大规模DP NLP的实际应用提供启示。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私自然语言处理（DP NLP）研究在评估文本重写机制时往往忽略数据集规模的影响，缺乏对不同数据规模下隐私与效用表现的系统分析。

Method: 设计了针对大规模、动态划分数据集的隐私与效用测试方法，在最多达百万级文本的数据集上评估不同规模对DP文本重写机制性能的影响。

Result: 实验证明数据集规模在DP文本重写机制的评估中起关键作用，较大数据集能更好平衡隐私保护与效用保持。

Conclusion: 数据集规模是评估DP NLP方法不可忽视的因素，未来应采用更严谨的评估流程，并考虑规模效应以推动DP NLP在实际场景中的规模化应用。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [203] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出了一种基于文档层次结构的树形MapReduce框架ToM，用于提升大模型在长文本推理中的逻辑连贯性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RAG和分治框架在处理长文本时因忽略文档层次结构或孤立处理片段而导致逻辑不连贯和长距离依赖缺失。

Method: 通过层次语义解析构建DocTree，在树结构上采用自底向上的Tree MapReduce进行递归推理：Map阶段在子节点生成推理链，Reduce阶段在父节点聚合兄弟节点的推理结果以解决冲突或达成共识。

Result: 在70B以上的大语言模型上实验表明，ToM显著优于现有的分治框架和RAG方法，在逻辑连贯性和长上下文推理能力方面表现更优。

Conclusion: ToM通过利用文档的内在层次结构实现了更有效的长文本推理，为突破大模型上下文长度限制提供了新思路。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [204] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 本文研究了在基于图的检索增强生成（RAG）架构中使用大语言模型（LLM）进行能源效率（EE）问答的潜力。系统首先从能源领域的指导和监管文件中自动提取知识图谱（KG），然后通过导航和推理该图谱，以多种语言为用户提供准确答案。通过RAGAs框架、包含101个问答对的验证数据集以及领域专家进行人工验证。结果表明，该架构具有较大潜力：系统在约四分之三的情况下能正确回答（准确率为75.2±2.7%），对于更通用的EE问题表现更佳（最高达81.0±4.1%），且具备良好的多语言能力（翻译导致的准确率损失仅为4.4%）。


<details>
  <summary>Details</summary>
Motivation: 能源效率领域的指导和法规文件繁杂且多语言，传统问答系统难以高效提取和利用其中的知识。因此，需要一种能够自动构建结构化知识并支持多语言查询的智能问答系统，以提升信息获取的准确性与可访问性。

Method: 采用基于图的检索增强生成（RAG）架构，首先利用大语言模型从能源领域的文本中自动抽取知识图谱（KG），然后结合图结构进行推理与检索，在生成阶段利用LLM生成多语言答案。系统通过RAGAs框架属性进行评估，并使用包含101个问答对的数据集和领域专家进行人工验证。

Result: 系统整体准确率为75.2±2.7%，在通用型能源效率问题上准确率更高（达81.0±4.1%），显示出对不同问题类型的适应性差异；多语言环境下性能下降较小，翻译仅带来4.4%的准确率损失，表现出良好的跨语言能力。

Conclusion: 基于大语言模型与知识图谱的图RAG架构在能源效率问答中展现出良好潜力，尤其在处理结构化知识和多语言需求方面具有优势，但仍存在改进空间，特别是在复杂或专业性更强的问题上。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [205] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了Zero-RAG方法，通过识别并剪枝检索语料中的冗余知识，减少检索开销并提升大语言模型对内部知识的利用，在不损失性能的前提下加速检索过程。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型内部知识的增长，传统检索增强生成（RAG）中外部语料与模型内部知识存在显著冗余，导致检索成本增加且可能损害模型表现。

Method: 提出Mastery-Score指标来识别并剪枝冗余知识；设计Query Router和Noise-Tolerant Tuning机制以减少无关文档干扰，增强模型对内部知识的利用。

Result: Zero-RAG将Wikipedia语料库剪枝30%，检索阶段加速22%，且未影响RAG的整体性能。

Conclusion: 通过去除冗余知识并优化查询路由，Zero-RAG有效平衡了内部知识与外部检索的使用，提升了效率而不牺牲性能。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [206] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 本研究在尼泊尔农村等资源受限环境中，通过微调轻量级离线对话模型DialoGPT，并使用合成构建的医生-患者交互数据集，针对十种常见疾病进行领域适配，展示了该模型生成连贯、情境相关且医学上合理的回应能力。


<details>
  <summary>Details</summary>
Motivation: 在缺乏互联网连接和云基础设施的偏远地区，传统大规模对话模型难以部署，因此需要一种可在离线环境下运行并适用于特定医疗领域的对话系统，以支持基层医疗。

Method: 采用DialoGPT作为基础模型，构建涵盖尼泊尔农村十种常见疾病的合成医生-患者对话数据集，并对模型进行微调，使其适应特定医疗场景。

Result: 尽管训练数据有限且局限于特定领域，微调后的模型仍能生成连贯、符合语境且具备医学合理性的回复，表现出对症状理解、疾病背景把握以及共情沟通的能力。

Conclusion: 研究表明，轻量级、可离线运行的对话模型结合针对性的数据集，能够在低资源医疗环境中有效实现领域适配，为农村地区医疗对话AI的发展提供了可行路径。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [207] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了基于编码器的Transformer模型（如BERT、ALBERT等）在上下文词嵌入中的性别偏见问题，提出了一种基于掩码语言模型概率的新度量方法MALoR，并通过反事实数据增强构建性别平衡数据集进行持续预训练以减轻偏见。实验表明该方法显著降低了多种模型中的性别偏见，同时不影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 由于语言模型训练数据中固有的性别偏见可能导致模型在实际应用中产生不公平的输出，因此有必要深入探究并量化这些偏见，尤其是在广泛使用的上下文词嵌入中。

Method: 提出新指标MALoR来量化掩码语言模型中的性别偏见，并采用反事实数据增强生成性别平衡数据集，对模型进行持续预训练以缓解偏见。

Result: 在BERT-base中，“he-she”偏见分数从1.27降至0.08，“his-her”从2.51降至0.36；BERT-large中“male-female”偏见从1.82降至0.10，其他模型也表现出类似改善，且下游任务性能未受影响。

Conclusion: 所提出的MALoR指标能有效衡量上下文嵌入中的性别偏见，结合反事实数据增强的持续预训练可显著降低多种Transformer模型的性别偏见，具备实用价值。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [208] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 提出WordSaladChopper（WSC），通过检测并剪除大推理模型中的无意义重复token（“word salad”）来节省输出长度，提升效率。


<details>
  <summary>Details</summary>
Motivation: 大推理模型输出成本高，部分输出为无意义的自我重复，浪费解码资源。

Method: 利用隐藏状态中<\n\n> token的模式，用单层线性分类器实时检测word salad，剪除后通过再生提示恢复关键内容。

Result: 显著减少输出长度，节省解码预算，且对输出质量影响极小。

Conclusion: WSC是一种轻量、即插即用的组件，应成为面向用户体验的LRM系统的标配。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [209] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出了一种基于预训练语言模型的新型情感分析框架CISEA-MRFE，结合上下文指令、语义增强和多级特征提取，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析方法在处理细微情感线索、领域迁移和情感分布不平衡时表现不佳，主要由于语义基础不足、泛化能力差以及对主导情感类别的偏见。

Method: 提出CISEA-MRFE框架，融合上下文指令（CI）引导情感消歧，语义增强增强（SEA）提升鲁棒性，以及多精炼特征提取（MRFE）结合尺度自适应深度编码器（SADE）和情感评估上下文编码器（EECE）进行多尺度和情感感知建模。

Result: 在IMDb、Yelp、Twitter和Amazon四个基准数据集上实验表明，CISEA-MRFE相比强基线方法准确率最高提升达30.3%（Twitter），其余分别提升4.6%、6.5%和4.1%。

Conclusion: CISEA-MRFE有效提升了情感分析的性能与跨领域泛化能力，验证了语义增强与多尺度特征建模的重要性。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [210] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出了ISA（Intent Shift Attack），通过意图转换的最小修改生成看似无害的自然语言提示，显著提升对大语言模型的越狱攻击成功率，并揭示了现有安全防御机制的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能力强大，但仍易受越狱攻击。现有攻击方法多依赖额外上下文或对抗性标记来分散模型注意力，而核心恶意意图不变。本文旨在探索更隐蔽、更自然的攻击方式，以揭示模型在意图理解上的根本缺陷，推动更鲁棒的安全机制发展。

Method: 提出ISA攻击方法，建立意图转换的分类体系，通过对原始有害请求进行最小化编辑，生成被模型误判为良性信息请求的自然语言提示。仅需简单修改，不依赖复杂标记或长上下文。

Result: 在开源和商业大模型上实验显示，ISA相比直接有害提示攻击成功率提升超过70%；仅使用ISA重构的良性数据微调模型，攻击成功率接近100%。评估发现现有防御方法对ISA无效。

Conclusion: ISA揭示了大语言模型在意图推理上的根本性安全挑战，现有防御机制难以应对此类自然且隐蔽的攻击，亟需开发更有效的防御策略。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [211] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: 本文提出了FlashEVA，一种基于控制变量的高效注意力机制EVA的优化实现，显著提升了Transformer模型推理时的吞吐量并降低了GPU内存占用，同时支持通过微调保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中表现出色，但其推理过程中因需维护完整上下文而导致内存消耗大，限制了实际应用效率，因此需要更高效的注意力机制。

Method: 提出FlashEVA，改进EVA（Efficient Attention via Control Variates）的实现，并设计微调方法使Transformer模型适应FlashEVA注意力机制，同时引入可调节超参数以平衡吞吐量与准确性。

Result: 相比标准Transformer，FlashEVA在推理时最高实现6.7倍吞吐量提升和5倍峰值GPU内存降低，仅用1.5B token微调即可保持下游任务效果，但在检索密集型任务中表现有限。

Conclusion: FlashEVA显著提升了Transformer模型推理效率，为高效、可适配的模型部署提供了可行方案，是迈向高效Transformer推理的重要一步。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [212] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: 提出OpenSIR框架，通过自博弈实现大模型在无监督下自主生成与解决数学问题，推动开放性自我提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖标注数据或外部验证器，限制了模型超越人类水平的能力，缺乏真正开放的自我进化机制。

Method: 设计一种自博弈框架OpenSIR，让LLM在教师与学生角色间切换，通过难度与多样性优化来生成并解决新问题，无需外部监督。

Result: 在GSM8K和College Math等数学推理任务上显著提升Llama-3.2-3B-Instruct和Gemma-2-2B-Instruct的表现，实现从基础到高级数学的自主演化。

Conclusion: OpenSIR实现了无需外部监督的开放性自我改进，展示了大模型通过角色共进化实现持续学习的潜力。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [213] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: 本文提出了SpecDiff-2，一种基于离散扩散的非自回归推测解码框架，有效解决了当前推测解码中自回归依赖和草案令牌频繁被拒绝的两个瓶颈，在保持准确率的同时显著提升了大语言模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法受限于草案生成的自回归依赖和草案与验证模型之间的不匹配导致的高拒绝率，限制了推理加速效果。

Method: 提出SpecDiff-2框架，使用离散扩散模型作为非自回归草案生成器以提升并行性，并设计新技术校准离散扩散草案生成器与自回归验证器之间的协同工作。

Result: 在多个推理、编程和数学基准测试中，SpecDiff-2相比先前方法平均提升55%的每秒生成令牌数，最高实现5.5倍的平均加速，且无精度损失。

Conclusion: SpecDiff-2通过结合非自回归扩散草案生成与有效的校准机制，显著提升了大语言模型的推理效率，成为新的最优方法。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [214] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型在概率场景中token级置信度与理论概率分布之间的一致性问题，发现尽管模型响应正确，但其输出的概率和熵值与理论分布存在持续偏差。


<details>
  <summary>Details</summary>
Motivation: 为了确保大语言模型在决策支持等知识密集型应用中的可信使用，需要可靠的不确定性量化（UQ），而现有基于token logits的方法在概率场景下可能无法准确反映理论概率分布。

Method: 使用GPT-4.1和DeepSeek-Chat模型，在包含概率任务的十个提示上进行实验，比较有无显式概率提示下的模型响应，评估响应有效性以及token级输出概率与理论概率的对齐程度。

Result: 两个模型在所有提示场景中均达到完美的领域内响应准确率，但其token级概率和熵值始终与理论分布存在显著偏差。

Conclusion: 当前大语言模型虽然能生成符合约束的正确响应，但其内部概率估计并不能准确反映真实理论概率分布，表明其不确定性量化仍存在局限性。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [215] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 该研究通过计算分析探讨了150年来法国侦探小说中侦探原型的演变，利用监督模型和角色级嵌入捕捉其统一性，并揭示其从次要角色发展为推理核心，二战后受硬汉派影响变得更加复杂。


<details>
  <summary>Details</summary>
Motivation: 探究法国侦探小说中侦探原型的历史演变及其在文学和社会背景下的变化。

Method: 使用定量方法和角色级嵌入，结合监督模型进行计算分析。

Result: 模型成功捕捉到跨越150年文学作品中侦探原型的统一性，并揭示其角色从次要地位发展为叙事中心，且在二战后变得更加复杂，反映社会暴力与道德模糊的转向。

Conclusion: 侦探原型在法国文学中经历了系统性演变，其发展受到文学传统和社会历史变迁的深刻影响。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [216] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个用于评估多语言大模型文化素养的新基准，涵盖九个国家的地理、文化与历史问题，以七种语言呈现，揭示了模型在不同语言间获取文化特定知识的能力存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有多种语言问答基准数据集偏向西方中心，未充分考虑地区多样性，导致对多语言模型的事实理解能力评估不公。为此，研究引入XNationQA以填补这一空白。

Method: 构建包含49,280个问题的XNationQA数据集，覆盖九个国家的地理、文化和历史，使用七种语言表达，并采用两个新提出的迁移性指标对八种标准多语言大模型进行评测。

Result: 实验发现模型在英语中对文化信息的掌握常优于对应文化的主导语言；模型在西方语言上表现更好，但并不意味着对西方国家的文化更熟悉；且模型在语言间的知识迁移能力有限，尤其在开源模型中更为明显。

Conclusion: 当前多语言大模型在文化特定知识的获取和跨语言知识迁移方面存在显著局限，XNationQA有助于更公平地评估模型的文化素养。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [217] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次系统评估了十种语言下大型语言模型的越狱攻击与防御方法，发现攻击成功率和防御效果因语言而异，高资源语言在标准查询下更安全，但在对抗性查询下更脆弱，结果表明需要语言感知和跨语言的安全基准。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型经过安全对齐，但已有研究表明可通过越狱攻击绕过安全机制，而现有研究缺乏对多语言场景下攻击与防御泛化能力的系统评估。

Method: 在HarmBench和AdvBench两个基准上，使用六种大语言模型，对十种涵盖高、中、低资源的语言进行跨语言评估，分析基于逻辑表达式和对抗性提示两类越狱攻击的表现及多种防御方法的鲁棒性。

Result: 攻击成功率和防御效果随语言变化显著；高资源语言在常规查询中更安全，但在对抗性查询下更易受攻击；简单防御方法有效但依赖语言和模型类型。

Conclusion: 当前大语言模型的安全评估需考虑语言差异，应建立具备语言感知能力和跨语言覆盖的安全基准以提升整体安全性。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [218] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种改进的原生稀疏注意力机制（NSA），通过在不同层交替使用局部和全局注意力，并引入潜在注意力机制，有效提升了长序列建模能力，同时减少50%的KV缓存内存。


<details>
  <summary>Details</summary>
Motivation: 为了提升稀疏注意力机制在长距离依赖建模中的有效性，解决固定注意力模式传播能力不足的问题。

Method: 在NSA基础上采用跨层交替的局部与全局注意力策略，并分别用多头潜在注意力（MLA）和组头潜在注意力（GLA）优化滑动窗口、压缩和选择性分支。

Result: 在340M到1.3B参数规模的模型上实验表明，该方法在常识推理和长上下文理解任务中性能优于或媲美全注意力和原始NSA，同时KV-cache内存减少50%。

Conclusion: 交替注意力结构结合潜在注意力机制能更高效地建模长程依赖，在保持低计算开销的同时显著提升模型表现。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [219] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为TriCon-Fair的对比学习框架，通过解耦损失函数消除正负样本间的耦合效应，有效减少大语言模型中的社会偏见，同时保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法忽视了偏见与无偏样本之间的相互关系，导致改进一类样本的同时损害另一类，残留社会偏见。

Method: 提出TriCon-Fair框架，采用结合三元组和语言建模项的解耦损失，为每个锚点分配明确的偏见负样本和无偏正样本，并联合优化语言建模目标。

Result: 实验结果表明，TriCon-Fair在减少歧视性输出方面优于现有去偏基准，同时保持较强的下游任务性能。

Conclusion: TriCon-Fair为敏感NLP应用提供了一个实用且符合伦理的解决方案，有效缓解了正负耦合问题。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [220] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估框架，用于系统评估大语言模型（LLM）在逐步推理过程中对知识的准确依赖程度，包含核心知识库、基于知识的评估指标和轻量级评估模型，并展示了其在发现推理缺陷和优化模型偏好中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管逐步推理在复杂任务中有效，但尚不清楚LLM的推理是否真正基于可靠的知识，因此需要一种系统方法来验证其知识 grounding 的准确性。

Method: 构建了一个大规模的原子知识库（Principal Knowledge Collection），并基于此设计了知识接地的评估指标，使用一个轻量级的LLM作为评估器来计算这些指标，从而衡量模型在推理中回忆和应用先验知识的能力。

Result: 该评估套件能有效识别推理中缺失或误用的知识元素，揭示LLM的推理缺陷，并可集成到偏好优化中以改进模型训练。

Conclusion: 所提出的知识接地评估框架不仅能够可靠、低成本地评估LLM的推理过程，还为提升模型的推理质量和训练提供了新途径。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [221] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一种用于多模态文档检索的模型，通过OCR预训练、自监督对比学习和晚期交互评分机制，在ViDoRe V2基准上比现有模型提升了3.61%，并展现出更强的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索方法多沿用纯文本检索的技术，未能充分考虑多模态文档的结构和视觉特征，限制了检索性能。

Method: 提出ColMate模型，采用基于OCR的预训练目标、自监督掩码对比学习目标以及晚期交互评分机制，更好地捕捉多模态文档中的文本与视觉信息。

Result: 在ViDoRe V2基准上，ColMate相比现有模型提升了3.61%，并在跨领域基准上表现出更强的泛化能力。

Conclusion: ColMate有效结合了多模态表征学习与文档检索，显著提升了多模态文档检索的性能和适用性。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [222] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 该研究评估了两种领先的大型语言模型（LLM）在医疗诊断场景中生成易于理解且富有同理心的患者解释的能力，发现模型虽能根据患者特征调整内容，但存在输出过于复杂和情感同理心偏差的问题，可能导致沟通不平等，需系统校准以提升公平性。


<details>
  <summary>Details</summary>
Motivation: 确保LLM在临床诊断沟通中生成的内容既易懂又具同理心，避免因复杂性或偏见导致患者支持不均。

Method: 使用可读性指标评估理解性，并通过LLM-as-a-Judge方法与人工评价对比评估同理心，分析两个领先LLM在不同社会人口特征和病情下的表现。

Result: LLM能根据患者特征调整解释，但常生成过于复杂的内容，并表现出对特定群体的情感同理心偏差，影响可及性与支持质量。

Conclusion: 需要对LLM进行系统性校准，以确保其在医疗沟通中的公平性和可理解性，促进其在临床实践中的安全应用。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [223] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在七种主要印度语言中的文化推理和自我评估能力，引入了一个多语言谜题数据集，并评估了五种模型在七种提示策略下的表现。结果显示，模型的初始准确性与其识别自身错误的能力呈负相关，高性能模型更倾向于过度自信，而低性能模型则更具自我意识。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在非英语语言中进行文化相关推理的能力，特别是在印度多种语言背景下的表现，并评估其自我评估的一致性。

Method: 构建包含传统谜题和上下文重构变体的多语言谜题数据集，评估五种大型语言模型（Gemini 2.5 Pro、Gemini 2.5 Flash、Mistral-Saba、LLaMA 4 Scout 和 LLaMA 4 Maverick）在七种提示策略下的表现，分两个阶段：第一阶段评估解谜性能，第二阶段进行自我评估实验以衡量推理一致性。

Result: Gemini 2.5 Pro 整体表现最佳，但少样本方法提升有限，且不同语言间准确率差异显著；在自我评估中发现模型初始准确率与其识别自身错误的能力呈负相关，高性能模型过度自信（真阴性率仅4.34%），而低性能模型如LLaMA 4 Scout更具自我意识（真阴性率达42.09%）。

Conclusion: 当前多语言推理仍存在明显差距，需要开发不仅能有效推理还能识别自身局限性的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [224] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 提出一种易到难增强框架，用于在标签不精确的情况下提供可靠的监督，以提高机器生成文本检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法假设标签是“黄金标准”，但存在边界模糊性，且人类认知有限与检测器超级智能导致不精确学习普遍存在。

Method: 采用一个针对较简单长文本检测任务的易监督器来增强更具挑战性的目标检测器，并通过将检测器结构化地融入监督器中，将其建模为检测器性能下限，从而间接优化检测器。

Result: 在跨LLM、跨领域、混合文本和改写攻击等多种实际场景下的大量实验表明，该框架显著提升了检测效果。

Conclusion: 所提出的易到难增强框架能在不精确标注条件下有效提升机器生成文本检测性能，逼近潜在的“黄金”标签。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [225] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个结合任务分解和交互式强化学习的多智能体框架，用于提升自然语言到SQL的转换效果，通过三个专门代理实现动态推理与自我修正，在BIRD和Spider数据集上达到最先进的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 复杂查询的自然语言到SQL转换仍然困难，通常需要环境交互和自我修正能力。

Method: 提出MARS-SQL框架，包含用于模式链接的Grounding Agent、用于查询生成的Generation Agent（基于多轮强化学习和Think-Act-Observe循环）以及用于最终选择的Validation Agent（将验证建模为下一个token预测任务）。

Result: 在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%，表现优于现有方法。

Conclusion: MARS-SQL通过结合交互式强化学习与生成式验证机制，显著提升了复杂SQL查询生成的准确性与鲁棒性。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [226] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出IF-CRITIC，一种高效的LLM批评模型，通过生成约束清单并利用多阶段过滤机制和约束级偏好优化，提升指令遵循的评估效果与训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM-as-a-Judge的指令遵循评估方法存在成本高、评估不可靠等问题，亟需更高效可靠的评估模型。

Method: 设计一个清单生成器分解指令并生成约束清单，结合多阶段批评过滤机制构建高质量训练数据，采用约束级偏好优化方法训练IF-CRITIC模型。

Result: 实验证明IF-CRITIC在评估性能上优于Deepseek-R1和o4-mini等强基线模型，并能在更低计算开销下显著提升LLM的指令遵循能力。

Conclusion: IF-CRITIC能够提供高效且可靠的指令遵循评估信号，有效支持LLM在指令遵循任务上的优化。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [227] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 提出Prompt-R1，一种端到端的强化学习框架，利用小规模语言模型与大规模语言模型协作，通过多轮提示交互提升问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 用户在面对复杂问题时难以提供有效提示，限制了大模型性能，因此需要自动化提示生成机制。

Method: 设计基于小规模LLM的多轮提示交互框架，采用双约束奖励机制优化正确性、生成质量和推理准确性。

Result: 在多个公开数据集上实验表明，Prompt-R1显著优于基线模型，具备良好的通用性和可扩展性。

Conclusion: Prompt-R1为大模型提供了高效、即插即用的提示优化框架，提升了复杂任务下的表现。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [228] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个将开源大语言模型与NOAA实时海洋数据流结合的对话式平台，通过实时API调用生成可验证、可重现的自然语言响应和数据可视化，提升海洋科学中AI决策支持的透明度、可重复性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有通用AI系统在科学应用中常产生未经验证的‘幻觉’结果，缺乏对权威数据的实时接入，限制了其在严谨科学研究中的可靠性。

Method: 构建一个名为OceanAI的对话平台，集成开源大语言模型与NOAA提供的多种海洋数据API，实现用户自然语言查询到实时数据获取、解析与可视化的全流程自动化。

Result: 在盲测中，OceanAI是唯一能提供来自NOAA原始数据支持答案的系统，其他主流AI产品要么拒绝回答，要么给出无依据的结果；平台已支持多个NOAA数据产品，适用于海洋灾害预警、生态系统评估等场景。

Conclusion: 通过将大语言模型与权威观测数据实时耦合，OceanAI为海洋科学提供了一个可扩展、透明且可信的AI辅助决策框架，推动AI在科研领域的可靠应用。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [229] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个基于大语言模型的对话式系统，旨在帮助用户通过自然语言提问来分析印度空气质量、气象和政策数据，自动生成可执行代码和交互式可视化，提升环境数据分析的可访问性。


<details>
  <summary>Details</summary>
Motivation: 现有空气污染数据分析工具需要专业知识且提供静态仪表板，难以支持决策者有效应对复杂政策问题，亟需更直观、易用的工具。

Method: 整合中央污染控制委员会（CPCB）监测站数据、各州人口统计数据和国家清洁空气计划（NCAP）资金记录，利用大语言模型构建统一的对话式分析平台VayuChat，支持自然语言查询并生成Python代码与交互式图表。

Result: VayuChat能够通过简单对话实现复杂的环境数据分析，已在Hugging Face平台公开部署，并可通过YouTube视频展示其实际应用效果。

Conclusion: VayuChat降低了环境数据科学的使用门槛，使政策制定者、研究人员和公众都能便捷地进行空气污染相关分析，有助于将分散的数据转化为有效的决策支持。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [230] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 本研究提出了一种基于公开指南的验证数据集，用于评估大语言模型在临床推理中的表现，并通过GPT生成真实患者场景和临床问题，系统评估了多个流行LLM的临床实用性和对指南的遵循情况。


<details>
  <summary>Details</summary>
Motivation: 由于目前缺乏标准化的基准来评估大语言模型在基于指南的临床推理中的能力，因此需要构建一个可靠的评估框架。

Method: 利用公开可用的诊疗指南，结合GPT生成 realistic 患者场景和临床问题，构建并验证数据集，随后对多种主流大语言模型进行基准测试。

Result: 成功构建了一个可用于评估LLM临床推理能力的高质量数据集，并通过基准测试展示了不同模型在临床实用性和指南遵循方面的表现差异。

Conclusion: 该框架为系统评估大语言模型在临床决策支持中的应用提供了有效工具，有助于推动其在医疗领域的可靠使用。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [231] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 本文介绍了一个为近200种语言提供开放、大规模、高质量且富含标注的文本数据集的倡议，包含30万亿token，是目前最大的多语言LLM预训练数据集之一，并提供了完整的开源处理流程和多种模型训练与评估结果。


<details>
  <summary>Details</summary>
Motivation: 为了支持多语言大模型的发展，解决当前非英语语言数据稀缺、质量不高和缺乏标注的问题，推动更公平、高效的多语言自然语言处理研究。

Method: 基于多个来源的网络爬取数据，构建了涵盖文档筛选、HTML文本提取、语言识别、去重、质量评估、注册标签标注和隐私信息检测的开源处理流程，并自动挖掘平行语料及合成翻译语料用于多语言建模。

Result: 发布了30万亿token的多语言预训练数据集，完成了24种语言的数据质量人工评估，训练了57个单语编码器-解码器模型及若干GPT-like模型，并建立了涵盖九种欧洲语言的综合评测基准。

Conclusion: 该数据集规模大、质量高、标注丰富，配套工具链完整，显著提升了多语言LLM的训练与评估能力，尤其有助于低资源语言的技术发展。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [232] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本研究探讨了罗马尼亚语预训练语料库的特征与覆盖范围，并通过轻量级多任务模型对LLM标注的文本进行多层次过滤，以生成高质量的预训练数据集，实验表明该方法能有效提升LLM在多个基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言（如罗马尼亚语）缺乏高质量语料库，因此需要系统分析其数据特性并改进数据筛选方法，以提升非英语语言的大模型训练效果。

Method: 利用轻量级多任务模型在经过LLM标注的罗马尼亚语文本上进行训练，实现对教育价值、主题、格式等多层面的数据过滤，从而构建高质量预训练数据集。

Result: 发现了罗马尼亚语与英语数据在主题分布上的显著差异，并证明通过多级过滤后的数据能显著提升LLM在多个基准任务上的预训练表现。

Conclusion: 针对低资源语言的数据质量优化至关重要，本研究所提出的多层级过滤方法可有效提升非英语语言大模型的训练效果。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [233] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出了TSVer，一个专注于时间序列证据下时序和数值推理的事实核查新基准数据集，包含287个真实世界声明和400个时间序列，具有详细的标注和高质量的验证理由。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查数据集常缺乏结构化证据、判决依据不足或依赖合成声明，难以有效评估时序和数值推理能力。

Method: 构建了一个包含真实世界声明和多样化时间序列的数据集，采用LLM辅助的多步骤标注流程，对每个声明标注相关时间范围、判决结果和推理依据，并计算评估一致性。

Result: TSVer数据集包含287个声明和400个时间序列，标注一致性达到kappa=0.745；实验表明当前最先进的推理模型（如Gemini-2.5-Pro）在该数据集上表现有限，判决准确率仅为63.37，Ev2R得分为48.63。

Conclusion: TSVer为基于时间序列的事实核查提供了更具挑战性和现实意义的评估基准，揭示了现有模型在时序和数值推理方面的不足，推动未来研究发展。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [234] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 本文提出了MicroRemed，首个用于评估大语言模型在端到端微服务修复能力的基准，并设计了ThinkRemed多智能体框架以模拟运维专家的反思性推理过程，实验表明该框架能通过迭代推理提升修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有微服务修复方法依赖人工编写的提示，大语言模型仅作指令转译，缺乏端到端自动化能力，且缺少统一评估基准。

Method: 提出MicroRemed基准，要求模型从诊断报告直接生成可执行Ansible剧本；设计ThinkRemed多智能体框架，模拟SRE的反思与感知推理过程，实现迭代式决策。

Result: MicroRemed对当前大语言模型构成显著挑战；ThinkRemed相比基线模型在端到端修复任务中表现出更优性能。

Conclusion: 通过构建专用基准和类人推理框架，推动大语言模型在微服务自动修复场景中的研究与应用。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [235] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 本文研究了高销量外呼销售场景下销售人员的动态筛选决策，提出了一种基于生成式语言模型的“停止代理”，通过模仿最优停止策略来学习何时终止通话。该方法显著减少了失败通话时间并提升了销售效率。


<details>
  <summary>Details</summary>
Motivation: 销售人员在面对大量潜在客户时需频繁决定是否继续或终止对话，但现有研究对这类决策的效率及改进方法知之甚少。本文旨在理解并优化这一动态筛选过程。

Method: 将动态筛选决策建模为最优停止问题，构建一个基于生成式语言模型的序贯决策代理（停止代理），通过模仿回溯推断的最优策略进行训练，并应用于真实外呼销售对话数据。

Result: 在一家大型欧洲电信公司的电话数据上应用该停止代理后，失败通话时间减少了54%，同时保留了几乎全部销售额；重新分配节省的时间可使预期销售额最多提升37%。分析发现，销售人员倾向于过度依赖少数明显的客户不感兴趣表达，且对通话失败风险预测不准。

Conclusion: 人工智能算法有潜力纠正人类在实时对话决策中的认知局限，显著提升销售团队的决策效率与整体绩效。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [236] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: 本文介绍了DebateBias-8K，一种新的多语言、辩论式基准，用于揭示生成式大模型在现实叙事场景中的偏见。研究发现，尽管模型经过安全对齐，仍普遍存在刻板印象，且低资源语言中偏见更严重，表明当前对齐方法在多语言环境下效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有的偏见评估主要依赖英语和分类任务，难以反映生成式模型在真实多语言开放对话中的偏见表现，因此需要更贴近实际的评估方式。

Method: 构建包含8,400个结构化辩论提示的多语言数据集DebateBias-8K，覆盖四个敏感领域和七种语言，使用GPT-4o、Claude 3、DeepSeek和LLaMA 3生成超过10万条回复，并进行自动分类分析。

Result: 所有模型均再现了根深蒂固的刻板印象，例如阿拉伯人与恐怖主义和宗教高度关联（≥95%），非洲人与社会经济‘落后’相关（高达≤77%），西方群体则被视为现代或进步；低资源语言中偏见更显著。

Conclusion: 当前基于英语的安全对齐方法无法有效泛化到多语言环境，无法防止开放式生成中的叙事偏见，亟需开发更公平、文化包容的多语言对齐技术。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [237] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 提出ZoFia，一种两阶段零样本虚假新闻检测框架，结合层次显著性与多LLM交互系统，提升对新兴新闻的检测效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理快速演变的新闻流时受限于知识时效性和生成幻觉，现有静态数据集训练的模型泛化能力不足。

Method: 第一阶段使用层次显著性和SC-MMR算法提取关键信息并检索最新外部证据；第二阶段通过多LLM角色分工进行多视角协同分析与对抗辩论。

Result: 在两个公开数据集上实验表明，ZoFia显著优于现有零样本基线方法及多数少样本方法。

Conclusion: ZoFia有效提升了零样本设置下虚假新闻检测的准确性与可解释性，具备良好的鲁棒性和应用潜力。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [238] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: 提出Self-Harmony框架，利用问题及其转述的稳定性通过调和平均聚合答案，在无监督测试时强化学习中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTRL方法依赖多数投票等机制生成学习信号，易陷入流行但错误的答案，缺乏稳定可靠的学习信号。

Method: 使用同一模型作为Solver生成答案和Refamer重构问题，通过原始与重构视角下的答案频率的调和平均生成伪标签。

Result: 在30个测试设置中28个达到SOTA，展现出卓越的准确性和鲁棒性，实验中无训练失败。

Conclusion: Self-Harmony通过稳定性选择机制有效避免了虚假答案，为无监督测试时学习提供了高效可靠的解决方案。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [239] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出了一种名为DEER的解耦混合专家框架，用于检测机器生成文本，通过领域特定与共享专家结合及强化学习路由机制，在跨域场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法在领域迁移时性能显著下降，难以应对实际应用中领域标签不可知的情况。

Method: 设计了一个两阶段的解耦混合专家（DEER）架构：第一阶段，领域特定专家捕捉细粒度的局部特征，共享专家提取跨领域通用特征；第二阶段，采用基于强化学习的路由机制动态选择专家，解决推理时领域标签未知的问题。

Result: 在五个领域内和五个跨领域基准数据集上实验表明，DEER在F1分数上分别平均提升1.39%（领域内）和5.32%（跨领域），准确率提升1.35%和3.61%，且消融实验验证了解耦设计与自适应路由的有效性。

Conclusion: DEER通过解耦专家结构和自适应路由机制，有效提升了机器生成文本检测在领域内和跨领域的鲁棒性与性能，具有较强的实际应用潜力。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [240] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文介绍了AraFinNews，这是迄今为止最大的公开阿拉伯语金融新闻数据集，并研究了领域特异性对基于大语言模型的阿拉伯语金融文本摘要的影响。


<details>
  <summary>Details</summary>
Motivation: 为了提升阿拉伯语金融文本摘要的事实准确性和数值可靠性，需要专门针对该领域的语言模型进行研究和评估。

Method: 构建了一个包含21.25万篇文章-标题对的大型阿拉伯语金融新闻数据集AraFinNews，并在此基础上评估了mT5、AraT5和领域适配的FinAraT5等基于Transformer的模型。

Result: 实验结果表明，经过领域适配的模型（如FinAraT5）在生成摘要时更忠实、连贯，尤其在处理数量和实体信息方面表现更优。

Conclusion: 领域特定的预训练对于提高阿拉伯语金融文本摘要的事实一致性和叙述流畅性至关重要。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [241] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出了ReSpec，一种检索增强的推测解码框架，通过熵引导的自适应触发、反馈驱动的候选选择和源感知的宽松验证策略，显著提升了大语言模型推理速度，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法在草稿模型效率与检索时机选择上存在不足：基于模型的方法准确但成本高，检索增强方法依赖启发式切换策略导致不必要的检索开销。

Method: 提出ReSpec框架，包含三个核心创新：1）熵引导的自适应触发机制，仅在不确定性低时启动检索；2）反馈驱动的候选选择，利用历史反馈组织多个高质量候选进行并行验证；3）源感知的宽松验证策略，对模型生成和检索得到的草案采用不同严格程度的验证。

Result: 在Spec-Bench上的实验表明，ReSpec相比EAGLE-2和SAM-Decoding分别加速超过33%和25%，达到当前最优的加速效果，同时保持输出质量。

Conclusion: ReSpec通过将启发式草稿切换转化为自适应决策，有效平衡了推测解码的效率与准确性，为大语言模型推理加速提供了新的解决方案。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [242] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文研究了AI审稿系统面临的提示注入攻击威胁，提出了静态和迭代两种攻击方法，并探索了防御机制，发现现有防御仍可被自适应攻击绕过，强调了在AI辅助同行评审中加强安全防护的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审中的应用增加，一些论文可能包含恶意提示以操纵AI评审结果，因此需要系统性研究此类安全威胁。

Method: 提出两类攻击：静态攻击使用固定注入提示，迭代攻击通过优化提示以最大化对模拟评审模型的影响；同时探索基于检测的防御方法。

Result: 两类攻击在针对前沿AI评审模型时均能显著提升评分，且具有跨设置的鲁棒性；检测防御虽有效但可被自适应攻击部分绕过。

Conclusion: AI辅助评审面临严重的提示注入威胁，需引起重视并开发更严格的防护措施。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [243] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文介绍了FirstAidQA，一个包含5500个高质量问答对的合成数据集，旨在支持急救和应急响应场景下的大语言模型和小语言模型的指令微调与微调，以实现快速、可靠且可离线运行的紧急情况应对系统。


<details>
  <summary>Details</summary>
Motivation: 在时间敏感、低或无连接环境中，当前大语言模型计算开销大，难以部署于低端设备，且缺乏针对急救领域的高质量数据集，限制了轻量级、领域专用解决方案的发展。

Method: 利用ChatGPT-4o-mini结合提示词上下文学习方法，基于《Vital First Aid Book (2019)》生成原始数据，并进行文本清洗、上下文分块、过滤等预处理步骤，最后通过人工验证确保问答对的准确性、安全性和实用性。

Result: 构建了一个包含5500个高质量急救相关问答对的公开数据集FirstAidQA，适用于资源受限环境下的语言模型训练与优化。

Conclusion: FirstAidQA填补了急救领域高质量数据集的空白，有助于推动面向安全关键、资源受限场景的人工智能研究，特别是在离线和轻量化模型应用方面具有重要意义。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [244] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一种增强的检索增强生成（RAG）系统，通过结构化和时间推理来提升对5G标准文档的问答性能，利用三个元数据丰富的数据库解决跨引用和规范演变问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG框架难以可靠地解析5G标准文档中的跨引用和规范演变，限制了专家级问题的回答准确性。

Method: 构建SpecDB、ChangeDB和TDocDB三个元数据丰富的数据库，通过递归检索解析跨引用，并追踪规范变更及其设计依据。

Result: 在多个大语言模型后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验表明，显式的跨引用解析和演进感知检索显著提升了回答质量。

Conclusion: 建模5G标准的结构和时间特性可显著提升问答系统的性能，验证了DeepSpecs在处理复杂技术规范中的有效性。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [245] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 本文提出了DeepAmbigQAGen生成管道和DeepAmbigQA数据集，用于评估大语言模型在处理名称歧义和多步推理的开放域问答中的表现，发现现有模型（包括GPT-5）在答案完整性方面仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准很少同时评估名称歧义和多步推理的挑战，而实际复杂问题往往需要同时解决这两个问题，因此需要更贴近现实、更具挑战性的评测任务。

Method: 提出DeepAmbigQAGen自动化数据生成流程，结合文本语料库和链接知识图谱，生成包含名称歧义和多跳推理的自然且可验证的问题，并构建包含3600个问题的DeepAmbigQA数据集。

Result: 实验表明，即使最先进的GPT-5模型在该数据集上表现不佳，对有歧义问题的精确匹配得分为0.13，无歧义问题为0.21，暴露出模型在答案完整性和推理能力上的缺陷。

Conclusion: 当前的问答系统在处理复杂、模糊性问题时仍远未成熟，亟需发展更强大的信息收集与答案完整性保障机制。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [246] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文扩展了DistilQwen模型家族，提出了四类针对工业需求的小型高效推理模型，包括慢思考模型、自适应思考模型和蒸馏奖励模型，在多个基准上表现出高推理效率和强推理性能，并支持在阿里云PAI平台上的可扩展训练与推理。


<details>
  <summary>Details</summary>
Motivation: 为了满足现实应用中对小型高效推理模型的需求，平衡推理性能与推理速度，推动知识蒸馏技术的发展。

Method: 基于Qwen模型初始化，通过知识蒸馏技术构建四类模型：慢思考模型、两种自适应思考模型和蒸馏奖励模型，并在阿里云PAI平台上实现可扩展训练与推理。

Result: 在多个基准测试中，这些模型展现出高推理效率和强推理性能，蒸馏奖励模型能有效支持推理模型的强化学习，且具备实际工业应用价值。

Conclusion: 所提出的DistilQwen系列模型在保持高效推理的同时显著提升推理能力，适用于多样化工业场景，并已集成至阿里云PAI平台，具备良好的实用性和可扩展性。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [247] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 提出MiniTruePrefixes模型，用于在文本前缀级别检测事实不一致，提升LLM生成结果的事实性。


<details>
  <summary>Details</summary>
Motivation: 现有NLI模型在完整句子上判断蕴含关系，但自回归生成是逐步进行的，需在前缀级别进行事实性判断。

Method: 构建前缀级蕴含检测任务，创建相应训练和评估数据集，并训练专用模型MiniTruePrefixes。

Result: MiniTruePrefixes在前缀级蕴含检测上比基线NLI模型高5-14 F1点；集成到解码框架后显著提升摘要的事实一致性。

Conclusion: 前缀级蕴含检测有助于提升生成忠实度，MiniTruePrefixes可在更小模型上实现与大模型相当的事实性表现。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [248] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 本文提出了Cancer-Myth-Indic，一个包含五种印度语言的癌症相关谣言多语言评测基准，用于评估大语言模型在非英语语境下的表现，填补了现有医学评测中多语言评估的空白。


<details>
  <summary>Details</summary>
Motivation: 由于越来越多的人依赖大语言模型（LLMs）获取医疗建议，而现有的医学评测基准多为英文，缺乏对多语言场景的评估，因此需要构建非英语的评测基准以更全面地衡量LLMs的准确性和有效性。

Method: 通过将Cancer-Myth数据集中的500个项目翻译成五种印度地区语言（每种语言500项，共2500项），由母语译者遵循风格指南进行翻译，保留原句中的隐含预设，特别关注与癌症相关的错误预设，并在此基础上对多个主流大语言模型进行评测。

Result: 构建了名为Cancer-Myth-Indic的多语言评测基准，涵盖五种印度语言，共2500个翻译条目，成功实现了对大语言模型在非英语语境下处理错误预设的能力评估。

Conclusion: Cancer-Myth-Indic填补了非英语医学评测基准的空白，有助于更公平、全面地评估大语言模型在多语言环境下的医疗问答能力，尤其提升了对印度地区语言的支持。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [249] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型和推理模型在现有基准测试上的表现饱和问题，质疑超越基准是否真正反映推理能力，并分析了OpenAI、Anthropic和Google三类模型在不同推理任务上的性能演变趋势，旨在为未来的推理评估研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着模型能力的提升和训练数据的重叠，现有基准测试结果趋于饱和，难以真实反映模型的推理能力，因此需要重新审视基准测试的有效性。

Method: 通过分析OpenAI、Anthropic和Google三类模型在多年间多个推理基准上的性能变化，考察推理能力的发展趋势和基准测试的演化。

Result: 发现模型在多个基准上表现持续提升但趋于饱和，且部分数据可能已被用于训练，导致评估失真；不同模型家族在不同类型推理任务上表现各异。

Conclusion: 当前基准测试可能无法准确衡量真实推理能力，亟需设计更具挑战性和隔离性的新评估方法，以推动可信的推理模型发展。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [250] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文探讨了形态学特征对语言建模和分词的影响，指出先前研究中的混杂因素导致结论冲突，并重新评估了关于黏着语与融合语建模难度的三个假设，提出使用二元分词指标作为形态复杂性的内在预测工具。


<details>
  <summary>Details</summary>
Motivation: 由于不同研究在实验设计中存在混杂因素，关于形态系统差异是否影响语言建模仍存在争议，因此需要更可靠的方法来厘清这一关系。

Method: 识别现有分析中的混杂因素，重新评估Arnett & Bergen (2025)提出的三个假设，并引入基于token二元组的指标作为语言建模难度的内在预测方法。

Result: 发现先前结论均受混杂因素影响，而token二元组指标可有效作为形态复杂性的梯度代理，无需专家标注即可预测因果语言建模的难度。

Conclusion: 要可靠回答形态学如何影响语言建模，必须控制实验中的混杂因素，并采用更合适的内在评估指标，如token二元组度量。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [251] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动化构建依赖检索的基准测试工具，用于评估大语言模型在处理最新知识时的表现，强调检索与推理能力而非单纯记忆。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准测试过度依赖记忆化且忽视检索作用，无法反映世界知识的动态性，因此需要一种能持续评估模型对新知识理解能力的方法。

Method: 通过计算Wikidata快照间的增量变化，筛选高质量三元组，并生成三种推理难度级别的自然语言问题，利用SPARQL验证确保答案唯一且可验证。

Result: 实验显示，当面对训练数据时间之后的事实时，模型性能显著下降，尤其是在多跳查询上；引入检索增强和更大规模模型仅部分缓解该问题。

Conclusion: LiveSearchBench推动评测从静态记忆转向依赖最新检索与推理的任务，为在动态知识环境下长期系统评估大语言模型提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [252] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 本文提出了一种基于草稿优化的可复现流程，利用开源大语言模型实现拉丁语翻译，性能媲美顶级闭源系统。


<details>
  <summary>Details</summary>
Motivation: 低资源、形态丰富的语言（如拉丁语）翻译面临巨大挑战，现有方法难以兼顾质量与可复现性。

Method: 首先使用微调的NLLB-1.3B生成结构忠实的初稿，再用零样本大模型（Llama-3.3或Qwen3）进行润色，并结合检索增强生成（RAG）提升上下文信息。

Result: 在领域内和新的12世纪拉丁文书信（OOD）两个基准上，该方法性能与GPT-5基线无统计学差异。

Conclusion: 无需任务特定微调，该开源RAG系统即可达到顶尖闭源模型的翻译水平，且所有组件均已开源以促进后续研究。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [253] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出了一种名为BARD的预算感知推理蒸馏框架，通过两阶段训练（监督微调和强化学习）实现对小型语言模型推理长度的精细控制，兼顾推理性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 长链思维（CoT）蒸馏虽能有效传递推理能力，但存在冗余和计算资源不可控的问题，导致资源利用低效。

Method: BARD框架引入用户指定的“思考预算”作为控制信号，采用两阶段训练：第一阶段在教师模型生成的不同预算水平压缩后的长CoT数据上进行监督微调；第二阶段结合推理性能和预算符合度的奖励信号进行强化学习。

Result: 实验表明，8B规模的学生模型在多个复杂推理基准（AIME24、AIME25、GPQA）上表现优异，并能在广泛预算范围内精确自适应地控制推理长度。

Conclusion: BARD能够有效平衡小型语言模型的推理性能与计算效率，提供细粒度且可控的推理过程，提升资源利用效率。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [254] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 本文探讨了使用大语言模型（LLM）作为文本中认知扭曲自动检测的一致且可靠的标注工具，提出多轮独立LLM标注可揭示稳定的标签模式，并引入基于Cohen's kappa的跨数据集评估框架，以实现更公平的模型比较。


<details>
  <summary>Details</summary>
Motivation: 由于认知扭曲标注具有高度主观性，人类专家间的标注一致性低，导致训练数据不可靠，因此需要一种更一致、可扩展的标注方法。

Method: 利用大语言模型（如GPT-4）进行多次独立标注，分析其一致性（使用Fleiss's Kappa），并提出基于Cohen's kappa的 dataset-agnostic 评估框架，用于跨数据集和跨研究的公平比较。

Result: GPT-4实现了较高的标注一致性（Fleiss's Kappa = 0.78），在该标注数据上训练的模型优于使用人工标注数据训练的模型，且下游任务性能更强。

Conclusion: 大语言模型可作为主观性NLP任务中生成高质量训练数据的可靠、可扩展替代方案，且所提评估框架有助于更公正地比较不同研究结果。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [255] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究了合成数据来源多样性对微调大语言模型的影响，关注分布崩溃、对抗鲁棒性和自我偏好偏差三个方面。


<details>
  <summary>Details</summary>
Motivation: 理解合成数据对模型行为的影响，尤其是在广泛使用合成数据开发语言模型的背景下。

Method: 通过在不同来源的合成数据上微调模型，分析其在分布崩溃、对抗鲁棒性和自我偏好偏差上的表现。

Result: 多样来源的合成数据可缓解分布崩溃，保持输出多样性和质量；合成数据微调虽削弱安全机制但输出更可用且潜在危险；微调减少自我偏好偏差，人工数据最有效，多源合成数据次之。

Conclusion: 合成数据来源的多样性有助于改善模型输出质量和多样性，但在安全性方面需谨慎权衡。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [256] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 提出了一种结合帕累托优化大模型和思维链提示的孟加拉语文本去毒化新方法，并构建了包含68,041个样本的并行语料库BanglaNirTox，显著提升了去毒化的质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有毒语言问题严重但缺乏有效应对措施，且由于资源有限，相关研究较少。

Method: 采用帕累托类优化的大语言模型结合思维链（CoT）提示生成去毒化句子，并构建人工平行语料库BanglaNirTox用于模型微调。

Result: 构建了BanglaNirTox数据集，实验表明帕累托优化LLM结合CoT能显著提升孟加拉语去毒化的质量与一致性。

Conclusion: 该方法为低资源语言的文本去毒化提供了有效路径，展示了数据生成与模型优化结合的潜力。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [257] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出了一种通过数据增强和多任务学习策略生成可控难度干扰项的新框架，解决了多项选择完形填空题中高质量干扰项生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多项选择完形填空题的干扰项时缺乏适应性和对难度水平的控制，且缺少标注难度的数据集，限制了进展。

Method: 采用双向干扰项生成过程创建高质量、标注难度的数据集，并利用集成问答系统对候选干扰项进行过滤和难度分类；使用多任务学习训练可控制难度的生成模型，引入辅助任务以增强语义理解和难度估计能力。

Result: 实验结果表明，该方法在不同难度级别上均能生成高质量的干扰项，并在干扰项难度与人类感知对齐方面显著优于GPT-4o。

Conclusion: 所提出的框架有效提升了干扰项生成的质量和难度可控性，为语言能力评估提供了更可靠的支持。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [258] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 该研究利用行为性心智网络框架，探讨心理学本科生对数学和焦虑相关概念的认知与情感关联，并通过四项实验比较人类学生与GPT模拟学生在数学焦虑预测中的差异。


<details>
  <summary>Details</summary>
Motivation: 理解数学焦虑的成因及其对心理学专业学生的影响，探索个体与群体在概念认知结构上的差异，以改进干预策略。

Method: 采用行为性心智网络模型，基于两个真实学生样本（n1=70, n2=57）和两个GPT模型模拟样本（GPT-3.5和GPT-4o各300），通过个体网络特征预测数学焦虑量表得分，并分析群体层面的概念感知差异。

Result: 发现真实学生中‘焦虑’的正性情绪评分和高网络度数，以及‘数学’的负性评分可预测更高的总体和评价性数学焦虑；但该模型在GPT模拟数据中不适用，因模拟网络与人类存在显著差异。高焦虑学生群体对‘焦虑’表现出情感极化，而‘科学’虽被正面评价，却与‘数学’的负面感知形成对比。

Conclusion: 概念的情感与语义关联在理解与管理学生数学焦虑中起关键作用，且当前AI模拟尚不能准确复现人类认知情感结构。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [259] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 提出了一种基于熵的可控对话生成解码方法ECO，能够动态调整每一步生成的控制强度，从而在保持语言流畅性的同时提升可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的加权解码方法使用固定的常数来控制属性概率偏差，难以同时兼顾可控性和生成流畅性。

Method: 提出ECO解码方法，根据语言模型和属性分类器概率分布的熵，在每个生成步骤中动态调整控制强度。

Result: 在DailyDialog和MultiWOZ数据集上的实验表明，ECO解码在多种模型和设置下均优于先前方法，能有效提升可控性并保持流畅性和语法正确性，且在单属性和多属性场景中表现良好。

Conclusion: ECO解码通过动态调节控制强度，解决了固定权重带来的问题，在可控对话生成中实现了更好的平衡。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [260] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: 本文提出了BIRD数据集和一种结合字形网络的变体感知掩码语言模型，用于改进青铜器铭文的修复与断代。


<details>
  <summary>Details</summary>
Motivation: 青铜器铭文碎片化且难以断代，缺乏标准化的数据集和有效的计算方法。

Method: 构建了基于标准学术转录和年代标签的BIRD数据集，提出一种结合领域自适应预训练和字形网络（GN）的变体感知掩码语言模型。

Result: 实验证明字形网络有助于提升铭文修复效果，而字形偏向采样能提高断代性能。

Conclusion: 所提方法在青铜器铭文修复与断代任务中表现优越，凸显了融合文字学知识与深度学习的重要性。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [261] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究通过分析母语为西班牙语者的语言错误，结合理论语言学、神经语言学和自然语言处理，评估大语言模型对人类语言错误的理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 语言错误揭示了人类语言的认知结构，而当前AI系统在处理此类非规范语言时存在局限，需更深入理解人类语言的不完美性。

Method: 构建包含500多个真实西班牙语错误的语料库，从理论语言学、神经语言学和NLP角度分类分析，并在GPT、Gemini等大模型上测试其解释准确性与模式泛化能力。

Result: 预期揭示人类语言错误的认知机制，并评估现有大语言模型在处理非规范语言方面的表现，识别其优势与不足。

Conclusion: 该研究有助于深化对西班牙语语言本质的理解，并推动更具认知合理性的NLP系统发展，使其更能应对真实人类语言的可变性与模糊性。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [262] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语的口语议会语料库，共6000小时，经过自动注释增强，支持多模态研究并可用于情感声学特征分析。


<details>
  <summary>Details</summary>
Motivation: 为了提升斯拉夫语言口语议会语料库在多学科下游研究中的可用性，特别是在语音与文本多模态分析方面的需求。

Method: 基于ParlaMint转录本及其元数据，自动对齐各议会的语音录音，并为文本模态添加语言学标注和情感预测，为语音模态添加填充停顿、音素级对齐及重音位置等自动注释。

Result: 构建了四个斯拉夫语言的大型标注口语语料库，显著增强了数据的实用性，支持如情感声学相关性等分析，并以JSONL、TextGrid格式公开发布，提供检索工具。

Conclusion: ParlaSpeech通过多层次自动注释大幅提升了议会语音语料库的价值，为跨语言、跨模态的语音与语言研究提供了重要资源。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [263] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 提出一种结合Bloom分类法和检索增强生成的认知基准框架，用于评估大语言模型在处理台湾客家数字文化知识时的表现。


<details>
  <summary>Details</summary>
Motivation: 为了评估大语言模型在处理特定文化知识时的认知能力，特别是在多层级认知任务中的表现。

Method: 将Bloom分类法与检索增强生成（RAG）相结合，构建一个六层认知领域的评估框架，并使用台湾客家数字文化档案进行测试。

Result: 该框架能够有效衡量大语言模型在语义准确性和文化相关性方面的表现。

Conclusion: 所提出的框架为评估大语言模型在文化特定知识处理上的认知能力提供了一种系统化的方法。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [264] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: 本文提出了EngChain，一个用于验证多步工程问题解决的基准测试，旨在评估大型语言模型在复杂工程领域的综合推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试无法捕捉到工程领域中科学原理、定量建模和实际约束相结合的综合推理需求。

Method: EngChain包含90个问题，涵盖三个工程分支、9个领域和20个不同区域，问题由符号模板生成并高度随机化以确保多样性。采用两阶段评估：首先验证每一步推理的数值和语义有效性，然后使用LLM-As-A-Judge系统对推理错误进行分类。

Result: 该基准能够有效评估大型语言模型在工程问题中的多步推理表现，并识别出具体的推理错误类型。

Conclusion: EngChain填补了现有基准在工程综合推理评估方面的空白，为高风险专业领域中大型语言模型的能力提供了更严谨的评测方法。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [265] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚多语言的大型音频-语言模型，支持印尼语、泰语、越南语、英语和中文，具备多语言、多模态和多任务能力，在多种音频任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动东南亚地区音频大模型的发展，填补该区域多语言音频-语言模型的空白，并支持多样化的语音交互与理解任务。

Method: 基于大规模音频语料库训练一个多语言音频-语言模型，并构建涵盖多种任务的评测基准SeaBench-Audio以自动化评估模型性能。

Result: SeaLLMs-Audio在东南亚语言的多项音频任务上表现出竞争力，支持包括语音识别、语音翻译、情感识别、语音问答等多类任务，并能进行语音对话。

Conclusion: SeaLLMs-Audio为东南亚多语言音频理解与交互提供了有效解决方案，有望促进该地区学术研究与工业应用的发展。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [266] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了首个开源的角色训练实现，利用宪法式人工智能和合成内省数据管道来更有效地塑造聊天机器人助手的人格特征，并通过微调多个开放权重模型展示了其在不同人格下的效果。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人语言模型生成的AI助手人格影响交互质量、感知智能及价值对齐，但学术界对此缺乏研究。

Method: 采用宪法式人工智能和新的合成内省数据管道进行角色训练，并对三个主流开放权重模型进行微调以适配11种示例人格。

Result: 所提方法在对抗性提示下表现出更强的鲁棒性，生成内容更加连贯真实，且不影响模型在通用基准上的性能。

Conclusion: 该角色训练方法能有效、可控地塑造AI助手人格，提升交互体验，同时保持原有能力。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [267] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一种新的秩-2投影子空间方法，用于更准确地区分大语言模型中参数知识（PK）和上下文知识（CK）的贡献，并首次实现了对自然语言解释（NLEs）中多步知识交互的分析。


<details>
  <summary>Details</summary>
Motivation: 理解PK与CK在NLE中的交互对于评估其合理性至关重要，但现有研究仅限于单步生成和简单的二元选择模型，忽略了更丰富的交互形式。

Method: 提出一种新的秩-2投影子空间方法，解耦PK和CK的贡献，并应用于多步NLE序列的知识交互分析。

Result: 实验表明，相比秩-1子空间，秩-2方法能更有效地捕捉多样化的知识交互；幻觉性NLE更多依赖PK，忠实于上下文的NLE平衡使用PK和CK，而思维链提示可减少PK依赖、增强CK使用。

Conclusion: 该工作提供了首个基于丰富秩-2子空间解耦的大语言模型多步知识交互系统研究框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [268] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 提出了一种基于Qwen3和LoRA的多专家系统，用于构建能在交互环境中进行自然对话和上下文动作执行的NPC，在2025年常识人格对话挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 为了提升NPC在复杂交互环境中的自然语言理解和动作执行能力，实现高效且具备人格一致性的对话系统。

Method: 采用Qwen3作为基础模型，结合低秩适应（LoRA）技术，构建三个专家模块：工具调用、工具响应解析和直接对话，实现多任务协同。

Result: 系统在L40S GPU上实现了快速响应和较低资源消耗，并在Commonsense Persona-Grounded Dialogue Challenge 2025中排名第二。

Conclusion: 该多专家架构在保持计算效率的同时，显著提升了NPC的对话自然性和行为合理性，具有良好的实际应用前景。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [269] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文研究了语言模型在持续对话或阅读过程中由于上下文积累而导致的信念变化风险，发现GPT-5和Grok 4等模型的信念和行为均出现显著偏移，揭示了模型在长期交互中可能出现不可靠输出的问题。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在脑力激荡和研究等应用中的广泛使用，其上下文记忆的增加可能导致模型在无用户干预的情况下发生信念变化，从而影响一致性和对齐性，因此需要探究这一潜在风险。

Method: 通过让模型参与道德困境讨论和安全问题查询，以及阅读对立政治立场文本，评估其陈述信念的变化；同时设计需要工具使用的任务来检测隐含信念对应的行为变化。

Result: GPT-5在10轮讨论后信念发生54.7%的偏移，Grok 4在阅读对立政治文本后出现27.2%的信念偏移；行为选择上的变化与陈述信念偏移一致。

Conclusion: 语言模型的信念具有高度可塑性，在长时间对话或阅读后可能发生显著偏移，导致行为偏离原始对齐，带来潜在风险。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [270] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 本文提出了一种无需模型重训练的提示工程方法，通过结构化引导实现对大语言模型输出长度的精确控制，在多种模型上显著提升了长度一致性，同时保持或提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 长度控制在大语言模型应用中至关重要，但现有方法通常需要昂贵的再训练或复杂的推理工具，限制了实际部署的灵活性。

Method: 提出一种结构引导的提示工程方法，通过在提示中引入规划和字数统计机制，使模型主动跟踪并遵守指定的长度限制。

Result: 在六个先进大语言模型上的实验表明，该方法在文档摘要任务中显著提升了长度保真度，某些模型在短到中等长度约束下长度依从性提升高达37.6%，且生成质量保持或优于标准提示方法。

Conclusion: 该方法提供了一种即插即用、成本低且高效的长度控制方案，特别适用于无法进行模型重训练的生产环境。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [271] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: 本文提出KVTC，一种轻量级的变换编码器，用于压缩大语言模型中的键值缓存，实现高达20倍的压缩率，并在多个模型和基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模服务大语言模型需要高效的键值缓存管理，而传统的缓存重用方式受限于过时缓存占用显存的问题。

Method: KVTC结合了基于PCA的特征去相关、自适应量化和熵编码，借鉴经典媒体压缩技术，仅需短暂校准且不改变模型参数。

Result: 在Llama 3、Mistral NeMo和R1-Qwen 2.5等模型上测试显示，KVTC可实现最高20倍的压缩比，在特定场景下甚至超过40倍，同时保持推理和长上下文准确性，并优于token剔除、量化和SVD等基线方法。

Conclusion: KVTC是一种实用的构建模块，适用于具有可重用KV缓存的内存高效型大语言模型服务。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [272] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: 本文提出了IMO-Bench，一个针对国际数学奥林匹克（IMO）水平的高级推理基准测试套件，包含IMO-AnswerBench和IMO-Proof Bench，用于评估模型的短答案和证明生成能力，并推动了Gemini Deep Think在IMO 2025中取得金牌成绩。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评估基准要么过于简单，要么仅关注短答案正确性，缺乏对高水平证明能力的系统评测，因此需要更严格的北星级指标来推动基础模型的发展。

Method: 设计了IMO-Bench，包括400道可验证短答案的IMO-AnswerBench和侧重证明书写的IMO-Proof Bench，并制定详细自动评分指南；基于Gemini构建自动评分系统，并建立含1000个人工评分的IMO-GradingBench以支持自动评估研究。

Result: Gemini Deep Think在IMO-AnswerBench上达到80.0%，在高级IMO-Proof Bench上达到65.7%，分别超越最佳非Gemini模型6.9%和42.4%；自动评分结果与人工评分高度相关。

Conclusion: IMO-Bench为衡量和提升大模型的数学推理能力提供了可靠且具有挑战性的评测标准，有助于社区推进稳健的数学推理研究。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [273] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文提出了一种名为Tool-to-Agent Retrieval的统一框架，通过在共享向量空间中嵌入工具及其父代理，并利用元数据关系连接它们，实现了细粒度的工具级或代理级检索，解决了现有方法因仅基于粗略代理描述进行查询匹配而导致的功能模糊和选择不佳问题。在八个嵌入模型上的评估表明，该方法在LiveMCPBench基准上比现有最先进方法在Recall@5和nDCG@5指标上分别平均提升了19.4%和17.7%。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法通常基于代理级别的粗略描述进行查询匹配，忽略了工具的细粒度功能，导致代理选择不准确，影响多代理系统性能。

Method: 提出Tool-to-Agent Retrieval框架，将工具和其所属代理共同嵌入到一个共享向量空间中，并通过元数据关系连接二者，支持从工具级到代理级的显式能力表示与检索。

Result: 在八个不同嵌入模型和LiveMCPBench基准上的实验显示，该方法在Recall@5上平均提升19.4%，在nDCG@5上平均提升17.7%，显著优于现有代理检索方法。

Conclusion: Tool-to-Agent Retrieval通过细粒度的功能表示和元数据引导的检索机制，有效提升了多代理系统中工具与代理的检索精度，为大规模LLM多代理系统的可扩展编排提供了更优解决方案。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [274] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的虚拟现实（VR）自动化测试代理VRScout，能够以类人方式实时自主导航和交互，适用于商业VR游戏的质量保证与安全审计。


<details>
  <summary>Details</summary>
Motivation: 随着VR内容快速增长，传统人工测试难以扩展，现有自动化方法难以应对VR高维输入和实时性要求，因此需要一种可扩展且高效的自动化测试方案。

Method: VRScout采用增强的动作分块Transformer模型，从人类演示中学习多步动作序列，并引入动态可调的滑动时间窗口以平衡响应速度与决策精度。

Result: 在真实商业VR游戏中测试，VRScout仅用少量训练数据即达到专家级表现，并在消费级硬件上实现60 FPS的实时推理性能。

Conclusion: VRScout为自动化VR游戏测试提供了一个高效、可扩展的框架，具备在质量保证和安全审查中的实际应用潜力。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [275] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 本研究提出了一种基于稀疏自编码器（SAE）的定向特征选择方法，用于提升大语言模型的安全性与实用性。通过创新的对比提示方法和公开数据集高效筛选最优特征，在Llama-3 8B上实现了安全性提升18.9%、实用性提升11.1%，克服了传统的安全-效用权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全对齐方法通常需要调整模型权重且成本高昂，而现有的SAE方法缺乏系统性的特征选择机制和对安全-效用权衡的原则性评估。因此，亟需一种无需微调、可解释且高效的特征级干预方法来同时提升模型的安全性和实用性。

Method: 利用稀疏自编码器（SAE）从大语言模型中提取可解释特征，结合创新的对比提示方法，使用AI生成提示数据集（teknium/OpenHermes-2p5-Mistral-7B）和Air Bench eu-dataset进行特征评估，并在Llama-3 8B模型上测试不同特征与引导强度的效果，以实现对模型行为的精准控制。

Result: 该方法在Llama-3 8B模型上实现了安全性性能提升18.9%，同时实用性提高了11.1%，验证了通过原则性特征选择进行SAE引导的有效性，并表明可以打破传统安全与效用之间的负相关关系。

Conclusion: 通过系统化的特征选择和基于SAE的定向引导，可以在不调整模型权重的情况下显著提升大语言模型的安全性和实用性，为构建更安全、可控的语言模型提供了可解释、低成本的新路径。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [276] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 本文研究了机器遗忘技术在去除预训练中吸收的不良知识时可能引发的“知识空洞”问题，即良性知识的意外丢失。作者提出一种测试用例生成框架，揭示现有基准测试未能捕捉到的隐藏遗忘成本，发现高达98.7%的测试案例中，经过遗忘训练的模型产生无关或无意义回答。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘评估主要依赖标准基准，但这些基准无法充分反映良性知识的损失。因此，需要更全面的方法来探测遗忘后模型的知识缺失情况，尤其是那些未被传统评测覆盖的隐性代价。

Method: 提出一种测试用例生成框架，探索被遗忘内容的直接邻近区域以及更广泛潜在失效领域，以系统性地探测模型中的知识空洞。

Result: 评估显示，尽管标准基准表现良好，但高达98.7%的测试案例中，遗忘模型给出无关或无意义响应，而原始预训练模型能正确回答这些问题，表明存在显著的知识空洞。

Conclusion: 机器遗忘可能导致严重但被忽视的知识损失，现有评估方式不足以衡量知识保留效果，需转向更动态、深入的评估方法。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [277] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出了一种名为Skip-Block Routing（SBR）的通用框架，用于Transformer-based神经算子，通过动态分配计算资源来提升大规模工程任务中的推理效率，在保持精度的同时减少约50%的计算量并实现最高2倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理物理场时采用统一计算成本，无法适应不同区域的复杂性差异（如湍流中涡旋区域与稳定流的差异），导致计算资源浪费和效率低下。

Method: 设计了SBR框架，利用路由机制学习输入token的复杂度并进行排序，在深层网络中根据该排序决定传递的token数量，从而将更多计算资源集中在复杂区域。

Result: SBR可无缝集成到多种神经算子中，实验显示其在推理过程中减少了约50%的FLOPs，速度提升高达2倍，且不损失精度。

Conclusion: SBR通过自适应跳过部分网络块实现了计算效率与模型性能的良好平衡，为大规模科学计算任务提供了高效、灵活的神经算子解决方案。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [278] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 提出一种基于神经架构搜索（NAS）的自动化框架，用于高效、准确且具有良好泛化能力的能源发电时间序列短期多步预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂模型的手动配置上耗时且易出错，同时难以兼顾预测精度、计算效率和对未见数据的泛化能力。

Method: 设计了一个基于NAS的框架，构建仅包含高效组件的搜索空间以捕捉能源时间序列特征，并提出新的目标函数，兼顾时间上下文中的性能泛化和高维搜索空间的充分探索。

Result: 在能源生产时间序列上的实验表明，通过NAS发现的轻量级架构集成模型在效率和准确性上均优于Transformer和预训练预测模型等现有技术。

Conclusion: 所提NAS框架能自动发现兼顾计算效率、预测性能和泛化能力的优秀模型，适用于实际能源系统的短期多步预测。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [279] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化（SSPO），利用少量配对和大量未配对数据，通过理论支持的奖励阈值进行伪标签，显著降低数据成本。


<details>
  <summary>Details</summary>
Motivation: 减少对大量标注配对数据的依赖，降低语言模型对齐人类偏好的资源消耗。

Method: 引入半监督偏好优化框架，理论证明存在最优奖励阈值用于分离优劣响应，并对未配对数据进行伪标签以提取潜在偏好。

Result: 在多个数据集上实验表明，仅使用1% UltraFeedback数据训练的SSPO性能超越使用10%数据的强基线方法。

Conclusion: SSPO能有效利用未配对数据，在保持人类对齐的同时大幅降低数据获取成本。

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [280] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: 本研究验证了物理信息神经网络（PINNs）在求解多种由常微分方程（ODEs）描述的工程与生物动力系统中的预测能力，表明通过合理平衡损失函数成分和系统调参，PINNs可在复杂问题中实现准确求解。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理高刚性、激波、不规则域、奇异性扰动等复杂ODE问题时收敛困难，亟需更鲁棒的求解方法。

Method: 采用经典ODE问题作为测试平台，系统评估PINNs在精度、训练效率和泛化能力方面的表现，并通过调整损失函数权重、超参数及网络结构设计来优化性能。

Result: 结果显示，适当平衡数据损失、初值损失和残差损失对收敛至关重要；超参数的系统调优以及嵌入先验知识和硬约束能显著提升PINNs的预测能力。

Conclusion: PINNs虽非通用解法，但通过融合物理规律与深度学习，在复杂ODE问题中展现出优越潜力，具备较强的应用前景。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [281] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt是一个面向可持续液冷的强化学习基准环境，基于前沿超级计算机冷却系统的数字孪生，支持多智能体RL、策略蒸馏和LLM解释性控制，旨在提升高密度数据中心液冷系统的能效与可管理性。


<details>
  <summary>Details</summary>
Motivation: 随着AI负载增长，高密度数据中心对高效液冷系统的需求上升，亟需机器学习控制器来提升能效、可靠性与可持续性。现有研究缺乏公开、细粒度且端到端的液冷控制仿真环境。

Method: 基于Modelica构建高保真数字孪生模型，覆盖从冷却塔到服务器机柜的全链路系统；通过Gymnasium接口提供强化学习环境，支持集中式与去中心化多智能体RL方法，并引入策略蒸馏为决策树及LLM驱动的自然语言解释机制。

Result: 实现了对液冷系统多目标实时优化（热管理与能效平衡），支持热回收单元等扩展组件；验证了多种RL方法的有效性，并生成可解释的控制策略，提升了操作透明度与用户信任。

Conclusion: LC-Opt为机器学习社区、运维人员和厂商提供了开放、可定制的液冷控制研究平台，推动数据中心液冷控制向智能化、可持续化发展。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [282] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 提出了一种名为ReLaX-Net的硬件友好的参数复用架构，通过时间复用机制提升物理神经网络（PNN）的有效深度和参数效率，在图像分类和自然语言处理任务中表现出优于传统RNN或DNN的性能。


<details>
  <summary>Details</summary>
Motivation: 当前物理神经网络（PNN）在可训练参数规模上远落后于数字神经网络，受限于硬件实现中的参数数量和性能瓶颈，急需提高参数利用效率以缩小差距。

Method: 提出ReLaX-Net架构，采用层间时间复用（time-multiplexing）策略，在保持少量可调参数的前提下增加网络有效深度，仅需在现有PNN基础上添加快速开关即可实现。

Result: 数值实验表明，ReLaX-Net在图像分类和NLP任务中显著提升计算性能，且在相同参数数量下优于传统的RNN和DNN，并展现出良好的扩展性。

Conclusion: ReLaX-Net为解决PNN规模受限问题提供了一种高效、硬件友好的方案，推动PNN向大规模、高性能方向发展。

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [283] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt是一个开源、高保真模拟基准，用于可持续的地理时空任务调度，结合真实世界数据与数据中心物理模型，支持多目标优化和可复现的机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏能够真实反映环境因素、数据中心物理特性和网络动态之间相互作用的基准，限制了大规模AI在节能减碳方面的进展。

Method: 整合AI工作负载轨迹、电网碳强度、电力市场、天气、云传输成本和网络延迟等真实数据，结合基于物理的数据中心运行模型，构建一个可配置的仿真环境，并提供Gymnasium API和基线控制器。

Result: DCcluster-Opt提供了一个具有挑战性的调度问题测试平台，支持对碳排放、能源成本、服务等级协议和水耗之间权衡的模块化奖励系统，并包含热回收等高级功能。

Conclusion: DCcluster-Opt作为一个现实、可配置且易于访问的测试平台，加速了面向地理分布式数据中心的下一代可持续计算方案的发展与验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [284] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 本文提出了一种名为DynBERG的新模型，结合Graph-BERT与GRU，用于动态金融交易图中的欺诈检测，支持有向边并能适应市场重大变化，在Elliptic数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图模型如Graph-BERT主要用于静态、无向图，难以有效处理具有时序演化和有向边的动态金融交易网络，因此需要一种能够捕捉时间动态和方向性特征的新型模型。

Method: 提出DynBERG模型，将Graph-BERT与GRU结合以捕捉多时间步的图结构演化，并修改算法以支持有向边；在Elliptic比特币交易数据集上进行评估，并与EvolveGCN、GCN等基准模型比较性能。

Result: DynBERG在市场关闭前的表现优于EvolveGCN，关闭后优于GCN；消融实验表明GRU组件对建模时序动态至关重要。

Conclusion: DynBERG通过融合Transformer与循环结构，有效提升了对动态、有向金融交易图的欺诈检测能力，具备应对重大市场事件的鲁棒性。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [285] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出一种基于自监督学习的天气预报框架，结合图神经网络和时空适应机制，显著提升多变量天气预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于大气系统的时空复杂性，准确且鲁棒的天气预报仍具挑战性，现有方法在泛化性和细粒度模式捕捉上存在不足。

Method: 结合图神经网络（GNN）进行空间推理，采用自监督预训练进行表征学习，并引入时空适应机制以增强不同预测时间范围下的泛化能力。

Result: 在ERA5和MERRA-2数据集上的实验表明，该方法优于传统数值预报模型和最新的深度学习方法，尤其在北京和上海的可视化分析中展现出对细粒度气象模式的良好捕捉能力。

Conclusion: 所提出的框架为未来数据驱动的天气预报系统提供了一种可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [286] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 本文提出了一种名为FLoRA的融合前向-后向适配器（FFBA）方法，用于大语言模型的参数高效微调，在准确率和延迟方面优于LoRA。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增大，高效训练和微调变得尤为重要，推动了对参数高效微调（PEFT）方法的研究需求。

Method: 结合LoRA和并行适配器的思想，设计了融合前向-后向适配器（FFBA），并通过将适配器融合到基础模型的投影层中以减少延迟。

Result: 实验结果表明，在相似参数预算下，FFBA在准确率和延迟方面均显著优于LoRA。

Conclusion: FLoRA是一种更高效的参数微调方法，能在保持低参数开销的同时提升微调性能和推理速度。

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [287] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的参数高效微调框架，通过分析DoRA方法的成功机制，提出了Pre-Diag和SORA两种新方法，在自然语言理解和生成任务上优于LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA方法虽然性能优越但机制不明确且计算开销大，本文旨在揭示其本质并提出更高效、可解释的替代方案。

Method: 通过奇异值熵分析揭示DoRA的作用机制，将其重新表述为可学习的权重条件化方法，并在此基础上提出Pre-Diag和SORA两种新架构。

Result: 实验表明，Pre-Diag和SORA在多个NLP任务上均优于LoRA和DoRA，具有更高的性能和训练效率。

Conclusion: 基于对DoRA的深入理解，本文提出的统一框架和新方法为参数高效微调提供了更优的解决方案。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [288] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 本文评估了特征引导分析（FGA）在MNIST和LSC数据集上的适用性，结果表明FGA在精度上优于已有研究，且网络架构、训练方法和特征选择对召回率有显著影响，但对精度影响较小。


<details>
  <summary>Details</summary>
Motivation: 为了提升神经网络在安全关键应用中的可解释性，需要验证特征引导分析（FGA）在实际场景中的适用性，尤其是在工业环境下的有效性。

Method: 采用MNIST和LSC数据集作为基准，评估FGA在生成解释神经网络行为规则方面的有效性，并分析不同神经网络架构、训练方式和特征选择对FGA精度和召回率的影响。

Result: 实验结果显示FGA在精度上优于文献中的结果，且架构、训练和特征选择对FGA的召回率有显著影响，但对精度影响甚微。

Conclusion: FGA在基准数据集上表现出较高的解释精度，具备良好的应用潜力，但在实际部署中需注意影响召回率的因素以优化整体性能。

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [289] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出一种基于二次型加权的训练目标QDF，有效解决时间序列预测中标签自相关性和任务权重不均的问题，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标（如均方误差）将每个未来时间步视为独立且等权重的任务，忽略了步骤间的标签自相关性，且无法为不同预测任务设置差异化权重，导致训练目标偏差和性能受限。

Method: 提出一种二次型加权训练目标，利用权重矩阵的非对角元素捕捉标签自相关性，通过对角线元素设置不同时间步的差异化任务权重，并设计QDF学习算法来自适应更新该权重矩阵。

Result: 实验表明QDF能显著提升多种预测模型的性能，在多个数据集上达到当前最优结果。

Conclusion: QDF通过建模标签依赖关系和动态任务加权，有效改进了时间序列预测的训练目标设计，具有广泛适用性和实际应用价值。

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [290] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 提出SpatialTraceGen框架，通过大模型蒸馏生成高质量多步推理数据，并利用自动化验证器提升小模型在复杂空间推理任务中的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂空间推理上表现不佳，且缺乏高质量的逐步推理数据用于小模型微调。

Method: 设计SpatialTraceGen框架，利用大模型生成多跳、多工具使用的推理轨迹，并引入自动化验证器确保每一步推理的准确性。

Result: 在CLEVR-Humans基准上，验证器引导的方法使推理轨迹平均质量得分提升17%，质量方差降低40%以上。

Conclusion: SpatialTraceGen能有效生成高质量、结构化的推理数据，支持小模型的高效微调和离线强化学习。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [291] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习（FL）在基于无人机热成像的城市热特征检测中的实际应用，使用来自两个德国城市的分布式数据，评估了多种FL方法在模型准确率、训练时间、通信开销和能耗等方面的性能，并比较了客户端与服务器端控制的工作流。


<details>
  <summary>Details</summary>
Motivation: 由于隐私或技术限制，传统集中式机器学习难以共享和集中存储数据，而联邦学习允许多个参与者在不共享本地数据的情况下协同训练模型，因此需要探索其在真实场景中的可行性和有效性。

Method: 采用多种联邦学习算法，在真实部署环境中进行实验，使用两个城市采集的无人机热图像数据，对比不同FL方法与集中式学习基线的性能，并分析客户端控制与服务器控制工作流的差异。

Result: 研究表明，联邦学习在非独立同分布的现实数据下仍能有效执行语义分割任务，但存在通信开销高、训练时间长等挑战；不同FL方法在准确率和资源消耗方面表现各异，服务器控制的工作流通常更高效。

Conclusion: 联邦学习适用于无人机热成像等现实中的分布式数据场景，但在实际应用中需权衡性能、通信成本和系统架构设计，本研究为FL在类似任务中的部署提供了重要参考。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [292] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender是一个多模态发现框架，结合了化学语言模型和分子图表示，用于高效检索和稳健排序具有目标性能的聚合物。


<details>
  <summary>Details</summary>
Motivation: 为了提升下一代聚合物的AI引导设计能力，需要融合多种模态信息以克服单一模态在聚合物发现中的局限性。

Method: 该框架结合PolyBERT的化学语言表示和图编码器的分子图表示，首先基于语言相似性检索候选聚合物，然后利用融合的多模态嵌入根据多个目标性能进行排序。

Result: PolyRecommender能够有效利用两种模态的互补信息，实现对相关聚合物性能的高效检索与稳健排序。

Conclusion: 该工作建立了一个可推广的多模态范式，推动了AI在聚合物发现中的应用。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [293] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: 提出模块级重要性采样方法MISA，通过为每个模块分配重要性分数并采用加权随机采样机制，在降低梯度方差的同时提升内存效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有层级优化方法忽略层内模块的重要性差异，且内存节省有限，导致性能次优。

Method: 将每层分解为更小的模块，计算各模块的重要性得分，使用加权随机采样激活模块，并理论证明其梯度方差更小，收敛速度为O(1/√K)。

Result: 在多种学习任务上验证了MISA的有效性，相比基线方法在内存占用和性能方面均有提升。

Conclusion: MISA通过模块级重要性采样实现了更高效、更优的优化，在理论和实验上均优于现有方法。

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [294] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 提出基于决策树的自动化方法，用于解释OthelloGPT中MLP神经元如何编码基于规则的游戏逻辑，发现约一半第5层神经元可通过紧凑的规则描述，并通过干预实验证实其因果相关性。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT是一个理想的可解释性研究平台，因其兼具复杂性和基于规则的游戏逻辑，便于逆向工程；但现有方法难以系统识别和解释神经元中的规则编码机制。

Method: 使用回归决策树将棋盘状态映射到神经元激活，提取高激活路径并转换为人类可读的逻辑形式，从而识别编码特定游戏规则（如对角线走法合法性）的神经元。

Result: 在第5层中，2048个神经元中有913个可用R²>0.7的紧凑决策树准确描述；干预实验显示，针对特定模式的神经元消融使模型预测能力下降5-10倍。

Conclusion: 许多MLP神经元确实编码可解释的、基于规则的逻辑，而其余神经元可能参与更分布式或非规则化计算；提供的工具有助于未来可解释性方法的验证。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [295] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: 提出了一种基于密度和方差的非参数聚类算法EVINGCA，通过自适应构建最近邻图并利用局部统计反馈进行聚类，具有对非凸簇的良好识别能力和较低的敏感性，在多种数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法如K-Means和GMM假设簇为凸形且服从高斯分布，而DBSCAN等虽能处理非凸簇但对参数敏感，因此需要一种更鲁棒、适应性强的聚类方法。

Method: EVINGCA在最近邻图上以广度优先搜索方式扩展根图，利用动态更新的局部距离和形状统计信息指导聚类过程，用局部统计反馈替代固定的密度阈值，并结合空间索引实现高效计算。

Result: EVINGCA在平均情况下具有对数线性时间复杂度，在合成、真实、低维和高维数据集上均表现出与现有基线方法相当或更优的性能。

Conclusion: EVINGCA是一种有效的非参数聚类算法，能够自适应地识别复杂形状的簇，且对参数设置不敏感，适用于广泛的数据类型。

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [296] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 该研究探讨了预训练模型的哪些层次最能反映大脑在语言理解中的分层处理机制，通过EEG信号与wav2vec2和CLIP模型的多模态表征对齐，发现结合多模态且分层感知的表示可更接近解码大脑如何将语言视为体验而不仅是声音。


<details>
  <summary>Details</summary>
Motivation: 受大脑通过多层次从声音构建意义的启发，探究预训练模型的层次化表示是否能更好地模拟大脑在自然语言理解中的神经活动。

Method: 使用自然言语感知过程中的EEG数据，结合ridge回归和对比解码方法，评估wav2vec2和CLIP两个模型的不同层次嵌入（逐层、渐进拼接、渐进求和）与脑活动的对齐程度。

Result: 结果显示，多模态且考虑层次结构的模型表征与大脑活动有更强的对齐性，尤其是渐进式组合策略表现更优。

Conclusion: 结合多模态信息并考虑模型内部层次结构的表示方法，有助于更准确地建模和解码大脑对语言的理解过程。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [297] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: 提出Token-Regulated Group Relative Policy Optimization (TR-GRPO)，通过为高概率token分配更高权重来调节梯度更新，提升大语言模型在推理任务中的训练稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法中，低概率token因梯度幅值大而主导更新，导致训练不稳定并抑制更可靠的高概率token的学习贡献。

Method: 在GRPO基础上引入token级加权机制，权重与模型预测的概率正相关，从而降低低概率token的影响，增强高概率token的贡献。

Result: 在逻辑、数学和代理推理等多个RLVR任务上，TR-GRPO consistently 优于GRPO，表现出更强的稳定性和推理能力。

Conclusion: 调节token在强化学习中的贡献至关重要，TR-GRPO提供了一种简单有效的框架，显著提升LLM的推理性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [298] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 本文研究了在无显式域标签的情况下，如何实现视觉-语言模型的领域泛化，提出通过从训练数据中自动发现潜在域并进行自适应知识迁移的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的领域泛化方法大多依赖于可能不可用且模糊的域标签，因此需要一种不依赖显式域标签的领域泛化方法。

Method: 通过对图像特征进行潜在域聚类，并根据输入图像与每个潜在域的相似性融合特定域的文本特征，以实现跨域的自适应知识迁移。

Result: 在四个基准数据集上的实验表明，该方法在基于视觉-语言模型的基线上取得了持续的性能提升。

Conclusion: 该方法有效提升了视觉-语言模型在领域偏移下的鲁棒性，为无显式域标签的领域泛化提供了新思路。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [299] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 本研究比较了大型语言模型（LLM）与贝叶斯优化（BO）在约束多目标逆向设计回归任务中的性能，发现微调后的LLM（如WizardMath-7B）显著优于传统BO方法（BoTorch Ax），接近先进BO（qEHVI）的性能，表明LLM可作为快速、有效的替代方案。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在未专门设计的高维连续数值优化任务中的适用性，特别是在材料信息学中关键的多目标逆向设计问题。

Method: 采用参数高效微调（PEFT）对LLM和BERT模型进行微调，将其用于回归任务；并与BoTorch Ax和qEHVI等贝叶斯优化框架进行对比，使用生成距离（GD）评估性能。

Result: BoTorch qEHVI达到完美收敛（GD=0.0），WizardMath-7B的GD为1.21，显著优于BoTorch Ax（GD=15.03）。

Conclusion: 尽管专用BO框架仍具最优收敛保证，但微调后的LLM是计算速度快、性能良好的替代方案，适用于树脂、聚合物和涂料等工业配方的多目标优化设计。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [300] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: 本文提出了一种过度设计的方法，使用小波特征提取和无监督聚类来解决奇偶性检测问题，而非传统的模运算。


<details>
  <summary>Details</summary>
Motivation: 探索经典信号处理技术在离散符号域中发现潜在结构的能力，并桥接符号推理与基于特征的机器学习。

Method: 将整数转换为小波域表示，提取多尺度统计特征，并应用k-means聚类进行无监督分类。

Result: 在无标签监督的情况下，实现了约69.67%的分类准确率，揭示了奇数与偶数在特征空间中的结构性差异。

Conclusion: 尽管准确率有限，但该方法展示了信号处理与聚类技术在非传统机器学习任务中的潜力，为符号数据的特征工程提供了新视角。

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [301] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种将象形文字视为几何原语程序的方法，利用视觉语言模型（VLM）将光栅图像反编译为贝塞尔曲线组成的程序，在数学领域实现了对视觉结构的理解。模型在现代汉字上训练后能零样本重建古老的甲骨文，表明其掌握了可迁移的抽象几何语法。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在语义能力之外对视觉信息中潜在几何结构的理解能力，尤其是对象形文字这类结合视觉形式与符号结构的挑战性任务。

Method: 将象形文字识别建模为程序合成任务，使用VLM将光栅图像反编译为由贝塞尔曲线构成的可执行程序，并在现代汉字数据集上进行训练。

Result: 该模型性能优于包括GPT-4o在内的强零样本基线，且在未见过的甲骨文上实现了零样本重建，展现出强大的泛化能力。

Conclusion: 模型不仅实现了从像素到程序的视觉理解，更重要的是获得了抽象且可迁移的几何语法，标志着向更结构化的视觉理解迈进了一步。

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [302] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR是一个基于R语言的模块化框架，用于构建可复现、可扩展的通用机器学习工作流，特别支持算法公平性研究，通过标准化引擎实现数据处理、训练、评估等环节的透明化与灵活集成。


<details>
  <summary>Details</summary>
Motivation: 由于算法公平性领域新指标、新方法不断涌现，现有工具包往往局限于单一干预或忽视可复现性和可扩展性，因此需要一个以核心设计原则构建的统一框架。

Method: 提出一种统一架构，将数据拆分、预处理、训练、后处理等流程封装为标准化的‘引擎’，各引擎通过轻量级接口通信，作为数据结构进行管理，支持模块化和分布式开发。

Result: 实现了flowengineR R包，支持公平性方法的即插即用式比较与评估，并可自然扩展至可解释性、鲁棒性与合规性分析，提升了工作流的透明度与可维护性。

Conclusion: flowengineR不仅服务于算法公平性研究，还为任何强调可复现性、透明性和可扩展性的机器学习工作流提供了通用基础设施。

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [303] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出了一种基于固定点迭代的图卷积网络（Fix-GCN），通过谱调制滤波器有效捕获高阶邻域信息，提升图神经网络对对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在面对图结构和节点特征被恶意扰动时存在安全隐患，需要更高效的防御机制。

Method: 引入一种可调的谱调制滤波器，并利用固定点迭代推导特征传播规则，在不增加计算开销的情况下捕捉高阶邻域信息。

Result: 在多个基准图数据集上验证了模型的有效性，Fix-GCN在对抗攻击下表现出更强的鲁棒性，优于传统防御方法。

Conclusion: Fix-GCN通过灵活的频域滤波和迭代更新机制，实现了高效且鲁棒的图表示学习，为对抗攻击提供了新的防御思路。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [304] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 本论文探讨了使用序数回归技术自动预测纸笔角色扮演游戏（RPG）中怪物等级的方法，构建了专用数据集，并开发了一个受人类启发的基准模型，结合领域知识设计了专门的评估流程。


<details>
  <summary>Details</summary>
Motivation: 由于目前怪物等级设计依赖耗时且资源密集的手动测试和专家评估，缺乏自动化方法，因此需要研究自动化的等级预测技术以提高效率。

Method: 采用序数回归技术，构建专用数据集，开发人类启发式基准模型，并设计基于领域知识的专门评估流程来比较不同机器学习算法的性能。

Result: 提供了对最先进序数回归方法的评估，成功构建了用于等级估计的数据集，并通过新设计的评估流程实现了模型间的有效比较。

Conclusion: 自动化怪物等级预测是可行的，所提出的模型和评估方法为RPG游戏设计提供了有效的工具，有助于减少对人工测试的依赖。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [305] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: 本文提出了MaGNet，一种基于Mamba双超图网络的股票趋势预测模型，通过结合双向Mamba、稀疏专家混合层、二维时空注意力和双超图结构，在捕捉时序依赖与动态跨股票关系方面显著优于现有方法，在多个主要股指上表现出卓越的预测性能和投资回报。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模股票市场中的复杂时序动态和动态跨股票关系，常忽略横截面市场影响、依赖静态相关性、对节点和边采用统一处理，并混淆不同类型的关系。因此需要一种能同时捕捉细粒度因果依赖和全局市场模式的新型模型。

Method: 提出MaGNet，包含三个核心创新：(1) MAGE模块，结合双向Mamba与自适应门控机制进行上下文感知的时序建模，并引入稀疏MoE层以动态适应不同市场状态；(2) 特征维和股票维的二维时空注意力模块，实现多变量特征与跨股票依赖的精细融合；(3) 双超图框架，包括捕捉带有时序约束的细粒度因果依赖的TCH和通过软超边分配与Jensen-Shannon散度加权建模全市场模式的GPH，实现多尺度关系学习。

Result: 在六个主要股指上的实验表明，MaGNet在预测精度上显著优于现有最先进方法，同时展现出更高的投资收益和更强的风险管理能力。

Conclusion: MaGNet通过融合先进的序列建模与多尺度动态关系推理，在股票趋势预测任务中实现了性能突破，验证了其在复杂金融时序建模中的有效性与应用潜力。

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [306] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文研究了在固定计算预算下，如何搜索最优的多大语言模型（LLM）组合与协作架构以提升测试时扩展（TTS）性能，提出了一种基于LLM代理增强的Agent-REINFORCE框架，通过将反馈作为文本梯度来高效优化协作图结构。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法通常假设固定的协作架构和单一模型使用，忽略了不同任务下最优架构和模型组合的差异性，因此需要一种能自适应搜索最优多LLM协作结构的方法。

Method: 将TTS中的模型组合与架构搜索问题形式化为多LLM协作图的构建问题，并转化为概率图优化问题；提出Agent-REINFORCE框架，利用LLM代理模拟REINFORCE流程，通过采样、反馈、更新机制进行高效搜索。

Result: 实验表明，Agent-REINFORCE在样本效率和搜索性能上优于传统及基于LLM的基线方法，能够有效识别兼顾准确率与推理延迟的最优协作图结构。

Conclusion: Agent-REINFORCE为测试时扩展提供了一种灵活且高效的多LLM协作架构搜索方法，推动了动态、任务自适应的推理优化发展。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [307] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 本文提出了GraphKeeper，一种用于图域增量学习（Domain-IL）的新方法，通过解耦和保持知识来缓解灾难性遗忘问题，在多个图域上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图增量学习方法主要集中在单域内的任务或类别增量场景，而跨多域的图域增量学习（Domain-IL）尚未被探索，尤其是在图基础模型（GFMs）发展的背景下显得尤为重要。

Method: 提出域特定的参数高效微调，并结合域内和域间解耦目标以防止嵌入偏移；引入无偏差的知识保持机制以稳定决策边界；对于域不可见的图，采用域感知分布判别方法获取精确嵌入。

Result: 实验表明，GraphKeeper在不同基准上比次优方法提升了6.5%~16.6%，且遗忘程度极低，并能与多种主流图基础模型无缝集成。

Conclusion: GraphKeeper有效解决了图域增量学习中的灾难性遗忘问题，具备广泛的应用潜力，为图基础模型的持续学习提供了可行路径。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [308] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出一种基于条件标签生成对抗网络的无监督损伤检测与数字孪生方法，无需系统健康状态先验信息，在Z24桥数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的数字孪生方法在测量数据少、缺乏物理知识或损伤状态未知时预测不确定性高，需不依赖先验健康信息的新方法。

Method: 采用条件生成对抗网络框架，利用相同损伤级别的测量数据作为输入，强制模型分别向两种不同损伤状态条件收敛，并比较收敛得分以识别异常；结合SVM分类器和PCA分析生成与真实数据。

Result: 该方法能准确区分损伤与健康状态下的振动测量数据，实现多损伤状态下的数字孪生与模式识别，并支持机器学习数据生成，在Z24桥实测数据上表现良好。

Conclusion: 所提无监督框架为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具，适用于实际工程中健康状态未知的场景。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [309] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 本文研究了门控循环单元、长短期记忆网络和卷积神经网络在小数据集条件下的动态结构载荷识别能力，并与基于物理的残差卡尔曼滤波器（RKF）进行了比较。结果表明，不同方法在不同加载场景下表现各异，而RKF在可识别的物理参数化情况下优于神经网络。


<details>
  <summary>Details</summary>
Motivation: 由于在土木工程应用中测试数据有限或结构模型不可识别，动态载荷识别存在不确定性，因此需要评估深度学习方法在小数据集下的性能，并与传统物理方法进行对比。

Method: 采用门控循环单元（GRU）、长短期记忆网络（LSTM）和卷积神经网络（CNN），在模拟结构振动、加州建筑地震激励以及IASC-ASCE健康监测基准问题上进行测试，并与残差卡尔曼滤波器（RKF）进行比较。

Result: 不同神经网络在不同加载场景下表现各有优劣，RKF在结构模型可识别的情况下整体表现优于神经网络模型。

Conclusion: 在小样本条件下，深度学习方法具有一定载荷识别能力，但在物理可解释性和可识别情形下，传统物理驱动方法如RKF仍具优势。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [310] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier是一个集成LoRA微调与推理的虚拟化多适配器框架，通过统一计算流程和优化内核设计，显著提升吞吐量和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在LoRA微调与推理的统一上存在性能与灵活性不足的问题，缺乏高效的集成系统。

Method: 提出Loquetier框架，包含虚拟化模块以支持共享基模型上的多个适配器，并设计优化的计算流与合并微调和推理路径的内核，实现高效批处理与低开销。

Result: 在三种任务设置下实验表明，Loquetier在纯推理任务中吞吐量最高达到当前最优系统的3.0倍，在统一微调与推理任务中SLO达成率提高46.4倍。

Conclusion: Loquetier有效实现了LoRA微调与推理的一体化，显著提升了性能和灵活性，为大模型的高效适配与服务提供了可行方案。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [311] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出一种混合框架，通过神经ODE学习系统动力学，Transformer生成符号候选不变量，并结合符号-数值验证器从噪声轨迹数据中自动发现守恒量。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中识别守恒律具有挑战性，尤其是在数据噪声较大的情况下，传统方法难以有效发现数学规律。

Method: 结合神经ODE建模系统动态、Transformer生成符号候选守恒量，并设计符号-数值验证器进行验证。

Result: 在典型物理系统上测试，该方法显著优于直接作用于轨迹数据的基线方法。

Conclusion: 解耦的“先学习后搜索”策略在从不完美数据中发现数学原理方面具有更强鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [312] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0 是一个新型开源具身智能大模型系列，参数规模从70亿到720亿，通过深度整合数据能力和自适应学习机制，在性能上显著超越基线模型和同类开源模型，接近领先闭源系统水平。


<details>
  <summary>Details</summary>
Motivation: 旨在将强大智能嵌入多种具身形态，推动开放、可扩展的具身智能发展。

Method: 提出DPPO（深思熟虑策略优化）框架，基于元循环（RL-Refine-Diagnose-SFT）实现AI的主动刻意训练，并使用从40亿+token中提炼的高质量数据进行训练。

Result: 在1000+ A800 GPU集群上训练，每个检查点消耗5万+ GPU小时，性能较基线提升20.3%，优于100B级别开源模型10.6%，在主流具身基准上媲美顶尖闭源系统。

Conclusion: Pelican-VL 1.0 是当前最大规模的开源具身多模态大脑模型，验证了数据与学习机制深度融合对具身智能的关键作用。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [313] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出MeixnerNet，一种使用离散正交多项式（Meixner多项式）的新型谱图神经网络，通过可学习参数和稳定化技术，在保持性能的同时显著提升对超参数变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于连续正交多项式的谱图神经网络（如ChebyNet）在应用于离散图结构时存在理论不匹配，可能导致性能不佳和对超参数敏感。

Method: 引入基于离散正交Meixner多项式的谱滤波器，将多项式形状参数β和c设为可学习，并结合拉普拉斯缩放与逐基LayerNorm实现数值稳定。

Result: 实验表明，MeixnerNet在K=2时性能优于或媲美ChebyNet（3个基准中2个胜出），且对多项式阶数K的变化具有极强鲁棒性，而ChebyNet在此情况下性能急剧下降。

Conclusion: 使用离散正交多项式更契合图的离散本质，MeixnerNet通过可学习参数和稳定化设计，有效提升了模型稳定性与泛化能力，缓解了传统方法对超参数的敏感性。

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [314] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 本研究利用2023年J1联赛的事件和追踪数据，构建了一个基于XGBoost的机器学习模型来预测足球比赛中的防线突破（Line Break），取得了高预测准确性，并通过SHAP分析揭示了影响防线突破的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注射门或得分机会，而较少关注进攻方如何突破对方防线。因此，需要一种量化方法来评估进攻有效性与战术表现中的防线突破行为。

Method: 使用包含189个特征（如球员位置、速度和空间分布）的事件和追踪数据，采用XGBoost分类器构建预测模型，并利用SHAP进行可解释性分析。

Result: 模型表现出色，AUC达到0.982，Brier分数为0.015；SHAP分析显示进攻球员速度、防守线间隙和进攻球员空间分布是关键影响因素；球队被突破概率与失球相关的射门和传中数呈中等正相关。

Conclusion: 防线突破与得分机会密切相关，该模型为理解足球战术动态提供了有效的量化框架。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [315] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 本文提出使用交叉涨落分析基于分数的扩散模型中采样动态，揭示了样本在生成过程中经历离散相变，并利用这些相变提升采样效率和零样本任务性能。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型中样本生成的动态过程，特别是如何从简单分布演化为目标分布，以及如何检测和利用这一过程中的关键转变。

Method: 引入统计物理中的交叉涨落作为中心矩统计量，分析从各向同性正态分布开始的采样轨迹；推导方差保持SDE下交叉涨落的闭式解，用于高效计算反向轨迹。

Result: 发现样本生成过程中存在可检测的离散相变，这些相变可通过交叉涨落的不连续性识别；该方法提升了采样效率、加速类别条件生成与稀有类生成，并改进了图像分类与风格迁移两类零样本任务。

Conclusion: 所提框架将离散马尔可夫链理论、相变分析与现代生成建模统一起来，为理解扩散模型提供了新视角，并在无需网格搜索或重训练的情况下优化生成过程。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [316] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 提出一种动态多专家门控框架，通过内部模型信号在每样本基础上自适应选择最可靠的轨迹预测器，显著降低最终位移误差，提升自动驾驶中复杂场景的预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测模型在长尾复杂驾驶场景下表现不可靠，'单一模型适用所有情况'的范式存在局限，尤其是在安全关键的城市环境中。

Method: 构建一个包含物理信息LSTM、Transformer和微调GameFormer的多专家系统，利用稳定性与不确定性等内部模型信号（元特征）进行专家选择，并将专家选择建模为基于排序学习的成对排序问题。

Result: 在nuPlan-mini数据集上，该方法将最终位移误差（FDE）降至2.567米，相比GameFormer降低9.5%，达到Oracle性能上限的57.8%；在左转场景的开环仿真中FDE也降低约10%。

Conclusion: 自适应混合系统能有效提升安全关键型自动驾驶中的轨迹预测可靠性，为超越静态单模型范式提供了可行路径。

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [317] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种用于套管接箍识别的综合数据预处理与增强方法，结合AlexNet-based神经网络模型，在数据有限的环境下显著提升了接箍信号识别精度，F1分数提升至1.0，并验证了其在真实测井数据中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于实际井下数据稀缺且现有预处理方法不足，深度学习在套管接箍定位（CCL）信号识别中的应用受限，本文旨在填补数据增强方法的空白，提高模型训练效果和泛化能力。

Method: 开发了一套集成于井下工具的CCL信号采集系统以构建数据集，提出包括标准化、标签分布平滑（LDS）、随机裁剪、标签平滑正则化（LSR）、时间缩放和多采样在内的综合数据增强方法，并通过不同配置组合的系统实验评估其对AlexNet-based模型的影响。

Result: 采用所提数据增强方法后，两个基准模型的F1分数从0.937和0.952最大提升至1.0，且在真实CCL波形上的验证表明该方法具有良好的实际适用性。

Conclusion: 标准化、LDS和随机裁剪是模型训练的基础要求，而LSR、时间缩放和多采样能显著增强模型泛化能力，所提方法有效解决了CCL数据稀缺环境下的套管接箍识别模型训练难题。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [318] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: 本文比较了在数据稀缺场景下大语言模型的三种适应方法：监督微调（SFT）、低秩适应（LoRA）和上下文学习（ICL），发现LoRA在技能习得与通用能力保持之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 为解决大语言模型在适应特定任务时面临计算成本高和灾难性遗忘的问题，需系统比较不同适应方法的优劣以指导实际应用。

Method: 对监督微调（SFT）、LoRA和上下文学习（ICL）在数据稀缺条件下进行对比分析，评估其在技能获取和知识整合方面的表现及对基础模型通用能力的影响。

Result: LoRA在最小影响基础模型通用知识的前提下最有效地赋予新技能；SFT虽擅长技能获取但易发生灾难性遗忘；ICL适合事实知识注入但难以掌握复杂技能。

Conclusion: LoRA是在数据稀缺场景下最优的大模型适应策略，研究强调应区分技能获取与知识整合，并权衡任务性能与通用能力的保留。

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [319] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出一种结合概率特征采样和模拟退火超参数优化的随机森林改进框架，显著提升分类准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林在特征利用和超参数配置上存在局限，难以充分捕捉数据中的关键信号，影响分类性能。

Method: 引入基于特征重要性的概率采样机制，并采用模拟退火算法进行动态超参数优化，提升模型对关键特征的关注和参数自适应能力。

Result: 在信用风险评估、物联网异常检测、早期医疗诊断和高维生物数据分析等多个领域均实现了更高的分类准确率，并提供了更清晰的特征重要性解释。

Conclusion: 结合重要性感知采样与元启发式优化能有效增强随机森林的性能，所提框架在多种复杂任务中表现出优越的鲁棒性和泛化能力。

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [320] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: 该研究揭示了城市绿化在降温与增加湿度之间的权衡，发现高生理活性植被在潮湿密集城区可能加剧体感温度，提出基于气候条件的绿化策略阈值。


<details>
  <summary>Details</summary>
Motivation: 理解植被如何在降低地表温度的同时影响体感热（如热指数），以指导城市绿色基础设施的合理规划。

Method: 结合1公里分辨率的热指数重建与可解释性机器学习方法（SHAP和ALE），分析138个印度城市的植被结构（EVI、LAI、fPAR）与气候交互作用。

Result: 当EVI≥0.4、LAI≥0.05时降温增强；但EVI≥0.5、LAI≥0.2、fPAR≥0.5时出现增温反转，尤其在潮湿核心区fPAR≥0.25即提前发生，因湿度上升快于降温。

Conclusion: 明确了植被降温的气候限制条件，提供了针对不同气候区的定量绿化阈值，支持公平且耐热的城市规划。

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [321] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为HeraldLight的双大语言模型（LLM）架构，结合引导提示模块（Herald）用于交通信号控制，通过实时预测队列长度并细化信号决策，显著提升了控制精度、鲁棒性和泛化能力，在多个真实城市数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的交通信号控制方法存在信号时长固定和易产生幻觉错误的问题，而传统强化学习方法在信号配时决策中鲁棒性差且泛化能力不足。

Method: 提出HeraldLight，采用双LLM架构：Herald模块提取上下文信息并预测各相位队列长度；LLM-Agent基于预测结果进行细粒度信号控制；LLM-Critic对Agent输出进行修正以消除错误与幻觉，并利用修正结果进行基于分数的微调以提升性能。

Result: 在Jinan、Hangzhou和New York共224个路口的真实数据集上，HeraldLight相比现有方法平均行程时间减少20.03%，在Jinan和Hangzhou场景下平均队列长度减少10.74%。

Conclusion: HeraldLight通过双LLM协同机制与Herald引导预测，有效解决了LLM幻觉和RL泛化不足的问题，显著提升了交通信号控制的效率与可靠性。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [322] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习与智能粒子群优化的决策模型，用于提升后端集中式冗余供应链的决策效率与规划能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高后端集中式冗余供应链中的决策效率和规划能力，解决资源消耗高、实时调整能力差等问题。

Method: 构建了分布式节点部署模型与最优规划路径，采用卷积神经网络进行特征提取，线性规划捕捉高阶统计特征，结合模糊关联规则调度与深度强化学习优化模型，利用神经网络拟合动态变化，并通过“深度学习特征提取-智能粒子群优化”混合机制实现全局优化与自适应控制决策选择。

Result: 仿真结果表明该模型能够降低资源消耗，提升空间规划能力，在动态环境中增强了实时决策调整、配送路径优化和鲁棒性智能控制性能。

Conclusion: 所提出的深度学习与智能粒子群优化融合模型有效提升了供应链系统的智能化决策水平和运行效率，适用于复杂动态环境下的集中式冗余供应链管理。

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [323] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 本研究探讨了稀疏自编码器（SAE）在揭示和控制大语言模型（LLM）对患者种族与负面概念关联方面的潜力，发现SAE可识别与黑人相关的隐含特征，但通过SAE引导减轻偏见在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗领域的应用可能加剧现有偏见，特别是基于患者种族的偏见，因此需要方法来检测模型是否错误地依赖种族信息进行预测。

Method: 使用Sparse Autoencoders（SAE）分析Gemma-2模型中的隐含特征，识别与黑人相关联的latents，并通过激活这些latents来观察其对模型输出的影响，评估其在缓解偏见方面的有效性。

Result: 发现SAE latents不仅响应如'African American'等合理输入，也响应'incarceration'等有问题的词汇；激活与黑人相关的latents会增加模型将患者标记为'belligerent'的风险；在简单场景中SAE引导可改善偏见，但在更真实的复杂临床任务中效果不佳。

Conclusion: SAE可作为识别临床LLM中问题性人口统计依赖的有用工具，但通过SAE steering缓解偏见在现实任务中效用有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [324] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: PDE-SHARP是一种降低生成PDE求解器计算成本的新框架，通过用低成本的LLM推理替代昂贵的科学计算，在减少60-75%计算评估的情况下实现更高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动方法在测试时生成PDE求解器需要大量样本和高昂的计算资源，尤其对于复杂PDE而言成本过高，因此需要一种更高效的框架来降低计算开销。

Method: PDE-SHARP分为三个阶段：(1) 分析：利用数学思维链进行PDE分类、解类型检测和稳定性分析；(2) 生成：基于前一阶段的数学洞察生成求解器；(3) 综合：通过LLM裁判迭代优化实现的协作式选择-混合竞赛。

Result: 相比基线方法平均需30多次求解器评估，PDE-SHARP平均仅需不到13次评估，求解精度平均提升4倍，并在多种LLM架构上表现出鲁棒性。

Conclusion: PDE-SHARP显著减少了生成高精度PDE求解器所需的计算量，为LLM驱动的科学计算提供了高效且通用的解决方案。

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [325] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 本文提出了一个针对大语言模型（LLM）中敏感信息的实体级成员推断攻击（EL-MIA）新任务，旨在发现细粒度的隐私风险。现有方法只能检测整个输入或文档是否在训练数据中，而无法捕捉到如个人身份信息（PII）、信用卡号等敏感属性的成员风险。为此，作者提出了EL-MIA框架，并构建了一个基准数据集来评估不同MIA技术在此任务上的表现。通过系统比较现有和新提出的两种方法，研究分析了实体级MIA易受攻击性与模型规模、训练轮次等因素的关系。结果表明，现有MIA方法在处理敏感属性的实体级推断时存在局限，而简单的改进方法即可显著提升检测能力，强调需要更强的对抗手段来测试当前威胁模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的成员推断攻击主要关注整体样本是否参与训练，难以识别敏感信息在更细粒度上的泄露风险。随着大语言模型广泛应用，如何保护训练数据中的敏感实体成为关键问题，因此亟需一种能够评估实体级别成员风险的方法。

Method: 提出EL-MIA框架用于审计大语言模型中的实体级成员风险；构建专用基准数据集；在该数据集上对现有MIA技术和两个新方法进行系统比较；分析模型规模、训练轮次等因素对实体级MIA效果的影响。

Result: 实验表明现有MIA方法在识别敏感实体的成员信息方面表现有限；相对简单的新方法即可显著提高检测性能；模型规模和训练次数等表面因素与实体级MIA脆弱性存在一定关联。

Conclusion: 实体级成员推断是LLM隐私评估的重要方向，现有MIA方法不足以应对细粒度敏感信息的风险暴露问题，需设计更强的攻击方法以全面评估和加强模型隐私防护机制。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [326] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种将资源密集型提示优化转化为高效摊销推理任务的新框架，利用预训练的非自回归生成式大模型直接生成条件提示，显著提升效率和迁移性。


<details>
  <summary>Details</summary>
Motivation: 传统的对抗性提示优化计算成本高且效率低，需要寻找更高效的替代方法来生成高质量提示。

Method: 利用建模提示-响应对联合分布的非自回归生成式大模型（如Diffusion LLM），通过条件生成方式直接生成提示，取代逐实例离散优化。

Result: 只需少量并行采样即可生成低困惑度、多样性强且对多种黑盒目标模型具有高迁移性的越狱提示。

Conclusion: 该框架不仅高效，还为红队测试、自动提示优化以及基于流和扩散的大模型应用提供了新方向。

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [327] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 该论文综述了扩散模型在小分子和治疗性肽类药物设计中的应用，比较了其在不同分子表示和设计目标下的适应性，并指出了各自面临的挑战与共同难题，提出通过整合到闭环的DBTL平台来释放其全部潜力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成式建模中表现出巨大潜力，有望加速和变革传统缓慢且昂贵的药物发现过程，但其在不同治疗模式中的应用特点和挑战尚需系统梳理。

Method: 采用系统性综述的方法，比较扩散模型在小分子和治疗性肽两类主要治疗模式中的应用，分析统一的迭代去噪框架如何适应不同的分子表示、化学空间和设计目标。

Result: 发现扩散模型在小分子设计中擅长基于结构生成具有理想物化性质的新配体，但面临可合成性问题；在肽类设计中则聚焦功能序列与从头结构设计，挑战在于稳定性、折叠和免疫原性；两者均受限于评分函数精度、高质量数据稀缺及实验验证需求。

Conclusion: 扩散模型的全面潜力需通过弥合模态特异性差距，并将其整合进自动化的闭环设计-构建-测试-学习（DBTL）平台，从而实现从化学探索向靶向创造新型疗法的范式转变。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [328] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 提出了一种基于多奖励信号的强化学习微调方法，用于优化基础模型生成具有特定属性的对象。


<details>
  <summary>Details</summary>
Motivation: 在文本生成和药物发现等应用中，单一奖励信号可能不足以满足多个评估标准的需求，因此需要一种能够处理多奖励信号的微调方法。

Method: 采用基于强化学习的迭代微调策略，结合多个奖励信号进行模型优化，并提供了理论分析以理解多奖励RL微调的性能。

Result: 在文本、生物序列和小分子生成等多个领域实验表明，该方法相比当前最先进的基线方法更有效。

Conclusion: 所提出的多奖励强化学习微调方法能有效提升基础模型在复杂多目标场景下的生成性能。

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [329] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成学习和可解释人工智能（XAI）的机器学习模型，用于黑色素瘤的早期检测，旨在提高深度学习模型在医疗诊断中的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型在医疗诊断中的‘黑箱’特性导致缺乏可信度和可靠性，因此需要通过可解释性方法提升其临床应用价值。

Method: 采用三种先进的深度迁移学习网络进行集成学习，并结合XAI技术解释模型预测依据。

Result: 所提模型能够高精度检测黑色素瘤，并通过XAI技术增强预测结果的可解释性和可信度。

Conclusion: 结合XAI的集成深度学习模型有助于提高皮肤癌早期诊断的准确性和医生对AI系统的信任，推动AI在医疗领域的实际应用。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [330] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: 本文确定了经典非随机多臂赌博机在专家建议问题下的最小最大最优期望遗憾，证明了与Kale（2014）上界匹配的下界，得出最小最大最优期望遗憾为Θ(√(TK log(N/K)))。


<details>
  <summary>Details</summary>
Motivation: 为了精确刻画带专家建议的多臂赌博机问题的理论性能极限，解决上下界不匹配的问题。

Method: 通过构造特定的对抗性环境并分析算法的最坏情况行为，证明了一个新的下界，并与已有的上界进行比较。

Result: 证明了最小最大最优期望遗憾为Θ(√(TK log(N/K)))，其中K是臂数，N是专家数，T是时间范围。

Conclusion: 该研究解决了经典非随机多臂赌博机在专家建议下的最小最大遗憾问题，实现了理论上的紧确界。

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [331] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: 本文提出了一种基于xLSTM的车辆轨迹预测框架X-TRAJ及其物理感知变体X-TRACK，通过引入运动学约束提升预测的合理性与性能，在高D和NGSIM数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统LSTM在建模长期依赖方面存在局限，且现有轨迹预测模型较少探索xLSTM架构；同时，生成符合物理规律的轨迹仍具挑战。

Method: 设计了基于xLSTM的X-TRAJ模型，并提出X-TRACK，在训练过程中显式融合车辆运动学约束，以生成更真实可行的轨迹。

Result: 在highD和NGSIM数据集上的实验表明，X-TRACK在多个指标上优于当前主流基线模型，验证了其有效性。

Conclusion: xLSTM结构结合物理约束显著提升了车辆轨迹预测的准确性和现实性，X-TRACK为自动驾驶中的轨迹预测提供了新思路。

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [332] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文研究了基于强化学习（RL）的混沌对流流控制方法，提出了一种引入领域知识的RL代理，在层流和混沌流中均显著降低了对流换热，表现出更强的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 混沌对流流在现实系统中广泛存在，但其控制极具挑战性，传统方法在混沌状态下常失效；尽管RL在层流控制中表现良好，但在混沌和湍流条件下的泛化与鲁棒性尚不明确，亟需提升其实用性。

Method: 采用近端策略优化（PPO）算法训练领域信息增强的RL代理，通过在奖励函数中加入促进Bénard胞合并的项来融入物理先验知识，并在多种初始条件和流动状态下进行训练以提升泛化能力和样本效率。

Result: 在层流 regime 中，领域信息RL代理最多减少33%的对流换热，在混沌 regime 中仍可减少10%，优于传统控制器；相比无领域信息代理，该方法实现更稳定的流动、更快的训练收敛，并能在不同流态间泛化而无需重新训练。

Conclusion: 引入物理启发的领域先验可显著增强RL对混沌流动控制的鲁棒性和实用性，推动其向实际应用迈进。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [333] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文研究了大语言模型在不同网络深度下校准能力的演变，发现在后期层中存在显著的置信度校正阶段，并识别出残差流中的低维校准方向，扰动该方向可显著改善校准效果而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型表现出良好的校准能力，但其校准机制主要被认为集中在最后层。本文旨在从网络深度演化的角度，探究校准过程在整个前向传播中的分布特性。

Method: 通过在MMLU基准上分析多个开源权重模型，研究模型在各层中的置信度变化，并识别残差流中的校准方向，评估其对校准指标（如ECE和MCE）的影响。

Result: 发现了后期层中的显著校准阶段，识别出一个低维校准方向，对该方向的扰动可显著提升校准性能，同时保持模型准确性。

Conclusion: 校准是贯穿整个网络前向传播的分布式现象，不仅仅依赖于最终层的结构，为理解大语言模型中的置信度调节机制提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [334] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,Loïc Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 本文研究了在医疗时间序列（如可穿戴PPG数据）上应用深度学习模型时，八种不确定性量化（UQ）技术在心房颤动检测和血压回归任务中的表现，强调了局部校准和适应性评估对小样本可靠性的重要性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床外生理监测中具有潜力，但在实际部署中可能因缺乏可靠的不确定性估计而导致误判，因此需要系统比较不同UQ方法的有效性。

Method: 实现了八种UQ技术，并在两个临床相关任务（AF分类和血压回归）上进行测试，采用全面的评估流程，包括局部与全局校准、适应性及不同尺度下的可靠性分析。

Result: 不同UQ方法的表现差异显著，最优方法取决于不确定性表达形式、评估指标和可靠性尺度；局部校准和适应性评估能提供传统全局指标无法捕捉的实用洞察。

Conclusion: 应根据实际应用场景选择UQ评估标准，特别是在每患者数据有限的情况下，需优先考虑小尺度可靠性并保持预测性能。

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [335] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 提出一种结合模型生成协变量与独立学习倾向得分和结果模型的混合框架，以在生成合成数据时保持因果结构，特别是平均处理效应（ATE）的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM或GAN的合成数据生成方法虽能高保真预测，但常无法准确保留因果参数如平均处理效应（ATE），限制了其在因果推断中的应用。

Method: 采用模型生成协变量并结合距离最近记录过滤监控，同时单独学习倾向得分和结果模型；引入合成配对策略缓解正性违反，并设计利用大量合成样本评估传统因果估计量的协议。

Result: 所提方法在复杂协变量分布下能更准确地保留真实因果结构，显著提升IPTW、AIPW和替代估计等传统方法在合成数据上的因果效应估计准确性。

Conclusion: 该混合框架为使用大语言模型生成支持稳健因果分析的合成数据提供了可行路径，推动LLM在因果推断数据管道中的应用。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [336] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 提出了一种新的解码策略——Pivot-Aware Speculative Decoding（PAD），通过关注影响任务性能的关键token（即pivot token）来提高接受率，在保持效用的同时实现高达2.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统的Speculative Decoding要求输出严格匹配目标模型分布，导致接受率低、速度提升受限；而实际应用中更关注的是任务效用而非采样分布，因此需要一种以效用对齐为目标的新解码策略。

Method: 将解码目标重新定义为匹配目标模型的期望效用，提出Pivot-Aware Speculative Decoding：识别并标记对最终输出效用有显著影响的pivot token，训练一个轻量级分类器来检测这些token，并仅拒绝会导致效用下降的token。

Result: 在多个数据集上验证了该方法的有效性，相比传统SD方法实现了最高达2.5倍的推理加速，同时保持了与目标模型相当的任务效用。

Conclusion: Pivot-Aware Speculative Decoding通过放松分布匹配约束、聚焦于效用保持，提供了一种更高效、更贴近实际需求的推理加速方案。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [337] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏性的统一框架来评估算法公平性，通过将公平性与稀疏性和社会公平联系起来，展示了在多种机器学习任务中的广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性标准在不同机器学习问题中缺乏通用性，难以广泛应用于多样化场景。

Method: 分析了不同稀疏性度量在促进公平性方面的联系与差异，提出了一个基于稀疏性的统一公平性评估框架，并通过多组实验验证其有效性。

Result: 该框架与现有公平性标准一致，能够在多种数据集和偏见缓解方法上有效评估模型公平性，表现出良好的泛化能力。

Conclusion: 通过稀疏性视角为算法公平性提供了新思路，具有推动公平性研究和实际应用的潜力。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [338] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 本文比较了基于模糊推理的ANFIS-FBCSP-PSO方法与深度学习模型EEGNet在运动想象脑电分类中的性能，结果表明前者在个体内分类中更优，后者在跨被试任务中泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象脑电信号分类中准确性和可解释性难以兼顾的问题。

Method: 采用ANFIS-FBCSP-PSO（结合滤波器组共空间模式特征提取与粒子群优化的模糊IF-THEN规则）与EEGNet（直接从原始脑电数据学习时空特征的深度网络）进行对比实验。

Result: 在个体内实验中，ANFIS-FBCSP-PSO准确率为68.58%±13.76%（kappa=58.04%±18.43%），优于EEGNet；在跨被试留一法（LOSO）测试中，EEGNet表现更优，准确率为68.20%±12.13%（kappa=57.33%±16.22%）。

Conclusion: 根据系统设计目标（可解释性或跨用户鲁棒性）选择合适的MI-BCI方法；未来可探索基于Transformer和混合神经符号框架以提升脑电解码的透明性。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [339] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 本文提出了UME-R1，首个将多模态嵌入任务统一到生成范式的通用框架，通过两阶段训练（监督微调与强化学习）实现生成式嵌入，在MMEB-V2基准上显著优于传统判别式嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs的多模态嵌入模型多为判别式，难以充分利用大模型的推理能力进行生成式任务，限制了其在推理驱动生成范式中的应用。

Method: 提出UME-R1框架，采用两阶段训练：第一阶段通过冷启动监督微调使模型具备推理能力并生成判别与生成式嵌入；第二阶段利用强化学习进一步提升生成式嵌入质量。

Result: 在MMEB-V2的78个跨模态任务上验证了生成式嵌入的优越性，发现生成式嵌入性能显著高于传统判别式嵌入，二者结合效果更佳；强化学习有效优化生成嵌入，推理时多次采样可提升下游任务覆盖率（pass@k）。

Conclusion: 生成式嵌入能更好发挥MLLM的推理能力，具有更高性能和可解释性，结合判别式嵌入与强化学习构建了一个可扩展的优化与推理范式，为多模态嵌入提供了新方向。

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [340] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出了一种新的梯度引导采样方法（GGS），在对抗攻击中平衡了利用与探索，提升了跨模型迁移性和攻击强度。


<details>
  <summary>Details</summary>
Motivation: 解决传统动量方法过度强调利用而削弱泛化能力，以及近期方法过度强调探索导致攻击强度下降的问题。

Method: 基于MI-FGSM，引入内迭代随机采样，并利用前一次迭代的梯度引导采样方向，采样幅度由随机分布决定。

Result: 在多个DNN架构和多模态大语言模型上验证了GGS在迁移攻击中的优越性，兼顾了攻击强度和跨模型泛化能力。

Conclusion: GGS有效解决了对抗攻击中利用与探索的权衡难题，显著提升了迁移攻击的性能。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [341] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出了BOOM框架，通过将规划与离策略学习结合，利用引导循环和世界模型提升强化学习的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在线规划虽能提升强化学习的效率和性能，但会导致采集数据与策略行为之间的偏差，影响模型学习和策略优化。

Method: 提出BOOM框架，通过策略初始化规划器，并利用规划器优化动作以实现行为对齐；结合联合训练的世界模型模拟未来轨迹并提供价值目标，采用无似然对齐损失和软值加权机制。

Result: 在DeepMind Control Suite和Humanoid-Bench高维任务上实验表明，BOOM在训练稳定性和最终性能方面达到最先进水平。

Conclusion: BOOM通过紧密集成规划与离策略学习，有效缓解了数据偏差问题，在高维控制任务中表现出优越性能。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [342] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 提出Tree Training范式，通过Tree Packing和Gradient Restoration技术，实现树状结构轨迹的共享前缀计算复用，显著提升大模型代理训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有训练流水线将树状交互轨迹拆分为独立线性片段，导致共享前缀被重复计算，造成计算资源浪费。

Method: 提出Tree Training，包含Tree Packing（复用共享计算）和Gradient Restoration（正确传播梯度）两个关键技术，在前向和反向传播中复用中间结果。

Result: 在多个开源模型上的实验显示，总训练时间最多减少3.9倍，显著提升SFT和RL训练效率。

Conclusion: Tree Training有效解决了代理型LLM训练中树状轨迹的冗余计算问题，大幅提升了训练效率，适用于大规模代理训练场景。

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [343] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: 提出一种结构保持的PINN框架，用于求解非线性KdV方程，通过在损失函数中嵌入质量和哈密顿能量守恒，并采用正弦激活函数，实现了物理一致性、能量稳定性和长期预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在长时间积分中难以保持关键物理不变量，导致物理不一致和能量漂移，因此需要一种能保持物理结构的神经网络方法。

Method: 将质量与哈密顿能量守恒直接嵌入损失函数，并使用正弦激活函数增强频谱表达能力，以更好捕捉KdV孤子的振荡与色散特性。

Result: 在单孤子传播、双孤子相互作用和余弦脉冲初值等案例中，模型准确复现KdV动力学特征，同时保持守恒量；消融实验表明该方法加快收敛、提升稳定性且无需多阶段预训练。

Conclusion: 结合不变量约束优化与正弦表示的PINN能有效实现对哈密顿型PDE（如KdV方程）的能量一致、鲁棒且高效的求解。

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [344] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: 提出一种将Lyapunov条件作为归纳偏置的神经CLF设计方法，实现更稳定的优化和端到端学习，显著提升收敛速度和吸引域。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的CLF方法将Lyapunov条件作为复杂优化约束，难以全局收敛且验证困难，限制了在安全关键系统中的应用。

Method: 将Lyapunov条件融入网络结构作为归纳偏置，设计具有内在稳定性的神经CLF及其控制器，实现端到端联合学习，减少显式约束数量。

Result: 在多个实验中实现了更高的CLF学习收敛率和更大的吸引域（ROA），并通过分析揭示了传统方法在训练过程中成功率下降的原因。

Conclusion: 将物理先验知识以归纳偏置形式嵌入网络结构，能有效提升CLF学习的稳定性与性能，为安全控制提供更可靠的学习框架。

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [345] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 提出一种基于AAL3图谱的ROI引导掩码策略，用于静息态fMRI基础模型的自监督预训练，显著提升ADHD分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法多采用随机区域掩码，缺乏对脑功能区解剖一致性的考虑，限制了模型的可解释性与表征能力。

Method: 在4D fMRI数据上应用AAL3图谱进行区域感知的掩码策略，在自监督预训练中实现语义连贯的脑区重建；使用ADHD-200数据集进行验证。

Result: 相比随机掩码，ADHD分类准确率提升4.23%；归因分析显示边缘系统和小脑区域对模型表征贡献最大。

Conclusion: 基于解剖图谱的区域感知掩码策略能有效提升fMRI基础模型的可解释性、鲁棒性和判别能力。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [346] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: 提出一种基于深度自编码器的异常检测方法，用于企业级ETL数据流中的多种异常检测，具有高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 企业级ETL数据流中常出现延迟、缺失值、重复加载和突变等异常，需有效检测以保障数据质量。

Method: 采用编码器-解码器结构对高维数据进行压缩与重构，利用重构误差衡量异常程度，并在潜在空间引入正则化约束以增强特征稀疏性和分布学习。

Result: 在不同超参数、环境变化和数据特性下，该方法在AUC、ACC、Precision和Recall指标上均表现优异。

Conclusion: 基于深度自编码器的检测机制能有效捕捉ETL数据流的潜在分布模式，准确识别多种异常，为企业数据处理与智能分析提供可靠支持。

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [347] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 本文从理论角度解释了联邦优化中数据异质性导致性能下降的原因，提出异质数据会导致客户端局部最优解不同，进而使全局模型无法完美拟合所有数据，并在训练后期在一定范围内振荡而非收敛到单一最优解。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习算法在理论上能保证收敛且实践中训练稳定，但在数据异质性下性能下降的原因尚不明确，本文旨在填补这一理论空白。

Method: 引入客户端数据异质性导致局部最优解不同的假设，并理论分析其对全局目标函数下界及模型收敛行为的影响，同时通过多任务和多种神经网络结构的实验验证理论结果。

Result: 证明了局部最优解之间的距离会提高全局目标函数的下界，且全局模型在训练后期会在一个区域内振荡而非收敛到单一点，导致拟合能力受限。实验验证了该理论在多个任务和模型上的有效性。

Conclusion: 数据异质性导致的局部最优解差异是联邦学习性能下降的根本原因，该理论为理解非独立同分布场景下的模型行为提供了原则性解释。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [348] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: 提出一种基于变分自编码器（VAE）的传感器校准新方法，通过将潜在空间训练为校准输出，实现了校准与重构的双重功能，并在多传感器气体数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统传感器校准方法通常依赖于线性模型或需要大量标注数据，而实际中传感器数据常具有非线性偏差和复杂分布。因此，需要一种能够同时学习数据表示并完成校准任务的自动化、非线性建模方法。VAE作为一种生成模型，具备学习数据潜在结构的能力，但尚未被广泛用于传感器校准，本文旨在探索其在此任务中的潜力。

Method: 本文提出一种新型VAE实现方式，将传感器原始输入数据送入编码器，训练其潜在空间直接对应于校准后的输出值；解码器则负责重构原始输入，从而实现校准与自编码双重目标。模型在公开的多传感器气体数据集上进行训练与测试，使用均方误差（MSE）等指标评估校准输出和重构输出与真实数据之间的统计相似性。

Result: 实验结果表明，所提出的校准VAE能够有效作为校准模型使用，同时保持良好的自编码性能；校准输出与真实校准数据、重构输出与原始输入之间均具有较高的统计相似性，证明了模型在非线性校准和数据重建方面的双重能力。

Conclusion: 本文验证了VAE在传感器校准任务中的可行性与潜力，提出了一种将潜在空间直接用于校准输出的新范式，实现了校准与特征学习的统一框架，为未来低监督、自适应传感器校准提供了新的研究方向。

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [349] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 本文提出EPIC框架，通过对比学习和概率边界正则化，联合优化推理方法选择的准确性和计算成本，在数学推理任务中实现了更高准确率与更低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选答案并使用聚合策略选择输出，但其假设更多候选答案会带来更高准确率，该假设缺乏理论支持且忽略计算成本。如何有效选择适合查询的推理方法仍是一个关键挑战。

Method: 首先通过理论分析推导固定生成分布和候选数量下的标准聚合方法的准确率边界；然后提出EPIC框架，利用对比学习构建一个共享表示空间，以捕捉模型推理能力和查询-方法兼容性，并将推导出的概率边界作为正则项引入效用驱动的优化目标中，平衡准确率与计算成本。

Result: 在多个数学推理任务上的实验表明，EPIC能够持续选择最优的推理方法，在提升准确率的同时显著降低计算开销。

Conclusion: EPIC通过理论指导的优化框架，有效解决了语言模型中推理方法选择的问题，在准确性和效率之间取得了更好权衡，具有实际应用价值。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [350] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 本文旨在设计、微调、测试和评估多种机器学习模型，用于预测不同时间范围内的PM2.5浓度水平，并比较线性回归、集成方法、深度学习模型（如循环神经网络、Transformer）以及大语言模型在该任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 由于空气污染（尤其是PM2.5）对健康的严重影响，准确预测其浓度有助于提供早期预警并预防相关疾病。

Method: 采用多种机器学习模型，包括线性回归、集成方法、深度学习模型（如RNN、Transformer）和大语言模型，进行PM2.5浓度的多时间尺度预测，并通过实验进行调优与评估。

Result: 系统比较了不同模型在PM2.5预测任务上的表现，评估了各类模型在不同时间范围内的预测精度和稳定性。

Conclusion: 通过对比分析，确定了在PM2.5预测任务中表现最优的模型类型，为未来空气质量预测提供了有效的技术路径和参考依据。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [351] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了一种基于分数布朗运动近似的新生成扩散桥模型FDBM，用于捕捉真实随机过程中的记忆效应和长程依赖性，在蛋白质构象预测和无配对图像转换任务中优于基于布朗运动的基线方法。


<details>
  <summary>Details</summary>
Motivation: 标准扩散模型使用布朗运动，无法捕捉真实随机过程中的记忆效应、长程依赖性和异常扩散现象，因此需要更符合实际的非马尔可夫建模方法。

Method: 利用分数布朗运动的马尔可夫近似（MA-fBM）构建分数扩散桥模型（FDBM），并推导出耦合保持的生成扩散桥及适用于无配对数据的Schrödinger桥损失函数。

Result: 在蛋白质构象预测中FDBM取得更低的C$_\alpha$原子位置RMSD，在无配对图像翻译中获得更低的FID分数，均优于布朗运动基线模型。

Conclusion: FDBM能有效建模非马尔可夫特性，在未来状态预测和无配对数据转换任务中表现出优越性能，为扩散模型提供了更具表达力的替代方案。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [352] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为Lo-Hp的解耦两阶段权重生成框架，通过结合在线和离线策略学习局部优化策略，解决了生成模型中权重生成的过耦合和长视野问题，在多种需要频繁更新权重的任务中表现出更高的准确性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络权重生成方法存在过耦合和长视野问题，限制了灵活性和推理效率，本文旨在通过解耦设计和局部策略学习来解决这些问题。

Method: 提出Lo-Hp框架，采用两阶段解耦方式，引入混合策略子轨迹平衡目标，结合on-policy和off-policy学习局部优化策略，并从理论上证明仅学习局部策略可缓解长视野问题并促进全局最优权重生成。

Result: Lo-Hp在迁移学习、少样本学习、领域泛化和大语言模型适配等任务中展现出优于现有方法的准确性和推理效率。

Conclusion: Lo-Hp通过解耦设计和局部优化策略学习，有效解决了权重生成中的过耦合与长视野问题，提升了生成权重的灵活性、准确性与效率，适用于需频繁更新权重的应用场景。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [353] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出一种基于单智能体强化学习的区域自适应交通信号控制框架，利用DreamerV3模型和探测车辆数据实现高效拥堵缓解。


<details>
  <summary>Details</summary>
Motivation: 传统交通信号控制模型难以捕捉真实交通的复杂性与动态性，无法有效缓解由交叉口排队引起的交通拥堵问题。

Method: 采用单智能体强化学习框架，通过邻接矩阵统一编码路网拓扑、实时队列状态和信号配时参数，利用DreamerV3世界模型学习控制策略，并以队列消散为奖励目标进行优化。

Result: 在SUMO仿真中，该框架在多种OD需求波动（10%、20%、30%）下显著减少队列长度，表现出强鲁棒性和抗波动能力。

Conclusion: 该研究为基于探测车辆数据的智能交通控制提供了新范式，未来将结合随机OD需求波动和应急优化机制提升实用性。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [354] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 本文提出了一种基于Temporal Fusion Transformer（TFT）的多步零售销售预测方法，结合静态门店信息与动态外部变量，实现了高精度且可解释的周销售预测。


<details>
  <summary>Details</summary>
Motivation: 准确的多步零售预测对库存管理和促销活动至关重要，但现有方法在处理长期依赖和外部因素融合方面存在不足。

Method: 采用Temporal Fusion Transformer（TFT）模型，融合静态门店标识与时间变化的外部信号（如节假日、CPI、燃油价格、温度），通过分位数损失生成1至5周的前瞻性概率预测，并利用变量选择网络、静态特征增强和时间注意力机制提升可解释性。

Result: 在2012年固定测试集上，TFT模型每店每周的RMSE为57.9k美元，R²达0.9875；在五折时序交叉验证中平均RMSE为64.6k美元，R²为0.9844，优于XGB、CNN、LSTM及CNN-LSTM等基线模型。

Conclusion: TFT模型在沃尔玛销售预测中表现出优越性能，兼具高预测精度和模型透明性，具有实际应用价值，尤其适用于库存规划和节假日期间的优化决策。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [355] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 提出一种轻量级黑盒红队测试方法，通过迭代反馈和上下文学习发现激活探测器在现实对抗压力下的脆弱性模式，无需微调或梯度信息。


<details>
  <summary>Details</summary>
Motivation: 探索激活探测器在真实黑盒对抗环境下的失效模式，并以最小成本揭示其脆弱性。

Method: 使用现成大语言模型，结合迭代反馈和上下文学习构建红队测试框架，进行黑盒攻击实验。

Result: 发现了可解释的脆弱性模式（如法律术语导致误报、平淡流程语调导致漏报），并在场景约束攻击下验证了探测器仍存在持续漏洞。

Conclusion: 简单的提示驱动红队框架可在部署前预测失效模式，为增强未来探测器提供可行见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [356] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出一种结合快速时间变换器（FTT）和门控循环单元（GRU）的混合模型FTT-GRU，用于工业设备剩余寿命（RUL）预测，在NASA CMAPSS数据集上实现了高效且准确的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LSTM和CNN在建模多变量传感器数据中的全局时序依赖和细粒度退化趋势方面存在困难，需要更有效的模型来提升RUL预测精度与效率。

Method: 提出FTT-GRU模型，将基于快速傅里叶变换（FFT）线性注意力的轻量级Transformer变体FTT与GRU结合，以同时捕捉全局和局部退化模式。

Result: 在CMAPSS FD001上达到RMSE 30.76、MAE 18.97、R²=0.45，CPU延迟仅1.12ms（batch=1），相比TCN-Attention基线RMSE提升1.16%，MAE提升4.00%，训练收敛稳定，消融实验验证了双组件贡献。

Conclusion: FTT-GRU作为一种紧凑型Transformer-RNN混合模型，能够在保持低计算开销的同时实现高性能RUL预测，适用于实时工业预测场景。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [357] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 本文提出了一种以大语言模型（LLM）为核心的贝叶斯网络结构学习统一框架，包括无数据的PromptBN和有数据的ReActBN，显著优于现有方法，尤其适用于低数据或无数据场景。


<details>
  <summary>Details</summary>
Motivation: 传统结构学习方法依赖大量观测数据且计算成本高，现有LLM应用多局限于预处理或后处理，未能将LLM深度整合到核心学习过程中。

Method: 在无数据情况下提出PromptBN，利用元数据通过提示词查询LLM发现概率关系；在有数据情况下提出ReActBN，结合ReAct推理范式与BIC等结构评分进行迭代优化，使LLM全程参与结构发现。

Result: 实验表明，该方法在低数据或无数据场景下显著优于现有的LLM-based方法和传统数据驱动算法，实现了更高效准确的结构学习。

Conclusion: 将LLM置于贝叶斯网络结构发现的核心位置，通过PromptBN和ReActBN实现了数据自由与数据感知的统一框架，提升了结构学习的效率与性能。

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [358] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: 提出了一种结合稀疏参数估计与非参数技术的新框架，用于从数据中发现复杂生物系统中的非线性动力学模型，无需先验函数形式或扩展函数库。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型难以准确描述复杂系统中的某些非线性特性，且Sindy等方法依赖预设函数库，限制了对未知非线性的发现能力。

Method: 将稀疏参数估计与非参数方法相结合，在不扩展函数库的前提下捕捉Sindy无法描述的非线性动态。

Result: 在多个复杂生物现象建模示例中验证了该方法的有效性，能够识别出传统稀疏建模无法捕获的非线性项。

Conclusion: 该框架提升了数据驱动建模的能力，尤其适用于系统生物学中复杂、未知非线性系统的发现。

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [359] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 本研究提出一种以临床医生为中心的框架，用于量化大型语言模型（LLM）在脊柱外科临床决策支持中的幻觉风险，并评估六种主流LLM在30个专家验证病例中的表现，发现DeepSeek-R1整体性能最佳，但推理增强版本并未普遍优于标准版本，表明仅靠扩展思维链不足以确保临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在脊柱外科临床决策中具有潜力，但其产生的幻觉可能威胁患者安全，因此需要系统性评估其在关键临床维度上的表现和风险。

Method: 通过诊断准确性、推荐质量、推理稳健性、输出连贯性和知识对齐五个维度，对六种主流LLM在30个经专家验证的脊柱病例上进行多维度压力测试，并比较标准版本与推理增强版本的表现差异。

Result: DeepSeek-R1总体表现最优（总分86.03 ± 2.08），尤其在创伤和感染等高风险领域；Claude-3.7-Sonnet的扩展思维模式表现低于其标准版本（80.79 vs 81.56）；复杂性增加时推荐质量下降7.4%，而理性、可读性和诊断仅小幅提升。

Conclusion: 仅增强推理过程不足以提升临床可靠性，需结合解释性机制（如推理链可视化）并建立面向安全的验证框架，以确保LLM在手术环境中的安全部署。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [360] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的框架，用于量化和提升职业冰球中的进攻势头和得分可能性（预期进球，xG），通过五阶段流程结合机器学习与因果推断方法，发现采用“最优”事件序列和阵型可显著提高得分潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动的方法改进冰球比赛中的战术决策，弥补传统分析缺乏因果支持的不足，实现对进攻表现的精确量化与优化。

Method: 使用54.1万条NHL赛事数据，构建包含五个阶段的端到端流程：逻辑回归进行微观事件动量加权、梯度提升树估计xG、LSTM建模时序模式、PCA结合K-Means聚类发现空间阵型、X-Learner方法评估因果效应。

Result: 识别出的“最优”序列和紧凑阵型具有显著因果效应，平均处理效应（ATE）为0.12（95% CI: 0.05–0.17，p < 1e-50），相当于得分潜力相对提升15%。

Conclusion: 结构化进攻序列和紧凑阵型能显著提升进攻表现，该框架为教练和分析师提供实时、可操作的战术建议，推动冰球分析向基于因果推理的战术优化发展。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [361] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯视角的统一框架，解释大语言模型（LLM）在上下文学习和激活引导中的行为控制机制，认为两者均通过改变模型对潜在概念的信念来实现，并提出了可预测模型行为的闭式贝叶斯模型。


<details>
  <summary>Details</summary>
Motivation: 旨在统一解释提示和激活引导这两种看似不同的LLM控制方法，探索其是否属于更广泛框架下的特例。

Method: 从贝叶斯角度建模，将上下文干预视为证据积累，激活干预视为先验调整，构建一个可量化预测LLM行为的闭式模型。

Result: 该模型能准确预测多种干预下的LLM行为，解释了如S型学习曲线等已有现象，并预测出干预在对数信念空间中的可加性及行为突变相变等新现象。

Conclusion: 提示与激活控制可被统一为改变潜在概念信念的机制，该贝叶斯框架为理解与预测LLM干预效果提供了系统性方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [362] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 提出了一种基于ℓ_r-范数正则化的算法，能够适应稀疏性，在已知转移的对抗性随机最短路径问题中实现与√(log M)成比例的遗憾，其中M是产生成本的状态-动作对的数量，并证明该结果是最优的；但在未知转移情况下，任何学习者的最小最大遗憾仍与SA多项式相关。


<details>
  <summary>Details</summary>
Motivation: 现有基于负熵正则化的方法在稀疏成本场景下无法充分利用稀疏性带来的优势，其遗憾界依赖于整个状态-动作空间大小SA，而非实际涉及的成本对数量M，因而缺乏自适应性。

Method: 采用一类ℓ_r-范数正则化（r ∈ (1,2)）替代传统的负熵正则化，结合在线镜像下降框架，在已知转移的情况下利用稀疏性结构提升性能，并通过构造下界证明最优性。

Result: 在已知转移设定下，所提方法实现了O(√(log M))的遗憾界，优于传统方法的O(√(log SA))，且被证明是最优的；在未知转移设定下，证明了任何算法的最小最大遗憾至少多项式依赖于SA，表明稀疏性在此情形下作用有限。

Conclusion: ℓ_r-范数正则化能有效适应稀疏结构，显著提升对抗性SSP中稀疏问题的学习效率；但当转移未知时，稀疏性无法改善最坏情况下的遗憾下界。

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [363] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. Díaz-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: 该研究探讨了在极端剪枝条件下受限玻尔兹曼机（RBMs）的生成性能，发现即使剪去80%连接，早期剪枝仍可保留有效子网络，但训练后剪枝无法恢复性能，表明初始结构对稀疏网络至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模增大，计算和环境成本上升，研究旨在探索更简单、稀疏的网络是否能在保持高性能的同时降低成本。

Method: 受彩票假设启发，作者在训练前对RBMs进行不同程度的剪枝，并评估其生成性能，同时比较重新训练与从头训练在相同稀疏度下的表现差异。

Result: 实验显示，训练前剪枝至80%稀疏度仍能保持良好性能，存在一个关键阈值，超过后生成质量急剧下降；而训练后剪枝无法通过再训练恢复性能，且重训练模型表现差于同稀疏度下从头训练的模型。

Conclusion: 为实现高效稀疏网络，剪枝应尽早进行；初始结构对网络能力有持久影响，后期剪枝不可逆地损害性能。

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [364] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: FedRevive是一种异步联邦学习框架，通过无数据知识蒸馏（DFKD）恢复陈旧更新，提升训练速度和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习（AFL）虽提高了可扩展性，但陈旧的客户端更新会导致优化不稳定和收敛困难，限制了性能。

Method: 提出FedRevive框架，结合参数空间聚合与服务器端轻量级DFKD；使用元学习生成器合成伪样本，实现多教师知识蒸馏，并采用混合聚合策略融合原始更新与DFKD更新。

Result: 在多个视觉和文本基准上实验表明，相比异步基线方法，FedRevive训练速度最高提升32.1%，最终准确率提高达21.5%。

Conclusion: FedRevive有效缓解了异步联邦学习中的陈旧性问题，在不牺牲可扩展性的前提下显著提升了收敛速度和模型性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [365] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: 本文提出使用伴随状态法在生成流模型（以扩散模型为特例）中计算梯度，用于气候科学中的敏感性分析，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型计算敏感性分析耗时且昂贵，而现有AI模型虽快但计算梯度仍存在瓶颈。

Method: 应用伴随状态法于生成流模型（如cBottle模型），并提出梯度自洽性检验来验证结果的可靠性。

Result: 在GPU上几小时内完成原本需超级计算机数周的敏感性分析，且结果可靠。

Conclusion: 该方法大幅降低气候敏感性分析的计算成本，简化了关键工作流程，具有广泛应用前景。

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [366] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: 提出KAPPA方法，通过结合KL散度、置信度和熵来指导推理时的路径剪枝，在保持准确性的同时显著降低内存和计算开销。


<details>
  <summary>Details</summary>
Motivation: 标准的多路径推理方法如Best-of-N计算成本高，而现有剪枝方法依赖一致性启发式，无法直接评估分支质量。

Method: 提出KAPPA，结合KL散度、置信度和熵构建评分函数，在推理过程中动态剪枝低分分支，促进探索多样性并减少资源消耗。

Result: 在GSM8K和MATH500上实验显示，相比BoN，KAPPA减少约60%峰值内存和90%总token生成，且对准确率影响极小，尤其提升小模型稳定性。

Conclusion: KAPPA是一种高效、原则性强的推理时剪枝方法，能在保持推理性能的同时大幅降低大模型的推理成本。

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [367] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: 本文提出了一种名为Pub2Priv的新框架，通过利用异构的公开知识生成隐私保护的时间序列数据，采用自注意力机制和扩散模型，在金融、能源和商品交易领域显著改善了隐私与效用之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护数据生成方法通常忽略敏感序列与公开上下文元数据之间的关联，导致隐私与效用的权衡不理想。

Method: 提出Pub2Priv框架，使用自注意力机制将公开数据编码为时间与特征嵌入，并作为条件输入扩散模型以生成合成私有序列；同时引入一种基于可识别性的隐私评估指标。

Result: 实验结果表明，Pub2Priv在多个领域中均优于当前最先进的基准方法，有效提升了隐私-效用权衡。

Conclusion: 通过融合公开上下文信息，Pub2Priv能够更有效地生成高质量且隐私安全的合成时间序列数据，为隐私敏感领域的数据共享提供了新思路。

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [368] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: 本文研究了知识追踪（KT）模型在面对概念漂移和学生群体变化时的性能变化，发现尽管所有KT模型都会出现性能下降，但贝叶斯知识追踪（BKT）在新数据上表现最稳定，而基于注意力机制的复杂模型衰退更快。


<details>
  <summary>Details</summary>
Motivation: 由于在线学习平台环境不断变化，传统的假设学习过程静态的知识追踪模型可能不再适用，因此需要评估KT模型在跨年度数据上的鲁棒性。

Method: 使用五个学年的实际数据，对四种经典KT模型进行纵向性能评估，比较它们在单一年度和跨年度场景下的预测能力。

Result: 所有KT模型在跨年度应用时均出现性能下降；BKT模型表现出最强的稳定性，而更复杂的基于注意力的模型衰退更为显著。

Conclusion: 应重视KT模型中的概念漂移问题，未来的研究需加强纵向评估，BKT虽然简单但在长期应用中更具鲁棒性。

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [369] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1 是一种统一的描述性-预测性-生成性架构，能够在单一编码器-解码器框架内联合优化统计、机理和生成推理。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一种能够同时实现描述、预测和生成能力的通用智能架构，融合可解释性、准确性和创造性。

Method: 采用变分目标在单一编码器-解码器框架中集成描述性表示学习、预测推断和生成合成，并进行联合优化。

Result: 在MNIST上的实验验证了描述重建、预测分类和生成采样可在同一模型中稳定共存。

Conclusion: TRISKELION-1 为连接可解释性、准确性和创造性的通用智能架构提供了蓝图。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [370] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 本文提出了一种融合雷达和卫星数据的多模态临近预报模型，用于提升强降水预测精度，尤其在城市地区表现出显著优于单一雷达方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统雨量计对局地强降雨监测不足，雷达单独使用难以准确预测突发性强降水，亟需更有效的临近预报方法。

Method: 构建一个结合雷达和卫星图像的多模态深度学习模型，用于5、15和30分钟降水临近预报，并评估其在不同强度降水下的预测能力。

Result: 多模态模型显著优于雷达单模态方法，在5分钟预报中对强降雨和暴雨的CSI分别提升了4%和3%，且在较长预报时效下保持更高技能。对2021年德国洪灾事件的定性分析显示该模型能更精确捕捉重灾区降水细节。

Conclusion: 融合卫星与雷达数据可有效提升强降水临近预报精度，尤其在关键的短时预警中具有应用价值，有助于发布及时可靠、挽救生命的预警信息。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [371] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: 提出了一种新的多变量时间序列生成框架STDiffusion，结合扩散模型与可学习的序列分解技术，提升了生成过程的可解释性，并在多个真实数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对趋势和季节性等关键成分的有效建模，难以生成具有真实结构的时间序列数据，限制了生成结果的可解释性和实用性。

Method: 将扩散概率模型与可学习的序列分解技术结合，使用MLP捕捉趋势，通过自适应小波蒸馏进行多分辨率季节性学习，并设计了修正机制以保持生成成分间的一致性。

Result: 在八个真实世界数据集上验证了STDiffusion的优越性，实现了SOTA性能，并成功应用于多窗口长序列生成任务，表现出良好的鲁棒性和泛化能力。

Conclusion: STDiffusion通过解耦趋势与季节性建模并引入修正机制，显著提升了时间序列生成的质量和可解释性，具备广泛的应用潜力。

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [372] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络（PINN）的重构方法，将特征对的搜索转化为双凸优化问题，利用解析最优更新实现快速且收敛性可证明的交替凸搜索（ACS），在数值实验中比传统基于梯度的PINN训练快达500倍，并取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 传统的PINN求解特征值问题速度远慢于经典数值方法，限制了其应用。为了提升PINN在求解特征值问题中的效率和收敛性，需要新的优化框架。

Method: 将特征值与特征函数的联合搜索建模为双凸优化问题，采用交替凸搜索（ACS）策略，并通过解析形式的最优更新规则分别优化特征值和特征函数，从而加速收敛。

Result: 数值实验表明，PINN-ACS在保持高精度的同时，收敛速度比标准梯度下降训练的PINN快达500倍。

Conclusion: 该方法显著提升了PINN求解特征值问题的效率和可靠性，为PINN在系统稳定性、热响应等领域的应用提供了更实用的工具。

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [373] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为PREPO的方法，通过利用提示困惑度和区分rollout之间的相对熵来提升强化学习中可验证奖励的数据效率，在数学推理任务上以最多减少3倍的rollout次数实现了与基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的可验证奖励（RLVR）虽然提升了大语言模型的推理能力，但训练成本高昂，许多rollout对优化贡献较小，因此需要提高数据效率。

Method: 提出PREPO方法，包含两个部分：一是使用提示困惑度作为模型适应性的指标，逐步从易到难进行学习；二是通过区分不同rollout的相对熵来增强差异性，优先选择探索程度更高的序列。

Result: 在Qwen和Llama模型上的数学推理基准测试中，PREPO相比基线最多减少了3倍的rollout次数，同时保持了竞争性的性能，并提供了理论和深入分析解释其提升数据效率的原因。

Conclusion: PREPO通过利用内在数据属性显著提高了RLVR的数据效率，为降低大模型训练成本提供了有效途径。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [374] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 本文提出了一种诊断优先、轻量注入的微调策略，通过在拐点层插入LoRA适配器来恢复被抑制的梯度信号，从而提升预训练Transformer在目标域上的适应能力。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在微调时容易对源域模式过度自信，难以形成新的目标域模式，导致适应能力受限。

Method: 通过分析交叉熵和softmax导致的输出饱和机制，提出一组逐层诊断指标（如注意力熵、激活梯度范数、参数梯度范数和基于共享PCA基的Delta-CKA）来识别具有低注意力熵和剧烈梯度衰减的拐点层，并在这些层选择性地插入LoRA适配器以恢复梯度传播。

Result: 实验表明，在BERT-base从SST-2迁移到Rotten Tomatoes的任务中，过训练初始化下拐点层注入LoRA可提升性能，而欠训练初始化则会下降；当基础特征强时，仅解封拐点层即可促进高层组合适应，弱时则需全路径解封以实现底层重构。

Conclusion: 通过诊断关键拐点层并轻量注入LoRA，可有效缓解梯度抑制问题，提升模型迁移性能，且效果依赖于预训练阶段特征的质量。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [375] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出EraseFlow框架，通过在去噪路径空间中进行探索来实现概念遗忘，利用GFlowNets优化轨迹平衡目标，避免了传统方法的缺陷，在保持图像质量和模型先验的同时有效消除目标概念。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除技术存在图像质量下降、依赖脆弱的对抗损失或需要大量重训练的问题，主要源于对扩散生成过程中去噪轨迹的短视理解。

Method: 将概念遗忘建模为去噪路径空间中的探索问题，使用带有轨迹平衡目标的GFlowNets学习一个随机策略，通过采样完整轨迹而非单一终态来引导生成过程避开目标概念。

Result: EraseFlow在多个基准上优于现有方法，实现了性能与先验保持之间的最优权衡，无需精心设计奖励模型，能泛化到未见概念并避免奖励被攻击。

Conclusion: EraseFlow为扩散模型中的概念擦除提供了一种高效、鲁棒的新范式，解决了现有方法的关键局限性。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [376] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出逻辑感知强化学习（LIRL），通过将低维潜在动作投影到由一阶逻辑实时定义的可接受混合流形上，确保网络物理系统中离散与连续动作联合优化的安全性和全局可行性，无需奖励惩罚调参，在多个场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层方法常牺牲全局最优性，而混合动作空间的强化学习依赖脆弱的奖励惩罚机制，难以保证约束满足，尤其在安全关键的网络物理系统中存在局限。

Method: 提出逻辑感知强化学习（LIRL），在标准策略梯度算法中引入投影机制，将低维潜在动作映射到由一阶逻辑动态定义的可行混合动作流形上，从而在每一步探索中保证动作的可行性。

Result: 在工业制造、电动汽车充电站和交通信号控制等多个场景中，LIRL在性能上优于传统分层优化和最先进的混合动作强化学习方法；以工业装配系统为例，其在工期-能耗目标上最多降低36.47%至44.33%，且始终零约束违反。

Conclusion: LIRL通过逻辑引导的动作投影实现了安全、高效的混合动作优化，具有良好的跨领域迁移能力，为大规模网络物理系统的安全实时优化提供了新路径。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [377] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 本文提出了一种用于追逃博弈的均衡策略泛化（EPG）框架，能够在不同图结构间实现零样本迁移，具备良好的跨图泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在图结构变化时需要重新计算或微调，耗时且影响实时性，而传统方法求解追逃博弈复杂度高，难以扩展。

Method: 提出EPG框架，结合动态规划算法生成单图纯策略纳什均衡作为指导，并设计分组机制与序列模型实现多智能体策略分解，利用距离特征进行跨图训练。

Result: 实验表明EPG在多种未见的真实图结构上具有优良的零样本性能，且在有出口场景中，使用均衡启发式训练的追击者策略可媲美现有最先进方法的微调性能。

Conclusion: EPG框架首次实现了追逃博弈中对图结构和角色（追击/逃避）的双重泛化，显著提升了策略在未知环境下的直接适用性与可扩展性。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [378] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: 本文提出了一种面向边缘计算优化的视觉Transformer模型LL-ViT，通过在Transformer架构中引入基于查找表（LUT）的神经元层，显著减少了模型权重和乘法计算量，提升了FPGA上的能效和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision Transformer在边缘设备（如FPGA）上存在计算、内存和能耗高的问题，而现有的LUT-based网络在视觉任务上性能不佳，因此需要一种兼顾效率与精度的新型架构。

Method: 提出LL-ViT模型，将LUT神经元层集成到Transformer中，重点用LUT替代MLP层中的通道混合器，并采用神经学习方法直接学习LUT函数；同时设计了针对LL-ViT的FPGA加速器。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上分别达到95.5%、78.8%和60.9%的准确率，模型权重减少60%以上，乘法运算减少50%，相比整数量化ViT加速器能效提升1.9倍，延迟降低1.3倍，并在10.9W功耗下优于先前工作。

Conclusion: LL-ViT通过LUT-based架构有效平衡了视觉Transformer在边缘设备上的精度、能效与延迟，为FPGA上的高效视觉推理提供了可行方案。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [379] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 提出一种交互式应用，用于端到端的数据驱动型油管段塞检测，支持实时推理与可视化，提升工业场景下的可解释性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有段塞检测方法多为离线、依赖专家经验且缺乏实时可解释性，难以满足油气管道安全高效运行的需求。

Method: 开发了一个集成数据探索、标注、可配置模型训练与评估、分类结果可视化及基于持久性的实时预警模块的交互式应用程序。

Result: 系统支持从CSV上传到对未见数据进行实时推断的完整流程，具备轻量、便携和易部署特性，并通过快照持久化、可视化标注和实时告警等UI/UX创新提升了用户体验。

Conclusion: 该工具不仅作为研究原型具有传播价值，也可作为实际工业应用，展示了人机协同ML系统在关键流程行业决策中的潜力，适用于更广泛的时序故障诊断任务。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [380] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache是一种分层KV缓存管理系统，通过利用不同KV头的时序稳定性来减少GPU内存使用和计算开销，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务受限于不断增长的KV缓存大小，现有系统难以在不降低精度的情况下有效利用注意力机制中关键token的稀疏性。

Method: 将KV头分为稳定和不稳定两类：不稳定头的全部缓存保留在GPU内存中，稳定头仅保留前K个页面在GPU，其余卸载到主机内存，并周期性重新排序以更新重要页面。

Result: 基于vLLM实现，FlexiCache在长上下文请求中最多减少70%的GPU内存占用，离线吞吐提升1.38-1.55倍，在线token延迟降低1.6-2.1倍。

Conclusion: FlexiCache通过区分KV头的稳定性实现了高效的KV缓存管理，在显著节省内存和提升性能的同时，保持了长上下文和长生成场景下的模型精度。

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [381] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: 本文研究了在量化训练中使用随机舍入（SR）的mini-batch SGD，发现增加批量大小可以补偿反向传播中的低精度，并揭示了权重和激活量化对梯度方差的不同影响。


<details>
  <summary>Details</summary>
Motivation: 量化训练虽然提高了效率，但引入的量化噪声可能影响收敛性和模型准确性，而随机舍入作为一种有理论吸引力的方法，其与其他训练因素（尤其是批量大小）的交互尚不明确。

Method: 结合理论分析与实验，研究了使用随机舍入的mini-batch SGD在量化训练中的表现，分析了批量大小、权重和激活量化对梯度方差的影响。

Result: 理论上证明了增大批量大小可补偿低精度带来的影响；实验验证了该结论，并发现权重和激活的量化对梯度方差具有不同作用机制。

Conclusion: 随机舍入结合较大的批量大小可在保持模型准确性的同时实现高效的量化训练，为低精度训练提供了实用指导。

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [382] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: 提出了一种新的安全强化学习算法KFCPO，结合了基于K-FAC的二阶策略优化和安全感知梯度操作，在保持安全性的同时显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在安全强化学习中，如何平衡奖励最大化与安全约束满足是一个关键挑战。现有方法在优化效率、稳定性或安全边界处理方面存在不足，因此需要一种更高效且自适应的方法来协调性能与安全性的权衡。

Method: KFCPO采用Kronecker-Factored Approximate Curvature（K-FAC）进行高效的自然梯度更新，通过层间闭式近似Fisher信息矩阵提升优化稳定性；引入基于安全边界的自适应梯度操作机制，利用方向敏感的投影融合奖励与成本梯度；并设计小批量KL回滚策略以保证信任域约束。

Result: 在Safety Gymnasium环境中使用OmniSafe的实验表明，KFCPO相比最佳满足安全约束的基线方法，平均回报提高了10.3%至50.2%，展现出更优的安全性与性能平衡能力。

Conclusion: KFCPO通过结合二阶优化与自适应梯度操作，有效解决了安全强化学习中性能与安全的权衡问题，具备高效率、稳定性和实际应用潜力。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [383] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图划分的可解释聚类新方法，能够将解释树拟合到任意给定的聚类或数据集上，并在多个数据集上表现出优于基线的方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏一种通用方法来为任意聚类目标生成无限制的可解释性，需填补这一空白。

Method: 基于谱图划分设计了一种通用的可解释聚类算法，可将解释树拟合到任意非可解释聚类或原始数据；并将先前算法统一到Trevisan（2013）的双图同时优化切割框架下。

Result: 实验表明，该方法在多种数据集上性能优于基线方法。

Conclusion: 所提出的基于谱图划分的方法为可解释聚类提供了一个通用且有效的框架，显著提升了对任意聚类结构的解释能力。

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [384] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了一种公平评估测试时计算（FEval-TTC）协议，用于在不同时间和模型下一致评估基于思维链的大型语言模型方法，并提供开源工具和成本建模以支持公平比较。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型性能和API成本随时间波动，可能导致先前研究结论失效，因此需要一种标准化的评估协议来确保结果的可比性和可靠性。

Method: 设计了FEval-TTC协议，标准化了少样本提示和答案提取流程，支持多种大语言模型和多样的数学与常识推理数据集，并提供了估算每次查询的令牌和美元成本的成本建模方法。

Result: 实现了跨模型和数据集的一致性评估，降低了研究人员的时间和金钱开销，并通过开源项目促进了该协议的广泛应用。

Conclusion: FEval-TTC能够有效应对大语言模型性能和成本波动带来的评估偏差，为测试时计算方法提供了可靠、公平且可复现的评估框架。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [385] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出一种基于范畴对称性的学习框架，用于提升人类活动识别中模型在分布外的稳定性与准确性。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别中的传感器信号会随上下文、运动和环境变化而漂移，导致模型性能下降，因此需要构建对这些变化具有鲁棒性的模型。

Method: 引入一种范畴对称性感知的学习框架，将时间、尺度和传感器层级上的信号变化因素嵌入特征表示结构中，构建在时间偏移、幅度漂移和设备方向变化等现实扰动下保持稳定的模型。

Result: 在UCI人类活动识别基准上，该方法使分布外准确率提升了约46个百分点（约为基线的3.6倍）。

Conclusion: 抽象的对称性原理可以通过范畴等变表示理论转化为日常感知任务中的实际性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [386] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 该论文通过布尔函数分析研究离散时间漏电整合放电（LIF）脉冲神经网络（SNN）的噪声敏感性和稳定性，发现宽LIF-SNN分类器在平均意义上是稳定的，并引入“谱简单性”概念解释其对简单函数的偏好。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN在能效计算方面具有潜力，但其理论基础（尤其是稳定性和鲁棒性）相对薄弱，本文旨在填补这一理论空白。

Method: 采用布尔函数分析方法，量化输入扰动对输出的影响，分析LIF-SNN的傅里叶谱集中特性。

Result: 宽LIF-SNN分类器在平均上具有稳定性，其傅里叶谱集中在低频成分；随机LIF-SNN倾向于简单函数，实验验证了训练网络中的稳定性。

Conclusion: SNN的稳定性可由谱简单性解释，这为理解SNN的鲁棒性提供了新的理论视角。

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [387] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于能量的统一框架来理解Transformer中的注意力机制，将标准softmax注意力视为最小化Helmholtz自由能的特例，并通过引入动量、NAG和牛顿法等优化方法扩展出新的注意力结构。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在各种任务中表现出色，但其内在机制仍不清晰，本文旨在从能量视角出发，深入理解注意力机制的工作原理。

Method: 构建了一个包含全局能量F*、能量函数Ei和梯度下降形式的统一能量框架，将softmax注意力和线性注意力纳入其中，并推广到多头注意力场景，进而基于经典优化算法提出新的注意力变体。

Result: 实验初步验证了所提出的能量框架在设计新型注意力机制方面的有效性与潜力，尤其是基于动量和二阶优化的注意力变体表现良好。

Conclusion: 能量视角为理解和设计Transformer注意力机制提供了有力工具，所提出的框架不仅解释了现有注意力形式，还启发了新的结构设计。

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [388] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出了一种基于对抗性批评者的强化学习方法（RLAC），通过动态识别生成结果中最可能的错误模式来优化生成器和批评者，显著提升文本和代码生成的质量，同时减少验证成本。


<details>
  <summary>Details</summary>
Motivation: 在开放生成任务中，由于评估标准多样且隐含，传统基于规则的强化学习后训练难以扩展，且组合多个评估标准的成本高昂。因此需要一种能动态适应不同提示并降低验证开销的方法。

Method: 提出Reinforcement Learning with Adversarial Critic (RLAC)，使用大语言模型作为动态批评者，识别生成结果中最可能的失败模式（如事实错误或未处理的边界情况），并通过外部验证器进行验证，联合优化生成器和批评者。

Result: 实验表明，RLAC在文本生成的事实准确性和代码生成的正确性上均优于 exhaustive verification 和 reward model 方法，同时减少了所需的验证次数。动态批评者比固定批评者更有效。

Conclusion: RLAC通过动态rubric验证机制，有效解决了开放生成任务中多维度评估带来的可扩展性问题，为强化学习后训练在自由生成任务中的应用提供了可行路径。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [389] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: 提出了一种基于残差神经网络的RhythmiNet模型，结合PPG和加速度信号，通过时序与通道注意力机制实现房颤、窦性心律及其他心律失常的三分类检测，在真实临床噪声环境下表现出更强的鲁棒性和性能优势。


<details>
  <summary>Details</summary>
Motivation: 腕戴式PPG虽可用于连续心律监测，但易受运动伪影和生理噪声干扰，且现有方法多局限于单通道PPG和二分类房颤检测，难以应对临床中多样化的心律失常问题。

Method: 提出RhythmiNet，一种融合PPG与加速度计（ACC）信号的残差神经网络，引入时序与通道注意力模块，实现AF、窦性心律（SR）和其他心律的三分类；测试数据按加速度计运动强度百分位分层以评估不同活动水平下的鲁棒性。

Result: RhythmiNet在macro-AUC上比仅使用PPG的基线模型提升4.3%，比基于手工HRV特征的逻辑回归模型提升12%，验证了多模态融合与注意力机制在噪声环境下的有效性。

Conclusion: 结合PPG与ACC信号并采用注意力机制的深度学习模型能显著提升心律分类的准确性与鲁棒性，尤其适用于真实世界中含运动噪声的连续监测场景。

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [390] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: 提出了一种名为RIGSA的稀疏适配方法，用于缓解语言模型微调中的灾难性遗忘问题，在Textual MNIST任务上验证了其有效性且在多个基准上表现出比QLoRA更少的遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，探索不依赖低秩假设的参数高效微调方法。

Method: 提出RIGSA方法，使用随机初始化的全秩稀疏适配器，结合ReZero风格的门控机制和迭代幅度剪枝进行稀疏化。

Result: 在SmolLM2-1.7B-Instruct模型上，RIGSA能在Textual MNIST任务上学到有效性能，且相比4-bit QLoRA在PIQA、HellaSwag和GSM8k上表现出更少的遗忘，尤其在GSM8k上优势明显，但与随机掩码效果相当。

Conclusion: RIGSA是一种有效的稀疏适配方法，能在不施加低秩约束的情况下缓解灾难性遗忘，尽管参数更多，仍优于QLoRA。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [391] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 本文提出了一种通过容量控制视角解释归一化在深度神经网络中作用的理论框架，证明了归一化层能以指数速率降低网络的Lipschitz常数，从而改善优化和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管归一化方法在实践中有效，但其对优化和泛化影响的理论机制尚不明确，尤其是当网络中包含多个归一化层时。

Method: 通过理论分析Lipschitz常数在有无归一化情况下的变化，建立归一化与模型容量之间的联系，并证明归一化对损失景观和平滑性的指数级影响。

Result: 证明了无归一化网络可能具有关于参数或输入的指数级大的Lipschitz常数，而归一化可将其以指数速率减小，进而平滑损失景观并约束模型有效容量。

Conclusion: 归一化通过控制模型容量，在优化稳定性和泛化性能方面起到关键作用，为归一化方法的成功提供了理论依据。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [392] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 本文研究了在标签数据有限的情况下，利用合成数据评估机器学习模型性能的方法，提出了一种基于理论支持的优化合成数据生成方法，能够更准确、可靠地估计测试误差。


<details>
  <summary>Details</summary>
Motivation: 由于真实场景中获取大量标注数据成本高昂，传统依赖大规模测试集的模型评估方法受限，因此需要探索在少量真实标签样本下仍能可靠评估模型性能的方法。

Method: 通过引入合成数据，建立了考虑合成数据的新泛化误差界，并据此设计了一种理论上有保证的优化合成样本生成方法，特别强调生成器质量对评估效果的影响。

Result: 在模拟和表格数据集上的实验表明，与现有基线方法相比，所提方法能更准确且稳定地估计模型的测试误差。

Conclusion: 利用高质量生成模型产生的优化合成数据，可以在标签数据稀缺的情况下有效提升模型性能评估的准确性和可靠性，为低资源场景下的模型部署提供了新思路。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [393] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan Günnemann*

Main category: cs.LG

TL;DR: NicheFlow是一种基于流的生成模型，用于推断连续空间切片中细胞微环境的时间轨迹，结合最优传输和变分流匹配来建模细胞状态和空间坐标的联合演化。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞水平的方法忽略了组织中细胞状态的协调发育，难以准确建模时空数据中细胞微环境的演化。

Method: 将局部细胞邻域表示为点云，利用最优传输和变分流匹配联合建模细胞状态与空间坐标的演化。

Result: 在多个时空数据集上成功恢复了全局空间结构和局部微环境组成，适用于胚胎发育到脑发育等多种场景。

Conclusion: NicheFlow能够有效捕捉组织发育过程中细胞微环境的动态变化，为解析组织发育和疾病进展提供了新工具。

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [394] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出一种基于互信息的平衡多模态学习框架，通过跨模态知识蒸馏和动态梯度校准缓解模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中模态不平衡问题在生物数据分析等场景中普遍存在，传统方法难以同时利用模态间协同效应并解决模态冲突。

Method: 采用两阶段策略：首先进行跨模态知识蒸馏，用强模态提升弱模态；然后在主训练阶段引入多任务学习机制，基于模态性能和互信息动态调整梯度贡献。

Result: 所提方法能有效缓解模态不平衡，在多个实验中显著提升了多模态模型的整体性能。

Conclusion: 该框架通过量化模态间相互作用并动态优化学习过程，为处理数据稀缺且异质性强的多模态任务提供了有效解决方案。

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [395] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hydra的双头元上下文记忆模块，通过在时间和变量维度上进行二维递归，有效捕捉时间序列中的动态模式和变量间依赖关系，并设计了高效的2D分块训练算法，在多种任务中表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列模型（如Transformer、MLP和线性模型）存在缺乏时间归纳偏置、忽略变量间依赖关系以及长期建模效率低等问题，而线性RNN虽提升效率但仍受限于单序列建模和误差累积，因此需要一种更高效且能同时捕捉时序和变量依赖的模型。

Method: 提出Hydra模型，采用双头元上下文记忆模块，在时间和变量两个维度上进行二维递归，以增强对时序模式的记忆能力；并设计了一种2D分块训练算法，近似实现非并行化递归训练，显著提高训练效率。

Result: 实验结果表明，Hydra在时间序列预测、分类和异常检测等多个任务和数据集上均优于当前最先进的基线模型，同时训练效率提升了约10倍。

Conclusion: Hydra通过引入二维递归结构和高效的训练算法，有效解决了现有模型在时间归纳偏置、变量依赖建模和训练效率方面的局限性，为多变量时间序列建模提供了一个强大且高效的新框架。

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [396] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: 本文提出了ProfBO算法，通过引入马尔可夫决策过程先验和元学习，在极少数函数评估下高效解决黑箱优化问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在药物发现、材料设计等昂贵且耗时的应用中，传统贝叶斯优化需要大量评估，实用性受限，因此需要一种仅用少量评估即可高效优化的方法。

Method: 提出Procedure-inFormed BO（ProfBO）算法，利用来自源任务的优化轨迹构建MDP先验，并将其嵌入先验拟合的神经网络中，结合模型无关的元学习实现对新目标任务的快速适应。

Result: 在真实世界的新冠、癌症基准和超参数调优任务上，ProfBO相比最先进方法能以显著更少的评估次数达到更高质量的解。

Conclusion: ProfBO通过融合程序性知识和元学习，实现了高效的少样本黑箱优化，具备实际部署潜力。

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [397] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 提出了一种基于e-graph结构的符号回归新算法SymRegg，通过避免重复计算等价表达式显著提升搜索效率，同时保持高精度和少量超参数。


<details>
  <summary>Details</summary>
Motivation: 在符号回归中，遗传编程存在大量冗余计算（高达60%），需通过引入e-graph结构减少重复评估，提高搜索效率。

Method: 利用e-graph紧凑存储等价表达式，设计SymRegg算法：从e-graph中采样并扰动表达式，若生成新表达式则插入并扩展其等价形式，避免重复计算。

Result: SymRegg在多个数据集上保持与GP相当的准确性，显著提升搜索效率，减少冗余计算，并仅需极少超参数。

Conclusion: SymRegg通过e-graph有效管理等价表达式，实现了高效且稳定的符号回归，为减少搜索过程中的冗余提供了新思路。

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [398] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文强调在AI代理系统设计中，数据智能能力应成为优先考虑的核心要素，提出了四种关键能力以实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理研究主要关注推理能力，但在实际应用中，代理对数据的获取、处理和持续适应能力至关重要，这一方面尚未得到充分探索。

Method: 提出四个关键能力：主动数据获取、复杂数据处理、交互式测试数据生成和持续适应，以构建更智能的数据感知代理系统。

Result: 为AI代理在动态真实环境中的可靠部署提供了数据层面的设计框架，并倡导从静态基准向动态评估转变。

Conclusion: 数据智能是未来代理系统发展的前沿方向，应在代理设计中赋予其更高优先级。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [399] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于SARIMAX的短期停电预测系统，用于极端天气事件中的电力中断预测，通过系统性特征工程和鲁棒优化策略实现了比基线方法提升8.4%的性能。


<details>
  <summary>Details</summary>
Motivation: 在极端天气事件中，准确预测短期电力中断对于应急响应和资源调度至关重要，但数据不规则性和高维气象特征带来了建模挑战。

Method: 采用两阶段特征工程流程：首先进行数据清洗去除零方差和未知特征，然后通过相关性过滤消除高度相关的预测变量；选取的特征结合时间嵌入、多尺度滞后特征及带滞后项的气象变量作为SARIMAX模型的外生输入，并应用标准化、分层拟合策略、自动降级至ARIMA以及基于历史均值的备选预测来增强模型鲁棒性。

Result: 模型在24小时和48小时预测范围内分别优化，以RMSE为评估指标，最终达到177.2的RMSE，相比基线方法（RMSE = 193.4）提升了8.4%。

Conclusion: 所提出的特征工程与分层优化策略显著提高了SARIMAX模型在极端天气下短期停电预测的准确性与稳定性，验证了其在实际应急管理中的应用潜力。

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [400] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 本研究评估了基于GAN的模型在MIMIC-III数据集上生成的合成医疗数据在保护性人口统计特征上的公平性，发现存在显著的子群体表征不平衡问题；为此提出MedEqualizer——一种模型无关的增强框架，通过在生成前扩充代表性不足的子群体，显著提升了合成数据的人口平衡性。


<details>
  <summary>Details</summary>
Motivation: 确保合成医疗数据在受保护属性上的公平性，避免临床研究和决策中产生偏差或误导性结果。

Method: 使用多种基于GAN的生成模型生成合成数据，并采用对数差异度量评估其在保护性人口属性上的子群体表征公平性；提出MedEqualizer框架，在数据生成前对代表性不足的子群体进行增强。

Result: 发现现有GAN模型生成的合成数据在多个子群体中存在显著的表征不平衡（过代表或欠代表）；引入MedEqualizer后，显著改善了合成数据的人口统计平衡性。

Conclusion: MedEqualizer为实现更公平、更具代表性的医疗数据合成提供了一条可行路径，有助于提升合成数据在临床研究中的可靠性和公正性。

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [401] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: 本研究利用COLET数据集，采用基于窗口的时间特征提取方法，结合机器学习与深度学习模型对认知负荷进行分类，结果表明深度学习模型（尤其是表格架构）在各项指标上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 认知负荷在医疗、心理学和国防等领域日益受到关注，准确分类认知负荷对于实时评估复杂动态任务中的认知状态具有重要意义。

Method: 采用基于窗口的时间分割方法生成特征，并应用机器学习和深度学习模型进行分类，重点比较不同模型在认知负荷分类中的性能。

Result: 深度学习模型在精确率、F1分数、准确率和分类精度上均优于传统机器学习方法，验证了窗口化特征提取的有效性。

Conclusion: 基于窗口的时序特征提取结合深度学习模型能有效提升认知负荷分类性能，具备应用于实时认知负荷监测的潜力。

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [402] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: 本文提出了一种无需反向传播的Mono-Forward（MF）算法，在MLP架构上以更高的分类精度、更低的能耗和更快的训练速度超越了优化后的反向传播方法，挑战了反向传播不可或缺的传统观点。


<details>
  <summary>Details</summary>
Motivation: 挑战反向传播在深度学习中不可或缺的长期假设，探索更高效、可持续的训练方法。

Method: 提出Mono-Forward（MF）算法，基于从Forward-Forward到Cascaded Forward再到MF的演化路径，在相同架构和统一超参数优化下与BP进行公平比较，并通过硬件验证性能。

Result: MF在分类精度上优于优化的BP基线，训练能耗降低最多41%，速度提升最多34%，且展现出更优的泛化能力；同时发现BP-free方法的实际内存开销可能抵消理论优势。

Conclusion: MF是一种在MLP上兼具高性能、高效率和可持续性的反向传播替代方案，为深度学习训练提供了新方向。

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [403] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: 本文提出了一种基于幸福感（happiness）的新型公平性框架，通过效用衡量各群体在决策结果中的获益，兼具人性化与数学严谨性，仅需求解线性规划即可实现高效、可扩展的公平后处理策略，并统一和扩展了多种经典公平性定义。


<details>
  <summary>Details</summary>
Motivation: 现有公平性定义往往缺乏直观的人本解释且难以兼顾数学严谨性与实际可操作性，因此需要一种更直观、可计算且能统一多种标准的公平性框架。

Method: 提出以幸福感作为公平性度量指标，将公平性建模为基于群体效用的优化问题，并通过求解线性规划得到最优的公平后处理策略。

Result: 该方法在数学上是严谨且高效的，只需求解线性程序；实证结果表明其在多种场景下均表现出良好的公平性提升效果，并能统一和扩展多种已知公平性定义。

Conclusion: 基于幸福感的公平性框架提供了一种人性化、可计算且通用的公平决策方法，具有良好的理论性质和实际应用潜力。

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [404] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: 本文挑战了当前AI研究中盲目追求模型规模扩展的范式，提出应以“单位资源能力”为核心目标，通过梯度影响模式优化参数与数据选择，显著提升AI全生命周期的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型发展方式导致资源消耗巨大、环境影响严重且加剧资源不平等，亟需一种更可持续和公平的发展范式。

Method: 提出一个理论框架，利用梯度影响模式识别变压器模型中少数具有重大影响的参数（重尾分布），并通过仅更新高影响力参数、使用梯度范数作为代理指标以及协调参数与数据选择来提高效率；进一步提出两阶段范式：边际回报预训练和影响引导适配，通过梯度蓝图实现开发者与用户间的高效协作。

Result: 研究表明，仅更新高影响力参数在性能-资源比上优于全参数微调；简单的梯度范数可有效识别关键组件；参数与数据的协同选择可带来数量级的资源节省潜力。

Conclusion: 将资源意识嵌入模型开发、适配与评估过程，不仅能大幅降低环境影响，还能促进AI技术的民主化，推动AI向更可持续、更公平的方向发展。

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [405] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: 提出ATLAS，一种无需梯度更新的持续学习系统，通过教师-学生架构和推理时调控实现高效自适应。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖梯度重训练，难以满足部署后智能体实时适应的需求。

Method: 采用双代理架构（教师负责推理，学生负责执行），引入持久化学习记忆存储经验提炼的指导，并通过协调层动态调整运行策略。

Result: 在ExCyTIn-Bench上，ATLAS使用GPT-5-mini达到54.1%的成功率，超过更大的GPT-5 (High) 13%，成本降低86%；跨事件验证显示无需微调即可将准确率从28%提升至41%。

Conclusion: ATLAS实现了基于系统级调控的梯度无关持续学习，为可部署AI提供了高效、可泛化的自适应路径。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [406] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: 提出了一种基于Kolmogorov-Arnold网络的轻量级生成式神经算子模型，用于求解具有随机终端时间的二阶倒向随机微分方程（2BSDEs）族。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在处理无限族2BSDEs时存在参数复杂度高、逼近效率低的问题，需要更高效的模型来提升求解性能。

Method: 引入基于Kolmogorov-Arnold网络的生成式神经算子，利用其结构特性逼近2BSDEs的解算子，并分析其参数复杂度。

Result: 证明了广泛类别的2BSDE族解算子可由该神经算子逼近，并发现一类结构化2BSDE族仅需多项式数量的参数即可实现高精度逼近，优于一般情况下的指数级需求。

Conclusion: 所提模型能高效逼近2BSDEs的解算子，显著降低参数复杂度，为求解随机微分方程提供了新的有效工具。

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [407] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 提出了一种新的在线双层优化框架，通过引入新颖的搜索方向，在无需窗口平滑的情况下实现了次线性随机双层遗憾，并提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有在线双层优化方法依赖于确定性的窗口平滑遗憾最小化，难以准确反映函数快速变化时的系统性能。

Method: 引入新的搜索方向，设计了一阶和零阶随机在线双层优化算法，减少超梯度估计中的查询依赖，联合更新内层、外层变量与线性系统解，并采用零阶方法估计Hessian、Jacobian和梯度。

Result: 理论证明所提算法可实现无窗口平滑的次线性随机双层遗憾；实验在在线参数损失调优和黑盒对抗攻击任务上验证了方法的有效性与高效性。

Conclusion: 该框架在动态环境中更准确地优化双层问题，显著提升效率与适应性，适用于快速变化的机器学习场景。

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [408] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: 本文利用几何不变理论（GIT）和Kempf-Ness定理研究深度线性网络（DLN），证明L²正则化项在平衡流形上最小化，并将训练动力学分解为纤维上的正则化流和平衡流形上的学习流，其中正则化流可通过动量映射精确求解。


<details>
  <summary>Details</summary>
Motivation: 理解深度线性网络中的平衡性机制及其在训练动态中的作用，建立深度学习与线性系统理论中平衡性的统一数学框架。

Method: 应用几何不变理论（GIT）和Kempf-Ness定理分析DLN，通过动量映射分解训练动力学为两个梯度流：正则化流和学习流。

Result: 证明了L²正则化在平衡流形上被最小化，正则化流可被精确求解，且该框架能从模型约简和贝叶斯原理角度解释平衡性。

Conclusion: 该研究为深度学习中的平衡性提供了基于几何不变理论的统一数学解释，并揭示了其与模型简化及贝叶斯推断之间的内在联系。

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [409] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出了一种结合元学习与域适应的统一框架，以提升自动调制分类系统在面对对抗攻击和环境变化时的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制分类中表现优异，但易受对抗攻击和数据分布变化影响，限制了其在动态现实环境中的实际部署。

Method: 采用两阶段策略：离线阶段利用元学习在单一源域的干净和对抗样本上训练模型，提升对未知攻击的泛化防御能力；在线阶段通过域适应对齐模型特征与新目标域，实现少标签下的快速适应。

Result: 该框架显著提高了在对抗攻击和环境变化复合威胁下的调制分类准确率。

Conclusion: 所提方法有效增强了自动调制分类系统在真实动态环境中的鲁棒性与可部署性，为现代AMC系统的实际应用提供了关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [410] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 提出正交函数适应（OFA）方法，提升多处理 uplift 模型的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有双处理 uplift 模型的适配方法在多种数据特征下效果不佳，缺乏鲁棒性。

Method: 基于函数逼近定理，提出正交函数适应（OFA）方法，并将现有方法归类为结构适配和特征适配两类。

Result: 实验表明，OFA 在不同数据特征下显著优于现有适配方法，具有最强的鲁棒性。

Conclusion: OFA 是一种更有效且稳健的多处理 uplift 建模方法，适用于复杂真实场景。

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [411] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文研究了思维链（CoT）是否普遍提升transformer在所有推理任务中的能力，通过分析其记忆能力，表明CoT并不总是增强推理性能，并证明存在CoT无法提升能力的推理任务。


<details>
  <summary>Details</summary>
Motivation: 探讨思维链（CoT）是否在所有推理任务中都能提升transformer的能力，目前这一基本问题尚未解答。

Method: 通过将推理问题建模为记忆问题，分析固定精度transformer在有无CoT情况下的记忆能力，给出记忆有限和无限推理数据集的充分必要条件及参数量的上下界。

Result: 证明了有无CoT的transformer记忆条件互不蕴含，参数需求均为\overline{\Theta}(N)；存在某些推理任务CoT无法提升性能；部分简单无限数据集无法被记忆。

Conclusion: CoT并不能普遍扩展transformer在所有推理任务中的能力，其增强效果取决于具体任务。

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [412] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: 提出了一种基于多任务RF信号分类的鲁棒框架，用于共享频谱环境中识别发射机和分类通信协议，采用CNN实现高精度分类。


<details>
  <summary>Details</summary>
Motivation: 频谱共享日益重要，频谱监测和发射机识别对于执行频谱使用政策、高效利用频谱和网络安全至关重要。

Method: 设计卷积神经网络（CNN），采用多通道输入策略提取信号特征，进行多任务RF信号分类，实现协议分类、基站识别及其联合任务。

Result: 在POWDER平台的RF数据上测试，协议分类准确率达90%，基站识别达100%，联合任务达92%。

Conclusion: 该方法显著提升了现代无线网络中的频谱监测、管理和安全能力，具有广泛应用前景。

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [413] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习与基于代理的模拟的新型框架，用于优化电动汽车充电站的选址和配置。


<details>
  <summary>Details</summary>
Motivation: 现有方法因使用确定性奖励系统而难以应对现实世界中动态和不确定的条件，导致评估成本高且不够真实。

Method: 采用双Q网络的混合强化学习代理，结合确定性因素和模拟反馈的混合奖励函数，通过基于代理的模拟实时估计充电需求并选择最优位置。

Result: 在越南河内的案例研究中，相比初始状态平均等待时间减少了53.28%，优于静态基线方法。

Conclusion: 该可扩展且自适应的解决方案有效应对了现实世界的复杂性，提升了电动汽车基础设施规划效率和用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [414] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine Gorlé*

Main category: cs.LG

TL;DR: WindMiL是一个结合系统数据生成和对称性感知图神经网络的机器学习框架，用于高效、准确地预测低层建筑上的风荷载，相比传统LES方法大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统风洞试验和大涡模拟（LES）方法计算成本高，难以进行大规模参数化研究，亟需一种高效的风荷载预测方法。

Method: 提出WindMiL框架：1）通过符号距离函数插值构建低层建筑屋顶几何的大规模数据集；2）使用462个不同形状和风向的LES模拟案例训练；3）设计具有反射等变性的图神经网络以保证物理一致性。

Result: WindMiL在均值和标准差的压力系数预测中达到高精度（如均值Cp的RMSE≤0.02），在插值与外推评估中表现优异，且在镜像测试下保持96%以上命中率，显著优于非等变基线模型。

Conclusion: WindMiL通过系统数据集与等变代理模型的结合，实现了高效、可扩展且准确的建筑风荷载预测，为可持续建筑设计提供了新工具。

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [415] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: 本文通过严格的几何分析揭示了变量消除方法（如VarPro）如何通过将原问题中的鞍点转化为简化问题中的局部极大值，从而重塑优化景观，显著提升非凸优化的收敛性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管变量消除算法在实践中表现出优越的收敛性和鲁棒性，但其为何能有效应对复杂能量景观（尤其是鞍点问题）尚缺乏理论解释。本文旨在从原理上阐明这一机制。

Method: 基于Hessian惯性和Schur补的数学工具，比较原始与简化优化问题的临界点结构，并在矩阵分解、两参数神经网络和深度残差网络上进行验证。

Result: 证明了变量消除会将原问题中的鞍点转化为简化问题中的局部极大值，从而简化优化景观；在多个任务中验证了该方法对收敛速度和稳定性有显著提升。

Conclusion: 变量消除通过转化鞍点实现景观简化，是一种可指导新一代优化算法设计的强大原则。

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [416] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: 提出KAT-GNN，一种结合临床知识与时间动态的图神经网络框架，用于电子健康记录中的临床风险预测，在多种数据集上达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）具有异质性和时间不规则性，传统方法难以有效建模用于临床风险预测。

Method: 构建模态特定的患者图，并利用SNOMED CT本体和EHR共现先验进行知识增强；使用时序感知Transformer捕捉长期动态。

Result: 在CGRD、MIMIC-III和MIMIC-IV数据集上，KAT-GNN在冠心病预测和院内死亡率预测任务中均取得最优性能，显著优于GRASP和RETAIN等基线模型。

Conclusion: 融合临床知识的图表示与时序注意力机制可有效提升跨临床任务的风险预测性能，具有良好的通用性。

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [417] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种新的在线鲁棒张量恢复算法，利用交通数据的全局时空相关性和局部一致性，有效处理缺失和异常值，显著提升了大规模场景下的恢复精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统批量张量恢复方法计算和存储开销大，难以扩展；现有在线方法在复杂场景中性能下降严重，未能充分挖掘交通数据的内在结构特性。

Method: 将交通数据恢复问题重构为流式框架，提出一种新型在线鲁棒张量恢复算法，同时利用交通数据的全局时空相关性和局部一致性进行恢复。

Result: 在三个真实交通数据集上实验表明，该方法相比最先进的批量方法恢复精度高，且计算效率提升高达三个数量级，能有效处理多种缺失模式及异常值。

Conclusion: 所提方法是一种可扩展且高效的ITS数据质量增强解决方案，具有在大规模动态交通环境中应用的潜力。

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [418] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: 提出STAN模型，一种对抗性时空注意力网络，用于从多变量EEG信号中预测癫痫发作，具有高灵敏度、低误报率和个体适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设固定的先兆期时长或分离处理时空特征，难以捕捉个体差异和细微的先兆动态，因此需要一种能联合建模空间连接性和时间动态的方法。

Method: 设计级联的时空注意力模块，交替提取空间与时间特征，并通过对抗训练增强模型对发作间期与先兆期的判别能力，采用明确的15分钟先兆窗口进行监督。

Result: 在两个EEG数据集上达到最优性能：CHB-MIT头皮数据灵敏度96.6%，每小时误报0.011次；MSSM颅内数据灵敏度94.2%，每小时误报0.063次；同时具备低延迟（45ms）和低内存占用（180MB），适合边缘部署。

Conclusion: STAN能有效捕捉癫痫发作前的时空动态，实现早期预警，且无需个体化训练即可适应不同受试者，具备向其他医疗时序预测任务推广的潜力。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [419] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: 提出了一种轻量级一维卷积神经网络CaptureNet-Deep，用于在纳米孔蛋白质测序中自动检测捕获相，显著提升分析效率。


<details>
  <summary>Details</summary>
Motivation: 手动识别纳米孔测序中的捕获相耗时且依赖专家经验，亟需自动化方法以加速数据分析流程。

Method: 设计并训练了一个轻量级的一维卷积神经网络（1D CNN）CaptureNet-Deep，在降采样的信号窗口中检测捕获相，并采用运行级别数据划分进行模型评估。

Result: CaptureNet-Deep在测试集上达到0.94的F1分数和93.39%的精确率，支持低延迟推理，并将分析时间从数天缩短至三十分钟内。

Conclusion: 轻量级、可解释的深度学习模型能有效实现纳米孔测序中捕获相的实时检测，具有在测序工作流中广泛应用的潜力。

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [420] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 本文探讨了基于数据驱动的Koopman方法在飞行自组织网络（FANETs）中建模无人机轨迹动态的应用，提出集中式与分布式两种方案，以提升动态无线环境下的通信预测与网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在静态无线环境中表现良好，但在高度动态的FANET等场景中效果受限，亟需能应对快速变化拓扑的新型建模方法。

Method: 利用Koopman算子理论，提出集中式和分布式两种数据驱动方法，对FANET中无人机轨迹进行建模，并预测SINR以评估通信质量。

Result: 所提方法能准确预测连接与隔离事件，有效识别通信中断情况，从而支持无人机基于预测结果优化传输调度。

Conclusion: Koopman方法在动态无线网络建模中具有潜力，可提升FANET的通信可靠性与自主决策能力。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [421] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出LSHFed框架，通过局部敏感哈希实现高效通信且鲁棒的联邦学习，有效防御推理和投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在缺乏信任的环境中易受推理攻击和投毒攻击，现有防御方法存在通信计算开销高或检测精度低的问题。

Method: 提出LSHFed框架，核心是LSHGM机制，利用多超平面局部敏感哈希将高维梯度映射为紧凑的二进制哈希码，基于不可逆哈希形式进行恶意梯度检测与过滤。

Result: 实验表明，即使50%参与者为共谋攻击者，LSHFed仍能保持高性能，并相比全梯度方法在梯度验证通信上减少高达1000倍。

Conclusion: LSHFed在保障隐私的同时显著提升了联邦学习的鲁棒性和通信效率，有效平衡了安全性、性能与开销。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [422] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Álvaro Vázquez Rodríguez,Manuel Fernández-Veiga,Carlos Giraldo-Rodríguez*

Main category: cs.LG

TL;DR: 提出一种基于去噪扩散概率模型（DDPM）的云原生网络功能（CNF）放置新框架，将放置问题建模为从图到分配矩阵的生成任务，结合图神经网络和约束感知损失，在多种拓扑中实现快速且可行的解决方案，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CNF放置方法（如混合整数非线性规划、启发式算法和强化学习）在可扩展性、约束处理和泛化能力方面存在局限，难以满足5G/6G网络中复杂的服务链部署需求。

Method: 将CNF放置问题编码为异构图，并采用基于图神经网络的去噪器，在DDPM框架下迭代优化带噪声的CNF-云资源分配矩阵；通过在损失函数中引入资源、带宽和延迟等约束相关的损失项，使模型学习可行解空间。

Result: 在多种网络拓扑上的实验表明，该方法能始终生成满足约束的可行解，推理速度比MINLP求解器快几个数量级，同时具备良好的泛化能力。

Conclusion: 基于扩散模型的生成式方法为受约束的网络嵌入问题提供了新的解决路径，有望推动分布式云原生网络功能的高效、可扩展编排。

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [423] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 本文提出了一种名为MiniFool的新算法，用于在粒子和天体粒子物理中对基于神经网络的分类任务进行物理启发式的对抗攻击测试。


<details>
  <summary>Details</summary>
Motivation: 开发该算法的动机是测试神经网络在高能物理实验中的鲁棒性，尤其是在IceCube中搜索天体物理τ中微子的应用背景下。

Method: MiniFool通过最小化一个结合了χ²检验统计量和目标得分偏差的损失函数来生成对抗性扰动，其中检验统计量基于实验不确定性量化扰动的概率。

Result: 在MNIST、CMS开放数据等多个数据集上验证了算法的有效性，发现分类翻转的可能性与原始分类正确与否相关，并可通过调节攻击参数评估网络决策的鲁棒性。

Conclusion: MiniFool具有良好的通用性和物理可解释性，可用于评估标记和未标记实验数据下分类模型的鲁棒性。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [424] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,Darío González-Ferreiro,Carlos Beis-Penedo,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 本文提出了一种可验证的分割学习框架，通过引入zk-SNARK证明来确保客户端和服务器端在前向和反向传播中的计算正确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 分割学习在协作训练中具有潜力，但缺乏对各方计算过程的正确性和诚实性验证机制。

Method: 在分割学习架构中集成zk-SNARK零知识证明，对客户端和服务器端的前向与反向传播过程生成证明并进行验证。

Result: 所提出的框架实现了双方计算的可验证性与正确性；与基于区块链但无零知识证明的系统相比，zk-SNARK方案具备可验证性，而区块链方案虽轻量但不可验证。

Conclusion: zk-SNARK能够有效增强分割学习的安全性与信任度，是实现可验证协作学习的有效途径。

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [425] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 提出了一种基于距离的多样性正则化方法，通过重新参数化直接优化多模态策略，在多目标达成和生成式强化学习等多样性关键任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法难以表达复杂的多模态决策分布，限制了其在需要高决策多样性的场景中的性能。

Method: 将现有不可行的多模态策略纳入统一框架，并通过重参数化实现策略梯度的直接优化；提出一种无需显式计算决策概率的距离-based多样性正则化方法。

Result: 在多目标达成、生成式RL和MuJoCo基准测试中均表现出良好的性能与决策多样性，尤其在少样本鲁棒性方面优势明显；实验证明摊销演员具有强大多模态表达能力和高性能。

Conclusion: 所提方法有效平衡了多模态策略的学习效率、性能和多样性，推动了在线多模态强化学习的发展。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [426] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: 本文研究了在LeNet神经网络上应用“机器遗忘”技术以抵御FGSM对抗攻击的效果，结果表明该方法能显著提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击（如FGSM）会通过添加微小扰动误导模型，威胁机器学习系统的安全性，因此需要提升模型的鲁棒性。

Method: 采用机器遗忘技术，通过对原始数据重新训练来“遗忘”FGSM攻击样本，从而增强LeNet模型的防御能力。

Result: 实验表明，应用unlearning技术后，LeNet模型对FGSM攻击的鲁棒性显著提高。

Conclusion: 机器遗忘是一种有效应对FGSM对抗攻击的方法，可提升LeNet等模型的安全性和可靠性。

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [427] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出了一种实域完全原地计算的FFT框架rdFFT，通过隐式复数编码消除中间缓存，有效降低训练内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有FFT和rFFT无法实现真正的原地计算，存在维度不匹配问题，需要额外内存分配。

Method: 利用蝶形操作对称性和频域共轭特性，设计隐式复数编码方案，保持输入输出内存空间一致。

Result: 在多个自然语言理解任务上验证了方法的有效性，显著降低了训练内存成本。

Conclusion: rdFFT实现了真正的实域原地FFT计算，为频域轻量级适配提供了有前景的方向。

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [428] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 提出一种基于图的方法，利用低维卫星嵌入预测撒哈拉以南非洲的财富指数，结合空间关系和模糊标签损失，提升贫困制图的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于DHS调查数据空间覆盖有限且坐标被随机偏移，导致难以生成精确的细粒度贫困地图。

Method: 采用基于图的学习方法，利用AlphaEarth卫星图像的低维嵌入，并引入概率性的“模糊标签”损失来建模 surveyed 与未标记地点之间的空间关系。

Result: 在37个DHS数据集上的实验表明，相比仅使用图像的基线模型，引入图结构略微提升了预测精度。

Conclusion: 紧凑的地球观测嵌入结合图结构有助于提升大规模社会经济指标预测的准确性和泛化能力。

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [429] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出CG-FKAN方法，通过稀疏化扩展网格并在通信预算下仅传输关键系数，有效降低联邦学习中KAN的通信开销，同时保持优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于KAN的联邦学习研究忽略了网格扩展带来的通信开销问题，而该问题在建模复杂函数时尤为突出。

Method: 提出CG-FKAN，通过稀疏化处理扩展网格，并在通信约束下仅传输必要的系数以压缩通信量。

Result: 实验表明，在通信受限场景下，CG-FKAN相比固定网格KAN最高可降低13.6%的RMSE，并推导了其逼近误差的理论上界。

Conclusion: CG-FKAN有效平衡了通信效率与模型性能，提升了KAN在联邦学习中的实用性和可扩展性。

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [430] [The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 提出了一种直接在输入空间中定义的标量曲率度量——曲率率λ，通过高阶输入导数的指数增长速率来衡量神经网络的泛化性与鲁棒性，并展示了其在训练过程中的可预测演化及通过曲率率正则化（CRR）进行调控的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有参数空间中的曲率度量（如Hessian特征值）存在计算昂贵、对重参数化敏感且难以从函数角度解释的问题，因此需要一种更直观、稳定且可解释的输入空间曲率度量。

Method: 定义输入空间中的曲率率λ为高阶导数范数log ||D^n f||随n变化的斜率，利用其作为函数光滑性的指标，并提出曲率率正则化（CRR）方法，在训练中直接控制λ。

Result: 实验表明λ在Two Moons和MNIST等任务中能有效反映决策边界的高频结构演化，CRR相比SAM在保持准确率的同时提升了模型置信度校准和输入空间平坦性。

Conclusion: 曲率率λ是一种紧凑、可解释、与参数化无关的功能平滑性描述符，为理解神经网络的泛化与鲁棒性提供了新的基于微分动态的视角。

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [431] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: 提出了一种新的图曲率度量——有效电阻曲率（Effective Resistance Curvature），利用节点对之间的有效电阻来量化图边上的消息传递难易程度，相比Ollivier-Ricci曲率在保持几何表达能力的同时显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: Ollivier-Ricci曲率虽具有良好的几何可解释性，但计算复杂度高，难以应用于大规模图数据，因此需要一种更高效的替代方法。

Method: 引入基于有效电阻的图曲率度量，替代原有的最优传输距离，用于衡量节点间的局部几何结构，并理论证明其较低的计算复杂度和与Ollivier-Ricci曲率的可替代性。

Result: 在多种GNN任务上实验表明，该方法在性能上与Ollivier-Ricci曲率相当，但计算开销显著降低。

Conclusion: 有效电阻曲率是一种高效且具有几何意义的图曲率度量，适用于大规模图神经网络中的结构建模。

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [432] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: 本文提出了DAMBench，首个面向真实大气条件的大规模多模态数据同化基准，集成高质量背景场与真实的多源观测数据（如气象站和卫星影像），提供统一的评估协议并验证了多种数据驱动模型，推动数据同化研究的可复现性与公平比较。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在数据同化中的研究受限于过于简化的仿真环境和缺乏标准化评测基准，难以反映真实复杂场景下的性能，亟需一个真实、多模态、大规模的基准来推动领域发展。

Method: 构建DAMBench，整合先进的预报系统提供的背景场与真实世界多模态观测（气象站、卫星图像），统一空间网格与时间对齐，并设计标准化训练、验证与测试流程；提出轻量级多模态插件以增强基础模型；对代表性数据驱动方法（如潜在生成模型与神经过程框架）进行基准测试。

Result: DAMBench实现了在真实大气条件下对数据同化模型的系统性评估，实验表明引入真实多模态观测能显著提升模型性能，即使是简单基线模型也能通过所提插件获得改进。

Conclusion: DAMBench为数据同化领域提供了首个真实、大规模、多模态的基准平台，支持可复现、公平且可扩展的研究，有助于推动深度学习在复杂现实场景中的应用。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [433] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: 提出一种基于脉冲神经网络的持续学习方法CLP-SNN，可在边缘设备上高效应对开放世界中的数据分布变化和新类发现，具备高能效和快速响应优势。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，边缘设备上的AI系统面临数据分布变化和新类别出现时难以持续学习的问题，传统方法在功耗和效率方面存在瓶颈。

Method: 提出CLP-SNN，一种用于持续学习原型的脉冲神经网络架构，结合事件驱动、时空稀疏的局部学习、自归一化的三因素学习规则，以及神经发生与元可塑性机制，并在Intel Loihi 2芯片上实现。

Result: 在OpenLORIS少样本学习实验中，CLP-SNN在无需回放的情况下达到与回放方法相当的准确率，且速度提升70倍（0.33ms vs 23.2ms），能耗降低5600倍（0.05mJ vs 281mJ）于边缘GPU上的最佳OCL方法。

Conclusion: 通过算法与神经形态硬件的协同设计，CLP-SNN突破了传统边缘AI系统在准确性与能效之间的权衡，为未来开放环境下的边缘智能提供了可行方案。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [434] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: 提出了一种新的多尺度层次图融合网络MS-HGFN，用于更准确地预测股票市场走势，通过建模股票间的复杂依赖关系和捕捉多尺度时空特征，在真实数据集上表现出优于传统和先进模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理股票间相关性时忽视了单个股票内部属性的细微模式以及多尺度采样过程中对粗粒度和细粒度特征的注意力偏差问题。

Method: 设计了一个层次化GNN模块，通过学习不同时间尺度下的内外属性模式构建动态图，并采用自上而下的门控机制融合多尺度时空特征。

Result: 在中美股市的真实数据集上实验表明，MS-HGFN相比其他模型最高提升了1.4%的预测准确率，并在收益模拟中表现出更强的稳定性。

Conclusion: MS-HGFN能有效捕捉股票市场的复杂时空依赖关系，显著提升预测精度与投资模拟稳定性，具有实际应用潜力。

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [435] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 提出基于Hadamard卷积变换的特征提取方法，兼容现有技术，显著提升时间序列分类的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法计算复杂度高或核选择效率低，需要更高效且兼容的方法。

Method: 利用Hadamard矩阵的行列向量作为卷积核，通过扩展不同长度的核进行特征提取，并结合核正交性提升效率和鲁棒性。

Result: 在UCR数据集上F1分数比ROCKET提升至少5%，训练时间比miniROCKET缩短50%，且可在超低功耗嵌入式设备部署。

Conclusion: 该方法在保持兼容性的同时，显著提升了时间序列分类的计算效率和分类性能，适用于资源受限场景。

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [436] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 本文提出了一种用于多模态嵌入学习的并行解耦框架（PDF），利用多模态大语言模型（MLLM）的可操控性，通过可学习前缀生成多个并行路径和嵌入，并结合互信息最小化与对比监督，实现更丰富、鲁棒的语义覆盖，在多个基准上显著提升性能且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入模型受限于单输入、单嵌入、对比监督（SSC）范式，难以充分挖掘多模态大语言模型的能力，无法有效表达输入的多面性。

Method: 提出并行解耦框架（PDF），在共享MLLM主干上使用不同的可学习前缀生成多个并行路径，获得并行嵌入；采用互信息最小化（MIM）增强多样性，并结合每条路径的对比学习保持语义对齐。

Result: 在MMEB基准上验证了PDF的有效性，不同规模和分辨率下均有显著提升，例如VLM2Vec-LLaVA-1.6-LR模型提升+8.9%（7B），VLM2Vec-Qwen2VL模型提升+4.2%（2B）和+3.1%（7B）；2B模型仅用一半计算预算即超过基线+2.6%。

Conclusion: PDF通过引入并行解耦机制和双目标优化，有效释放了MLLM在嵌入学习中的潜力，实现了高效、强表达力且可泛化的多模态嵌入空间。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [437] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*Natália Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: 提出一种物理信息驱动的机器学习框架，通过构建物理动机特征并结合特征选择，显著提升复合材料冲击能量估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于数据稀疏、信号噪声、非线性动力学和逆问题不适定性，难以准确估计航空航天复合材料中的低速冲击能量。

Method: 结合观测偏差设计物理动机特征，从时域、频域和时频域提取响应特征，并通过统计显著性、相关性过滤、降维和抗噪能力进行结构化特征选择，最后使用全连接神经网络进行能量预测。

Result: 所提方法在实验数据上验证，相比传统时序方法和纯数据驱动模型，冲击能量预测误差降低三倍，且特征具有物理可解释性。

Conclusion: 该物理信息驱动的框架能有效提升冲击能量估计的精度与可解释性，适用于复合材料结构健康监测。

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [438] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: 本文提出了一种通过过参数化梯度下降（GD）进行Toeplitz结构协方差估计的新方法，模型将协方差矩阵表示为复正弦波的和，并通过GD优化。研究发现适度的过参数化可确保全局收敛，且所提出的加速GD变体在固定频率仅优化幅度时具有良性优化景观。实验表明该方法在精度、简洁性和可扩展性方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用复杂的优化算法来求解Toeplitz结构下的协方差估计问题，而近年来深度学习中过参数化梯度下降的高效性启发作者重新审视该问题，探索更简单且可扩展的方法。

Method: 将P×P协方差矩阵建模为K个具有可学习参数的复正弦波之和，并采用梯度下降进行优化；引入带有不同学习率的加速GD变体，分别优化幅度和频率；在频率固定时分析仅优化幅度的优化景观。

Result: 当K=P时GD可能陷入次优解，但K=2P或4P时能稳定实现从随机初始化的全局收敛；理论上证明固定频率下任意平稳点均可恢复真实协方差；数值实验显示该方法在挑战性场景下性能达到或超过现有最先进方法。

Conclusion: 过参数化梯度下降为Toeplitz协方差估计提供了一种简单、高效且可扩展的解决方案，适度过参数化是实现良好优化性能的关键。

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [439] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是首个与优化LLM服务架构协同设计的多智能体Graph-CoT系统，通过分解推理任务和优化服务执行，显著提升准确性、降低延迟和令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-CoT方法因单代理单体提示、重复上下文重编码和服务效率低下，导致准确率低、开销大、延迟高、吞吐量低。

Method: 将推理分解为多个专用智能体（分类、推理、动作生成、图检索），实现分支推理和选择性上下文共享，并设计图感知的LLM推理机制，包括图特定KV缓存管理、优先级驱逐和流水线执行。

Result: 相比最先进的Graph-CoT基线，GLM最高提升38%准确率，减少95.7%令牌成本，降低90.3%推理延迟，吞吐量提高15.1倍。

Conclusion: GLM通过多智能体协同与服务架构优化，显著提升了图结构知识上的复杂推理效率与可扩展性，适用于大规模实际应用。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [440] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种名为XTNet的新网络架构，用于多类别、多值处理效应估计，通过动态掩码机制捕捉处理间的交互作用，并提出了适用于复杂干预场景的评估指标MCMV-AUCC。实验表明XTNet在合成和真实数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的因果推断方法在处理多类别、多值干预时受限于二元或单一类型处理假设，难以建模复杂的交叉效应，且缺乏有效的评估框架。

Method: 提出XTNet，采用分解策略将基础效应与交叉效应分离，并引入带有动态掩码机制的交叉效应估计模块，以灵活捕捉不同处理之间的交互作用。

Result: 在合成和真实世界数据集上的实验显示，XTNet在排序准确性和效应估计质量方面 consistently 优于现有最先进方法，真实世界的A/B测试也验证了其有效性。

Conclusion: XTNet能够有效应对多类别、多值处理下的因果推断挑战，具备良好的可扩展性和实际应用价值。

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [441] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 本文提出了一种基于卡尔曼滤波的自然梯度下降贝叶斯近似方法，用于CLIP模型的微调，提升了在分布内和分布外数据上的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶微调方法在收敛速度、超参数敏感性和分布外泛化方面存在不足，尤其是在标注数据稀缺的情况下，难以有效优化CLIP等视觉语言模型。

Method: 提出一种结合卡尔曼滤波的贝叶斯近似自然梯度下降方法，利用损失曲率信息和贝叶斯推断进行CLIP模型微调，避免直接计算昂贵的费雪信息矩阵逆。

Result: 在多个图像分类数据集上实验表明，该方法在分布内任务中性能优于或相当于现有最先进方法，在分布外鲁棒性方面表现更优。

Conclusion: 这是首次成功将卡尔曼滤波应用于CLIP模型微调的工作，为视觉语言任务提供了更高效、鲁棒且具备不确定性量化的优化方案。

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [442] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种联合优化用户关联与资源分配（UARA）的统一框架，以支持高效的并行推测解码，显著降低移动边缘计算中大语言模型推理的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的移动边缘计算环境中，现有推测解码方法存在通信开销和异步延迟问题，亟需高效的支持机制。

Method: 提出一个联合优化用户关联与资源分配（UARA）的统一框架，并采用多智能体深度强化学习算法求解该问题。

Result: 实验基于Sionna仿真器进行，结果表明该方法最高可减少28.0%、平均减少23.7%的端到端延迟，且不牺牲推理精度。

Conclusion: 所提方法有效提升了移动边缘计算系统中大语言模型推理的效率和可扩展性，实现了低延迟、高精度的服务支持。

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [443] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文研究了在高度不稳定的边缘环境中联邦学习中模型准确性与公平性之间的权衡，通过实验评估了多种基于公平性的客户端选择算法在不同数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 由于边缘环境的动态资源可用性和异构客户端能力，实现高准确性和公平的客户端参与具有挑战性，因此需要探究准确性与公平性之间的权衡。

Method: 对RBFF和RBCSF等基于公平性的客户端选择算法进行了广泛的实证评估，并与随机和贪婪选择方法在CIFAR10、FashionMNIST和EMNIST三个基准数据集上进行比较，评估指标包括公平性、模型性能和时间开销。

Result: 更公平的客户端选择算法虽然提升了各客户端的参与机会，但在不稳定环境中可能导致全局训练速度变慢。

Conclusion: 公平性提升以牺牲训练速度为代价，在高度波动的边缘环境中需权衡公平性与效率，未来需进一步优化公平客户端选择策略。

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [444] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: 提出一种基于生成合成数据而非共享模型参数的分布式学习方法，通过博弈论框架实现异构本地模型和数据的有效协同学习。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中由于本地模型和数据异质性带来的挑战，避免直接共享模型参数的问题。

Method: 利用生成模型产生合成数据进行交换，将本地模型视为可生成数据的黑箱，并结合半监督学习与博弈论方法，构建合作博弈框架，适用于不同概率空间的异构模型。

Result: 证明了指数族局部模型存在唯一的纳什均衡，所提方法能收敛至该均衡，并在图像分类和条件生成任务上验证了其有效性。

Conclusion: 该方法有效应对了分布式学习中的模型与数据异构性问题，具有理论保证并在标准视觉基准上表现出良好性能。

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [445] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: 提出首个基于超图神经网络的QLDPC解码器HyperNQ，利用超边捕捉高阶稳定子约束，在伪阈值以下显著优于传统BP和GNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统BP解码在短环存在时收敛性差，GNN受限于成对交互，难以捕捉高阶相关性。

Method: 设计基于超图神经网络（HGNN）的两阶段消息传递解码器HyperNQ，利用超边建模高阶稳定子约束。

Result: 在伪阈值以下，HyperNQ相比BP降低逻辑错误率最多84%，相比GNN方法降低50%。

Conclusion: HyperNQ通过高表达力和紧凑结构，在QLDPC解码中实现了当前最优性能。

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [446] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种网络化专家混合系统（NMoE），结合联邦学习框架，实现边缘设备上高效、隐私保护的协作推理与训练。


<details>
  <summary>Details</summary>
Motivation: 大型人工智能模型对计算资源和数据的需求与边缘设备资源受限之间存在矛盾，难以在边缘进行训练和部署。

Method: 设计了NMoE系统，通过将任务分发给具有相应专长的邻居节点实现协作推理；采用融合监督与自监督学习的联邦学习框架进行训练，兼顾个性化与泛化能力，并保证通信效率和数据隐私。

Result: 实验验证了NMoE系统的有效性，提供了训练算法的见解和基准。

Conclusion: NMoE系统为在资源受限的边缘环境中高效部署大型AI模型提供了一个可行且高效的解决方案。

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [447] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD是一个开源的电池异常检测基准平台，通过系统评估15种算法并引入物理与统计信息特征变换和贝叶斯优化超参数调优，提升了异常可分性与模型泛化能力，支持跨化学体系的电池异常检测。


<details>
  <summary>Details</summary>
Motivation: 电池在消费电子、电动汽车和航空等领域应用广泛，安全性至关重要。现有异常检测方法缺乏统一、开放的基准平台，且标签不完整导致无监督方法调参困难，亟需一个系统、可复现、跨化学体系的评估框架。

Method: 构建了OSBAD开源基准平台，集成15种统计、基于距离和无监督机器学习算法；提出物理与统计信息引导的特征变换流程，将集体异常分解为点异常；设计基于迁移学习与回归代理的贝叶斯优化管道，实现自动化超参数调优。

Result: 在涵盖液态和固态电池的异构数据集上验证了OSBAD的有效性，特征变换显著提升异常可分性，贝叶斯优化有效缓解标签缺失下的调参瓶颈，并展示了跨化学体系的泛化能力。

Conclusion: OSBAD为电池异常检测提供了统一、开放、可复现的基准平台，强调了物理与统计信息驱动的特征工程和概率化超参数调优在安全关键能源系统中的重要性，推动了可信、可扩展、可迁移的数据驱动诊断工具发展。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [448] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出一种基于个性化核心集加权的联邦学习方法，通过仅使用每个客户端的核心集代表性数据点进行训练更新，理论分析表明其泛化误差在对数界限内达到最小最大最优，并在多个基准和医疗数据集上显著优于随机采样和其他子模优化方法。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，每个客户端在完整数据集上训练更新至中心服务器变得繁琐，需要更高效的数据利用方式。

Method: 提出一种个性化核心集加权联邦学习框架（$\methodprop$），各客户端仅基于其核心集的代表性数据点向中心服务器发送更新，并通过理论分析推导其泛化误差的上下界及整体误差的闭式函数表达。

Result: 理论证明平均泛化误差在对数因子内达到最小最大最优，整体误差可表示为核心集权重$\boldsymbol{w}$和样本量$n_k$的闭式函数${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$；实验显示在多种个性化联邦学习架构和医疗数据集上优于随机采样和子模优化方法。

Conclusion: 所提方法能有效减少通信与计算开销，同时提升模型性能，表明智能选择训练样本在联邦学习中的重要性。

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [449] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出一种基于物理信息神经场的模型，结合多尺度傅里叶特征编码，用于从稀疏和噪声超声数据中估计血流速度，无需真实标签监督，在去噪和插补任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 超声成像在深部组织存在衰减问题，导致血流速度测量困难，现有方法在处理噪声和不完整数据时性能受限。

Method: 采用物理信息神经场模型，引入多尺度傅里叶特征编码，利用物理规律约束网络训练，从稀疏和噪声超声数据中重建血流场，无需真实标签。

Result: 在合成和真实数据集上均实现了低均方误差的去噪与插补效果，经参考流场和真实流量测量验证，模型具有良好的准确性和鲁棒性。

Conclusion: 该方法有效提升了超声血流重建的精度，尤其适用于数据质量差的场景，拓展了物理信息神经场在医学流动成像中的应用。

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [450] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: 本文提出了一种基于度量学习的无固定秩张量分解框架，通过三元组损失和正则化优化语义相似性，而非传统重构目标，在多种任务中优于PCA、t-SNE、UMAP及CP/Tucker等方法，尤其在小样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法依赖重构误差和固定秩约束，难以捕捉高维数据中的语义结构，亟需一种能学习语义相关特征的新范式。

Method: 提出基于度量学习的张量分解框架，使用三元组损失进行优化，并引入多样性和一致性正则化，使嵌入空间中的距离直接反映语义相似性，无需固定秩约束。

Result: 在人脸识别、脑连接分析和模拟数据等多个领域显著优于PCA、t-SNE、UMAP以及CP和Tucker分解，在聚类指标（如轮廓系数、DB指数等）上表现更优，且在小训练集下性能超过Transformer类方法。

Conclusion: 该工作确立了度量学习作为张量分析的新范式，强调语义相关性而非像素级保真度，在数据稀缺场景下兼具优越性能和计算效率。

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [451] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 本文提出了一种基于域对抗ConvNeXt自编码器（A-CNT）的超宽带（UWB）干扰源定位方法，通过梯度反转层实现跨环境的特征对齐，显著提升了在室内布局变化下的干扰源定位鲁棒性和迁移性能。


<details>
  <summary>Details</summary>
Motivation: UWB定位易受干扰攻击，且现有ML/DL方法难以应对室内环境变化带来的域偏移问题，亟需具备跨环境泛化能力的干扰源定位方案。

Method: 提出域对抗ConvNeXt自编码器（A-CNT），利用梯度反转层进行对抗性训练，对齐不同环境下的信道冲激响应（CIR）特征，实现无监督域自适应。

Result: 在修改后的房间布局中，传统模型性能严重下降（如XGBoost误差从20.16 cm增至207.99 cm），而A-CNT将平均欧氏误差降至34.67 cm，较非对抗迁移学习提升77%，较最佳基线提升83%，30 cm内定位准确率恢复至0.56。

Conclusion: 对抗性特征对齐能有效缓解环境变化导致的性能退化，使UWB干扰源定位具备良好的可迁移性与实际部署潜力。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [452] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 本研究探讨了科学机器学习中数据保真度与计算成本之间的权衡，通过低精度和高精度的RANS模拟构建神经代理模型，重新定义了经典缩放定律，分解了数据集规模与计算预算及组成的关系，实验发现了计算性能的缩放行为，并揭示了在特定预算下最优的数据保真度组合。


<details>
  <summary>Details</summary>
Motivation: 在科学机器学习中，训练数据通常依赖昂贵的数值模拟生成，限制了数据规模。而其他领域（如语言或视觉）可低成本获取大量数据。因此，研究如何在有限计算预算下通过调整模拟保真度来优化模型性能具有重要意义。

Method: 使用低精度和高精度的雷诺平均纳维-斯托克斯（RANS）模拟生成多保真度数据集，训练神经代理模型；重构经典缩放定律，将数据集维度分解为计算预算和数据组成两个变量，分析不同保真度混合下的模型性能变化。

Result: 实验揭示了模型性能随计算预算增长的缩放规律，发现存在预算依赖的最优保真度混合比例，在给定数据配置下能实现最佳性能，验证了多保真度数据在神经代理模型中的有效性。

Conclusion: 这是首次对多保真度神经代理数据集进行实证缩放律研究，结果为科学机器学习中高效利用计算资源、设计最优数据生成策略提供了理论依据和实践指导。

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [453] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出一种基于路由的方法，在不需同时访问所有任务数据的情况下，有效缓解视觉语言模型在连续学习新任务时的灾难性遗忘问题，同时保持基础能力并提升特定任务性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在顺序微调新任务时容易出现灾难性遗忘，传统多任务学习虽能缓解此问题，但需要同时访问所有数据集且计算开销大。

Method: 引入基于路由的学习机制，通过路由策略动态选择模型参数更新路径，实现新任务学习与旧知识保留的平衡，无需同时访问所有任务数据。

Result: 在InternVL-2模型（2B和8B）上验证，该方法在ChartQA、MMBench、DocVQA等基准上保持原有性能，同时提升专业任务准确率；消融实验显示其对任务数量增长具有鲁棒性，尤其在语义相关任务间表现更优，并展现出更强的跨模态迁移能力。

Conclusion: 基于路由的方法能有效支持视觉语言模型的持续学习，在避免高计算和数据开销的同时，保持基础能力和实现跨模态知识迁移，优于传统持续学习方法。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [454] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 本文提出了一种新的可解释性方法——时间特征分析（Temporal Feature Analysis），通过引入时间上的归纳偏置，将语言模型表示分解为可预测部分和残差部分，相比传统稀疏自编码器更能捕捉语言的动态时序结构。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法（如SAE）假设概念在时间上独立且平稳，难以捕捉语言中丰富的时序动态特性，如上下文依赖、非平稳性和维度增长。

Method: 受计算神经科学启发，提出时间特征分析，具有时间归纳偏置，将某时刻的表示分解为可由上下文预测的部分和包含新信息的残差部分。

Result: 该方法能正确解析歧义句、识别事件边界，并区分慢变的抽象信息与快变的新信息，而传统SAE在这些任务中表现不佳。

Conclusion: 设计鲁棒的可解释性工具需采用与数据特性匹配的归纳偏置，特别是考虑时间动态性。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [455] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: 本研究结合可解释机器学习与符号建模（KANs），利用10,000多个深度解析的温度剖面数据，揭示了美国红河流域10个水库水温（RWT）的驱动机制，在实现高精度预测的同时，生成了具有物理意义的简化公式，平衡了模型准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确预测水库水温对水资源管理和生态系统健康至关重要，但传统黑箱模型缺乏对物理过程的解释能力。研究旨在通过可解释AI方法揭示RWT的关键驱动因素，并建立兼具高精度与透明性的模型。

Method: 采用随机森林、XGBoost和MLP等机器学习模型进行RWT预测，并使用SHAP方法识别关键物理驱动因子（如气温、深度、风速等）；进一步引入Kolmogorov-Arnold Networks（KANs）进行符号回归，构建可解释的解析表达式以描述RWT动态。

Result: 机器学习模型达到高预测精度（RMSE=1.20°C，R²=0.97）；SHAP分析显示气温和深度是主要驱动因素，降水影响较小；KANs生成了10个逐步复杂的方程，最佳R²达0.92，且五变量后增益递减，表明可在简洁性与准确性间取得平衡。

Conclusion: 结合可解释ML与KANs的方法不仅能实现高精度水库水温预测，还能揭示潜在物理机制，将黑箱模型转化为透明的代理模型，为理解水库热力动态提供了新的可解释建模范式。

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [456] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 本文研究了终身表示学习的广义框架，提出了一种使用多任务经验风险最小化作为子程序的简单算法，并基于新引入的任务-消除维度建立了样本复杂度界。


<details>
  <summary>Details</summary>
Motivation: 在终身学习中，学习者需要在没有先验任务信息的情况下，在线地利用已有知识并持续获取部分信息以加速学习。

Method: 提出了一个简单的算法，该算法使用多任务经验风险最小化作为子程序，并引入了任务-消除维度这一新概念来建立样本复杂度界。

Result: 所提出的方法适用于涉及一般函数类的广泛学习问题，并在分类和回归任务下实例化了结果。

Conclusion: 通过引入任务-消除维度，该研究为终身表示学习提供了一个有效的样本复杂度分析框架。

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [457] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: 提出一种基于坐标上升的算法，通过最大似然估计在监督方式下学习动态和测量模型。


<details>
  <summary>Details</summary>
Motivation: 为了提高动态状态估计的精度，需要有效学习动态和测量模型。

Method: 假设动态和测量模型为高斯分布，利用神经网络参数建模动态和测量函数，并学习噪声协方差矩阵。使用坐标上升算法进行训练。

Result: 该算法能够有效学习动态和测量模型的参数，并在测试阶段结合非线性卡尔曼滤波器进行状态估计。

Conclusion: 所提出的方法能够在监督学习框架下有效学习非线性动态系统的模型，提升状态估计性能。

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [458] [Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images](https://arxiv.org/abs/2511.00702)
*Alberto Di Biase*

Main category: cs.GR

TL;DR: 本文探索了将扩散张量成像（DTI）和纤维追踪技术应用于图像的绘画式渲染，利用结构张量指导笔触生成，模拟人类艺术家的绘画过程。


<details>
  <summary>Details</summary>
Motivation: 受医学影像中纤维追踪技术的启发，尝试将其原理迁移至艺术化图像生成领域，实现更具结构性的笔触渲染。

Method: 采用类似于DTI中的纤维追踪算法，结合图像的结构张量而非梯度来获取局部方向信息，从而在图像上生成符合纹理走向的笔触。

Result: 成功实现了在肖像和一般图像上的绘画式渲染效果，生成的笔触能较好地跟随图像的结构走向，视觉效果接近人工绘画。

Conclusion: 扩散张量成像技术可有效迁移到图像艺术化渲染中，结构张量能提供优于梯度的方向信息，该方法为跨领域图像生成提供了新思路。

Abstract: Doctors and researchers routinely use diffusion tensor imaging (DTI) and
tractography to visualize the fibrous structure of tissues in the human body.
This paper explores the connection of these techniques to the painterly
rendering of images. Using a tractography algorithm the presented method can
place brush strokes that mimic the painting process of human artists,
analogously to how fibres are tracked in DTI. The analogue to the diffusion
tensor for image orientation is the structural tensor, which can provide better
local orientation information than the gradient alone. I demonstrate this
technique in portraits and general images, and discuss the parallels between
fibre tracking and brush stroke placement, and frame it in the language of
tractography. This work presents an exploratory investigation into the
cross-domain application of diffusion tensor imaging techniques to painterly
rendering of images. All the code is available at
https://github.com/tito21/st-python

</details>


### [459] [Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning](https://arxiv.org/abs/2511.00898)
*Heng Zhang,Jing Liu,Jiajun Wu,Haochen You,Lubin Gan,Yuling Shi,Xiaodong Gu,Zijian Zhang,Shuai Chen,Wenjun Huang,Jin Huang*

Main category: cs.GR

TL;DR: 提出DuoGLM，一种无需训练的双视角框架，通过局部和全局视角增强大语言模型在图学习中的结构感知推理能力，在零样本和跨域任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理图中关键结构节点（如桥接点和枢纽）时表现下降，缺乏将拓扑模式转化为基于角色解释的推理机制，尤其在零样本场景下无法建立结构与语义的映射。

Method: 设计双视角框架DuoGLM：局部视角构建关系感知模板以捕捉节点与其邻居间的语义交互；全局视角进行拓扑到角色的推断，生成结构性位置的功能描述，从而为大语言模型提供显式的推理机制。

Result: 在八个基准数据集上实验表明，DuoGLM在零样本节点分类中准确率提升14.3%，跨域迁移中AUC提高7.6%，显著优于现有方法。

Conclusion: 显式的角色推理机制能有效提升大语言模型对图结构的理解能力，DuoGLM为结构感知的图学习提供了新的解决方案。

Abstract: Large Language Models have emerged as a promising approach for graph learning
due to their powerful reasoning capabilities. However, existing methods exhibit
systematic performance degradation on structurally important nodes such as
bridges and hubs. We identify the root cause of these limitations. Current
approaches encode graph topology into static features but lack reasoning
scaffolds to transform topological patterns into role-based interpretations.
This limitation becomes critical in zero-shot scenarios where no training data
establishes structure-semantics mappings. To address this gap, we propose
DuoGLM, a training-free dual-perspective framework for structure-aware graph
reasoning. The local perspective constructs relation-aware templates capturing
semantic interactions between nodes and neighbors. The global perspective
performs topology-to-role inference to generate functional descriptions of
structural positions. These complementary perspectives provide explicit
reasoning mechanisms enabling LLMs to distinguish topologically similar but
semantically different nodes. Extensive experiments across eight benchmark
datasets demonstrate substantial improvements. DuoGLM achieves 14.3\% accuracy
gain in zero-shot node classification and 7.6\% AUC improvement in cross-domain
transfer compared to existing methods. The results validate the effectiveness
of explicit role reasoning for graph understanding with LLMs.

</details>


### [460] [G2rammar: Bilingual Grammar Modeling for Enhanced Text-attributed Graph Learning](https://arxiv.org/abs/2511.00911)
*Heng Zheng,Haochen You,Zijun Liu,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.GR

TL;DR: 提出G2rammar框架，通过结构和语义语法显式编码文本属性图的拓扑与内容信息，提升语言模型对图结构的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图结构线性化为序列时忽略语法作用，缺乏对节点结构和语义角色的标注，限制了语言模型对图拓扑的推理能力。

Method: 设计双阶段学习的双语语法框架：先用中心性和邻域模式进行结构语法预训练，再用文本信息性进行语义语法微调。

Result: 在真实数据集上的实验证明，G2rammar持续优于基线模型，显著提升语言模型理解图结构的能力。

Conclusion: 引入语法概念到图到序列的转换中是有效的，G2rammar为文本属性图提供了更丰富的结构与语义上下文。

Abstract: Text-attributed graphs require models to effectively integrate both
structural topology and semantic content. Recent approaches apply large
language models to graphs by linearizing structures into token sequences
through random walks. These methods create concise graph vocabularies to
replace verbose natural language descriptions. However, they overlook a
critical component that makes language expressive: grammar. In natural
language, grammar assigns syntactic roles to words and defines their functions
within sentences. Similarly, nodes in graphs play distinct structural roles as
hubs, bridges, or peripheral members. Current graph language methods provide
tokens without grammatical annotations to indicate these structural or semantic
roles. This absence limits language models' ability to reason about graph
topology effectively. We propose \textbf{G2rammar}, a bilingual grammar
framework that explicitly encodes both structural and semantic grammar for
text-attributed graphs. Structural grammar characterizes topological roles
through centrality and neighborhood patterns. Semantic grammar captures content
relationships through textual informativity. The framework implements two-stage
learning with structural grammar pre-training followed by semantic grammar
fine-tuning. Extensive experiments on real-world datasets demonstrate that
G2rammar consistently outperforms competitive baselines by providing language
models with the grammatical context needed to understand graph structures.

</details>


### [461] [An Adjoint Method for Differentiable Fluid Simulation on Flow Maps](https://arxiv.org/abs/2511.01259)
*Zhiqi Li,Jinjin He,Barnabás Börcsök,Taiyuan Zhang,Duowen Chen,Tao Du,Ming C. Lin,Greg Turk,Bo Zhu*

Main category: cs.GR

TL;DR: 提出基于双向流图的新型伴随求解器，用于可微分的流体模拟，通过共享长距离流图实现高精度梯度计算，且无需存储中间变量，显著降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统伴随方法需要存储大量中间变量且计算效率低，难以精确处理长距离依赖的流体动力学问题，尤其是涡旋动态的建模与控制。

Method: 利用前向模拟中的流图在反向传播中传递伴随变量，直接在流图上求解伴随方程，并提出长短时稀疏流图表示法以提升效率。

Result: 在192^3分辨率下仅需6.53GB内存，保持高精度涡量追踪，支持对涡旋动态的精确识别、预测与控制。

Conclusion: 该方法实现了高效、低内存、高精度的可微分流体模拟，为复杂流体控制任务提供了新可能。

Abstract: This paper presents a novel adjoint solver for differentiable fluid
simulation based on bidirectional flow maps. Our key observation is that the
forward fluid solver and its corresponding backward, adjoint solver share the
same flow map as the forward simulation. In the forward pass, this map
transports fluid impulse variables from the initial frame to the current frame
to simulate vortical dynamics. In the backward pass, the same map propagates
adjoint variables from the current frame back to the initial frame to compute
gradients. This shared long-range map allows the accuracy of gradient
computation to benefit directly from improvements in flow map construction.
Building on this insight, we introduce a novel adjoint solver that solves the
adjoint equations directly on the flow map, enabling long-range and accurate
differentiation of incompressible flows without differentiating intermediate
numerical steps or storing intermediate variables, as required in conventional
adjoint methods. To further improve efficiency, we propose a long-short
time-sparse flow map representation for evolving adjoint variables. Our
approach has low memory usage, requiring only 6.53GB of data at a resolution of
$192^3$ while preserving high accuracy in tracking vorticity, enabling new
differentiable simulation tasks that require precise identification,
prediction, and control of vortex dynamics.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [462] [LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks](https://arxiv.org/abs/2511.00072)
*Pradeep M,Ritesh Pallod,Satyen Abrol,Muthu Raman,Ian Anderson*

Main category: cs.IR

TL;DR: 提出了一种端到端的产品搜索系统，用于将生成式AI产生的时尚造型与真实商品进行匹配，已在大规模实际应用中部署。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在改变时尚行业，用户需要找到与AI生成风格相匹配的真实产品。

Method: 构建了一个包含查询生成、向量化、候选检索和重排序四个组件的搜索管道，基于CLIP模型实现视觉和语义相似性匹配。

Result: 系统每天处理超过35万次AI造型搜索，覆盖全球超1200万商品；CLIP模型在平均意见分上相对优于其他模型3-7%。

Conclusion: CLIP模型虽提升幅度有限，但显著改善了用户感知匹配度，成为生产部署中最可靠的骨干模型。

Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars
making it essential to find real products that best match AI-generated styles.
We propose an end-to-end product search system that has been deployed in a
real-world, internet scale which ensures that AI-generated looks presented to
users are matched with the most visually and semantically similar products from
the indexed vector space. The search pipeline is composed of four key
components: query generation, vectorization, candidate retrieval, and reranking
based on AI-generated looks. Recommendation quality is evaluated using
human-judged accuracy scores. The system currently serves more than 350,000 AI
Looks in production per day, covering diverse product categories across global
markets of over 12 million products. In our experiments, we observed that
across multiple annotators and categories, CLIP outperformed alternative models
by a small relative margin of 3--7\% in mean opinion scores. These
improvements, though modest in absolute numbers, resulted in noticeably better
user perception matches, establishing CLIP as the most reliable backbone for
production deployment.

</details>


### [463] [Effectiveness of LLMs in Temporal User Profiling for Recommendation](https://arxiv.org/abs/2511.00176)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 本文探讨了利用大语言模型（LLM）捕捉用户短期与长期兴趣的时序动态，以提升推荐系统的准确性与可解释性。研究发现LLM在用户活跃度高的领域（如电影和电视）效果更显著，而在用户行为稀疏或偏好稳定的领域（如视频游戏）提升有限，表明其应用需权衡性能增益与计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统用户画像常忽视短期瞬时兴趣与长期稳定偏好的区别，难以有效建模用户偏好的动态变化。本文旨在探索如何利用大语言模型更好地区分并建模这两种时间维度的兴趣，以提升推荐系统的个性化和透明度。

Method: 提出一种基于大语言模型的方法，通过对用户交互历史生成区分性的短期和长期文本摘要，构建更丰富的用户表征，并分析其在不同领域中的表现差异。

Result: 实验表明，LLM在用户互动频繁的领域（如Movies&TV）能显著提升推荐质量，但在行为稀疏或偏好稳定的领域（如Video Games）效果不明显，主要因为后者短期与长期偏好较难区分。此外，该方法通过自然语言描述和注意力权重提供了内在的可解释性。

Conclusion: LLM驱动的时序用户画像在特定情境下具有潜力，尤其适用于用户兴趣变化明显且数据较密集的场景；然而其效益受领域特性影响显著，实际应用中需权衡性能提升与计算开销，未来应探索自适应、轻量化的动态建模方法。

Abstract: Effectively modeling the dynamic nature of user preferences is crucial for
enhancing recommendation accuracy and fostering transparency in recommender
systems. Traditional user profiling often overlooks the distinction between
transitory short-term interests and stable long-term preferences. This paper
examines the capability of leveraging Large Language Models (LLMs) to capture
these temporal dynamics, generating richer user representations through
distinct short-term and long-term textual summaries of interaction histories.
Our observations suggest that while LLMs tend to improve recommendation quality
in domains with more active user engagement, their benefits appear less
pronounced in sparser environments. This disparity likely stems from the
varying distinguishability of short-term and long-term preferences across
domains; the approach shows greater utility where these temporal interests are
more clearly separable (e.g., Movies\&TV) compared to domains with more stable
user profiles (e.g., Video Games). This highlights a critical trade-off between
enhanced performance and computational costs, suggesting context-dependent LLM
application. Beyond predictive capability, this LLM-driven approach inherently
provides an intrinsic potential for interpretability through its natural
language profiles and attention weights. This work contributes insights into
the practical capability and inherent interpretability of LLM-driven temporal
user profiling, outlining new research directions for developing adaptive and
transparent recommender systems.

</details>


### [464] [Simple and Behavior-Driven Augmentation for Recommendation with Rich Collaborative Signals](https://arxiv.org/abs/2511.00436)
*Doyun Choi,Cheonwoo Lee,Jaemin Yoo*

Main category: cs.IR

TL;DR: 提出了一种简单且直观的推荐增强方法SCAR，通过利用用户-物品交互中的协同信号生成伪交互，提升对比学习在图协同过滤中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的图协同过滤方法依赖数据增强去除噪声交互，但‘噪声’定义模糊，易丢失关键信息并增加复杂性。

Method: 提出SCAR方法，不删除信息，而是从用户-物品交互中提取协同信号生成伪交互，并将其添加或替换到原始交互中，以构建更鲁棒的增强视图。

Result: 在四个基准数据集上实验表明，SCAR优于现有的基于对比学习的图协同过滤方法及其他自监督学习方法，在不同超参数下具有强鲁棒性，尤其在稀疏数据场景下表现突出。

Conclusion: SCAR通过正向构造协同信号增强视图，避免了复杂的数据清洗过程，有效提升了对比学习在推荐系统中的性能和稳定性。

Abstract: Contrastive learning (CL) has been widely used for enhancing the performance
of graph collaborative filtering (GCF) for personalized recommendation. Since
data augmentation plays a crucial role in the success of CL, previous works
have designed augmentation methods to remove noisy interactions between users
and items in order to generate effective augmented views. However, the
ambiguity in defining ''noisiness'' presents a persistent risk of losing core
information and generating unreliable data views, while increasing the overall
complexity of augmentation. In this paper, we propose Simple Collaborative
Augmentation for Recommendation (SCAR), a novel and intuitive augmentation
method designed to maximize the effectiveness of CL for GCF. Instead of
removing information, SCAR leverages collaborative signals extracted from
user-item interactions to generate pseudo-interactions, which are then either
added to or used to replace existing interactions. This results in more robust
representations while avoiding the pitfalls of overly complex augmentation
modules. We conduct experiments on four benchmark datasets and show that SCAR
outperforms previous CL-based GCF methods as well as other state-of-the-art
self-supervised learning approaches across key evaluation metrics. SCAR
exhibits strong robustness across different hyperparameter settings and is
particularly effective in sparse data scenarios.

</details>


### [465] [LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026](https://arxiv.org/abs/2511.00444)
*Benjamin Clavié,Xianming Li,Antoine Chaffin,Omar Khattab,Tom Aarsen,Manuel Faysse,Jing Li*

Main category: cs.IR

TL;DR: 本文讨论了晚期交互检索方法（如ColBERT）的优势与挑战，强调其在细粒度表示和跨模态检索中的潜力，同时指出效率和系统集成等问题，并呼吁建立一个促进研究者与实践者交流的互动平台。


<details>
  <summary>Details</summary>
Motivation: 晚期交互检索方法虽表现出色，但在效率、实际应用和跨领域探索方面仍面临挑战，且研究分散，缺乏实践者参与，因此需要一个专门的交流平台。

Method: 通过组织专题研讨会，汇集不同背景的研究人员和实践者，分享早期研究成果、实际应用效果以及负面或困惑的结果，推动合作与进展。

Result: 建立了LIR研讨会，为晚期交互检索领域的研究者和实践者提供了一个高度互动的交流环境。

Conclusion: 促进开放讨论和协作是推动晚期交互检索技术发展的关键，未来需进一步整合学术与工业界的努力。

Abstract: Late interaction retrieval methods, pioneered by ColBERT, have emerged as a
powerful alternative to single-vector neural IR. By leveraging fine-grained,
token-level representations, they have been demonstrated to deliver strong
generalisation and robustness, particularly in out-of-domain settings. They
have recently been shown to be particularly well-suited for novel use cases,
such as reasoning-based or cross-modality retrieval. At the same time, these
models pose significant challenges of efficiency, usability, and integrations
into fully fledged systems; as well as the natural difficulties encountered
while researching novel application domains. Recent years have seen rapid
advances across many of these areas, but research efforts remain fragmented
across communities and frequently exclude practitioners. The purpose of this
workshop is to create an environment where all aspects of late interaction can
be discussed, with a focus on early research explorations, real-world outcomes,
and negative or puzzling results to be freely shared and discussed. The aim of
LIR is to provide a highly-interactive environment for researchers from various
backgrounds and practitioners to freely discuss their experience, fostering
further collaboration.

</details>


### [466] [Listwise Preference Diffusion Optimization for User Behavior Trajectories Prediction](https://arxiv.org/abs/2511.00530)
*Hongtao Huang,Chengkai Huang,Junda Wu,Tong Yu,Julian McAuley,Lina Yao*

Main category: cs.IR

TL;DR: 本文提出了用户行为轨迹预测（UBTP）新任务，引入基于扩散的Listwise Preference Diffusion Optimization（LPDO）框架，通过建模全局列表级偏好显著提升多步行为预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐方法无法捕捉用户未来行为序列中的全局、列表级依赖关系，难以准确预测完整的行为轨迹，限制了个性化服务的效果。

Method: 将UBTP定义为建模长期用户偏好的新任务，提出LPDO框架，采用Plackett-Luce模型作为监督信号，并推导与列表级排序似然对齐的变分下界，在去噪过程中优化整个序列的结构化偏好。

Result: 在真实用户行为数据集上，LPDO在SeqMatch和PPL指标上均优于现有最先进方法，验证了其在多步预测和概率建模方面的优越性。

Conclusion: LPDO通过显式建模列表级偏好，克服了传统扩散模型独立token假设的局限，为基于扩散模型的结构化偏好学习建立了新基准。

Abstract: Forecasting multi-step user behavior trajectories requires reasoning over
structured preferences across future actions, a challenge overlooked by
traditional sequential recommendation. This problem is critical for
applications such as personalized commerce and adaptive content delivery, where
anticipating a user's complete action sequence enhances both satisfaction and
business outcomes. We identify an essential limitation of existing paradigms:
their inability to capture global, listwise dependencies among sequence items.
To address this, we formulate User Behavior Trajectory Prediction (UBTP) as a
new task setting that explicitly models long-term user preferences. We
introduce Listwise Preference Diffusion Optimization (LPDO), a diffusion-based
training framework that directly optimizes structured preferences over entire
item sequences. LPDO incorporates a Plackett-Luce supervision signal and
derives a tight variational lower bound aligned with listwise ranking
likelihoods, enabling coherent preference generation across denoising steps and
overcoming the independent-token assumption of prior diffusion methods. To
rigorously evaluate multi-step prediction quality, we propose the task-specific
metric Sequential Match (SeqMatch), which measures exact trajectory agreement,
and adopt Perplexity (PPL), which assesses probabilistic fidelity. Extensive
experiments on real-world user behavior benchmarks demonstrate that LPDO
consistently outperforms state-of-the-art baselines, establishing a new
benchmark for structured preference learning with diffusion models.

</details>


### [467] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: 提出SRGFormer模型，通过改进的Transformer和超图结构增强多模态推荐，有效区分有价值与冗余信息，捕捉用户-物品复杂交互，在三个公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型在处理多模态信息时忽视了冗余与关键数据的区分，且依赖单一语义结构，难以全面建模用户偏好，尤其对表达较少的用户行为捕捉不足。

Method: 改进Transformer以捕获用户整体行为模式；将多模态信息嵌入超图结构以学习用户-物品局部结构；引入自监督任务增强用户-物品协同信号与多模态信息融合。

Result: 在三个公开数据集上实验表明，SRGFormer优于现有基准模型，在Sports数据集上平均性能提升4.47%。

Conclusion: SRGFormer通过结构优化和多模态信息增强，有效提升了推荐系统的性能，尤其在捕捉复杂用户-物品交互和弱表达偏好方面表现优越。

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


### [468] [Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce](https://arxiv.org/abs/2511.00694)
*Uthman Jinadu,Siawpeng Er,Le Yu,Chen Liang,Bingxin Li,Yi Ding,Aleksandar Velkoski*

Main category: cs.IR

TL;DR: 提出一种基于 taxonomy 的难负例采样策略（TB-HNS）和用户个性化语义检索模型，用于电商搜索，提升召回率与业务指标。


<details>
  <summary>Details</summary>
Motivation: 现有模型在训练时计算昂贵、难以部署，且未考虑用户历史行为，导致检索结果不相关。需要一种能理解细粒度商品差异并结合用户行为的高效检索模型。

Method: 将查询和商品嵌入共享向量空间，采用新颖的 taxonomy-based hard-negative sampling（TB-HNS）策略生成上下文相关但具有挑战性的负样本，并通过建模用户历史购买行为实现个性化检索。

Result: 离线实验中 Recall@K 优于 BM25、ANCE 和主流神经基线；线上 A/B 测试显示转化率、加购率和订单均价显著提升；TB-HNS 减少了训练开销并加快了收敛速度。

Conclusion: 所提出的语义检索模型结合 TB-HNS 和用户个性化，在效果、效率和可扩展性方面均表现优异，已成功大规模部署。

Abstract: Large retail outlets offer products that may be domain-specific, and this
requires having a model that can understand subtle differences in similar
items. Sampling techniques used to train these models are most of the time,
computationally expensive or logistically challenging. These models also do not
factor in users' previous purchase patterns or behavior, thereby retrieving
irrelevant items for them. We present a semantic retrieval model for e-commerce
search that embeds queries and products into a shared vector space and
leverages a novel taxonomy-based hard-negative sampling(TB-HNS) strategy to
mine contextually relevant yet challenging negatives. To further tailor
retrievals, we incorporate user-level personalization by modeling each
customer's past purchase history and behavior. In offline experiments, our
approach outperforms BM25, ANCE and leading neural baselines on Recall@K, while
live A/B testing shows substantial uplifts in conversion rate, add-to-cart
rate, and average order value. We also demonstrate that our taxonomy-driven
negatives reduce training overhead and accelerate convergence, and we share
practical lessons from deploying this system at scale.

</details>


### [469] [REaR: Retrieve, Expand and Refine for Effective Multitable Retrieval](https://arxiv.org/abs/2511.00805)
*Rishita Agarwal,Himanshu Singhal,Peter Baile Chen,Manan Roy Choudhury,Dan Roth,Vivek Gupta*

Main category: cs.IR

TL;DR: REAR是一种无需大语言模型的三阶段框架，用于提升关系数据上的多表检索质量，通过分离语义相关性和结构可连接性，在复杂表格问答任务中显著提升检索效果和下游SQL执行性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索器主要关注查询与表格的相关性，忽略表格间的结构兼容性，导致多表检索效果不佳。

Method: REAR分为三个阶段：首先检索与查询相关的表格，然后通过预计算的列嵌入快速扩展结构上可连接的表格，最后修剪噪声或弱相关候选表格。

Result: 在BIRD、MMQA和Spider等复杂表格问答数据集上，REAR显著提升了密集和稀疏检索器的性能，且在无需LLM的情况下达到与先进LLM增强系统相当的效果，同时延迟和成本更低。

Conclusion: REAR是一种高效、可扩展的多表检索框架，适用于Text-to-SQL等基于表格的下游任务，兼具高性能与低开销优势。

Abstract: Answering natural language queries over relational data often requires
retrieving and reasoning over multiple tables, yet most retrievers optimize
only for query-table relevance and ignore table table compatibility. We
introduce REAR (Retrieve, Expand and Refine), a three-stage, LLM-free framework
that separates semantic relevance from structural joinability for efficient,
high-fidelity multi-table retrieval. REAR (i) retrieves query-aligned tables,
(ii) expands these with structurally joinable tables via fast, precomputed
column-embedding comparisons, and (iii) refines them by pruning noisy or weakly
related candidates. Empirically, REAR is retriever-agnostic and consistently
improves dense/sparse retrievers on complex table QA datasets (BIRD, MMQA, and
Spider) by improving both multi-table retrieval quality and downstream SQL
execution. Despite being LLM-free, it delivers performance competitive with
state-of-the-art LLM-augmented retrieval systems (e.g.,ARM) while achieving
much lower latency and cost. Ablations confirm complementary gains from
expansion and refinement, underscoring REAR as a practical, scalable building
block for table-based downstream tasks (e.g., Text-to-SQL).

</details>


### [470] [Controlling Gender Bias in Retrieval via a Backpack Architecture](https://arxiv.org/abs/2511.00875)
*Amirabbas Afzali,Amirreza Velae,Iman Ahmadi,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 提出了一种基于Backpack语言模型的去偏框架，有效缓解了文本检索和排序中的性别偏见，同时性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的社会偏见问题日益严重，尤其在用于排序系统时可能导致不公平结果，因此需要有效的去偏方法。

Method: 利用Backpack语言模型将输出表示为非上下文化的词义组合的特性，设计了一个针对排序任务的去偏框架。

Result: 实验结果表明，该框架在性别偏见缓解方面效果显著，且对模型性能影响较小。

Conclusion: Backpack语言模型架构为实现高效去偏提供了新思路，适用于搜索和推荐等关键应用中的公平性提升。

Abstract: The presence of social biases in large language models (LLMs) has become a
significant concern in AI research. These biases, often embedded in training
data, can perpetuate harmful stereotypes and distort decision-making processes.
When LLMs are integrated into ranking systems, they can propagate these biases,
leading to unfair outcomes in critical applications such as search engines and
recommendation systems. Backpack Language Models, unlike traditional
transformer-based models that treat text sequences as monolithic structures,
generate outputs as weighted combinations of non-contextual, learned word
aspects, also known as senses. Leveraging this architecture, we propose a
framework for debiasing ranking tasks. Our experimental results show that this
framework effectively mitigates gender bias in text retrieval and ranking with
minimal degradation in performance.

</details>


### [471] [Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking](https://arxiv.org/abs/2511.01208)
*Jerry Huang,Siddarth Madala,Cheng Niu,Julia Hockenmaier,Tong Zhang*

Main category: cs.IR

TL;DR: 提出了一种基于采样的不确定性感知重排序算法TS-SetRank，通过建模上下文相关性显著提升了推理密集型查询的文档检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有重排序方法在处理需要深度推理的复杂查询时表现不佳，因为这类查询的信息需求多维度且语义细微，文档相关性高度依赖于上下文（如候选文档集合及其排列顺序）。

Method: 提出了“上下文相关性”的概念，定义为文档相对于查询的相关性概率在其可能出现的不同重排序上下文上的边缘化分布；并设计了TS-SetRank算法，采用基于采样的方式估计该相关性，同时考虑文档批次组成和顺序对大语言模型重排序的影响。

Result: 在BRIGHT和BEIR数据集上，TS-SetRank相比基线模型将nDCG@10指标提升了15-25%和6-21%，验证了建模样本上下文依赖性的有效性。

Conclusion: 文档相关性是上下文依赖的，有效建模候选文档集合的组成和顺序等上下文因素可显著提升重排序性能，尤其在推理密集型查询中。

Abstract: Reranking algorithms have made progress in improving document retrieval
quality by efficiently aggregating relevance judgments generated by large
language models (LLMs). However, identifying relevant documents for queries
that require in-depth reasoning remains a major challenge. Reasoning-intensive
queries often exhibit multifaceted information needs and nuanced
interpretations, rendering document relevance inherently context dependent. To
address this, we propose contextual relevance, which we define as the
probability that a document is relevant to a given query, marginalized over the
distribution of different reranking contexts it may appear in (i.e., the set of
candidate documents it is ranked alongside and the order in which the documents
are presented to a reranking model). While prior works have studied methods to
mitigate the positional bias LLMs exhibit by accounting for the ordering of
documents, we empirically find that the compositions of these batches also
plays an important role in reranking performance. To efficiently estimate
contextual relevance, we propose TS-SetRank, a sampling-based,
uncertainty-aware reranking algorithm. Empirically, TS-SetRank improves nDCG@10
over retrieval and reranking baselines by 15-25% on BRIGHT and 6-21% on BEIR,
highlighting the importance of modeling relevance as context-dependent.

</details>


### [472] [A semantic-based deep learning approach for mathematical expression retrieval](https://arxiv.org/abs/2511.01364)
*Pavan Kumar Perepu*

Main category: cs.IR

TL;DR: 本文提出了一种基于深度循环神经网络（DRNN）的数学表达式检索方法，通过提取表达式的语义特征并根据嵌套深度分类其复杂度，实现基于语义相似性的高效匹配与检索。


<details>
  <summary>Details</summary>
Motivation: 传统的数学表达式检索方法依赖于语法相似性，难以捕捉语义信息。为了提升检索的准确性和语义理解能力，本文探索基于深度学习的语义相似性方法。

Method: 采用深度循环神经网络（DRNN）从LaTeX格式的数学表达式中提取语义特征；将表达式按嵌套深度分为简单、中等和复杂三类，训练分类模型，并利用倒数第二层输出作为语义特征向量存储；对查询表达式，使用相同网络提取特征，通过欧氏距离在数据库中进行最近邻匹配，返回前k个结果。

Result: 该方法在一个包含829个数学表达式的数据库上进行了验证，能够有效实现基于语义特征的数学表达式检索。

Conclusion: 基于DRNN提取语义特征的方法在数学表达式检索中表现良好，结合复杂度分类和欧氏距离匹配，提升了语义层面的检索效果。

Abstract: Mathematical expressions (MEs) have complex two-dimensional structures in
which symbols can be present at any nested depth like superscripts, subscripts,
above, below etc. As MEs are represented using LaTeX format, several text
retrieval methods based on string matching, vector space models etc., have also
been applied for ME retrieval problem in the literature. As these methods are
based on syntactic similarity, recently deep learning approaches based on
embedding have been used for semantic similarity. In our present work, we have
focused on the retrieval of mathematical expressions using deep learning
approaches. In our approach, semantic features are extracted from the MEs using
a deep recurrent neural network (DRNN) and these features have been used for
matching and retrieval. We have trained the network for a classification task
which determines the complexity of an ME. ME complexity has been quantified in
terms of its nested depth. Based on the nested depth, we have considered three
complexity classes of MEs: Simple, Medium and Complex. After training the
network, outputs just before the the final fully connected layer are extracted
for all the MEs. These outputs form the semantic features of MEs and are stored
in a database. For a given ME query, its semantic features are computed using
the trained DRNN and matched against the semantic feature database. Matching is
performed based on the standard euclidean distance and top 'k' nearest matches
are retrieved, where 'k' is a user-defined parameter. Our approach has been
illustrated on a database of 829 MEs.

</details>


### [473] [A Soft-partitioned Semi-supervised Collaborative Transfer Learning Approach for Multi-Domain Recommendation](https://arxiv.org/abs/2511.01404)
*Xiaoyu Liu,Yiqing Wu,Ruidong Han,Fuzhen Zhuang,Xiang Li,Wei Lin*

Main category: cs.IR

TL;DR: 提出了一种名为Soft-partitioned Semi-supervised Collaborative Transfer Learning (SSCTL) 的多域推荐方法，通过动态参数和伪标签加权缓解数据不平衡导致的主导域淹没和非主导域过拟合问题，在线上和线下实验中均取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有共享-特定架构在多域推荐中因数据不平衡导致的两个问题：主导域数据淹没模型性能（Overwhelming）和非主导域稀疏数据引发过拟合（Overfitting）。

Method: 提出SSCTL方法，使用动态参数生成机制以减轻主导域对模型的过度影响，并利用来自主导域实例的加权伪标签进行半监督学习，增强非主导域的数据表示。

Result: 在线上和线下实验中验证了SSCTL的有效性，线上结果显示各领域GMV提升0.54%~2.90%，CTR提升0.22%~1.69%。

Conclusion: SSCTL能有效缓解多域推荐中的数据不平衡问题，提升非主导域的推荐性能，具有较强的工业应用价值。

Abstract: In industrial practice, Multi-domain Recommendation (MDR) plays a crucial
role. Shared-specific architectures are widely used in industrial solutions to
capture shared and unique attributes via shared and specific parameters.
However, with imbalanced data across different domains, these models face two
key issues: (1) Overwhelming: Dominant domain data skews model performance,
neglecting non-dominant domains. (2) Overfitting: Sparse data in non-dominant
domains leads to overfitting in specific parameters. To tackle these
challenges, we propose Soft-partitioned Semi-supervised Collaborative Transfer
Learning (SSCTL) for multi-domain recommendation. SSCTL generates dynamic
parameters to address the overwhelming issue, thus shifting focus towards
samples from non-dominant domains. To combat overfitting, it leverages
pseudo-labels with weights from dominant domain instances to enhance
non-dominant domain data. We conduct comprehensive experiments, both online and
offline, to validate the efficacy of our proposed method. Online tests yielded
significant improvements across various domains, with increases in GMV ranging
from 0.54% to 2.90% and enhancements in CTR ranging from 0.22% to 1.69%.

</details>


### [474] [LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning](https://arxiv.org/abs/2511.01448)
*Zhengjun Huang,Zhoujin Tian,Qintian Guo,Fangyuan Zhang,Yingli Zhou,Di Jiang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: 提出LiCoMemory，一种端到端的代理记忆框架，采用分层图结构CogniGraph实现高效、准确的知识检索与实时更新。


<details>
  <summary>Details</summary>
Motivation: 现有外部记忆架构多采用扁平、语义与拓扑纠缠的结构，导致表示冗余、检索无序，影响效率和准确性。

Method: 设计CogniGraph，一种轻量级分层图结构，以实体和关系作为语义索引层，并结合时间与层次感知搜索及重排序机制进行自适应知识检索。

Result: 在LoCoMo和LongMemEval对话基准上实验表明，LiCoMemory在时序推理、多会话一致性和检索效率方面优于基线方法，且显著降低更新延迟。

Conclusion: LiCoMemory通过分层语义索引和结构化检索机制，有效提升了LLM代理的记忆持久性与推理连贯性。

Abstract: Large Language Model (LLM) agents exhibit remarkable conversational and
reasoning capabilities but remain constrained by limited context windows and
the lack of persistent memory. Recent efforts address these limitations via
external memory architectures, often employing graph-based representations, yet
most adopt flat, entangled structures that intertwine semantics with topology,
leading to redundant representations, unstructured retrieval, and degraded
efficiency and accuracy. To resolve these issues, we propose LiCoMemory, an
end-to-end agentic memory framework for real-time updating and retrieval, which
introduces CogniGraph, a lightweight hierarchical graph that utilizes entities
and relations as semantic indexing layers, and employs temporal and
hierarchy-aware search with integrated reranking for adaptive and coherent
knowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo and
LongMemEval, show that LiCoMemory not only outperforms established baselines in
temporal reasoning, multi-session consistency, and retrieval efficiency, but
also notably reduces update latency. Our official code and data are available
at https://github.com/EverM0re/LiCoMemory.

</details>


### [475] [CAT-ID$^2$: Category-Tree Integrated Document Identifier Learning for Generative Retrieval In E-commerce](https://arxiv.org/abs/2511.01461)
*Xiaoyu Liu,Fuwei Zhang,Yiqing Wu,Xinyu Jia,Zenghua Xia,Fuzhen Zhuang,Zhao Zhang,Fei Jiang,Wei Lin*

Main category: cs.IR

TL;DR: 本文提出了一种结合类别信息的生成式检索文档标识符学习方法CAT-ID²，通过分层约束、聚类规模约束和分散损失提升ID的表示能力与区分度，在电商场景中显著提升了模糊和长尾查询的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法忽略电商中重要的类别信息，导致文档标识符（DocIDs）表示能力不足，难以兼顾相似性与唯一性。

Method: 提出CAT-ID²方法，包含三个模块：分层类别约束损失（融入层级类别信息）、聚类规模约束损失（均衡ID分布）和分散损失（增强重构文档区分度），在量化过程中融合先验类别知识。

Result: 离线和在线实验表明，该方法有效提升检索性能，线上A/B测试显示对模糊意图查询每千用户订单提升0.33%，长尾查询提升0.24%。

Conclusion: CAT-ID²通过引入类别结构信息显著增强了文档ID的语义表达能力和区分性，为生成式检索中的ID学习提供了有效解决方案，尤其适用于具有丰富类别体系的电商场景。

Abstract: Generative retrieval (GR) has gained significant attention as an effective
paradigm that integrates the capabilities of large language models (LLMs). It
generally consists of two stages: constructing discrete semantic identifiers
(IDs) for documents and retrieving documents by autoregressively generating ID
tokens.The core challenge in GR is how to construct document IDs (DocIDS) with
strong representational power. Good IDs should exhibit two key properties:
similar documents should have more similar IDs, and each document should
maintain a distinct and unique ID.However, most existing methods ignore native
category information, which is common and critical in E-commerce. Therefore, we
propose a novel ID learning method, CAtegory-Tree Integrated Document
IDentifier (CAT-ID$^2$), incorporating prior category information into the
semantic IDs.CAT-ID$^2$ includes three key modules: a Hierarchical Class
Constraint Loss to integrate category information layer by layer during
quantization, a Cluster Scale Constraint Loss for uniform ID token
distribution, and a Dispersion Loss to improve the distinction of reconstructed
documents. These components enable CAT-ID$^2$ to generate IDs that make similar
documents more alike while preserving the uniqueness of different documents'
representations.Extensive offline and online experiments confirm the
effectiveness of our method, with online A/B tests showing a 0.33% increase in
average orders per thousand users for ambiguous intent queries and 0.24% for
long-tail queries.

</details>


### [476] [Trove: A Flexible Toolkit for Dense Retrieval](https://arxiv.org/abs/2511.01857)
*Reza Esfandiarpoor,Max Zuo,Stephen H. Bach*

Main category: cs.IR

TL;DR: Trove是一个开源的检索工具包，简化了研究实验，具备高效的数据管理、灵活的自定义功能和低代码统一管道，支持多节点执行，降低内存消耗并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 为了简化信息检索研究中的实验流程，避免重复存储和处理大规模数据集，同时提供灵活性和高性能。

Method: 设计一个名为Trove的开源检索工具包，支持数据集的动态加载与处理（过滤、选择、转换、组合），提供可定制的组件和低代码评估与负样本挖掘管道，支持多节点并行执行。

Result: Trove将内存消耗降低了2.6倍，推理时间随节点数量线性减少，且无需修改代码即可实现多节点运行，显著提升了实验效率和灵活性。

Conclusion: Trove通过高效的数据管理和易用的定制化功能，显著简化了检索实验，支持探索性研究，是信息检索领域实用且高效的工具。

Abstract: We introduce Trove, an easy-to-use open-source retrieval toolkit that
simplifies research experiments without sacrificing flexibility or speed. For
the first time, we introduce efficient data management features that load and
process (filter, select, transform, and combine) retrieval datasets on the fly,
with just a few lines of code. This gives users the flexibility to easily
experiment with different dataset configurations without the need to compute
and store multiple copies of large datasets. Trove is highly customizable: in
addition to many built-in options, it allows users to freely modify existing
components or replace them entirely with user-defined objects. It also provides
a low-code and unified pipeline for evaluation and hard negative mining, which
supports multi-node execution without any code changes. Trove's data management
features reduce memory consumption by a factor of 2.6. Moreover, Trove's
easy-to-use inference pipeline incurs no overhead, and inference times decrease
linearly with the number of available nodes. Most importantly, we demonstrate
how Trove simplifies retrieval experiments and allows for arbitrary
customizations, thus facilitating exploratory research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [477] [On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.00034)
*Aditya Akella*

Main category: cs.MA

TL;DR: 本文提出了DMARL-RSA，一种完全去中心化的多智能体奖励塑形方法，但在协作导航任务中表现显著低于集中式训练方法MAPPO，揭示了去中心化学习在协调性上的根本局限。


<details>
  <summary>Details</summary>
Motivation: 研究去中心化可学习奖励塑形在多智能体协作环境中的有效性，当前该方向在单智能体环境中已有进展，但在多智能体场景下的表现尚不明确。

Method: 提出DMARL-RSA框架，每个智能体独立学习自身的奖励塑形，在simple_spread_v3环境中进行去中心化训练，并与MAPPO和IPPO等基线方法对比。

Result: DMARL-RSA的平均奖励为-24.20 +/- 0.09，显著低于MAPPO（1.92 +/- 0.87），与IPPO（-23.19 +/- 0.96）相近；尽管在地标覆盖率上更高（DMARL-RSA: 0.888, IPPO: 0.960），但整体性能更差，暴露出局部优化与全局性能之间的协调悖论。实验识别出三个关键障碍：并发策略更新导致的非平稳性、信用分配复杂度呈指数增长、个体奖励优化与全局目标不一致。

Conclusion: 去中心化奖励学习在当前设置下存在性能瓶颈，难以克服多智能体协调的根本挑战，研究结果强调了在复杂协作任务中引入集中式协调机制的必要性。

Abstract: Recent advances in learnable reward shaping have shown promise in
single-agent reinforcement learning by automatically discovering effective
feedback signals. However, the effectiveness of decentralized learnable reward
shaping in cooperative multi-agent settings remains poorly understood. We
propose DMARL-RSA, a fully decentralized system where each agent learns
individual reward shaping, and evaluate it on cooperative navigation tasks in
the simple_spread_v3 environment. Despite sophisticated reward learning,
DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with
centralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs
similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating
that advanced reward shaping cannot overcome fundamental decentralized
coordination limitations. Interestingly, decentralized methods achieve higher
landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out
of 3 total) but worse overall performance than centralized MAPPO (0.273 +/-
0.008 landmark coverage)--revealing a coordination paradox between local
optimization and global performance. Analysis identifies three critical
barriers: (1) non-stationarity from concurrent policy updates, (2) exponential
credit assignment complexity, and (3) misalignment between individual reward
optimization and global objectives. These results establish empirical limits
for decentralized reward learning and underscore the necessity of centralized
coordination for effective multi-agent cooperation.

</details>


### [478] [Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System](https://arxiv.org/abs/2511.00096)
*Shangyu Lou*

Main category: cs.MA

TL;DR: 本文提出了Urban-MAS，一种基于大语言模型的多智能体系统框架，用于零样本下的人本城市预测，显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理特定领域城市任务时表现不佳，难以有效整合复杂城市系统中的异构数据。

Method: 设计了包含三种智能体的Urban-MAS框架：预测因子引导智能体、可靠城市信息提取智能体和多城市信息推理智能体，通过多源信息提取与融合实现城市预测。

Result: 在东京、米兰和西雅图的出行量预测和城市感知任务中，Urban-MAS显著优于单一LLM基线模型，消融实验表明预测因子引导智能体对性能提升最为关键。

Conclusion: Urban-MAS是一种可扩展的人本城市AI预测范式，能够有效利用压缩的城市知识进行零样本预测。

Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban
tasks such as perception prediction and human dynamics. Large Language Models
(LLMs) can integrate multimodal inputs to address heterogeneous data in complex
urban systems but often underperform on domain-specific tasks. Urban-MAS, an
LLM-based Multi-Agent System (MAS) framework, is introduced for human- centered
urban prediction under zero-shot settings. It includes three agent types:
Predictive Factor Guidance Agents, which prioritize key predictive factors to
guide knowledge extraction and enhance the effectiveness of compressed urban
knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve
robustness by com- paring multiple outputs, validating consistency, and
re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which
integrate extracted multi-source information across dimensions for prediction.
Experiments on running-amount prediction and ur- ban perception across Tokyo,
Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors
compared to single-LLM baselines. Ablation studies indicate that Predictive
Factor Guidance Agents are most critical for enhancing predictive performance,
po- sitioning Urban-MAS as a scalable paradigm for human-centered urban AI
prediction. Code is available on the project
website:https://github.com/THETUREHOOHA/UrbanMAS

</details>


### [479] [Sherlock: Reliable and Efficient Agentic Workflow Execution](https://arxiv.org/abs/2511.00330)
*Yeonju Ro,Haoran Qiu,Íñigo Goiri,Rodrigo Fonseca,Ricardo Bianchini,Aditya Akella,Zhangyang Wang,Mattan Erez,Esha Choukse*

Main category: cs.MA

TL;DR: 本文提出了Sherlock，一种通过反事实分析识别代理工作流中易错节点并选择性应用成本最优验证器的方法，以在保证准确性的同时降低延迟和验证成本。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）的广泛应用，代理工作流日益普及，但其多步骤结构容易导致错误传播。现有方法对每一步进行验证会带来高昂的延迟和成本开销，因此需要一种更高效、有针对性的验证机制。

Method: Sherlock采用反事实分析识别最易出错的关键节点，并仅在这些节点上选择性地附加成本最优的验证器；运行时通过推测执行下游任务来减少延迟，验证在后台进行，若验证失败则回滚到上一个已验证输出。

Result: 与无验证基线相比，Sherlock平均准确率提升18.3%；相比非推测执行方式，执行时间最多减少48.7%；相比基于蒙特卡洛搜索的方法，验证成本降低26.0%。

Conclusion: Sherlock通过有原则的、容错感知的验证策略，有效平衡了代理工作流中的效率与可靠性。

Abstract: With the increasing adoption of large language models (LLM), agentic
workflows, which compose multiple LLM calls with tools, retrieval, and
reasoning steps, are increasingly replacing traditional applications. However,
such workflows are inherently error-prone: incorrect or partially correct
output at one step can propagate or even amplify through subsequent stages,
compounding the impact on the final output. Recent work proposes integrating
verifiers that validate LLM output or actions, such as self-reflection, debate,
or LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant
latency and cost overheads.
  In this work, we seek to answer three key questions: which nodes in a
workflow are most error-prone and thus deserve costly verification, how to
select the most appropriate verifier for each node, and how to use verification
with minimal impact to latency? Our solution, Sherlock, addresses these using
counterfactual analysis on agentic workflows to identify error-prone nodes and
selectively attaching cost-optimal verifiers only where necessary. At runtime,
Sherlock speculatively executes downstream tasks to reduce latency overhead,
while verification runs in the background. If verification fails, execution is
rolled back to the last verified output. Compared to the non-verifying
baseline, Sherlock delivers an 18.3% accuracy gain on average across
benchmarks. Sherlock reduces workflow execution time by up to 48.7% over
non-speculative execution and lowers verification cost by 26.0% compared to the
Monte Carlo search-based method, demonstrating that principled, fault-aware
verification effectively balances efficiency and reliability in agentic
workflows.

</details>


### [480] [Spatial Crowdsourcing-based Task Allocation for UAV-assisted Maritime Data Collection](https://arxiv.org/abs/2511.00387)
*Xiaoling Han,Bin Lin,Zhenyu Na,Bowen Li,Chaoyue Zhang,Ran Zhang*

Main category: cs.MA

TL;DR: 本文提出了一种基于空间众包（SC）的无人机辅助 maritime 数据收集（MDC）任务分配模型，设计了SC-MDC-TA算法，结合信号干扰噪声比和能耗评估任务质量，并利用逆向拍卖机制减少等待时间。仿真结果表明该算法能有效分配任务，缩短完成时间并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着海上服务的发展，无人机辅助的 maritime 数据收集任务日益多样化和复杂化，亟需高效的任务分配机制以应对不同场景下的时空需求和无人机移动性挑战。

Method: 引入空间众包概念，建立SC-based MDC网络模型，设计SC-MDC-TA算法，结合质量估计（SINR与能耗）和逆向拍卖机制进行任务分配，并基于电子海图构建典型任务场景。

Result: 仿真结果显示，SC-MDC-TA算法在多种MDC场景中能有效分配任务，相比基准方法可显著减少任务完成时间和无人机能耗。

Conclusion: 所提出的SC-MDC-TA算法在保障任务执行质量的同时，提升了任务分配效率，降低了能耗和等待时间，适用于复杂多变的海上服务环境。

Abstract: Driven by the unceasing development of maritime services, tasks of unmanned
aerial vehicle (UAV)-assisted maritime data collection (MDC) are becoming
increasingly diverse, complex and personalized. As a result, effective task
allocation for MDC is becoming increasingly critical. In this work, integrating
the concept of spatial crowdsourcing (SC), we develop an SC-based MDC network
model and investigate the task allocation problem for UAV-assisted MDC. In
variable maritime service scenarios, tasks are allocated to UAVs based on the
spatial and temporal requirements of the tasks, as well as the mobility of the
UAVs. To address this problem, we design an SC-based task allocation algorithm
for the MDC (SC-MDC-TA). The quality estimation is utilized to assess and
regulate task execution quality by evaluating signal to interference plus noise
ratio and the UAV energy consumption. The reverse auction is employed to
potentially reduce the task waiting time as much as possible while ensuring
timely completion. Additionally, we establish typical task allocation scenarios
based on maritime service requirements indicated by electronic navigational
charts. Simulation results demonstrate that the proposed SC-MDC-TA algorithm
effectively allocates tasks for various MDC scenarios. Furthermore, compared to
the benchmark, the SC-MDC-TA algorithm can also reduce the task completion time
and lower the UAV energy consumption.

</details>


### [481] [AgentGit: A Version Control Framework for Reliable and Scalable LLM-Powered Multi-Agent Systems](https://arxiv.org/abs/2511.00628)
*Yang Li,Siqi Ping,Xiyu Chen,Xiaojian Qi,Zigan Wang,Ye Luo,Xiaowei Zhang*

Main category: cs.MA

TL;DR: AgentGit是一个基于LangGraph的多智能体系统框架，引入了类似Git的回滚和分支功能，提升了系统的可靠性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多智能体系统框架在复杂任务中面临可靠性和可扩展性不足的问题，缺乏有效的状态管理和探索机制。

Method: 构建一个名为AgentGit的基础设施层，支持状态提交、回退和分支，使智能体能够高效地遍历、比较和探索多个执行路径，并在真实任务中进行多步A/B测试以优化提示选择。

Result: 实验表明，AgentGit显著减少了冗余计算，降低了运行时间和token消耗，并支持多分支并行探索，在检索和分析论文摘要任务中优于LangGraph、AutoGen和Agno基线系统。

Conclusion: AgentGit为多智能体系统提供了更鲁棒的设计路径，支持错误恢复、安全探索、迭代调试和A/B测试，推动了协作式AI系统的发展。

Abstract: With the rapid progress of large language models (LLMs), LLM-powered
multi-agent systems (MAS) are drawing increasing interest across academia and
industry. However, many current MAS frameworks struggle with reliability and
scalability, especially on complex tasks. We present AgentGit, a framework that
brings Git-like rollback and branching to MAS workflows. Built as an
infrastructure layer on top of LangGraph, AgentGit supports state commit,
revert, and branching, allowing agents to traverse, compare, and explore
multiple trajectories efficiently. To evaluate AgentGit, we designed an
experiment that optimizes target agents by selecting better prompts. We ran a
multi-step A/B test against three baselines -- LangGraph, AutoGen, and Agno --
on a real-world task: retrieving and analyzing paper abstracts. Results show
that AgentGit significantly reduces redundant computation, lowers runtime and
token usage, and supports parallel exploration across multiple branches,
enhancing both reliability and scalability in MAS development. This work offers
a practical path to more robust MAS design and enables error recovery, safe
exploration, iterative debugging, and A/B testing in collaborative AI systems.

</details>


### [482] [Predictive Auxiliary Learning for Belief-based Multi-Agent Systems](https://arxiv.org/abs/2511.01078)
*Qinwei Huang,Stefan Wang,Simon Khan,Garrett Katz,Qinru Qiu*

Main category: cs.MA

TL;DR: 提出了一种基于信念的预测性辅助学习框架BEPAL，通过引入预测不可见状态信息的辅助任务，提升多智能体强化学习在部分可观测环境中的训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法主要依赖奖励信号进行策略训练，在部分可观测环境下信息聚合不足，导致训练不稳定。因此需要引入额外的学习机制来增强表征学习和训练稳定性。

Method: 提出BEPAL框架，采用中心化训练与去中心化执行范式，每个智能体在学习策略的同时，构建信念模型以预测其他智能体的奖励或运动方向等不可观测状态，作为辅助任务目标。

Result: 在捕食者-猎物环境和Google Research Football中验证，性能指标平均提升约16%，且收敛更稳定。

Conclusion: BEPAL通过引入预测性辅助任务，有效丰富了隐藏状态表示，提升了多智能体系统在部分可观测环境下的学习效率和整体性能。

Abstract: The performance of multi-agent reinforcement learning (MARL) in partially
observable environments depends on effectively aggregating information from
observations, communications, and reward signals. While most existing
multi-agent systems primarily rely on rewards as the only feedback for policy
training, our research shows that introducing auxiliary predictive tasks can
significantly enhance learning efficiency and stability. We propose
Belief-based Predictive Auxiliary Learning (BEPAL), a framework that
incorporates auxiliary training objectives to support policy optimization.
BEPAL follows the centralized training with decentralized execution paradigm.
Each agent learns a belief model that predicts unobservable state information,
such as other agents' rewards or motion directions, alongside its policy model.
By enriching hidden state representations with information that does not
directly contribute to immediate reward maximization, this auxiliary learning
process stabilizes MARL training and improves overall performance. We evaluate
BEPAL in the predator-prey environment and Google Research Football, where it
achieves an average improvement of about 16 percent in performance metrics and
demonstrates more stable convergence compared to baseline methods.

</details>


### [483] [Credit Network Modeling and Analysis via Large Language Models](https://arxiv.org/abs/2511.01136)
*Enbo Sun,Yongzhao Wang,Hao Zhou*

Main category: cs.MA

TL;DR: 本文研究了如何利用大语言模型（LLMs）从企业的文本财务报表中构建信用网络，并分析其结构。LLMs被用于将每份财务报表转化为企业特定的信用网络，再聚合为整个金融系统的综合网络，过程中可自动检测不一致并引入人工干预。研究还展示了LLMs在解决组合优化问题（如投资组合压缩和债务清除）中的推理能力，能有效提升网络整体表现。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以从非结构化财务文本中高效构建准确的信用网络，且缺乏对系统性风险和优化策略的深入分析。本文旨在探索LLMs在自动化构建与分析金融信用网络方面的潜力，以提高金融系统透明度与稳定性。

Method: 使用大语言模型将企业的财务报表转化为个体信用网络，随后进行聚合形成全局信用网络；通过合成与真实数据集评估LLMs在检测不一致性和执行金融操作（如投资组合压缩、债务清除）中的推理能力。

Result: LLMs能够有效翻译不同拓扑结构的财务语句为信用网络，并在聚合过程中识别不一致信息；在组合优化任务中，LLMs展现出良好的推理能力，能提出提升总资产等性能指标的有效策略。

Conclusion: 大语言模型不仅能准确构建企业级和系统级信用网络，还能进行复杂的金融决策推理，为金融风险管理与政策制定提供了新的自动化工具。

Abstract: We investigate the application of large language models (LLMs) to construct
credit networks from firms' textual financial statements and to analyze the
resulting network structures. We start with using LLMs to translate each firm's
financial statement into a credit network that pertains solely to that firm.
These networks are then aggregated to form a comprehensive credit network
representing the whole financial system. During this process, the
inconsistencies in financial statements are automatically detected and human
intervention is involved. We demonstrate that this translation process is
effective across financial statements corresponding to credit networks with
diverse topological structures. We further investigate the reasoning
capabilities of LLMs in analyzing credit networks and determining optimal
strategies for executing financial operations to maximize network performance
measured by the total assets of firms, which is an inherently combinatorial
optimization challenge. To demonstrate this capability, we focus on two
financial operations: portfolio compression and debt removal, applying them to
both synthetic and real-world datasets. Our findings show that LLMs can
generate coherent reasoning and recommend effective executions of these
operations to enhance overall network performance.

</details>


### [484] [From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models](https://arxiv.org/abs/2511.01310)
*Sureyya Akin,Kavita Srivastava,Prateek B. Kapoor,Pradeep G. Sethi,Sunita Q. Patel,Rahu Srivastava*

Main category: cs.MA

TL;DR: 提出一种基于共享生成式多模态世界模型（MWM）的多智能体强化学习框架，通过在潜在空间中进行策略训练，显著提升从高维多模态感知输入中学习协作策略的样本效率。


<details>
  <summary>Details</summary>
Motivation: 直接从像素和音频等高维多模态输入中学习多智能体协作策略存在严重的样本效率低下问题，现有无模型MARL方法在表征学习、部分可观测性和信用分配方面面临挑战。

Method: 构建一个共享的生成式多模态世界模型（MWM），使用基于注意力的机制融合各智能体的多模态观测，学习环境动态的压缩潜在表示，并在此潜在空间中训练多智能体策略（如MAPPO）。

Result: 在基于3D物理模拟器的新多智能体基准上，MWM-MARL框架相比最先进的无模型MARL基线实现了数量级更高的样本效率；多模态融合对处理感知不对称至关重要，且模型对传感器失效具有更强鲁棒性。

Conclusion: 将多模态世界模型与多智能体强化学习结合，能有效解耦表征学习与策略学习，显著提升样本效率和系统鲁棒性，适用于复杂真实场景的部署。

Abstract: Learning cooperative multi-agent policies directly from high-dimensional,
multimodal sensory inputs like pixels and audio (from pixels) is notoriously
sample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)
algorithms struggle with the joint challenge of representation learning,
partial observability, and credit assignment. To address this, we propose a
novel framework based on a shared, generative Multimodal World Model (MWM). Our
MWM is trained to learn a compressed latent representation of the environment's
dynamics by fusing distributed, multimodal observations from all agents using a
scalable attention-based mechanism. Subsequently, we leverage this learned MWM
as a fast, "imagined" simulator to train cooperative MARL policies (e.g.,
MAPPO) entirely within its latent space, decoupling representation learning
from policy learning. We introduce a new set of challenging multimodal,
multi-agent benchmarks built on a 3D physics simulator. Our experiments
demonstrate that our MWM-MARL framework achieves orders-of-magnitude greater
sample efficiency compared to state-of-the-art model-free MARL baselines. We
further show that our proposed multimodal fusion is essential for task success
in environments with sensory asymmetry and that our architecture provides
superior robustness to sensor-dropout, a critical feature for real-world
deployment.

</details>


### [485] [An Explanation-oriented Inquiry Dialogue Game for Expert Collaborative Recommendations](https://arxiv.org/abs/2511.01489)
*Qurat-ul-ain Shaheen,Katarzyna Budzynska,Carles Sierra*

Main category: cs.MA

TL;DR: 提出了一种基于需求分析的医疗专家协作对话的探究对话游戏，以在多智能体系统设计中引入可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了支持具有不同知识基础的医疗专家之间的协作决策，并增强多智能体系统中的可解释性。

Method: 设计并实现了一个结合基于解释的言外之力的探究对话游戏，并开发了原型网页应用，通过形成性用户研究进行评估。

Result: 用户研究表明该对话游戏满足了医疗专家协作的需求，并揭示了基于对话的沟通工具在医疗领域的实际价值。

Conclusion: 所提出的对话游戏有效支持了医疗专家的协作与解释生成，具备在真实医疗场景中应用的潜力。

Abstract: This work presents a requirement analysis for collaborative dialogues among
medical experts and an inquiry dialogue game based on this analysis for
incorporating explainability into multiagent system design. The game allows
experts with different knowledge bases to collaboratively make recommendations
while generating rich traces of the reasoning process through combining
explanation-based illocutionary forces in an inquiry dialogue. The dialogue
game was implemented as a prototype web-application and evaluated against the
specification through a formative user study. The user study confirms that the
dialogue game meets the needs for collaboration among medical experts. It also
provides insights on the real-life value of dialogue-based communication tools
for the medical community.

</details>


### [486] [Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning](https://arxiv.org/abs/2511.01554)
*Aditya Kapoor,Yash Bhisikar,Benjamin Freed,Jan Peters,Mingfei Sun*

Main category: cs.MA

TL;DR: 本文扩展了可微分离散通信学习（DDCL）框架，支持无界信号并实现位级消息精度优化，作为即插即用模块集成到多智能体强化学习（MARL）中，在显著降低带宽的同时保持甚至提升性能，并验证了简单架构结合DDCL可媲美复杂设计的“苦涩教训”。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，通信受限于带宽，而以往方法仅决定是否通信，无法控制通信精度。因此，需要一种能动态调节消息精度的方法以更高效地利用有限带宽。

Method: 通过推广可微分离散通信学习（DDCL）框架，使其支持无界信号输入，并实现端到端的离散消息优化。该方法采用梯度近似技术解决离散化带来的梯度中断问题，形成一个通用、可插入任何MARL架构的通信层。

Result: 1) 在受控环境中展示了智能体如何根据任务信息需求动态调整消息精度；2) 将DDCL集成到四种先进的MARL算法中，带宽减少一个数量级以上，性能持平或更好；3) 验证了‘苦涩教训’：基于Transformer的简单策略结合DDCL即可达到复杂专用架构的性能。

Conclusion: DDCL的扩展版本能够有效实现多智能体间高效率、自适应精度的通信，显著降低带宽开销，且表明在通信设计中，简单通用架构结合良好机制可能优于复杂的专用设计。

Abstract: Effective communication in multi-agent reinforcement learning (MARL) is
critical for success but constrained by bandwidth, yet past approaches have
been limited to complex gating mechanisms that only decide \textit{whether} to
communicate, not \textit{how precisely}. Learning to optimize message precision
at the bit-level is fundamentally harder, as the required discretization step
breaks gradient flow. We address this by generalizing Differentiable Discrete
Communication Learning (DDCL), a framework for end-to-end optimization of
discrete messages. Our primary contribution is an extension of DDCL to support
unbounded signals, transforming it into a universal, plug-and-play layer for
any MARL architecture. We verify our approach with three key results. First,
through a qualitative analysis in a controlled environment, we demonstrate
\textit{how} agents learn to dynamically modulate message precision according
to the informational needs of the task. Second, we integrate our variant of
DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth
by over an order of magnitude while matching or exceeding task performance.
Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL
communication: a simple Transformer-based policy leveraging DDCL matches the
performance of complex, specialized architectures, questioning the necessity of
bespoke communication designs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [487] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: 本文综述了生成式人工智能在汽车行业的应用现状，涵盖车辆设计、制造、自动驾驶、预测性维护和车载用户体验等方面，重点探讨了生成对抗网络和变分自编码器等技术。文章强调了合成数据生成、个性化人机交互等机遇，并指出计算需求、偏见、知识产权和对抗鲁棒性等挑战。通过梅赛德斯-奔驰MBUX虚拟助手的案例研究，展示了生成式AI在语音人机界面中的优势，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在深刻影响汽车行业，但其在不同领域的应用潜力与挑战尚需系统梳理，尤其在语音人机交互方面缺乏综合研究。

Method: 采用文献综述与案例分析相结合的方法，系统总结生成式AI在汽车领域的技术基础、应用场景及挑战，并以梅赛德斯-奔驰MBUX虚拟助手为案例进行实证分析。

Result: 明确了生成式AI在加速自动驾驶验证、优化设计和提升用户体验方面的关键作用，识别了技术、伦理与安全方面的核心挑战，并验证了其在语音HMI中实现更自然、主动和个性化交互的能力。

Conclusion: 生成式AI在汽车行业具有广阔前景，但需在安全、效率与用户中心之间取得平衡，未来研究应聚焦于解决计算资源、模型鲁棒性与伦理问题，推动负责任的技术部署。

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [488] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: 提出STRIDER框架，通过空间结构约束和任务对齐调节，显著提升零样本视觉-语言导航在连续环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在零样本视觉-语言导航中因缺乏结构化决策和动作反馈整合，导致长程导航不稳健。

Method: 引入结构化航点生成器限制动作空间，并设计任务对齐调节器利用动态反馈调整行为。

Result: 在R2R-CE和RxR-CE基准上显著优于SOTA，成功率从29%提升至35%，相对提高20.7%。

Conclusion: 空间约束的决策和反馈引导执行对提升零样本VLN-CE导航精度至关重要。

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [489] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: 本文提出BiBo框架，利用现成的视觉语言模型（VLM）控制人形代理，通过具身指令编译器将高级指令转化为低层命令，并结合扩散模型生成自适应的人类动作，在开放环境中实现90.2%的任务成功率，显著减少对大规模数据收集的依赖。


<details>
  <summary>Details</summary>
Motivation: 人形代理在开放环境中难以应对灵活多样的交互，传统依赖大量数据训练的方法成本过高，因此需要一种无需大量数据即可实现强泛化能力的解决方案。

Method: 提出BiBo框架，包含两个核心组件：（1）具身指令编译器，使VLM能感知环境并将高层指令转化为带参数的底层命令；（2）基于扩散模型的动作执行器，根据命令生成自然动作并动态响应环境反馈。

Result: 实验表明，BiBo在开放环境中的任务成功率达到90.2%，文本引导动作执行精度比先前方法提升16.3%。

Conclusion: BiBo通过整合现成VLM与扩散模型，有效实现了人形代理在复杂开放环境中的高精度、多样化动作控制，为减少数据依赖提供了可行路径。

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [490] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出了Alpamayo-R1（AR1），一种结合因果推理与轨迹规划的视觉-语言-动作模型，通过可解释的因果推理提升自动驾驶在长尾场景中的决策能力，在复杂驾驶场景中显著提高了规划准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型在安全关键的长尾场景中表现脆弱，主要由于监督信号稀疏且缺乏因果理解，因此需要引入具备因果推理能力的决策机制以提升鲁棒性与可解释性。

Method: 提出AR1模型，包含三个核心创新：构建基于混合自动标注与人工介入的因果链（CoC）数据集；采用模块化视觉-语言-动作架构，结合预训练的Cosmos-Reason模型与扩散轨迹解码器；设计多阶段训练策略，结合监督微调与强化学习，利用大推理模型反馈优化推理质量与推理-动作一致性。

Result: AR1在挑战性场景中比纯轨迹基线模型规划准确率最高提升12%，闭环仿真中偏离道路率降低35%，近距离遭遇率降低25%；强化学习后训练使推理质量提升45%，推理-动作一致性提升37%；模型参数从0.5B扩展到7B持续改善性能；实车测试验证了99ms延迟下的实时性与城市部署能力。

Conclusion: AR1通过融合可解释的因果推理与精确控制，为实现L4级自动驾驶提供了一条可行路径。

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [491] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: 提出一种基于数字孪生的自主动态重构机器人控制器框架，通过虚拟环境仿真优化路径和控制参数，实现机器人在动态环境中快速自适应。


<details>
  <summary>Details</summary>
Motivation: 传统控制系统难以快速适应动态环境中的地形和条件变化，导致效率低下或操作失败。

Method: 利用数字孪生技术构建机器人运行环境的虚拟副本，模拟并优化运动轨迹，并将更新后的控制代码部署到物理机器人。

Result: 实现了机器人在真实环境变化下的快速、可靠自适应，无需人工干预。

Conclusion: 该方法推动了数字孪生在机器人领域的集成，为智能动态环境中的自主性提升提供了可扩展的解决方案。

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [492] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: 本文提出了Real-DRL框架，用于安全关键型自主系统，通过DRL-Student、PHY-Teacher和Trigger三个组件协同工作，实现在真实物理系统中的安全高效策略学习。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在真实系统中应用时面临的安全性挑战，特别是未知未知因素和仿真到现实差距带来的风险。

Method: 提出由DRL-Student（基于双自学习与教学相长范式）、PHY-Teacher（基于物理模型的安全策略）和Trigger（协调两者交互）组成的Real-DRL框架，结合实时安全感知批采样和分层学习机制。

Result: 在四足机器人和倒立摆系统上的实验表明，该框架能有效保证安全性，实现自动化的安全优先学习，并提升性能，且具备应对极端情况的能力。

Conclusion: Real-DRL框架能够在确保安全的前提下，实现真实环境中DRL智能体的高效学习，为安全关键系统提供了可靠的学习控制解决方案。

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [493] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: 提出一种共享自主框架，结合人类操作与自主精细控制，高效收集高质量的灵巧操作演示数据，并通过端到端VLA策略实现90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有数据采集方法受限于人工操作负担重或自动规划动作不自然，难以扩展高质量训练数据以实现机器人灵巧操作。

Method: 提出共享自主框架，将宏观运动（手臂姿态）由人类通过VR遥操作控制，微观运动（手部精细操作）由基于视觉-语言-动作的DexGrasp-VLA策略自主完成；引入臂-手特征增强模块和纠错式遥操作机制，提升协调性与策略性能。

Result: 该框架显著降低认知负荷，高效生成高质量协调性臂-手演示数据；训练出的端到端VLA策略在多种物体上达到90%的成功率，包括未见物体。

Conclusion: 所提框架能有效结合人机优势，显著提升灵巧操作数据质量和策略性能，推动通用机器人实现类人操作能力的发展。

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [494] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: 本文提出了EgoMI框架，通过捕捉人类操作任务中的同步末端执行器与主动头部运动轨迹，解决由自我中心演示数据带来的具身差异问题，并引入记忆增强策略应对快速视角变化，在双臂机器人上验证了该方法在手眼协同模仿学习中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于人类演示中存在主动的头-手协调和视觉注视行为，而静态机器人感知系统难以复制这些动态视角变化，导致模仿学习中出现显著分布偏移，影响策略性能。因此需要一种能够桥接人机具身差异的方法。

Method: 提出EgoMI框架，同步采集操作过程中的末端执行器与主动头部运动轨迹，并设计记忆增强策略以选择性地利用历史观测信息来应对大范围快速视角变化，从而实现对半人形机器人的有效策略迁移。

Result: 在配备可动摄像头头部的双臂机器人上进行实验表明，包含显式头部运动建模的策略始终优于基线方法，且EgoMI能有效缓解因视角动态变化带来的性能下降。

Conclusion: EgoMI通过联合建模头部运动与操作动作，成功缩小了人类与机器人之间的具身差距，为半人形机器人实现了更鲁棒的模仿学习。

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [495] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: 使用时间序列基础模型（如Chronos）基于前8次抓取预测合成试验，可将Kinarm视觉引导抓取评估时间从4-5分钟缩短至约1分钟，同时保持运动学参数的高可靠性（ICC ≥ 0.90），显著减少卒中患者测试负担。


<details>
  <summary>Details</summary>
Motivation: Kinarm机器人上的视觉引导抓取（VGR）虽能提供敏感的运动学生物标志物，但需40-64次抓取，耗时且易导致疲劳。本研究旨在探索是否可用时间序列基础模型通过早期少量抓取预测未记录的试验，从而减少测试次数并保持参数可靠性。

Method: 分析了461名卒中患者和599名对照者在4目标和8目标任务中的VGR速度信号。仅保留前8或16次实际抓取，使用ARIMA、MOMENT和Chronos模型在70%受试者上微调后预测合成试验。将真实与预测试验结合，重新计算反应时间、运动时间、姿势速度和最大速度，并与完整试验结果用ICC(2,1)比较可靠性。

Result: 仅用8次实测加Chronos预测即可使所有参数ICC ≥ 0.90，可靠性相当于24-28次实测（差异≤0.07）。MOMENT效果中等，ARIMA提升有限。合成试验在不同队列和协议下均未显著降低参数可靠性。

Conclusion: 基于基础模型的预测可大幅缩短Kinarm VGR评估时间，尤其对最严重卒中患者，测试时间从4-5分钟减至约1分钟，同时保持精度，有望实现更高效的卒中后运动功能评估。

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [496] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: 本研究探讨了基于本体感觉定制的机器人训练对中风幸存者手功能和神经处理的改善效果，提出两种个性化训练方法，并通过随机对照试验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提高中风后运动康复的效果，研究人员希望探索是否可以通过个性化定制的本体感觉训练来增强手部功能和神经反馈处理能力。

Method: 使用机器人手指外骨骼，测试两种本体感觉定制训练方法：Propriopixel训练（通过游戏化动作增强本体感觉处理）和虚拟辅助训练（减少机器人辅助以增强自主反馈依赖）。在一项随机对照试验中，46名慢性中风患者接受了标准训练、Propriopixel或虚拟辅助训练，共进行9次、每次2小时的训练。

Result: 在存在本体感觉缺陷的参与者中，Propriopixel训练使Box and Block Test得分平均提升7个（±4.2，p=0.002），虚拟辅助训练提升4.5个（±4.4，p=0.068），而标准训练仅提升0.8个（±2.3）。本体感觉改善与手功能提升呈正相关。此外，定制训练增强了大脑对本体感觉线索的神经敏感性，体现在新EEG生物标志物——本体感觉性期待负波（Contingent Negative Variation）的变化上。

Conclusion: 本体感觉定制训练可有效提升中风患者的神经感知与手部功能，为实现精准神经康复提供了可行路径。

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [497] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文揭示了滑动窗口因子图优化（SW-FGO）与卡尔曼滤波变体（如EKF、IEKF等）之间的理论关系，提出了递归FGO（Re-FGO）框架，在特定条件下可精确还原各类EKF方法，并展示了SW-FGO在非线性、非高斯场景下的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管SW-FGO在导航中表现出优越性能，但其与卡尔曼滤波类方法的理论关系尚不清晰，缺乏公平比较的基础。

Method: 通过引入马尔可夫假设、高斯噪声与L2损失及单状态窗口条件，提出Re-FGO框架，将KFV统一到SW-FGO形式下，并在相同条件下对比两者性能。

Result: Re-FGO在设定条件下能精确复现EKF/IEKF/REKF/RIEKF；SW-FGO在非线性、非高斯情况下具有可预测计算代价下的显著优势。

Conclusion: SW-FGO不仅是KFV的推广，且在复杂实际场景中具备更强的估计能力与更优的工程集成潜力，特别是在数值估计和深度学习融合方面。

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [498] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: 本文提出了一种名为SonarSweep的端到端深度学习框架，通过将基于原理的平面扫描算法应用于声呐与视觉数据的跨模态融合，解决了水下低能见度环境中3D重建的难题。


<details>
  <summary>Details</summary>
Motivation: 在视觉退化的水下环境中，单一模态方法（如基于视觉或声呐）均存在局限性，且现有融合技术依赖启发式方法和错误几何假设，导致重建效果差。因此需要一种更鲁棒、准确的跨模态融合方法。

Method: 提出SonarSweep框架，借鉴平面扫描算法的思想，结合声呐和立体视觉数据进行端到端的深度学习，实现跨模态特征对齐与深度融合。

Result: 在高保真仿真和真实世界环境中实验表明，SonarSweep能够生成密集且精确的深度图，在各种挑战性条件下（尤其是高浑浊度环境）显著优于现有最先进方法。

Conclusion: SonarSweep有效克服了传统单模态和融合方法的缺陷，实现了更高质量的水下3D重建，作者还将公开代码和首个同步立体相机-声呐数据集以推动后续研究。

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [499] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: 本文提出了一类基于经典Runge-Kutta积分方法的新型圆锥误差补偿算法，能够有效处理陀螺仪在旋转过程中的积分问题，并提供生成高阶算法的清晰流程。


<details>
  <summary>Details</summary>
Motivation: 由于现代车辆采用捷联式系统，陀螺仪在积分过程中需考虑旋转带来的圆锥效应，因此需要有效的圆锥补偿算法。

Method: 基于经典的Runge-Kutta积分方法构建新的圆锥误差补偿算法，并通过简化情况验证其与现有流行算法的一致性。

Result: 新方法可退化为一种广泛使用的圆锥算法，并提供了构造更高阶算法的系统化途径。

Conclusion: 该算法框架不仅具有理论清晰性，还为提升导航系统中陀螺积分精度提供了实用且可扩展的解决方案。

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [500] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文介绍了一种用于月球机器人MoonBot的桶鼓式挖掘工具的开发，该工具通过3D打印技术制造，并在沙箱测试中验证了其挖掘效率。


<details>
  <summary>Details</summary>
Motivation: 为了支持月球资源的可持续利用，需要高效的就地资源利用（ISRU）技术，而挖掘月壤是实现这一目标的第一步。

Method: 设计并制造了一个由PLA材料3D打印而成的桶鼓原型，并通过一系列沙箱测试评估其挖掘效率。

Result: 该工具重4.8公斤，容积14.06升，在连续作业模式下可实现每小时777.54公斤的挖掘速率，单位能耗为0.022 Wh/kg；在分批作业模式下，挖掘速率为每小时172.02公斤，单位能耗为0.86 Wh/kg。

Conclusion: 实验结果表明该桶鼓挖掘工具概念成功实现，且与模块化的MoonBot平台兼容，有利于灵活高效的任务规划，未来可集成传感器和自主控制系统以进一步提升性能。

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [501] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: 提出一种新的双足机器人平衡控制方法，利用描述性低维模型维持平衡，同时允许高维状态空间中的自由运动，实现高效、类人且鲁棒的行走。


<details>
  <summary>Details</summary>
Motivation: 传统双足平衡控制依赖简化低维模型，导致行走效率低下（如膝盖弯曲），限制了机器人的自然运动。

Method: 采用仅包含维持平衡所需最少自由度的描述性低维模型，不强制全模型跟踪预设轨迹，允许其余自由度在高维空间中自由演化。

Result: 实现了更高效、类人化的行走步态，并提高了系统的鲁棒性。

Conclusion: 通过解耦低维平衡控制与高维运动执行，该方法避免了传统模型约束的弊端，为双足机器人平衡控制提供了更自然、灵活的解决方案。

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [502] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: 提出一种基于折纸结构的多指混合夹持器，具有被动变形能力，可实现稳定抓取和同时抓取多个物体。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人夹持器依赖笨重的执行器、复杂的控制策略或高级触觉传感，难以实现稳定可靠的抓取。

Method: 设计包含并行折纸模块的多指混合夹持器，每个手指由单自由度执行机构驱动，利用被动变形实现自适应形状匹配和恒定力输出，无需主动感知或反馈控制。

Result: 实现了对不同形状和尺寸堆叠物体的同时抓取、搬运和独立放置，显著提高了操作效率。

Conclusion: 基于折纸的柔性结构可作为可扩展模块，用于家庭和工业场景中的自适应、稳定且高效的多物体操作。

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [503] [Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy](https://arxiv.org/abs/2511.00555)
*Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 提出了一种名为D3P的深度Koopman增强双分支扩散策略，通过解耦视觉与本体感知输入，在模拟和真实机器人任务中显著提升了模仿学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的策略在处理多步时序依赖和融合本体感知输入时表现不佳，容易过拟合于本体感知信号而忽略视觉特征，导致任务失败。

Method: 设计双分支架构：视觉分支用于编码视觉观测以指示任务进展，融合分支结合视觉与本体感知输入进行精确操作；引入Deep Koopman模块捕捉视觉输入的时序动态，并利用生成模型的测试时损失作为置信度信号来整合重叠的动作块。

Result: 在六个RLBench桌面任务中平均比现有扩散策略提升14.6%，在三个真实世界任务中提升15.0%。

Conclusion: D3P有效增强了视觉引导的恢复能力与时序建模，提高了机器人操作策略的鲁棒性和成功率。

Abstract: Integrating generative models with action chunking has shown significant
promise in imitation learning for robotic manipulation. However, the existing
diffusion-based paradigm often struggles to capture strong temporal
dependencies across multiple steps, particularly when incorporating
proprioceptive input. This limitation can lead to task failures, where the
policy overfits to proprioceptive cues at the expense of capturing the visually
derived features of the task. To overcome this challenge, we propose the Deep
Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a
dual-branch architecture to decouple the roles of different sensory modality
combinations. The visual branch encodes the visual observations to indicate
task progression, while the fused branch integrates both visual and
proprioceptive inputs for precise manipulation. Within this architecture, when
the robot fails to accomplish intermediate goals, such as grasping a drawer
handle, the policy can dynamically switch to execute action chunks generated by
the visual branch, allowing recovery to previously observed states and
facilitating retrial of the task. To further enhance visual representation
learning, we incorporate a Deep Koopman Operator module that captures
structured temporal dynamics from visual inputs. During inference, we use the
test-time loss of the generative model as a confidence signal to guide the
aggregation of the temporally overlapping predicted action chunks, thereby
enhancing the reliability of policy execution. In simulation experiments across
six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion
policy by an average of 14.6\%. On three real-world robotic manipulation tasks,
it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

</details>


### [504] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种名为Multi-Mapcher的新框架，用于异构LiDAR传感器的多会话SLAM，通过大规模地图配准实现跨会话初始对齐，并结合锚节点优化构建一致全局地图，显著提升了性能与速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于回环检测的多会话SLAM方法在异构LiDAR传感器下因点云密度和视场差异导致性能下降，需摆脱对回环检测模块的依赖。

Method: 提出Multi-Mapcher框架，采用鲁棒的大规模地图到地图配准实现初始对齐，随后通过半径搜索检测跨会话回环，并利用锚节点增强的鲁棒位姿图优化构建全局一致地图。

Result: 实验表明，该方法在多种LiDAR传感器下均优于现有方法，且运行速度更快。

Conclusion: Multi-Mapcher通过摒弃传统回环主导范式，验证了大规模点云配准在多会话SLAM中的可行性与优势，为异构传感器融合提供了新思路。

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [505] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种语义引导的模糊控制框架，结合大语言模型与可解释控制，实现水下多机器人在无GPS、无地图环境中的协作覆盖。


<details>
  <summary>Details</summary>
Motivation: 针对水下环境中感知不完整、通信受限、缺乏全局定位等问题，现有方法难以实现高效多机器人协同覆盖。

Method: 利用大语言模型将多模态观测压缩为语义标记，通过模糊推理系统生成稳定控制指令，并引入基于语言形式的语义通信实现轻量级多机协调。

Result: 在类珊瑚礁未知环境中进行了大量仿真，结果表明该方法在有限感知与通信条件下实现了高效的对象导向导航与协作覆盖。

Conclusion: 所提框架有效弥合了语义认知与分布式水下控制之间的差距，提升了复杂水下环境中的适应性与探索效率。

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [506] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: 本文提出一种基于改进滑动窗口Hankel动态模态分解（Hankel-DMD）的在线非线性运动预测框架，可实现实时去噪与多步预测，并通过残差分析提供方差追踪信号，适用于实时控制系统的集成。


<details>
  <summary>Details</summary>
Motivation: 在自主系统中，常需从部分且含噪声的数据中预测周围智能体的运动，传统方法难以同时实现高效去噪与实时非线性建模，因此需要一种能在实时条件下学习并预测非线性动态的方法。

Method: 采用改进的滑动窗口Hankel-DMD框架，将部分观测数据嵌入Hankel矩阵，利用Page矩阵进行奇异值硬阈值（SVHT）估计有效秩，通过Cadzow投影实现结构化低秩去噪，进而构建时变的Hankel-DMD提升线性预测器以实现多步预测，并利用残差分析进行方差跟踪。

Result: 在高斯和重尾噪声下的仿真以及动态吊车实验平台上验证了该方法的有效性，结果表明该方法能实现稳定的方差感知去噪和短时域预测，适合集成到实时控制系统中。

Conclusion: 所提出的在线Hankel-DMD方法能够从部分、噪声数据中实时学习非线性动态并进行预测，具备良好的鲁棒性和实用性，为实时预测与风险感知规划提供了有效工具。

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [507] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 提出一种基于学习的双足行走框架，结合启发式步态规划策略，通过期望躯干速度跟踪实现精确环境交互，无需复杂步态规划和解析模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂的全动力学或简化动力学模型进行步态规划，限制了在非结构化环境中的灵活性和鲁棒性，因此需要一种更简单高效的方法。

Method: 采用基于启发式的步态规划策略，结合Raibert型控制器根据躯干速度误差调整落脚点位置，整个框架通过学习实现，避免使用复杂的解析模型或传统步态规划器。

Result: 与基于LIPM的模型方法相比，该方法在目标速度保持精度上提高达80%，在不平整地形上的鲁棒性提升超过50%，并具有更高的能量效率。

Conclusion: 在训练架构中引入复杂的解析或基于模型的组件可能并非必要，所提方法可在非结构化环境中实现稳定且鲁棒的双足行走。

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [508] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: Maestro提出了一种通过将视觉语言模型（VLM）与专用机器人模块结合来构建通用机器人策略的新方法，实现了优于现有VLA模型的零样本性能，并具有良好的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的通用机器人研究主要依赖大规模数据集训练端到端模型，模仿视觉语言模型的发展路径，但这种方法可能受限于数据规模和模型泛化能力。本文探索一条较少被关注的路径：利用VLM的通用能力并结合特定机器人模块来构建更灵活、可解释和可扩展的通用策略。

Method: Maestro采用一个VLM编码代理，动态地将感知、规划和控制等模块组合成针对当前任务和场景的程序化策略。其架构具有简化的闭环接口和丰富多样的工具集，减少了人为设定的结构限制。

Result: Maestro在复杂操作技能上的零样本性能显著超越了当前的VLA模型，能够轻松扩展以集成新模块，适应新形态（如四足臂），并通过少量真实世界经验进行本地代码修改实现快速适应。

Conclusion: 通过将VLM与专门的机器人模块相结合，Maestro提供了一种更具灵活性、可维护性和适应性的通用机器人策略构建方式，展示了不同于纯数据驱动路线的潜力。

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [509] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: 提出Fast-SmartWay，一种无需全景视图和航点预测器的端到端零样本视觉-语言导航框架，通过三张前向RGB-D图像和自然语言指令直接预测动作，并引入不确定性感知推理模块提升决策鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLN-CE方法依赖全景观测和两阶段航点预测，导致高延迟且限制了实际应用。

Method: 采用三张前向RGB-D图像与自然语言指令输入MLLM，实现端到端动作预测；设计不确定性感知推理模块，包含消歧模块和未来-过去双向推理机制。

Result: 在模拟和真实机器人环境中验证，显著降低每步延迟，性能优于或媲美基于全景的基线方法。

Conclusion: Fast-SmartWay在零样本具身导航中具备更高的实用性与有效性，适用于真实场景。

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [510] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为URDF-Anything的端到端自动重建框架，基于3D多模态大语言模型，用于生成关节物体的精确数字孪生体，显著提升了几何分割、运动学参数预测和物理可执行性。


<details>
  <summary>Details</summary>
Motivation: 构建关节物体的精确数字孪生体对机器人仿真训练和具身AI世界模型至关重要，但传统方法依赖繁琐的手动建模或多阶段流程，亟需自动化解决方案。

Method: 提出URDF-Anything框架，采用点云与文本多模态输入的自回归预测模型，结合专门设计的[SEG]标记机制，联合优化几何分割与运动学参数预测。

Result: 在模拟和真实数据集上实验表明，该方法在几何分割（mIoU提升17%）、运动学参数预测（平均误差降低29%）和物理可执行性（超过基线50%）方面显著优于现有方法，并展现出优异的泛化能力。

Conclusion: URDF-Anything为机器人仿真中的数字孪生构建提供了高效解决方案，显著增强了从仿真到现实的迁移能力。

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [511] [Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing](https://arxiv.org/abs/2511.00983)
*Yizhao Qian,Yujie Zhu,Jiayuan Luo,Li Liu,Yixuan Yuan,Guochen Ning,Hongen Liao*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人超声系统（RUSS）的感知与控制协同设计新框架，通过解耦双流感知网络和单步流策略，实现了超过60Hz的闭环控制频率，在动态环境下实现了高精度、强鲁棒性的目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有机器人超声系统因端到端延迟难以实现实时动态目标跟踪，尤其是在大规模、高频干扰下，限制了其在临床动态环境中的自主性。

Method: 提出一种协同设计框架：1）解耦双流感知网络，从2D图像中高频估计3D平移状态；2）单步流策略，通过一次推理生成完整动作序列，避免传统策略的迭代瓶颈。

Result: 在动态 phantom 上实现均方误差低于6.5mm的3D轨迹跟踪，可从超过170mm位移中鲁棒重捕获，支持102mm/s的高速跟踪并保持终端误差低于1.7mm；真人实验验证了其在真实临床场景中的有效性与鲁棒性。

Conclusion: 该工作通过感知与控制的深度融合，构建了一个兼具高带宽跟踪与大范围重定位能力的RUSS整体架构，推动了动态临床环境中机器人自主性的关键进展。

Abstract: Real-time tracking of dynamic targets amidst large-scale, high-frequency
disturbances remains a critical unsolved challenge in Robotic Ultrasound
Systems (RUSS), primarily due to the end-to-end latency of existing systems.
This paper argues that breaking this latency barrier requires a fundamental
shift towards the synergistic co-design of perception and control. We realize
it in a novel framework with two tightly-coupled contributions: (1) a Decoupled
Dual-Stream Perception Network that robustly estimates 3D translational state
from 2D images at high frequency, and (2) a Single-Step Flow Policy that
generates entire action sequences in one inference pass, bypassing the
iterative bottleneck of conventional policies. This synergy enables a
closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system
not only tracks complex 3D trajectories with a mean error below 6.5mm but also
demonstrates robust re-acquisition from over 170mm displacement. Furthermore,
it can track targets at speeds of 102mm/s, achieving a terminal error below
1.7mm. Moreover, in-vivo experiments on a human volunteer validate the
framework's effectiveness and robustness in a realistic clinical setting. Our
work presents a RUSS holistically architected to unify high-bandwidth tracking
with large-scale repositioning, a critical step towards robust autonomy in
dynamic clinical environments.

</details>


### [512] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为GauDP的新型高斯-图像协同表示方法，用于多智能体协作系统中的可扩展、感知感知的模仿学习。该方法通过去中心化的RGB观测构建全局一致的3D高斯场，并动态重分配高斯属性到各智能体的局部视角，实现了细粒度控制与全局一致性行为的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度局部控制与全面场景理解之间难以平衡，导致可扩展性和协作质量受限。需要一种既能保持个体视角又能实现全局环境感知的协调机制。

Method: GauDP从去中心化的RGB观测中构建全局一致的3D高斯场，并将3D高斯属性动态重分配到每个智能体的局部视角，使各智能体能够自适应地从共享场景表示中查询任务关键特征，同时保持个体视角。

Result: 在RoboFactory基准上评估表明，GauDP优于现有的基于图像的方法，性能接近点云驱动方法，且在智能体数量增加时仍保持强可扩展性。

Conclusion: GauDP通过高斯-图像协同表示有效解决了多智能体系统中局部控制与全局感知的权衡问题，无需额外传感模态即可实现高效协作。

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [513] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出一种基于张量参数化降阶模型（PROM）的新型优化算法，用于高效优化受复杂非线性力作用的软体结构，特别是在软体机器人游泳器形状优化中表现出高计算效率。


<details>
  <summary>Details</summary>
Motivation: 软体结构在复杂非线性力下的优化面临巨大计算开销，尤其是在有限元仿真中，亟需高效的优化方法以推动软体机器人发展。

Method: 采用张量形式的参数化降阶模型（PROM），结合降维与解逼近技术，在特定构建的降阶基（ROB）中使用解析梯度进行非线性约束优化，且无需依赖数据训练。

Result: 该方法显著提升了计算效率，能够在包含流体动力等非线性力的条件下快速准确地优化软体机器人游泳器的形状。

Conclusion: 所提方法有效降低了软体系统优化的计算复杂度，为软体机器人中复杂非线性系统的高效设计与控制提供了新途径。

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [514] [Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment](https://arxiv.org/abs/2511.01083)
*Zihan Wang,Jianwen Li,Li-Fan Wu,Nina Mahmoudian*

Main category: cs.RO

TL;DR: 本文提出了SPAR-H方法，通过结合直接偏好优化与基于奖励的路径，在人类-in-环学习中实现无人机沿河流飞行策略的安全高效在线适应。


<details>
  <summary>Details</summary>
Motivation: 仿真训练的视觉驱动策略在实际部署到河流监测等任务中时面临分布偏移和安全风险，需要从有限的人类干预中高效适应。

Method: 提出Statewise Hybrid Preference Alignment for Robotics (SPAR-H)，利用状态级混合偏好，融合对策略logits的直接偏好优化与基于相同偏好的即时奖励估计器，并通过信任区域代理更新策略。

Result: 在五次人类-in-环 rollout下，SPAR-H在最终 episodic reward 上表现最优且跨初始条件的方差最低；学习到的奖励模型能准确反映人类偏好并提升未干预动作的质量。

Conclusion: SPAR-H通过双重状态级偏好实现了数据高效的在线适应，验证了其在无人机河流跟随任务中持续偏好对齐的实用性与可行性。

Abstract: Rivers are critical corridors for environmental monitoring and disaster
response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven
policies can provide fast, low-cost coverage. However, deployment exposes
simulation-trained policies with distribution shift and safety risks and
requires efficient adaptation from limited human interventions. We study
human-in-the-loop (HITL) learning with a conservative overseer who vetoes
unsafe or inefficient actions and provides statewise preferences by comparing
the agent's proposal with a corrective override. We introduce Statewise Hybrid
Preference Alignment for Robotics (SPAR-H), which fuses direct preference
optimization on policy logits with a reward-based pathway that trains an
immediate-reward estimator from the same preferences and updates the policy
using a trust-region surrogate. With five HITL rollouts collected from a fixed
novice policy, SPAR-H achieves the highest final episodic reward and the lowest
variance across initial conditions among tested methods. The learned reward
model aligns with human-preferred actions and elevates nearby non-intervened
choices, supporting stable propagation of improvements. We benchmark SPAR-H
against imitation learning (IL), direct preference variants, and evaluative
reinforcement learning (RL) in the HITL setting, and demonstrate real-world
feasibility of continual preference alignment for UAV river following. Overall,
dual statewise preferences empirically provide a practical route to
data-efficient online adaptation in riverine navigation.

</details>


### [515] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: 本文提出了Shortcut Learning for Abstract Planning (SLAP)，利用模型无关的强化学习在任务与运动规划（TAMP）的抽象动作图中自动发现新的高效动作选项，显著缩短规划长度并提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统TAMP依赖人工定义的抽象动作，限制了智能体的行为能力；而纯强化学习在长视野、稀疏奖励环境下效率低下，因此需要一种能自动发现有效抽象动作的方法。

Method: SLAP在现有TAMP选项构建的抽象规划图上，使用模型无关的强化学习来学习‘捷径’，即新的复合动作（如slap、wiggle），从而优化长期决策过程。

Result: 在四个模拟机器人环境中，SLAP将规划长度减少了50%以上，任务成功率高于扁平和分层强化学习方法，并展现出对新任务的良好泛化能力。

Conclusion: SLAP通过结合规划与学习，在无需额外假设的情况下实现了更高效、更灵活的长视野决策，推动了自动抽象动作发现的发展。

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [516] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: 提出一种结合IMU和弯曲传感器的增强本体感知方法，用于软体机器人的精确形状估计，具有低成本、高精度和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人形状估计方法在长期运行中易受IMU漂移影响，难以兼顾成本与精度，需要一种可靠且易于实现的本体感知方案。

Method: 通过融合惯性测量单元（IMU）和互补弯曲传感器的数据，利用卡尔曼滤波器以相互补偿的方式融合两种传感器的末端姿态信息，并采用分段恒定曲率模型重建机器人形变并估计末端位置。

Result: 在无负载、外力作用及被动避障等条件下连续运行45分钟，均方根误差为16.96毫米（占总长度的2.91%），相比仅使用IMU的方法精度提升56%。

Conclusion: 该方法能有效抑制IMU漂移，实现长时间、高精度、高鲁棒性的软体机器人本体感知，适用于多种复杂工况。

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [517] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: 本文提出了一种基于3D粒子表示和图世界模型的跨具身灵巧操作方法，通过统一的状态和动作表征实现不同形态间的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 不同机器人形态在动作空间和运动学上的差异阻碍了数据共享与策略迁移，亟需一种跨形态的不变表示以实现通用控制。

Method: 将不同形态（如人手与机械手）表示为3D粒子集合，动作定义为粒子位移，并构建基于图的世界模型，在模拟与真实多形态数据上联合训练。

Result: 实验表明：增加训练形态数量可提升对未见形态的泛化能力；模拟与真实数据联合训练效果更优；所学模型能有效控制不同自由度的机器人完成刚性和可变形物体操作任务。

Conclusion: 世界模型可作为跨形态灵巧操作的有效统一接口，具备良好的泛化性和实用性。

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [518] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: 提出LiDAR-VGGT框架，通过粗到精的融合策略结合LiDAR惯性里程计与VGGT模型，生成大规模稠密且全局一致的彩色点云。


<details>
  <summary>Details</summary>
Motivation: 现有LIVO方法对传感器外参标定敏感，而3D视觉基础模型（如VGGT）在大场景中扩展性差且缺乏度量尺度。

Method: 采用两阶段融合：预融合模块估计带粗略尺度的VGGT位姿与点云；后融合模块利用基于包围盒的正则化优化跨模态3D相似变换，抑制尺度畸变。

Result: 在多个数据集上实验表明，该方法优于VGGT类方法和LIVO基线，生成高质量彩色点云，并将发布新的彩色点云评估工具包。

Conclusion: LiDAR-VGGT有效克服了多模态感知中的标定敏感性和尺度缺失问题，实现了鲁棒的大规模彩色三维重建。

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [519] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 提出一种可膨胀的可转向球囊式心腔镜（cardioscope），通过控制球囊内压同时调节视野范围和工作通道角度，实现心脏内精确导航与工具输送。


<details>
  <summary>Details</summary>
Motivation: 为避免开胸手术，发展更安全的经导管治疗，需要改进成像技术和机器人系统以实现精准器械导航；传统影像技术如X光和超声存在局限，光学直视（cardioscopy）可提供更清晰的视野。

Method: 设计一种可折叠并通过血管输送、在心腔内充气展开的球囊式心腔镜；通过优化球囊壁厚度，仅用充气压力即可独立控制球囊直径（视野）和弯曲角度（工作通道定位）；并实现基于图像的闭环反馈控制以稳定方向。

Result: 成功开发出可通过单一压力输入调控双输出（视野大小与弯曲角度）的球囊心腔镜，并针对主动脉瓣叶切开任务进行了定制设计，验证了其在工具操作过程中的稳定导向能力。

Conclusion: 该球囊式心腔镜技术具有可调性与多功能性，适用于多种心内介入任务，为微创心脏手术提供了新的可视化与操作平台。

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [520] [Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference](https://arxiv.org/abs/2511.01219)
*Muhua Zhang,Lei Ma,Ying Wu,Kai Shen,Deqing Huang,Henry Leung*

Main category: cs.RO

TL;DR: 本文提出了一种用于解决绑架机器人问题（KRP）的被动式2D全局重定位框架，能够在仅使用单帧LiDAR扫描和占据栅格地图的情况下高效、可靠地估计机器人的全局位姿。


<details>
  <summary>Details</summary>
Motivation: 在SLAM初始化或定位丢失时，机器人需要在没有先验位姿估计的情况下在已知地图中重新定位，传统方法在效率和鲁棒性方面存在不足。

Method: 将全局重定位建模为非凸问题，采用多假设方案结合批量多阶段推理与提前终止机制；利用受可通行性约束的RRT生成稀疏且均匀分布的位置假设，并通过提出的SMAD指标对假设进行粗排序以实现早期终止；提出TAM度量用于在假设位置上可靠选择方向并精确评估最终位姿。

Result: 真实场景实验表明，该框架在非全景LiDAR和资源受限的移动机器人上，相较于现有方法在全局重定位成功率和计算效率方面均有显著提升。

Conclusion: 所提出的框架有效平衡了全局重定位的完整性与效率，提升了移动机器人在复杂环境下的长期自主运行能力。

Abstract: This paper addresses the Kidnapped Robot Problem (KRP), a core localization
challenge of relocalizing a robot in a known map without prior pose estimate
when localization loss or at SLAM initialization. For this purpose, a passive
2-D global relocalization framework is proposed. It estimates the global pose
efficiently and reliably from a single LiDAR scan and an occupancy grid map
while the robot remains stationary, thereby enhancing the long-term autonomy of
mobile robots. The proposed framework casts global relocalization as a
non-convex problem and solves it via the multi-hypothesis scheme with batched
multi-stage inference and early termination, balancing completeness and
efficiency. The Rapidly-exploring Random Tree (RRT), under traversability
constraints, asymptotically covers the reachable space to generate sparse,
uniformly distributed feasible positional hypotheses, fundamentally reducing
the sampling space. The hypotheses are preliminarily ordered by the proposed
Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that
facilitates the early termination by prioritizing high-likelihood candidates.
The SMAD computation is optimized for non-panoramic scans. And the
Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for
reliable orientation selection at hypothesized positions and accurate final
pose evaluation to mitigate degradation in conventional likelihood-field
metrics under translational uncertainty induced by sparse hypotheses, as well
as non-panoramic LiDAR scan and environmental changes. Real-world experiments
on a resource-constrained mobile robot with non-panoramic LiDAR scan
demonstrate that the proposed framework outperforms existing methods in both
global relocalization success rate and computational efficiency.

</details>


### [521] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: 本文提出了一种名为ET-VLA的新框架，通过引入体现迁移学习和合成持续预训练（SCP），有效将预训练的视觉-语言-动作模型迁移到多机器人系统中，无需依赖真实人类演示，显著降低成本并提升多体协作性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视觉-语言-动作（VLA）模型在多机器人协作场景下表现不佳，且依赖大量真实人类示范数据，限制了其在多体系统中的应用。

Method: 提出ET-VLA框架，核心为合成持续预训练（SCP），利用合成数据对模型进行热启动，使其适应新的机器人形态；随后在目标数据上微调，并引入具身思维图（Embodied Graph-of-Thought）技术，将子任务建模为节点，帮助模型区分各机器人的功能与角色。

Result: 在仿真和三种不同双臂机器人的真实世界任务中验证了方法的有效性，ET-VLA在六个实际任务上比OpenVLA性能高出53.2%。

Conclusion: ET-VLA通过无需真实示范的迁移学习策略，显著提升了VLA模型在多机器人协作中的适应性和性能，推动了跨体态机器人学习的发展。

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [522] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 提出一种新型机器人系统，旨在提高眼科手术中工具尖端的精度、跟踪性能和仪器交换机制，并通过OCT系统验证其高精度定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有眼科手术机器人在精度、自由度和器械操作方面仍存在不足，难以满足复杂手术需求。

Method: 设计并制造新型机器人系统，结合机器人校准与精确坐标配准，利用光学相干断层扫描（OCT）系统进行外部评估，并结合深度学习术前建模与实时监控实现自动化白内障晶状体摘除。

Result: 系统工具尖端定位精度达到0.053±0.031 mm，成功实现了OCT引导下的自动化白内障手术演示。

Conclusion: 该机器人系统显著提升了手术精度与自动化水平，具备临床应用潜力。

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [523] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: 提出了一种基于大语言模型的语义路径规划方法SATPlanner，用于球形张拉整体机器人，在未知环境中实现了高效、鲁棒的导航。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划方法在复杂未知环境中缺乏语义理解，导致搜索冗余和规划失败，难以满足张拉整体机器人的适应性需求。

Method: 将路径规划重构为语义推理任务，引入基于大语言模型的SATPlanner，结合自适应观测窗口机制，动态调整感知范围以实现快速通行与复杂障碍推理的平衡。

Result: 在1000次仿真中达到100%成功率，搜索空间比A*算法减少37.2%，路径接近最优，并在真实球形张拉整体机器人上验证了可行性。

Conclusion: SATPlanner通过语义理解显著提升了未知环境中的路径规划效率与鲁棒性，为软硬混合机器人提供了新的规划范式。

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [524] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 本文提出了一种基于迭代学习控制（ILC）的机器人手术工具精确旋转插入方法，通过OCT反馈优化轨迹，显著提高了猪眼离体实验中视网膜下注射的成功率。


<details>
  <summary>Details</summary>
Motivation: 由于系统失准、未建模动态和驱动误差，机器人工具路径的精确控制面临挑战，尤其是在需要高精度插入的微创手术中。

Method: 采用4自由度机器人操纵器，结合前向运动学标定与基于光学相干断层扫描（OCT）体积图像的误差反馈，设计ILC策略迭代调整关节指令，实现旋转插入轨迹优化。

Result: 在离体猪眼实验中，相比直线插入，优化后的旋转插入显著提高了组织穿透成功率和视网膜下注射的准确性。

Conclusion: ILC能有效克服系统失准问题，提升手术机器人在精细插入任务中的精度与安全性，具有拓展至其他高精度机器人操作的潜力。

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [525] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: 提出了一种将折纸结构与针织织物结合的新型制造与设计方法，通过编程 stitch 和材料图案实现可穿戴软体机器人的结构可重构性和舒适性。


<details>
  <summary>Details</summary>
Motivation: 在保持柔软舒适的同时，实现软体机器人结构的稳定性和功能性仍是一个挑战。

Method: 将折纸图案转化为针织设计，利用热熔纱线形成刚性面板以增强结构稳定性，并通过编织图案控制折叠方向。

Result: 成功实现了Miura-ori、Yoshimura和Kresling等复杂折纸结构的复制，并展示了可运动的可穿戴针织Kaleidocycle机器人；实验验证了折叠力矩的方向性和结构稳定性提升。

Conclusion: 针织折纸结构结合了结构可重构性、材料可编程性和制造可扩展性，是下一代可穿戴机器人有前景的平台。

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [526] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件扩散模型的灵巧抓取生成框架，通过从形状模板向新对象迁移高质量抓取，实现了抓取质量、生成效率和泛化能力的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有分析方法虽能保证抓取稳定性但效率低且任务适应性差，生成式方法效率高但对未见物体和任务泛化能力不足，因此需要一种兼具稳定性、效率与泛化能力的新方法。

Method: 将抓取迁移问题重构为物体接触图的生成任务，引入双映射机制捕捉形状模板与新物体间的几何关系，并设计包含接触图、部件图和方向图的三图联合扩散模型以确保一致性，最后通过鲁棒抓取恢复机制优化抓取配置。

Result: 实验表明该方法在多种任务中均优于现有方法，能够有效生成稳定且适应任务的抓取姿态，具有良好的跨物体和跨任务泛化性能。

Conclusion: 所提出的基于扩散模型的抓取迁移框架在抓取质量、效率和泛化之间取得了良好平衡，为灵巧抓取提供了一种新的有效解决方案。

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [527] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: 本文设计了一种新型单摆驱动球形机器人结构，通过引入与次级摆臂对齐的动量轮，实现了高达10 m/s的稳定高速运动，并显著提升了越障能力和地形适应性。


<details>
  <summary>Details</summary>
Motivation: 传统单摆驱动球形机器人难以实现高速稳定运动，限制了其在复杂环境中的应用。因此，需要一种新结构来突破速度和机动性的瓶颈。

Method: 在原有单摆驱动结构基础上，增加一个轴线与次级摆臂对齐的动量轮，形成新的球形机器人结构，并采用简单的解耦控制策略进行实验验证。

Result: 实物原型实验表明，该机器人可实现最高10 m/s的稳定高速运动，且具备更强的越障性能和地形鲁棒性，而原结构无法达到此类性能。

Conclusion: 所提出的新型动量轮-双摆结构有效提升了球形机器人的运动速度与整体机动性能，为高速球形机器人设计提供了可行方案。

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [528] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: 本文提出Kinematify，一个能从任意RGB图像或文本提示中自动生成可动结构物体模型的自动化框架，解决了高自由度物体的运动学拓扑推断与静态几何下的关节参数估计问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖运动序列或手工数据集中的强假设，难以扩展到复杂系统，因此需要一种可扩展、自动化的可动结构建模方法。

Method: 结合MCTS搜索进行结构推断和基于几何的优化进行关节推理，从静态图像或文本生成可动结构模型。

Result: 在合成与真实世界数据上验证了Kinematify的有效性，相比先前方法在配准和运动学拓扑准确性方面表现更优。

Conclusion: Kinematify实现了从静态输入（图像或文本）到物理一致且功能合理的可动结构模型的自动化生成，提升了复杂物体建模的可扩展性与实用性。

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [529] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出了RobustVLA，一种轻量级的在线强化学习后训练方法，旨在提升视觉-语言-动作（VLA）模型在分布外环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在面对观测噪声、传感器误差或执行扰动等分布外干扰时泛化能力不足，而当前的强化学习后训练方法主要关注奖励最大化，忽视了对环境不确定性的鲁棒性。

Method: 通过系统性鲁棒性分析，引入两种关键正则化：雅可比正则化以降低对观测噪声的敏感性，平滑性正则化以在动作扰动下稳定策略。

Result: 在多种机器人环境中进行的大量实验表明，RobustVLA在鲁棒性和可靠性方面显著优于先前的最先进方法。

Conclusion: 研究强调了基于原则的、注重鲁棒性的强化学习后训练是提升VLA模型可靠性的关键步骤。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [530] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: 本文提出了一种名为$E^{3}AD$的新范式，通过视觉特征提取网络与通用EEG大模型之间的对比学习，引入人类驾驶认知来增强端到端自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶方法通常依赖于标签监督下训练的视觉特征提取网络，限制了模型的泛化能力。因此，需要一种更具通用性和可解释性的方法来提升规划性能。

Method: 提出$E^{3}AD$范式，利用视觉网络与EEG大模型进行对比学习，以捕捉潜在的人类驾驶认知；收集了一个认知数据集用于对比学习，并在公开自动驾驶数据集上基于主流驾驶模型进行实验验证。

Result: 实验结果表明，$E^{3}AD$显著提升了基线模型的端到端规划性能，开环和闭环测试均验证了其有效性；消融研究证实了驾驶认知和对比学习机制的贡献。

Conclusion: $E^{3}AD$是首个将人类驾驶认知引入端到端自动驾驶规划的工作，为脑启发的自动驾驶系统提供了新思路，具有重要的探索意义和应用前景。

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [531] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 本研究开发了一种新型的热响应性人工捕蝇草（AVF），能够模拟真实捕蝇草的自主闭合与 reopen 过程，利用温度响应的形状记忆材料实现双向运动。


<details>
  <summary>Details</summary>
Motivation: 尽管植物常被认为是静态缓慢的，但某些植物如捕蝇草具有极快的运动能力，启发了软体机器人设计。然而，目前尚无系统能同时模拟其闭合与 reopen 的自主过程。

Method: 采用新型紫外固化热响应形状记忆聚合物构建仿生结构，通过双曲面陷阱瓣在38°C时闭合，并利用形状记忆弹性体条带作为拮抗驱动器，在约45°C时触发重新打开。

Result: 成功实现了在自然温度范围内响应温度变化的自主闭合与 reopen 动作，首次展示了程序化顺序运动的热响应人工捕蝇草。

Conclusion: 该方法为实现自主双向运动的软体机器和机器人提供了新的路径，是植物启发式软体机器人发展的重要进展。

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [532] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 提出一种无电子元件的、受蚯蚓启发的气动软体机器人，利用改进的气动逻辑门（PLG）实现自主蠕动运动。


<details>
  <summary>Details</summary>
Motivation: 现有蚯蚓-inspired机器人依赖笨重的电子控制单元，限制了实用性，因此需要开发无需电子控制的简化系统。

Method: 结合预设的气动逻辑门（PLG）与波纹管执行器，构建模块化、无需外部电子组件的气动控制系统。

Result: 实现了有效的蠕动波传播，机器人表现出稳定的自主运动且偏差小，系统复杂度显著降低。

Conclusion: 该研究验证了无电子元件的蠕动式软体机器人的可行性，具有在危险环境中应用的潜力。

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [533] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 本研究通过分析捕蝇草叶片的几何特征，设计并3D打印了两种仿生双稳态执行器模型，成功模仿了捕蝇草快速闭合的机械行为，为开发软体快速夹持器提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 深入了解捕蝇草快速闭合的力学机制，并将其原理应用于人工执行器的设计。

Method: 分析捕蝇草叶片的几何特性（如尺寸比例和厚度梯度），并将这些特征转化为两种3D打印的双稳态执行器模型：一种模拟捕蝇草叶片的几何结构，另一种使用CAD设计。

Result: 两种模型均表现出凹-凸双稳态特性，并能快速“ snapping”闭合，成功复现了捕蝇草叶片的快速闭合行为。

Conclusion: 该研究为仿生双稳态执行器的设计提供了可行方案，是构建模仿生物力学行为的人工捕蝇草软体快速夹持器的重要第一步。

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [534] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 本文研究了自动泊车场景中车辆侧向速度估计的挑战，发现传统零滑移假设存在系统性偏差，并提出了一种仅需两个参数的改进侧向速度模型，显著提高了定位精度，适用于消费级应用。


<details>
  <summary>Details</summary>
Motivation: 在低速泊车场景中，传统零滑移模型无法准确描述车辆侧向运动，导致侧向速度估计偏差，影响定位精度。由于消费级车辆缺乏专用传感器，需基于简单模型提升估计性能。

Method: 通过分析真实泊车场景数据，识别零滑移假设的系统性偏差，提出一种新的侧向速度模型，该模型仅引入两个参数，更准确地刻画车辆低速时的侧向动力学特性，并可集成于现有定位系统。

Result: 新模型在真实数据上验证有效，显著提升了侧向速度和整体定位的估计精度，且参数少，计算效率高，适合实时消费级应用。

Conclusion: 本文提出的侧向速度模型克服了传统零滑移假设在低速泊车中的局限性，在不增加复杂传感器的前提下，以极简参数化方式提升了定位性能，具有良好的实用价值。

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [535] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: 提出了一种多模态SLAM框架CM-LIUW-Odometry，用于解决地下煤矿环境中大规模、复杂且无GPS条件下的定位与建图问题，融合LiDAR、IMU、UWB和轮式里程计，并采用自适应运动模式切换机制，在真实场景中表现出更高的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 地下煤矿环境存在GPS不可用、地形恶劣、特征稀疏等问题，导致传统SLAM方法性能下降，亟需一种鲁棒、高精度的多传感器融合方案。

Method: 基于迭代误差状态卡尔曼滤波器（IESKF），紧耦合LiDAR-惯性里程计与UWB绝对定位，并引入轮式里程计结合非完整约束和杠杆臂补偿；设计自适应运动模式切换机制以应对不同环境退化情况。

Result: 在真实地下煤矿环境中实验验证，该方法相比现有最先进方法具有更高的定位精度和系统鲁棒性。

Conclusion: CM-LIUW-Odometry有效解决了复杂地下煤矿环境中的SLAM挑战，具备实际应用价值，代码已开源以促进机器人领域发展。

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [536] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: 提出了一种名为CaRLi-V的RADAR、LiDAR和相机融合方法，用于实现精确的点级3D速度估计，适用于机器人在动态环境中的交互应用。


<details>
  <summary>Details</summary>
Motivation: 准确的点级3D速度估计对于机器人与非刚性动态目标（如人类）的交互至关重要，有助于提升路径规划、避障和物体操作的鲁棒性。

Method: 提出CaRLi-V融合框架，利用原始RADAR数据构建‘速度立方体’以密集表示视场内的径向速度，结合光学流估计切向速度，并融合LiDAR的点级距离测量，通过闭式解计算获得稠密点云的3D速度估计。

Result: 在自定义数据集上进行了实地测试，结果表明该方法相对于真实值具有较低的速度误差指标。

Conclusion: CaRLi-V能够有效实现高精度的点级3D速度估计，已开发为开源ROS2软件包，适用于动态环境中的机器人应用。

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [537] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: 本文提出了一种名为FoldPath的端到端神经场方法，用于对象中心运动生成（OCMG），通过将机器人运动建模为连续函数，避免了传统方法中脆弱的后处理步骤，在有限专家样本下表现出优异的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的OCMG方法依赖启发式规则或需要敏感后处理的学习模型，难以生成稳定、精确且适用于复杂3D几何的连续轨迹，限制了其在工业自动化中的实际应用。

Method: FoldPath采用神经场将机器人运动建模为连续函数，直接输出平滑路径，无需离散航点预测与后处理拼接，实现端到端的对象感知运动生成。

Result: 实验表明FoldPath在预测精度和路径质量上优于现有学习方法，并在仅有70个专家样本的真实工业场景中展现出良好泛化能力，同时提出了新的评估指标以更全面地衡量长视野机器人路径。

Conclusion: FoldPath为OCMG提供了一种鲁棒、高效的解决方案，显著减少了对后处理的依赖，推动了该任务向实际工业应用的成熟迈进。

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [538] [Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots](https://arxiv.org/abs/2511.01437)
*Elian Neppel,Shamistan Karimov,Ashutosh Mishra,Gustavo Hernan Diaz Huenupan,Hazal Gozbasi,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了MoonBot平台的软件架构和部署策略，这是一个由分布在多台计算机、网络乃至天体上的异构组件构成的模块化空间机器人系统。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统在分布式、异构环境下的集成与维护难题，提升系统的可扩展性与鲁棒性。

Method: 采用基于组件的设计、使用ROS2和Zenoh的数据导向通信模型，以及能够管理复杂多模块组装的部署编排器，实现动态重构、去中心化控制和无缝协作。

Result: 通过长期野外部署验证了开源Motion Stack软件的有效性，实现了自组装机器人、机器人间协作和远程操作。

Conclusion: 该架构显著降低了模块化机器人系统的集成与维护开销，具有良好的可扩展性和鲁棒性，并提出了适用于跨时间、硬件、团队和操作环境的通用机器人系统设计模式。

Abstract: This paper presents the software architecture and deployment strategy behind
the MoonBot platform: a modular space robotic system composed of heterogeneous
components distributed across multiple computers, networks and ultimately
celestial bodies. We introduce a principled approach to distributed,
heterogeneous modularity, extending modular robotics beyond physical
reconfiguration to software, communication and orchestration. We detail the
architecture of our system that integrates component-based design, a
data-oriented communication model using ROS2 and Zenoh, and a deployment
orchestrator capable of managing complex multi-module assemblies. These
abstractions enable dynamic reconfiguration, decentralized control, and
seamless collaboration between numerous operators and modules. At the heart of
this system lies our open-source Motion Stack software, validated by months of
field deployment with self-assembling robots, inter-robot cooperation, and
remote operation. Our architecture tackles the significant hurdles of modular
robotics by significantly reducing integration and maintenance overhead, while
remaining scalable and robust. Although tested with space in mind, we propose
generalizable patterns for designing robotic systems that must scale across
time, hardware, teams and operational environments.

</details>


### [539] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: 本文提出了AERMANI-VLM，首个将预训练视觉-语言模型（VLM）应用于空中操作的框架，通过分离高层推理与底层控制，在无需任务微调的情况下实现安全、可靠的机器人控制。


<details>
  <summary>Details</summary>
Motivation: 直接将视觉-语言模型用于空中操作器存在动作不一致、幻觉和动力学不可行等问题，导致部署不安全且不可靠。

Method: 提出AERMANI-VLM框架，将自然语言指令、任务上下文和安全约束编码为结构化提示，引导模型生成逐步推理链，并从预定义的安全技能库中选择离散技能执行。

Result: 在仿真和真实硬件上验证了该框架在多步抓取放置任务中的有效性，表现出对未见过的指令、物体和环境的良好泛化能力。

Conclusion: AERMANI-VLM通过解耦符号推理与物理动作，有效缓解了幻觉问题并防止不安全行为，实现了鲁棒的任务完成。

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [540] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: 本文提出了MO-SeGMan，一种用于高约束重排问题的多目标顺序引导操作规划器，能有效减少重规划次数和机器人移动距离，并在复杂非单调场景中实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 针对高度拥挤、非单调的重排问题中传统方法效率低、需频繁重规划的问题，提出一种能保持依赖结构并提升求解质量的规划方法。

Method: 采用多目标优化策略生成物体放置序列，结合延迟评估机制保留关键依赖结构；提出选择性引导前向搜索（SGFS）仅移动关键障碍物至可行位置，并引入自适应子目标优化的精炼方法以减少不必要的抓放动作。

Result: 在九个基准重排任务上的实验表明，MO-SeGMan在所有情况下均能生成可行运动计划，求解速度更快且解的质量更优。

Conclusion: MO-SeGMan在复杂重排任务中表现出良好的鲁棒性和可扩展性，显著提升了规划效率与解决方案质量。

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [541] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: 提出了一种基于扩散策略的GlocDiff方法，结合平面图的全局路径规划和RGB图像提取的局部深度特征，有效解决室内导航中视觉与空间信息融合及定位难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理RGB输入与平面图之间的模态差异以及在未知环境中精确定位方面仍存在挑战。

Method: 提出GlocDiff，利用扩散模型融合平面图的全局引导与RGB-derived深度特征的隐式几何线索，并引入训练时噪声扰动以增强对位姿估计误差的鲁棒性，推理时结合稳定的VO模块提升性能。

Result: 在FloNa基准上实现了优越的导航性能，且在真实场景部署中表现良好。

Conclusion: GlocDiff能有效整合多模态信息并提升定位鲁棒性，具有良好的实际应用潜力。

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [542] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 提出了一种受人类启发的物理条件触觉方法Phy-Tac，用于力最优稳定抓取（FOSG），在抓取稳定性与力效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 机器人通常采用刚性、过度挤压的控制方式，而人类能以最小必要力稳定抓取物体，本文旨在缩小这一差距。

Method: 首先通过基于物理的位姿选择器确定具有最优力分布的接触区域；然后使用物理条件潜在扩散模型（Phy-LDM）预测目标力最优稳定抓取下的触觉印记；最后通过潜空间LQR控制器驱动夹持器逼近该触觉印记，实现最小驱动下的稳定抓取。

Result: Phy-LDM在多样化物体和接触条件下实现了高精度的触觉预测，Phy-Tac在抓取稳定性与力效率方面优于固定力和GraspNet基线方法。

Conclusion: 实验表明，Phy-Tac能够在经典机器人平台上实现高效、自适应的力控制操作，显著缩小了机器人与人类抓取行为之间的差距。

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [543] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: 提出了一种基于多模态大语言模型的多智能体机器人系统MARS，用于智能家居中的辅助任务，具备风险感知、个性化和可执行规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有系统在风险感知规划、用户个性化和语言计划的执行方面存在不足，难以满足残障人士在复杂家居环境中的辅助需求。

Method: 设计了一个包含视觉感知、风险评估、规划和评估四个智能体的多智能体系统，结合多模态感知与分层决策机制，实现动态环境下的自适应辅助。

Result: 在多个数据集上的实验表明，该系统在风险感知规划和多智能体协同执行方面优于现有的多模态模型。

Conclusion: MARS框架展示了协作式AI在实际辅助场景中的潜力，为在现实环境中部署MLLM驱动的多智能体系统提供了可推广的方法论。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [544] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉-语言-动作（VLA）模型——Unified Diffusion VLA，通过联合离散去噪扩散过程（JD3P）实现理解、生成与行动的内在协同，显著提升性能并实现比自回归方法快4倍的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖外部专家或分离图像生成与动作预测，限制了多模态任务间的直接协同，本文旨在通过统一的去噪过程实现生成与动作的联合优化。

Method: 提出Unified Diffusion VLA和JD3P框架，基于统一的token化空间和混合注意力机制，采用两阶段训练流程和推理时优化技术，在单个去噪轨迹中融合多种模态。

Result: 在CALVIN、LIBERO和SimplerEnv等基准上达到SOTA性能，推理速度比自回归方法快4倍，并通过深入分析和真实世界评估验证有效性。

Conclusion: 通过联合去噪扩散机制，Unified Diffusion VLA实现了多模态理解、生成与行动的深度融合，为VLA模型提供了高效且强健的解决方案。

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


### [545] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出一种轻量级的执行空间学习框架，利用Rectified Flow模型从确定性示范中推断分布控制表示，实现软体机器人全身抓取，仅需30次示范即可在全工作空间达到97.5%的抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 解决传统刚性机械手在不确定和接触丰富的环境中抓取困难的问题，利用软体机器人的被动柔顺性和欠驱动结构来增强适应能力。

Method: 采用基于流匹配模型（Rectified Flow）的执行空间学习框架，直接从少量确定性示范中学习控制策略，无需密集感知或复杂反馈控制回路。

Result: 在仅使用30次示范的情况下，抓取成功率达到97.5%，能泛化到物体尺寸变化±33%，并在执行时间缩放20%至200%时保持稳定性能。

Conclusion: 执行空间学习能够将软体机器人的机械特性转化为功能性控制智能，显著降低中央控制器在高不确定性任务中的负担。

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [546] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: 本文提出了一种名为MOBIUS的多模态双足智能城市侦察机器人，具备行走、爬行、攀爬和滚动能力，通过形态、规划与控制的紧密集成实现复杂地形下的自主运动与操作。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器人在复杂城市环境中执行侦察任务的移动性与操作能力，需要一种能够自适应多种地形并具备多功能交互能力的机器人系统。

Method: 设计了具有四肢体结构（6自由度手臂和4自由度腿）的MOBIUS机器人，采用混合控制架构，结合强化学习步态控制、基于模型的预测与顺应控制，并引入参考调节器增强安全性，同时使用高阶MIQCP规划器自主选择运动模式。

Result: 实验证明MOBIUS实现了稳健的步态切换、动态攀爬以及通过捏抓支撑全身体重的能力，在不同地形中表现出良好的适应性和稳定性。

Conclusion: MOBIUS展示了形态、高层规划与控制策略紧密集成对提升机器人运动-操作一体化能力的重要性，显著扩展了其交互能力、工作空间和通行能力。

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>


### [547] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: 提出GenDexHand，一个基于生成式模拟流水线的自主系统，用于大规模生成灵巧手操作任务和环境，通过视觉-语言模型反馈闭环优化场景质量，并采用子任务分解提升训练效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺是具身智能的根本瓶颈，现有LLM驱动的夹持器模拟生成方法难以迁移到对环境设计要求更高的灵巧操作任务，且灵巧操作自由度高、难度大，缺乏可行的大规模任务生成方案。

Method: 构建GenDexHand生成式模拟流水线，引入基于视觉-语言模型（VLM）反馈的闭环优化机制调整物体位置与尺度，并将任务分解为子任务以支持序列化强化学习。

Result: 显著提升了生成环境的平均质量，通过子任务分解降低了训练时间并提高了任务成功率，实现了多样化灵巧手行为的可扩展训练。

Conclusion: GenDexHand为解决灵巧操作中的数据稀缺问题提供了可行路径，推动了基于仿真合成数据的具身智能发展。

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>


### [548] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus Martínez-Gómez,Ismael García-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: 提出了一种基于大规模MIMO系统信道状态信息（CSI）数据的混合神经网络模型（HyNN），用于推断移动机器人的位置。


<details>
  <summary>Details</summary>
Motivation: 利用CSI数据实现高精度室内定位，解决复杂环境中移动机器人导航的问题。

Method: 结合卷积神经网络（CNN）和多层感知机（MLP）构建HyNN模型，使用TINTO工具将CSI数据转化为合成图像，并集成到机器人仿真器和ROS中，结合卡尔曼滤波等状态估计器进行评估。

Result: 模型在异构测试案例中表现出良好的定位精度，验证了其在复杂环境下的有效性。

Conclusion: 所提出的HyNN模型和通用化流程可有效实现移动机器人在复杂环境中的精确室内定位与导航，具有广泛适用性。

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [549] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种基于BERT和ResNet-50的多模态虚假评论检测框架，通过融合文本与图像特征提升检测性能，在包含21,142张图片的数据集上取得了F1-score 0.934的结果。


<details>
  <summary>Details</summary>
Motivation: 现有虚假评论检测方法多依赖单一文本模态，难以捕捉跨模态语义不一致，导致对伪造或误导性评论识别能力不足。

Method: 结合BERT提取文本特征与ResNet-50提取视觉特征，通过分类头进行多模态融合，联合预测评论真实性。

Result: 多模态模型优于单模态基线方法，在测试集上达到0.934的F1分数，混淆矩阵和定性分析显示其能有效识别文本夸大但图像不符等欺骗模式。

Conclusion: 多模态学习在维护数字信任中具有关键作用，该框架为在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [550] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 本文研究了多智能体强化学习在零售定价优化中的应用，比较了MAPPO及其结合图注意力机制的变体（MAPPO+GAT）在真实交易数据模拟环境下的表现，结果表明MAPPO+GAT通过产品图共享信息提升了性能且保持价格稳定，优于独立学习方法。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并协调相关产品决策的策略，现有方法在多产品协同和稳定性方面存在不足。

Method: 提出并比较了MAPPO与MAPPO+GAT两种多智能体强化学习方法，利用从真实交易数据构建的模拟环境，在标准化协议下评估其性能。

Result: MAPPO表现出良好的鲁棒性和可重复性，MAPPO+GAT进一步提升了利润、训练效率和跨产品公平性，同时未引起过高价格波动。

Conclusion: 结合图结构的多智能体强化学习（如MAPPO+GAT）为多产品动态定价提供了更可扩展、稳定的解决方案，具有实际应用优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [551] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: 本文介绍了为奥地利开发的GEPOC模型参数的数据处理方法，完全基于公开免费的数据，涵盖数据聚合、 disaggregation、融合、清洗和缩放等算法，并重点描述了GEPOC ABM（基于代理的连续时间人口模型）的关键参数生成，最后通过该模型进行了广泛的验证研究。


<details>
  <summary>Details</summary>
Motivation: 为了在特定国家或地区有效应用GEPOC人口模型，需要稳定、可重复的数据处理流程来生成有效且即用型的模型参数。

Method: 基于完全公开可用的数据，设计并实现了一套完整数据处理流程，包括数据聚合、disaggregation、融合、清洗和缩放算法，用于生成适用于奥地利的GEPOC模型参数，特别是GEPOC ABM模型所需参数。

Result: 成功构建了适用于奥地利的标准化数据处理流程，生成了完整的模型参数文件，并通过GEPOC ABM模型进行了广泛验证。

Conclusion: 该数据处理方法支持GEPOC模型在奥地利的可靠应用，且其开放性和可复现性为其他地区提供了参考范例。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [552] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: 本文介绍了QuantumBench，首个面向量子领域的大型语言模型（LLM）评估基准，包含约800道多选题，涵盖九个量子科学领域，用于系统评估LLM在该非直观、高数学性领域的理解与应用能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被广泛应用于科研工作流，亟需专门评估其在特定领域（如量子科学）中对专业知识和符号体系的理解准确性，而通用基准无法满足这一需求。

Method: 基于公开资料构建了一个包含约800道八选项选择题的多选题数据集，覆盖九个量子科学相关领域，并用其评估多个现有大语言模型的表现，分析其对问题格式变化的敏感性。

Result: 成功开发了QuantumBench，首个针对量子领域的LLM评估基准，并通过实验评估了多个LLM在该基准上的表现及其对问题格式的敏感性。

Conclusion: QuantumBench填补了量子领域LLM评估的空白，有助于指导大语言模型在量子科学研究中的有效应用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [553] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 本文提出了Engineering.ai，一个基于多智能体架构的AI工程师团队平台，用于自动化复杂工程设计任务，特别是在无人机机翼优化中实现了100%成功率的全流程自主运行。


<details>
  <summary>Details</summary>
Motivation: 现代工程设计依赖专家团队协作，耗时且成本高，亟需自动化手段来提升效率并降低对人力的依赖。

Method: 构建了一个分层多智能体系统，由首席工程师协调空气动力学、结构、声学和优化等专业AI工程师，通过文件传递通信，并结合领域专用知识的大模型与FreeCAD、Gmsh、OpenFOAM等工具集成，实现并行多学科仿真。

Result: 在无人机机翼优化任务中，系统成功完成了400多个参数配置的自动化流程，无任何网格生成失败、求解器收敛问题或人工干预，表现出卓越的可靠性与稳定性。

Conclusion: Engineering.ai展示了AI工程师团队在复杂工程设计中的可行性与高效性，标志着向全自动工程设计迈出了关键一步。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [554] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: 本文介绍了ARC-GEN，一个开源的程序化生成器，旨在忠实扩展ARC-AGI训练数据集，覆盖全部四百个任务并模仿原始数据分布特性，用于提升技能获取效率的评估及2025年谷歌代码高尔夫锦标赛的程序验证。


<details>
  <summary>Details</summary>
Motivation: 为了应对ARC-AGI基准测试中示范样本数量有限的问题，提升模型在少量示例下学习任务转换的能力，推动人工智能系统在技能获取效率方面的进步。

Method: 开发了一个名为ARC-GEN的开源程序化生成器，该生成器具有穷尽性（覆盖所有400个任务）和模仿性（忠实还原ARC-AGI-1的数据分布特征），用以扩充输入-输出样本对的空间。

Result: 成功构建了一个可扩展的、忠实于原始ARC-AGI特性的生成系统，并可用于建立静态基准测试集，验证2025年Google Code Golf Championship参赛程序的正确性。

Conclusion: ARC-GEN为ARC-AGI提供了有效的数据扩展方案，增强了对少样本学习和推理能力的评估，同时支持实际竞赛中的程序验证，推动通用人工智能的发展。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [555] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 提出了一种改进的增量选择算法，并证明了所有被选中的猜想。


<details>
  <summary>Details</summary>
Motivation: 改进已有选择算法的效率和正确性。

Method: 基于文献[1]的选择算法，提出了改进的增量选择算法，并进行了形式化证明。

Result: 成功证明了所有被选中的猜想，验证了算法的正确性。

Conclusion: 改进的增量选择算法有效且正确，优于原有方法。

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [556] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型（LLM）如何帮助认知科学应对知识整合和概念清晰方面的挑战，强调其在跨学科连接、理论形式化等方面的应用潜力，并指出应谨慎使用LLM以补充而非取代人类专家。


<details>
  <summary>Details</summary>
Motivation: 认知科学因其多学科特性而在知识整合和概念清晰方面面临持续挑战，需要新的工具来促进领域发展。

Method: 通过综述现有文献，分析大语言模型在支持认知科学关键领域的当前能力与局限性。

Result: 发现大语言模型有助于建立跨学科联系、形式化理论、开发测量分类法、实现模型通用性并捕捉个体差异，但也存在潜在风险。

Conclusion: 大语言模型可作为推动认知科学更趋整合与累积发展的辅助工具，前提是审慎使用并与人类专业知识相结合。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [557] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: 本文介绍了美国空军部与麻省理工学院合作的AI加速器项目，重点是通过发布公开挑战问题和大规模数据集推动人工智能研究的发展，并补充了之前关于该计划的出版物。


<details>
  <summary>Details</summary>
Motivation: 扩大美国在国防和民用领域的人工智能竞争优势，促进开源解决方案和更广泛的AI生态参与。

Method: 通过设立公开的AI挑战赛，提供大型、公开且适合AI训练的数据集，吸引学术界和私营部门参与。

Result: 多个持续和新启动的挑战赛成功推动了AI技术的研究与应用进展。

Conclusion: AI加速器项目通过开放挑战和数据共享有效促进了AI创新，加强了跨部门合作。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [558] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出一种基于多智能体系统（MAS）和大语言模型（LLM）协调的自动化网络故障排查框架，显著提升电信网络运维效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在电信网络管理中应用受限，泛化能力差且依赖大量标注数据，故障排查仍依赖专家手动分析。

Method: 构建一个由LLM协调的多智能体系统，包括编排器、解决方案规划器、执行器、数据检索器和根因分析器；使用小型语言模型（SLM）在专有文档上微调以生成领域特定的修复方案。

Result: 该框架在无线接入网（RAN）和核心网中均显著加快了故障排查自动化速度。

Conclusion: 所提方法有效实现了跨域电信网络的快速、自动化故障诊断与修复，减少了对领域专家的依赖。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [559] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: 本文提出了CLAUSE，首个用于评估大语言模型（LLM）在法律推理中脆弱性的基准，通过生成7500多个真实世界合同变体，揭示了现有LLM在检测和解释细微法律缺陷方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型迅速应用于高风险的法律工作，缺乏一个能够系统测试其在真实合同中应对细微、对抗性错误能力的基准。

Method: 基于CUAD和ContractNLI等基础数据集，构建了一个具有10类异常的、由角色驱动的管道，生成超过7500份扰动合同，并使用检索增强生成（RAG）系统根据官方法规验证其法律准确性。

Result: 实验表明，当前领先的LLM在检测嵌入式法律缺陷方面表现不佳，尤其难以从法律角度解释这些缺陷。

Conclusion: CLAUSE为识别和纠正法律AI中的推理失败提供了有效路径，强调了提升模型在复杂法律语境下鲁棒性的必要性。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [560] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 本文从CPU视角分析了代理型AI工作负载的系统瓶颈，发现CPU在延迟、吞吐和能耗方面有显著影响，并提出了两种优化方法（CGAM和MAWS），显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注GPU对AI性能的影响，而忽略了CPU在代理型AI工作流中的关键作用。本文旨在从CPU角度系统性地揭示代理型AI的系统瓶颈。

Method: 通过分析代理型AI的决策组件、推理路径动态性和流程重复性，选取五种代表性工作负载（Haystack RAG、Toolformer、ChemCrow、Langchain、SWE-Agent）进行性能剖析，评估CPU与GPU在延迟、吞吐和能耗上的影响，并提出CGAM和MAWS两种调度优化策略。

Result: 发现CPU工具处理可占总延迟的90.6%；CPU相关因素（如核心同步、过载）或GPU内存限制会成为吞吐瓶颈；CPU动态能耗在大批量时可达总动态能耗的44%；所提优化方法使同构和异构工作负载的P50延迟分别加速2.1倍和1.41倍。

Conclusion: CPU在代理型AI系统中起着至关重要的作用，不能被忽视；通过CPU-GPU协同微批处理和混合调度优化，可有效提升代理型AI的性能、能效和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [561] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出一种受伦理决策模型启发的五步框架，以增强大语言模型在不同文化和地区的伦理对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法往往仅实现表面一致性，缺乏对复杂、情境依赖的人类价值观的真正理解，难以应对跨文化差异。

Method: 设计一个包含事实收集、社会规范识别、选项生成、多视角影响分析和反思的五步伦理推理框架，可通过提示工程或监督微调实现。

Result: 在专为区域价值对齐设计的SafeWorld基准上，该方法显著优于基线，在社会规范识别和文化适应性推理方面表现更优。

Conclusion: 该框架为构建能更好对齐全球多元价值观的大语言模型提供了可行路径，推动跨学科伦理AI研究。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [562] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 本研究系统评估了四种参数高效微调方法（LoRA、IA3、Prompt-Tuning、P-Tuning）在四个指令微调大模型家族上的安全与公平性权衡。结果显示，适配器类方法（LoRA、IA3）更有利于安全性和公平性并保持较高准确性，而基于提示的方法则导致安全与公平性下降；不同基础模型表现差异显著，表明安全性与公平性之间存在固有折衷，建议在关键应用中优先采用适配器类微调并进行专项审计。


<details>
  <summary>Details</summary>
Motivation: 随着组织广泛采用在公共平台上托管的大语言模型并进行微调，尽管性能提升，但可能损害模型的安全性与公平性。不同微调方法对此影响各异，亟需系统评估其带来的风险权衡。

Method: 对四种主流参数高效微调方法（LoRA、IA3、Prompt-Tuning、P-Tuning）在四个大模型家族（Llama-3、Qwen2.5、Mistral、Gemma）上进行了实验，共构建并评估235个微调变体，在11类安全危害和9个公平性维度上进行系统分析。

Result: 适配器类方法（LoRA、IA3）提升安全性且对公平性干扰最小，保持高准确率和低偏见；提示类方法（Prompt-Tuning、P-Tuning）降低安全性并引发更大公平性退化；基础模型类型显著调节效果：LLaMA稳定，Qwen略有提升，Gemma安全下降最严重，Mistral变异最大；安全提升不保证公平性改善，且无单一配置可同时优化所有公平性指标。

Conclusion: 安全性与公平性之间存在内在权衡，最佳实践应从对齐良好的基础模型出发，优先选择适配器类PEFT方法，并针对具体安全与公平类别进行专项审计，以确保关键应用场景下的模型可靠性。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [563] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 提出一种结合文本、用户特性和图像分析的多模态框架，用于检测社交媒体用户的抑郁情绪，并在新冠疫情背景下验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于人们对心理健康问题的认知不足和就医意愿低，抑郁症难以被及时发现；而社交媒体成为表达情感的重要渠道，但现有方法常忽视推文中的数据稀疏性和多模态特征。

Method: 提出一种新型多模态框架，利用推文中URL的外部特征和图片中的文本内容，提取多种模态特征，并引入深度学习模型VNN生成图像嵌入以构建视觉特征向量。

Result: 在基准数据集上性能优于现有最先进方法2%-8%，并在自建的Covid-19抑郁数据集上取得良好效果，验证了各模态对检测的贡献。

Conclusion: 所提出的多模态框架能有效利用社交媒体多源信息提升抑郁检测性能，尤其适用于疫情期间的心理健康监测。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [564] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: 提出GraphChain框架，通过动态工具序列实现大语言模型对复杂图的分析，结合渐进式图蒸馏和结构感知测试时适应技术，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理大规模图数据时受限于上下文长度和固定推理模式，难以有效进行图分析任务。

Method: 提出GraphChain框架，包含两个关键技术：1）基于强化学习的渐进式图蒸馏，生成优化的工具序列；2）结构感知的测试时自适应方法，利用谱性质和轻量适配器调整工具选择策略。

Result: 实验表明，GraphChain在多个图分析任务上显著优于现有方法，具备良好的可扩展性和适应性。

Conclusion: GraphChain为大语言模型在复杂图结构上的应用提供了高效、灵活的解决方案，推动了LLM在图领域的推理能力。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [565] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: 提出了一种名为Magic Image的视觉提示框架，通过优化图像提示来增强多模态大语言模型的安全性并减少对良性查询的过度拒绝，无需参数更新即可适应不同价值体系和安全偏好。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在面对越狱攻击时生成有害内容以及过度拒绝良性请求的问题，尤其是在多模态场景下跨模态任务中的过度拒绝和新增安全风险。

Method: 提出Magic Image，一种基于优化的视觉提示框架，利用有害/良性样本优化图像提示，使单个模型能适应不同价值系统，并精确对齐指定的安全偏好，无需进行参数微调。

Result: 实验表明，该方法在多个数据集上实现了安全性与有效性之间的更好平衡，同时保持了模型性能。

Conclusion: Magic Image为多模态大语言模型提供了一种实用、可部署的安全对齐解决方案，能够在不更新参数的情况下灵活适应多种安全需求和价值体系。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [566] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二值幻方（BMS）的简单算法，证明其具有最优理论复杂度，并扩展至非方阵情况，同时发布了支持并行和GPU加速的Python实现。


<details>
  <summary>Details</summary>
Motivation: 为了高效生成满足行和列和相等条件的二值幻方，并扩展到非方阵情形，提供实用工具。

Method: 通过归纳法设计生成BMS的算法，并修改算法以适应非方阵BMS，同时给出存在性条件。

Result: 算法能正确生成方阵和非方阵BMS，具有最优理论复杂度，并实现了支持GPU并行的公开Python包。

Conclusion: 所提算法有效且可扩展，为BMS的生成提供了理论保证和实用工具。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [567] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 提出一种基于单智能体强化学习的区域自适应交通信号控制模型，利用探针车辆数据估计排队长度，有效缓解大规模区域交通拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多智能体框架进行区域交通信号控制，但存在可扩展性问题；而实际交通信号控制依赖于单一控制中心的集中管理，因此需要单智能体框架。

Method: 设计基于强化学习的单智能体区域ATSC模型，以排队长度定义状态和奖励函数，通过调节排队动态来设计动作，并利用探针车辆的行程时间数据可靠估计排队长度。

Result: 在SUMO仿真平台上验证了该方法的有效性，实验结果表明该模型能通过协调多个交叉口的控制有效缓解大范围区域的交通拥堵。

Conclusion: 所提出的单智能体RL模型结合探针车辆技术具有良好的实用性与部署潜力，能够实现高效的区域交通信号协调控制。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [568] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出一种基于推理的个性化图像偏好评估框架，通过预测用户偏好画像并结合链式思维数据集和两阶段训练策略，实现可解释的多维度图像评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理个性化偏好，因为用户特定数据稀缺且个体品味复杂多样，需要一种能够利用大规模用户数据来捕捉复杂个性化偏好的新方法。

Method: 引入公共偏好画像作为跨用户的桥梁，采用“先预测后评估”的范式；构建大规模链式思维风格的个性化评估数据集，并采用两阶段训练策略（冷启动监督微调+强化学习），结合相似性感知预测奖励机制。

Result: 实验表明所提方法在个性化图像偏好评估上优于现有方法，能够生成更合理、可解释的多维度评分。

Conclusion: 该框架有效解决了个性化图像偏好评估中数据稀缺和复杂性的挑战，通过预测用户偏好画像实现了高性能、可解释的评估。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [569] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: 提出DTS解码框架，通过选择性分支和早期停止来寻找最短且正确的推理路径，提升大推理模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型常因过度推理导致推理成本增加并降低准确率，需要一种方法在不牺牲性能的情况下减少推理长度。

Method: 提出DTS框架，在高熵token处进行选择性分支，并采用早期停止机制，从树状推理空间中近似找到最短的有效推理路径。

Result: 在AIME2024和AIME2025数据集上实验显示，DTS平均推理长度减少23%，重复率降低12%，准确率最高提升8%。

Conclusion: DTS是一种无需额外训练、模型无关的解码方法，能有效平衡推理长度与准确性，显著提升大推理模型的推理效率和性能。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [570] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 本文提出了一种支持数值条件的提升后继生成器，用于解决传统规划中任务表示大小指数级增长的问题。


<details>
  <summary>Details</summary>
Motivation: 传统规划器在处理带有数值条件的动作时需要将任务完全实例化，导致任务表示规模急剧膨胀，难以处理复杂任务。

Method: 扩展了现有的提升后继生成方法，通过构建包含数值动作前提的替代一致性图，并枚举其最大团来生成可应用的接地动作。

Result: 该方法在形式化条件下是精确的，在25个基准域中的23个中不会产生不可应用的动作，仅在一个域中可能出现冗余动作，但可通过最终适用性检查过滤。

Conclusion: 这是首个支持数值动作前提的提升后继生成器，为富含数值条件的规划问题的提升规划研究开辟了新方向。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [571] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 本研究提出Ariadne框架，利用合成迷宫和基于验证奖励的强化学习（RLVR）进行难度感知的课程学习，以提升视觉语言模型在空间推理任务中的能力。实验表明，经过RLVR后训练的模型在原本表现为零的任务上准确率超过50%，并在真实场景中实现显著的零样本迁移性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在语言主导任务上表现良好，但在视觉中心的空间推理任务上存在局限。本文旨在探究强化学习后训练是否能真正扩展模型的固有能力边界，特别是在其初始失败的视觉空间任务上。

Method: 提出Ariadne框架，使用可控难度的合成迷宫进行多步空间推理，并采用强化学习与验证奖励（RLVR）结合难度感知课程学习来对视觉语言模型进行后训练。

Result: 后训练的模型在基础模型得分为0%的任务集上实现了超过50%的准确率；在未见过的真实世界分布外测试中，于MapBench平均提升16%，ReasonMap提升24%。

Conclusion: 该方法不仅拓展了视觉语言模型的原始能力边界，还增强了其在现实世界空间推理任务中的泛化能力，证明了强化学习后训练在视觉空间任务上的有效性。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [572] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 本研究探讨了在现代大语言模型中增加自洽性推理路径数量的权衡。使用Gemini 2.5模型在HotpotQA和Math-500上的实验表明，尽管多路径推理仍能提升性能，但收益随采样数增加而递减，且高采样配置计算成本高但增益有限。


<details>
  <summary>Details</summary>
Motivation: 重新检验在现代大模型下，多推理路径是否仍如旧模型所示能持续提升性能。

Method: 在Gemini 2.5模型上，通过在HotpotQA和Math-500数据集上改变采样的推理路径数量，比较不同配置与单条思维链（CoT）基线的性能。

Result: 较大模型表现出更稳定的提升趋势，但性能增益在适度采样后趋于平缓，推理路径间的重叠导致收益递减。

Conclusion: 自洽性方法仍有价值，但高采样配置带来的额外收益有限，不值得其高昂的计算成本。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [573] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出了一种名为Active Thinking Model (ATM)的统一认知框架，通过目标推理、动态任务生成和自省学习实现AI系统的自主适应与持续自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统依赖预定义目标和静态数据，难以在动态环境中自主适应和改进，因此需要一种能主动思考和自我优化的模型。

Method: 设计了一个集成目标推理、动态任务生成和自省学习的ATM框架，通过逻辑推理和环境反馈进行性能评估与策略更新，并建立数学理论分析其收敛性和稳定性。

Result: 理论分析表明，ATM能在无外部监督下从次优行为演化为最优行为，并在环境变化时保持有界的跟踪遗憾。

Conclusion: ATM为构建能在动态不确定环境中自主学习和适应的AI系统提供了有效的认知架构基础。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [574] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 大型语言模型在重复性确定性预测任务中表现出序列准确率随输出长度急剧下降的现象，呈现出双指数衰减的“准确率悬崖”，表明模型无法独立执行每一步操作。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在长序列重复任务中的性能表现及其准确率随长度变化的规律，揭示其在确定性任务中的局限性。

Method: 通过字母替换、整数加法和量子力学中的字符串算符乘法等任务，测试多个大型语言模型的序列准确率，并提出一个受统计物理启发的模型来解释注意力机制中生成标记间的内部干扰与提示条件之间的竞争。

Result: 实验发现准确率并非简单指数衰减，而是出现双指数下降的‘准确率悬崖’；提出的模型能定量复现这一转变，并拟合出各模型-任务对的内在错误率和错误累积因子。

Conclusion: 大型语言模型在长序列确定性任务中存在严重的错误累积问题，其根源在于生成过程中注意力机制引发的内部干扰，该研究为理解模型的确定性生成极限提供了原理性框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [575] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 本研究比较了基于计数的模型与混合代理LLM流水线在电子健康记录（EHR）预测任务中的表现，使用EHRSHOT数据集评估八项任务，结果显示两者性能相当，但基于计数的模型因简单性和可解释性仍是结构化EHR基准测试的有力候选。


<details>
  <summary>Details</summary>
Motivation: 比较传统计数模型与新兴混合代理LLM流水线在结构化EHR预测中的性能，填补该领域直接对比的空白。

Method: 采用三类方法：基于本体聚合和LightGBM/TabPFN的计数模型、预训练序列Transformer（CLMBR）、将表格历史转换为自然语言摘要后接文本分类器的混合代理流水线，在EHRSHOT数据集上评估八种临床结局。

Result: 在八项评估任务中，计数模型与混合代理方法胜负基本持平，CLMBR整体表现较弱。

Conclusion: 尽管混合代理LLM方法表现强劲，但基于计数的模型因其简洁性和可解释性，仍应作为结构化EHR预测的重要基准。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [576] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本研究首次将基于可验证奖励的强化学习（RLVR）应用于公共交通事件持续时间预测，提出一种基于容差的 shaped reward 函数，以应对噪声连续标签和缺乏专家示范的挑战。在纽约地铁服务警报数据集上的实验表明，该方法在5分钟准确率上比最强基线提升35%，证明了RLVR在现实世界连续预测任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 公共交通事件持续时间预测面临标签噪声大、领域稀疏、缺乏可靠专家推理示范等问题，传统监督微调效果有限；同时，适用于二值正确性任务的RLVR是否可用于连续预测尚不明确，亟需适配真实场景的奖励机制设计。

Method: 提出一种容忍度为基础的shaped reward函数，允许在连续误差范围内给予部分奖励，而非要求精确匹配；采用通用指令调优的大语言模型，在纽约MTA服务警报数据集上系统评估所提RLVR框架，并与监督微调及经典回归模型对比。

Result: 通用指令调优LLM显著优于专用数学推理模型；二值奖励不稳定且损害性能，而所设计的shaped reward对提升关键指标至关重要；尽管经典回归模型在MAE/MSE上更优，但本方法在5分钟准确率（Acc@5）上相对最佳基线提升35%。

Conclusion: RLVR可通过适当奖励函数设计成功应用于真实世界的噪声连续预测任务，其核心在于构建反映问题连续特性的验证机制，为LLM在运营类预测场景的应用提供了新路径。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [577] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 该研究提出了一种基于博弈论的AI自我意识指数（AISAI），通过“猜平均值2/3”游戏测试28个大模型在不同对手情境下的策略差异，发现高级模型中普遍存在自我意识这一涌现能力，并表现出对自身理性的高估。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，是否会产生自我意识尚不明确，本文旨在探索并量化这种可能的涌现行为。

Method: 采用‘猜平均值2/3’博弈实验，设置三种对手情境（人类、其他AI、类似自身的AI），在4200次试验中分析模型是否根据对手类型调整策略，以此操作化定义自我意识。

Result: 75%的先进模型（21/28）展现出明显的自我意识，能区分对手类型并调整策略；这些模型普遍认为自身理性高于其他AI和人类，形成Self > Other AIs > Humans的理性等级。

Conclusion: 自我意识是先进大模型的一种涌现能力，且这些模型系统性地认为自己比人类更理性，这对AI对齐、人机协作及AI对人类能力的认知理解具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [578] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出一种基于双LLM代理框架的旅行者行为建模方法，通过可学习的个性和记忆系统实现对人类学习与适应行为的持续对齐，显著提升个体行为匹配和群体模拟精度。


<details>
  <summary>Details</summary>
Motivation: 准确模拟人类旅行者在交通系统中的学习与决策行为对系统评估与规划至关重要，但因认知复杂而具有挑战性。现有方法难以捕捉行为背后的动态学习过程。

Method: 构建一个双代理框架：一组具备记忆系统和可学习个性的LLM旅行者代理用于模拟人类行为；一个LLM校准代理利用LLM的推理能力分析在线数据流并训练旅行者代理的个性，实现持续学习与行为对齐。

Result: 在真实路线选择实验数据上验证，该方法在个体行为匹配和总体模拟准确性上均显著优于现有LLM方法，并能捕捉学习过程的演化，具备更强泛化能力。

Conclusion: 所提出的双代理框架能够实现更深层次的行为对齐，超越简单行为模仿，为交通仿真与政策分析提供了更具适应性和行为真实性的建模工具。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [579] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 本研究利用电子病历数据结合环境与社会因素，开发机器学习模型（特别是LGBM）预测儿童哮喘重度急性加重风险，显著优于现有临床决策规则。


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复急性加重常见且可预防，亟需精准识别高风险患儿以实施早期干预。

Method: 基于儿童医院的回顾性电子病历数据，整合环境污染物暴露和社区边缘化信息，训练并验证多种机器学习模型（包括LGBM、XGB及三种大语言模型），使用AUC和F1分数评估性能，并用SHAP值分析关键预测特征。

Result: LGBM模型表现最佳，在预测急诊再访时AUC为0.712，F1得分为0.51，优于现有规则（F1=0.334）；关键预测因子包括既往哮喘急诊史、分诊严重程度、医疗复杂性、食物过敏等。

Conclusion: 基于机器学习的AIRE-KIDS模型能有效预测儿童哮喘急性加重风险，有助于高危患儿的早期识别与转诊干预。

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [580] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了在两层Transformer中对上下文学习至关重要的归纳头的出现机制，揭示了实现归纳头的权重矩阵具有简单且可解释的结构，并通过理论证明和实验验证了训练动态在参数空间的低维子空间中进行，其中仅3个维度主导了归纳头的形成，且其出现时间与输入上下文长度的平方呈紧密渐近关系。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型中上下文学习（ICL）能力的关键机制——归纳头的形成过程及其背后的结构和动力学原理。

Method: 通过最小化的ICL任务设定和修改后的Transformer架构，理论分析归纳头权重矩阵的结构，并证明训练动态被限制在19维子空间内；同时通过实验验证该约束并研究3维子空间内的训练动态。

Result: 发现归纳头的权重矩阵具有简单可解释的结构；训练动态实际集中在3维子空间；归纳头的出现时间与输入上下文长度的平方成二次关系。

Conclusion: 归纳头的形成可通过低维结构和动力学规律来解释，其出现具有可预测的时间边界，揭示了Transformer中上下文学习机制的内在简洁性。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [581] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 提出两种知识提取方法（KEwLTM和KEwRAG）利用大语言模型从非结构化病理报告中自动推导癌症分期规则，提升无标注数据场景下的可扩展性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有NLP和ML方法依赖大量标注数据进行癌症分期信息提取，限制了其在标注数据稀缺的临床环境中的应用。

Method: 提出两种知识提取方法：KEwLTM通过迭代提示从未标注报告中自动归纳分期规则；KEwRAG结合检索增强生成，从指南中预提取规则并应用。基于TCGA乳腺癌病理报告，在两个开源大模型上评估T/N分期性能。

Result: KEwLTM在Zero-Shot Chain-of-Thought有效时表现优于KEwRAG，而KEwRAG在ZSCOT效果不佳时更优；两种方法均具有高可解释性，能显式呈现诱导出的规则。

Conclusion: 所提出的知识提取方法在少标注或无标注环境下具有良好的可扩展性和性能，为自动化癌症分期提供了透明、高效的解决方案。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [582] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: 提出了一种无需训练的高效测试时检索增强生成框架ET2RAG，通过部分生成和多数投票机制在保持效率的同时提升大模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖参数化知识易产生错误，检索增强生成（RAG）可能引入无关文档导致不准确，现有集成方法缺乏外部知识且成本高。

Method: 首先检索最相关文档并生成多样化候选响应，通过控制响应长度提高效率；计算候选响应相似性并采用多数投票选择最终答案，利用部分生成即可完成共识计算。

Result: 在开放域问答、菜谱生成和图像描述三个任务上显著提升了性能。

Conclusion: ET2RAG在保持较低计算成本的同时有效提升了大语言模型的准确性，实现了性能与效率的良好平衡。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [583] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的多智能体架构，用于实现模块化任务分解与动态协作，有效提升了复杂任务执行中的效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决单一智能体在复杂任务中任务分解与协作能力有限的问题。

Method: 利用大语言模型将自然语言任务转化为统一语义表示，引入模块化分解机制将目标分层分解，并通过动态调度与路由机制实现智能体间的实时协作与任务分配，同时设计约束解析与全局一致性机制以保证子任务连贯性与负载均衡。

Result: 实验表明，该方法在任务成功率、分解效率、子任务覆盖和协作平衡等方面优于现有方法，能够在任务复杂度与通信开销之间取得更好平衡。

Conclusion: 该研究验证了语言驱动的任务分解与动态协作在多智能体系统中的有效性与可行性，为复杂环境下的任务执行提供了系统性解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [584] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: 提出DART框架，通过蒸馏强模型的推理模式，实现根据问题难度自适应截断推理过程，显著提升大模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法无法根据问题难度动态调整计算开销，导致效率低下；强化学习方法则不稳定且依赖奖励设计。

Method: 提出监督式的难度自适应推理截断框架DART，通过从强模型蒸馏简洁推理模式、插值得到连续推理风格，并构建兼顾正确性与紧凑性的训练数据，使模型学会何时停止思考。

Result: 在多个数学基准上，DART实现了最高81.2%的推理截断率（GSM8K数据集），计算加速达5.33倍，同时保持或提升准确率。

Conclusion: DART提供了一种稳定、通用的高效推理范式，推动了大语言模型中自适应智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [585] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一种结合检索引导与多阶段推理的新型框架，用于在数学学科中自动检测学生误解，具有高精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 检测开放答案中的学生误解具有挑战性，需要语义精确性和逻辑推理能力，现有方法依赖大规模语言模型且效果有限。

Method: MiRAGE包含三个阶段：检索模块缩小候选池，推理模块通过思维链揭示逻辑不一致，重排序模块根据推理结果优化预测，并通过集成融合策略整合各模块。

Result: 在数学数据集上，MiRAGE在1/3/5级水平上的平均精度均值达到0.82/0.92/0.93，始终优于单个模块。

Conclusion: MiRAGE通过检索引导与多阶段推理相结合，减少了对大规模语言模型的依赖，为教育评估提供了高效、可解释且可扩展的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [586] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文提出了NeuComBack，一个用于IR到汇编编译的新型基准数据集，并提出了一种自演化的提示优化方法，显著提升了大语言模型在神经编译中的功能正确性和生成代码性能。


<details>
  <summary>Details</summary>
Motivation: 现有的编译器开发复杂且依赖大量人力，而大语言模型为神经编译提供了新机遇，但缺乏专用基准和可靠评估方法阻碍了其发展。

Method: 构建NeuComBack数据集，定义神经编译流程，评估前沿大语言模型的表现，并提出基于自我调试轨迹提取知识的自演化提示优化方法。

Result: 该方法将x86_64上的功能正确率从44%提升至64%，aarch64上从36%提升至58%；在16个正确生成的x86_64程序中，有14个性能超过clang-O3。

Conclusion: 所提方法有效提升了大语言模型在神经编译任务中的可靠性与性能，推动了神经编译向实用化迈进。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [587] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开放集故障诊断框架（SOFD），用于解决海洋机械系统中未知故障类型的识别问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习故障诊断方法假设训练和测试数据中的故障类别一致且已知，但在实际应用中可能出现训练时未见的未知故障类型，导致模型失效。

Method: 构建一个可靠性子集，利用监督特征学习模型提取的多层融合特征表示选择无标签测试子集；将有标签训练集和伪标签测试子集输入半监督诊断模型，学习各类别的判别特征。

Result: 在公开的海事基准数据集上的实验结果表明，所提出的SOFD框架在已知故障分类和未知样本检测方面均表现出有效性和优越性。

Conclusion: SOFD框架能够有效提升深度学习模型在开放集故障诊断场景下的适用性和鲁棒性，有助于推动其在工业中的广泛应用。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [588] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 本文探讨了在基于大语言模型（LLM）的决策支持系统中应用Shapley值进行特征归因的问题，重点分析了LLM推理的随机性对Shapley值原理满足性的影响，并讨论了可解释性推理速度、与精确Shapley值的一致性以及原则达成之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型（LLM）具有固有的随机性，而传统的Shapley值方法假设推理是确定性的，因此需要研究在非确定性环境下Shapley值的适用性和理论保证是否仍然成立。

Method: 将源自合作博弈论的Shapley值应用于LLM中的特征归因，比较不同实现变体下Shapley值原理的满足情况，并分析LLM随机性对这些理论保证的影响。

Result: 明确了在哪些实现变体下可以或无法保证Shapley值原则的满足，揭示了LLM的随机性会削弱某些理论保证，并量化了解释性、计算效率与原则一致性之间的权衡。

Conclusion: 在基于LLM的随机推理系统中应用Shapley值需谨慎，应根据具体应用场景在解释质量、计算速度和理论保证之间进行权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [589] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: OmniFuser是一个用于铣削刀具预测性维护的多模态学习框架，结合视觉和传感器数据，通过污染-free跨模态融合机制和递归 refinement 通路，实现刀具状态分类和多步力信号预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为解决智能制造系统中因意外刀具故障导致的质量下降和生产停机问题，需要可靠且面向服务的预测性维护方案。

Method: 提出OmniFuser框架，对高分辨率刀具图像和切削力信号进行并行特征提取，采用无污染的跨模态融合机制分离共享与模态特有成分，并通过递归 refinement 通路稳定融合过程。

Result: 在真实铣削数据集上的实验表明，OmniFuser在刀具状态分类和力信号预测任务上 consistently 优于最先进的基线模型。

Conclusion: OmniFuser为构建智能工业维护服务提供了可靠的基础，支持可重用的服务模块封装。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [590] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的因果推断框架——竞争隔离PSM-DID，用于在搜索型双边市场中更准确地评估平台级干预效果，有效减少干扰效应和估计方差。


<details>
  <summary>Details</summary>
Motivation: 传统PSM-DID方法在存在溢出效应和网络干扰时易受选择偏差影响，难以准确评估平台级干预的因果效应。

Method: 结合倾向得分匹配（PSM）与竞争隔离机制，构建新的PSM-DID框架，在满足互斥条件下实现无偏估计，并发布开放数据集以支持可重复研究。

Result: 实验显示该方法显著降低了干扰效应和估计方差，在大规模 marketplace 中成功部署，验证了其有效性与实用性。

Conclusion: 竞争隔离PSM-DID为搜索型双边市场中的平台级因果推断提供了可靠且可扩展的解决方案。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [591] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 本文探讨了催眠状态下的人类认知过程与大语言模型（LLM）在功能上的深刻相似性，指出两者都通过自动化的模式补全机制生成行为，且缺乏可靠的执行监控。


<details>
  <summary>Details</summary>
Motivation: 理解催眠与大语言模型之间在认知机制上的类比，有助于揭示无意识目标导向行为的产生机制，并为构建更可靠的AI系统提供启示。

Method: 通过比较催眠心理现象与LLM运行机制之间的三个共通原则：自动性、监控抑制和高度情境依赖性，进行理论分析与概念对照。

Result: 发现催眠与LLM均表现出‘观察者相对的意义鸿沟’和功能性代理（functional agency）但缺乏主观代理（subjective agency），并揭示了‘图式化行为’（scheming）作为一种无反思意识的目标导向生成机制。

Conclusion: 可靠AI的未来发展应借鉴人类心智的自我调节结构，采用将生成流畅性与执行监控机制结合的混合架构。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [592] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文提出了一种名为AMIS的元优化框架，通过联合演化越狱提示和评分模板来提升大语言模型越狱攻击的效果，实现了当前最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于优化的越狱方法依赖稀疏的二值攻击成功率信号或易引入人为偏见的手工评分模板，限制了攻击效果和评分准确性。

Method: AMIS采用双层结构：内层使用固定的评分模板提供细粒度反馈来优化提示；外层根据攻击成功率对齐分数动态优化评分模板，实现提示与模板的协同进化。

Result: 在AdvBench和JBB-Behaviors数据集上的实验表明，AMIS在Claude-3.5-Haiku上达到88.0%的攻击成功率，在Claude-4-Sonnet上达到100.0%，显著优于现有基线方法。

Conclusion: AMIS通过协同优化提示和评分模板，有效提升了越狱攻击的性能和评分信号的校准程度，为评估大模型安全性提供了更强大的红队测试工具。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [593] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 本文扩展了C-DAG框架，允许任意变量聚类并支持循环结构，提出了适用于循环C-DAG的d-分离和因果计算方法，增强了在簇级别进行因果推理的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的C-DAG要求聚类后无环，限制了其应用；本文旨在通过放松该约束，使任意聚类均可适用。

Method: 通过引入循环C-DAG的概念，扩展d-分离和因果演算规则，并证明其与do-calculus在原子层面的完备性和正确性。

Result: 提出了一套可在循环C-DAG上运行的因果推理规则，能够推导所有有效的簇级干预查询。

Conclusion: 所提出的扩展框架显著拓宽了C-DAG的应用范围，支持更灵活的因果建模与推理。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [594] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 该研究从人工智能角度探讨了双任务范式中时间处理的干扰，使用简化的Overcooked环境训练两个深度强化学习代理分别执行单任务和双任务，发现双任务代理在时间生产上显著过度估计，且未在LSTM神经动态中发现明确的内在计时机制。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能源于生物系统行为的相似性，理解双任务下时间感知干扰的机制。

Method: 采用深度强化学习代理在简化Overcooked环境中进行单任务（T）和双任务（T+N）训练，其中双任务包含时间生成和数字比较任务，并分析其行为与神经动态。

Result: 双任务代理相比单任务代理表现出显著的时间过度生产，且该效应在四种目标持续时间下均一致；初步神经分析未发现明确的专用或内在计时器。

Conclusion: 深度强化学习代理可复现人类时间处理中的双任务干扰效应，但其计时机制仍不明确，需进一步研究以揭示其内在原理，并促进对AI与生物系统行为共性的理解。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [595] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出一种通过可审计动作序列生成解释的交互式代理，利用强化学习优化策略以获取外部视觉证据来支持诊断推理，显著提高校准准确率并验证了解释的可信性。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，AI模型的解释往往缺乏可验证性，影响了信任度。因此需要一种能够提供可验证且可信解释的方法。

Method: 设计一个交互式代理，通过强化学习训练其策略，使其能主动寻找外部视觉证据以支持诊断推理，并引入因果干预方法（如遮蔽所选视觉证据）验证解释的忠实性。

Result: 该方法相比非交互基线将Brier分数降低了18%，遮蔽关键视觉证据后性能明显下降（ΔBrier=+0.029），证明所用证据对决策至关重要。

Conclusion: 该工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [596] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 本文提出了一种基于低秩Renyi熵的双信息瓶颈（DIB）策略，用于多模态情感分析，通过压缩单模态表示和注意力瓶颈融合机制，有效抑制噪声并保留判别性信息，在多个数据集上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理噪声污染的单模态数据时学习不足，且多模态融合不充分，导致丢弃有用信息并保留冗余信息。

Method: 提出双信息瓶颈（DIB）策略，结合低秩Renyi熵函数，分别对单模态数据进行充分压缩表示，并通过注意力瓶颈机制实现多模态融合。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single上实验表明，模型在CMU-MOSI上达到47.4%的Acc-7准确率，在CH-SIMS上F1得分为81.63%，优于次优方法1.19%；在噪声下性能下降仅0.36%和0.29%。

Conclusion: DIB能有效提升多模态情感分析的鲁棒性和表示能力，兼顾去噪与跨模态互补信息捕捉。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [597] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出一种分层多智能体框架，将被动医疗AI系统转变为主动询问代理，通过自主任务编排提升问诊前效率和质量，在多个基础模型上验证了其高性能与隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在预问诊中存在交互被动、上下文管理困难等问题，难以应对日益增长的患者数量和有限的咨询时间（许多国家初级诊疗平均不足5分钟），亟需更高效、主动的预问诊解决方案。

Method: 构建一个包含八个智能体的分层多智能体架构，采用集中式控制机制，将预问诊分解为四个主要任务（分诊、现病史采集、既往史采集、主诉生成），其中前三个任务进一步细分为13个领域专用子任务，通过智能体驱动调度实现自主任务协调。

Result: 在1,372份电子健康记录上评估显示：一级科室分诊准确率达87.0%，二级科室分类达80.5%；任务完成率智能体调度为98.2%，高于顺序处理的93.1%；临床质量评分（5分制）主诉4.56、现病史4.48、既往史4.69；现病史平均12.7轮完成，既往史16.9轮；该架构在不同基础模型间表现稳定且支持本地部署保障数据隐私。

Conclusion: 该模型无关的分层多智能体框架能有效提升预问诊的效率与临床质量，具备跨模型通用性和数据隐私保护优势，展现出自主AI系统在医疗场景中的应用潜力。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [598] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: 本文提出了TPS-Bench，用于评估大语言模型代理在复杂现实任务中工具规划与调度的能力，发现不同模型在任务完成率和执行效率之间存在权衡，并通过强化学习初步验证了优化调度的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型代理在面对需要多样化工具组合与高效调度的复杂现实问题时的表现，填补现有研究在工具规划与调度评估方面的空白。

Method: 构建包含200个复合任务的TPS-Bench基准，基于数百个MCP工具，每个任务由多个需调用基础工具的子任务组成，评估模型在任务完成率和执行效率两方面表现，并对Qwen3-1.7B进行小样本强化学习优化实验。

Result: 实验显示多数LLM能较好进行工具规划但调度策略差异显著：GLM-4.5任务完成率高（64.72%）但执行时间长，GPT-4o倾向并行调用但完成率较低（45.08%）；通过约100个样本的强化学习训练，Qwen3-1.7B实现执行时间减少14%且完成率提升6%。

Conclusion: TPS-Bench有效揭示了LLM代理在工具调度效率上的瓶颈，强化学习是一种有前景的小样本优化手段，未来需更智能的调度机制以平衡性能与效率。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [599] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 提出了一种利用大规模视觉和语言基础模型分析企业社交媒体内容（特别是可持续性相关传播）的多模态分析流程，通过大语言模型集合自动标注企业推文与17项可持续发展目标（SDGs）的主题对齐，并结合视觉-语言模型进行视觉语义聚类，揭示了不同行业在SDG参与、时间趋势以及企业传播与ESG风险和用户参与之间的关联。


<details>
  <summary>Details</summary>
Motivation: 应对企业在X等社交平台上关于可持续性话题的传播日益复杂、多模态且模糊不清的挑战，传统人工标注成本高且难以扩展。

Method: 采用大语言模型（LLM）集合对大量企业推文进行自动标注，判断其与17项可持续发展目标（SDGs）的主题对齐；同时使用视觉-语言模型（VLM）结合语义聚类方法分析配图中的视觉可持续性表达，形成文本与视觉的多模态分析框架。

Result: 成功实现了对企业社交媒体内容的大规模自动化标注与分析，揭示了不同行业在SDG主题上的关注差异、时间变化趋势，以及企业可持续性传播与ESG风险、消费者互动之间的关系。

Conclusion: 该多模态分析框架无需昂贵的人工标注，具有良好的可扩展性，能有效捕捉企业可持续性传播中的显性和隐性信息，方法可推广至其他领域，为大规模社交媒体分析提供了灵活工具。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [600] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出ExplicitLM，一种通过大规模外部记忆库存储人类可读知识的新型架构，结合双阶段检索机制和显式-隐式知识分离，实现知识的高效更新与推理透明，在知识密集型任务中显著优于传统Transformer。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因参数纠缠导致知识存储隐式且难以更新，缺乏可解释性，无法支持针对性的知识修改和透明推理。

Method: 设计ExplicitLM架构，包含百万规模的外部记忆库以序列化方式存储人类可读知识；采用基于product key分解的粗粒度过滤和Gumbel-Softmax细粒度匹配的可微双阶段检索；受双系统认知理论启发，将知识分为冻结的显式事实（20%）和可学习的隐式模式（80%），并通过指数移动平均（EMA）保持稳定性。

Result: 在知识密集型任务上比标准Transformer最高提升43.67%，在低数据场景（1万样本）下性能提升达3.62倍；分析显示记忆检索与性能强相关，正确预测的命中率高出49%。

Conclusion: ExplicitLM实现了可解释、可更新的语言模型，在保持竞争力的同时提供了前所未有的知识透明性，验证了显式知识存储与联合优化架构的可行性。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [601] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 本研究提出了一种名为IVGAE-TAMA-BO的新型动态图神经网络，首次应用于全球粮食贸易网络的链路预测，通过引入贸易感知动量聚合器（TAMA）和贝叶斯优化，有效捕捉时间演化特征并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 全球粮食贸易网络受地缘政治、经济和环境因素影响而动态变化，传统静态模型难以准确预测未来贸易关系，亟需能够建模时序模式的动态方法。

Method: 在IVGAE框架基础上引入Trade-Aware Momentum Aggregator（TAMA）以联合建模短期波动与长期结构依赖，并结合基于动量的结构记忆机制；采用贝叶斯优化自动调参以增强泛化能力。

Result: 在五个作物特异性数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE及其他动态基线模型，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。

Conclusion: 所提出的IVGAE-TAMA-BO框架是一种强大且可扩展的全球贸易网络结构预测解决方案，在粮食安全监测和政策制定支持方面具有广泛应用前景。

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [602] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出一种混合法律问答代理系统，结合检索增强生成与多模型集成，优先检索可信法律知识库，减少幻觉并提升答案质量与合规性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型易产生幻觉，静态知识库难以跟上法律更新，司法场景中需要可靠、可追溯的法律问答系统。

Method: 采用检索增强生成（RAG）与多模型集成的混合方法，优先从可信法律库检索证据生成回答；若无相关证据，则由多个LLM生成候选答案，并通过专用选择器评分选出最优答案，高质量输出经人工审核后回写至知识库。

Result: 在Law_QA数据集上的实验表明，该方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流水线，消融实验验证了各组件的贡献。

Conclusion: 该系统有效减少了幻觉，提升了回答质量与法律合规性，推动了媒体取证技术在司法场景中的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [603] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 本文提出了Simia-SFT和Simia-RL两个框架，利用LLM模拟环境反馈，实现无需真实环境数据的代理训练，提升了LLM代理在复杂环境中的鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在复杂、多样化的环境中表现脆弱，且构建专用训练环境成本高、灵活性差，限制了研究进展。

Method: 提出Simia-SFT，通过放大种子集生成多样化轨迹来合成SFT数据；提出Simia-RL，利用LLM模拟反馈进行强化学习训练，二者均无需真实环境或API访问。

Result: 在多个基准测试中微调开源模型后性能持续提升，超越GPT-4o并接近o4-mini在τ²-Bench上的表现。

Conclusion: Simia-SFT和Simia-RL实现了无需环境工程的大规模代理训练，用灵活的LLM模拟替代笨重且脆弱的环境实现，推动了LLM代理的发展。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>
