<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 58]
- [cs.CL](#cs.CL) [Total: 77]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.RO](#cs.RO) [Total: 33]
- [cs.LG](#cs.LG) [Total: 90]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter是一种用于从自由稀疏图像中快速进行表面重建的新方法，通过端到端的高斯初始化、相机参数估计和几何增强优化，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点阵化方法在稀疏、未标定图像下重建效果差，易出现表面不完整和过拟合问题，因此需要一种能在自由稀疏视角下实现高质量重建的方法。

Method: 提出FSFSplatter，采用大型Transformer编码多视图图像，通过自分裂高斯头生成密集且几何一致的高斯场景初始化；结合基于贡献度的剪枝、深度与多视图特征监督，并在优化中使用可微相机参数来缓解过拟合。

Result: 在DTU和Replica数据集上优于当前最先进的方法，实现了更准确、完整的表面重建。

Conclusion: FSFSplatter有效解决了稀疏输入下的高斯点阵化重建难题，实现了快速、高质量的表面重建，适用于真实场景中的低重叠、少视角条件。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [2] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型（VLM）中OCR增强生成在韩语和英语中的应用，提出了双语OCR基准KLOCR，并发布了用于韩语VQA的KOCRBench数据集。实验表明，OCR提取的文本显著提升了开源和商业模型的性能。


<details>
  <summary>Details</summary>
Motivation: 为了推动多语言环境下视觉语言模型与OCR结合的研究，特别是在韩语等资源较少的语言中，需要一个强大的双语OCR基础模型和相应的评估基准。

Method: 训练并发布KLOCR模型，一个基于1亿实例的双语OCR基线模型；构建KOCRBench韩语VQA评测集；分析不同提示方法对性能的影响。

Result: OCR提取的文本显著提升多种VLM在双语VQA任务上的表现；KLOCR在OCR增强生成中展现出强大性能；不同提示方法的效果得到系统分析。

Conclusion: OCR信息有效增强多语言VLM的生成能力，尤其在韩语VQA任务中表现突出，为多语言OCR增强生成提供了新的研究基础和方向。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [3] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: 本文提出了一种名为ROGR的新方法，通过生成性重光照模型重建多视角捕获物体的可重光照3D模型。该方法利用多光照环境下的外观采样，训练一个光照条件化的NeRF模型，实现任意环境光照下的高效前向重光照。


<details>
  <summary>Details</summary>
Motivation: 为了实现对3D物体在任意环境光照下的真实感重光照，避免传统方法中复杂的每光照优化或光传输模拟过程。

Method: 采用生成性重光照模型，在多个光照环境下采集物体外观，构建数据集；使用新型双分支架构的光照条件化NeRF，分别编码整体光照效果和镜面反射特性。

Result: 在TensoIR和Stanford-ORB数据集上优于现有最先进方法，并在真实物体捕捉场景中展示了有效性。

Conclusion: ROGR能够高效生成高质量的可重光照3D模型，无需每光照优化或复杂仿真，显著提升了重光照的真实感与灵活性。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [4] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 本文提出了一种名为Oracle-RLAIF的新框架，通过使用通用的Oracle排序器替代训练好的奖励模型，并引入基于排名的GRPO_rank损失函数，实现了更高效、灵活的视频语言模型对齐方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频语言模型规模增大，依赖人类反馈的微调成本高昂，现有AI反馈强化学习（RLAIF）框架依赖专门训练的奖励模型，成本高且限制多。因此需要一种更低成本、更灵活的微调框架。

Method: 提出Oracle-RLAIF框架，用通用的Oracle排序器直接对候选响应进行排序而非打分；同时提出GRPO_rank损失函数，基于组相对策略优化，利用序数反馈和排名感知优势进行优化。

Result: 在多个视频理解基准上，Oracle-RLAIF consistently 优于现有的先进VLM微调方法，展现出更高的数据效率和灵活性。

Conclusion: Oracle-RLAIF为大规模多模态视频模型提供了一条无需标量奖励、基于排名反馈的高效对齐路径，推动了RLAIF框架向更灵活、低成本方向发展。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards -- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [5] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: 本文提出PhysHMR，一种统一的从单目视频中重建物理合理人体运动的框架，通过直接学习视觉到动作的策略，在物理模拟器中实现视觉对齐且物理合理的运动重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于运动学估计，缺乏物理约束，导致结果不真实；两阶段方法存在误差累积问题，限制了重建质量。

Method: 提出PhysHMR框架，采用像素即射线（pixel-as-ray）策略将2D关键点提升为3D空间射线并转换到全局空间，作为策略输入提供鲁棒的全局姿态引导；结合预训练编码器的局部视觉特征，并通过从动捕专家模型到视觉条件策略的知识蒸馏，再结合物理动机的强化学习奖励进行优化。

Result: 实验表明，PhysHMR在多种场景下生成高保真、物理合理的运动，在视觉准确性和物理真实性方面均优于先前方法。

Conclusion: PhysHMR通过统一的视觉到动作策略和软性全局定位机制，有效实现了高质量、物理可信的人体运动重建，解决了传统两阶段方法的误差累积问题。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [6] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 本研究探讨了人类与机器在人脸识别决策中的协作机制，提出了“邻近准确率规则”（PAR），并发现当人类与机器的基线准确率接近时，融合决策效果更优。研究确定了一个较大的“关键融合区”，在此区域内，尽管人类准确性低于机器，但结合二者能提升整体系统性能。通过智能选择合适的人类参与者进行融合，实现了优于纯机器或随机融合的准确性，为人工智能在人脸识别中的合理应用提供了实证依据。


<details>
  <summary>Details</summary>
Motivation: 由于人脸识别算法的决策可能具有重大影响，需要人类审查以形成人机协同系统。然而，个体差异可能导致协作效果不一，因此需要明确在何种情况下人机协作能够真正提升识别准确性。

Method: 基于专家与非专家的人脸识别数据，分析人-人与人-机协作的效果，提出并验证“邻近准确率规则”（PAR），利用该规则界定“关键融合区”，并通过图论方法评估全人类协作系统的上限性能，同时比较不同融合策略的系统准确性。

Result: 发现协作效益随合作者之间基线准确率差异减小而增加，PAR能有效预测融合效益；存在一个较大的关键融合区，在此区间内人机融合可提升系统准确性；智能人机融合比单独使用机器或所有人类-机器判断融合更准确；全人类协作系统的最佳表现接近智能人机协作的平均水平，但后者更能减少低水平人类对整体性能的影响。

Conclusion: 人类与机器在人脸识别中各有作用，通过基于PAR的智能融合策略，可以显著提升系统整体准确性，研究为AI在人脸识别中的合理部署提供了实证指导。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [7] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文提出了首个用于生成式视频模型的不确定性量化（UQ）框架，包括一种基于鲁棒秩相关估计的校准评估指标、一种名为S-QUBED的黑箱UQ方法（可分解为偶然性和认知性不确定性），以及一个用于基准测试的UQ数据集。实验表明，S-QUBED能有效估计与任务准确率负相关的校准不确定性。


<details>
  <summary>Details</summary>
Motivation: 生成式视频模型虽然广泛应用，但存在事实性错误的“幻觉”问题。与大语言模型类似，其不确定性量化研究尚属空白，缺乏安全保证，因此亟需针对视频生成模型的UQ方法。

Method: 提出S-QUBED框架：在潜在空间中进行条件生成，通过潜在建模将预测不确定性分解为偶然性（aleatoric）和认知性（epistemic）两部分；设计无需强假设的鲁棒秩相关指标来评估模型校准性；构建专用UQ数据集以支持基准测试。

Result: 在多个基准视频数据集上的实验证明，S-QUBED能够生成校准良好的总不确定性估计，且该估计与任务准确率呈负相关；同时能有效分离并计算偶然性和认知性不确定性成分。

Conclusion: 本工作是生成式视频模型不确定性量化的首次尝试，所提出的S-QUBED框架和评估指标为提升视频生成模型的可靠性和安全性提供了有效工具，填补了该领域的研究空白。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [8] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 提出一种名为Prompt Embedding Optimization (PEO) 的方法，通过优化简单提示的文本嵌入来提升预训练文本到图像扩散模型的美学质量，无需训练且不依赖特定模型结构。


<details>
  <summary>Details</summary>
Motivation: 在仅使用简单、未经修饰的提示时，现有预训练文本到图像模型生成图像的美学质量仍有提升空间，因此需要一种无需重新训练的高效优化方法。

Method: 利用预训练扩散模型作为骨干，通过一个包含美学保真度、文本嵌入一致性以及提示保留项的三元目标函数，优化输入提示的文本嵌入。

Result: 在定量和定性评估中，PEO方法优于或相当于当前最先进的文本到图像生成和提示适应方法。

Conclusion: PEO是一种无需训练、模型无关的提示优化方法，能有效提升简单提示下生成图像的美学质量。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [9] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 提出了一种新型无标记多摄像头系统，用于在真实野外条件下精确捕捉3D手势及其与物体的交互，结合了背戴式八摄像头装置和Meta Quest 3头显，实现了高精度3D手势姿态标注。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多在受控实验室环境中采集，缺乏环境多样性，限制了模型泛化能力，因此需要在真实无约束场景下实现高精度3D手势追踪。

Method: 设计了一个轻量级、背戴式多摄像头捕捉系统，包含八个外部摄像头和用户佩戴的Meta Quest 3头显（提供两个自我中心视角），并构建了一个ego-exo跟踪流水线以生成准确的3D手势姿态真值。

Result: 成功采集并标注了一个包含同步多视角图像和精确3D手势姿态的数据集，验证了该系统的高质量3D标注能力，显著降低了环境真实感与3D标注精度之间的权衡。

Conclusion: 所提出的系统能够在接近无约束的真实环境中实现高精度的3D手势捕捉，为面向实际应用的视觉模型训练和评估提供了更具多样性和真实性的数据支持。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [10] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本文提出一种基于输入人体姿态的视频蒸馏方法，通过引入输入感知的稀疏注意力和蒸馏损失，将多步扩散模型压缩为少步学生模型，在保持高质量的同时实现实时语音驱动视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的语音驱动视频生成方法因多步去噪和高计算成本的注意力机制而速度慢，难以实现实时应用。

Method: 利用输入的人体姿态关键点指导注意力机制，设计输入感知的稀疏注意力以减少冗余计算并增强身体部位的时间一致性；同时提出输入感知的蒸馏损失，提升唇部同步和手势动作的真实感。

Result: 该方法在多个数据集上实现了实时生成，并在视觉质量和运动连贯性方面优于现有的音频驱动和输入驱动方法。

Conclusion: 结合姿态引导的稀疏注意力与输入感知蒸馏损失，能有效提升扩散模型蒸馏效率与生成质量，推动语音驱动视频生成向实时应用迈进。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [11] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 本文提出了一种基于低秩适应（LoRA）的新型条件机制FunLoRA，用于持续学习中的生成模型，仅使用当前任务数据训练即可有效缓解灾难性遗忘，并在性能、内存开销和采样时间上优于现有扩散模型方法。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在文本和视觉应用中广泛应用，但持续学习中的灾难性遗忘问题严重限制了其发展；现有基于合成数据回放的方法存在训练时间增长和性能退化的问题。

Method: 设计了一种新的低秩适应机制FunLoRA，采用秩为1的矩阵并通过函数重参数化提升表达能力，实现动态条件控制，使模型仅需在当前任务数据上训练即可保持历史知识。

Result: 在从零训练的基于流匹配的模型上进行实验，结果显示该方法在分类准确率上超过现有的扩散模型SOTA方法，同时显著降低内存消耗和采样时间。

Conclusion: FunLoRA是一种高效、可扩展的参数微调方法，能有效解决生成模型持续学习中的灾难性遗忘问题，且具备更低的资源开销和更好的长期性能。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [12] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 本文提出了一种基于多源数据集的双视场、序列保持的鲁棒性框架，用于美国交通信号灯和标志识别，结合特征压缩、防御性蒸馏和基于熵的异常检测，并通过时序投票提升性能，在真实场景中显著降低了攻击成功率和高风险误分类率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆对交通信号识别的鲁棒性要求极高，现有方法缺乏对时间连续性、多视角感知以及数字与自然退化因素联合影响的考虑。

Method: 构建包含aiMotive、Udacity、Waymo及自录德州区域视频的多源数据集，对高速、夜间、雨天和城市四种运行设计域进行中长期RGB图像序列对齐，提出融合特征压缩、防御性蒸馏、基于熵的异常检测和时序投票的三层次统一防御框架。

Result: 该方法在真实异常检测应用中实现了79.8 mAP，将攻击成功率降低至18.2%，高风险误分类减少至32%，优于YOLOv8、YOLOv9和BEVFormer，且验证了物理可迁移性。

Conclusion: 所提出的统一防御堆栈在应对数字与自然干扰下的交通信号识别任务中表现出优越的鲁棒性和安全性，具备实际部署潜力。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [13] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: 提出了Smart-GRPO，首个针对流匹配模型中强化学习优化噪声扰动的方法，通过迭代搜索策略提升奖励优化与图像质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型的确定性使其难以适用于强化学习，而现有引入随机性的方法效率低且不稳定，因此需要更有效的扰动优化方法。

Method: 提出Smart-GRPO，采用迭代搜索策略：生成候选扰动，通过奖励函数评估，并优化噪声分布以聚焦高奖励区域。

Result: 实验表明，Smart-GRPO在奖励优化和视觉质量上优于基线方法。

Conclusion: Smart-GRPO为流匹配框架中的强化学习提供了可行路径，弥合了高效训练与人类对齐生成之间的差距。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [14] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: 本文提出了MoGIC，一个融合意图建模和视觉先验的统一多模态运动合成框架，通过联合优化动作生成与意图预测，显著提升生成质量与可控性，并引入混合注意力机制实现条件令牌与运动子序列的有效对齐。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动运动生成方法难以捕捉行为背后的因果逻辑与人类意图，且缺乏视觉 grounding 导致时空细节表达不足，限制了生成的精确性与个性化。

Method: 提出MoGIC框架，结合意图预测与视觉先验进行联合优化；设计自适应范围的混合注意力机制以实现局部对齐；构建包含440小时数据的Mo440H基准用于训练与评估。

Result: 在HumanML3D和Mo440H上微调后FID分别降低38.6%和34.6%；在运动描述任务中超越基于大语言模型的方法；支持意图预测与视觉条件生成。

Conclusion: MoGIC通过引入意图建模与视觉先验，提升了多模态运动生成的质量、可控性与语义一致性，推动了对人类意图的理解与应用。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [15] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 提出一种运动自适应的动态3D重建框架，通过语义和运动先验优化控制点分布，提升重建质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏控制方法因仅基于几何分配控制点，导致静态区域冗余、动态区域不足，难以有效建模复杂运动场景。

Method: 利用视觉基础模型提取语义与运动先验，建立patch-token-node对应关系，采用运动自适应压缩策略；通过迭代体素化和运动趋势评分调整控制密度，并引入基于样条的轨迹参数化（由2D tracklet初始化）替代MLP形变场。

Result: 在多个实验中显著优于当前最先进方法，实现了更高质量的重建、更平滑的运动表示和更稳定的优化过程，同时提升了计算效率。

Conclusion: 该运动自适应控制点分配与样条轨迹建模方法有效解决了控制密度与运动复杂性之间的不匹配问题，为单目视频的动态3D重建提供了高效且鲁棒的解决方案。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [16] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Net2Net的新方法，结合了未训练网络和预训练网络的优势，通过正则化去噪（RED）框架实现真实场景下的噪声去除，在少标签或无标签数据情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法依赖手工先验，难以应对真实噪声的复杂性和多样性；基于深度学习的方法需要大量标注数据且泛化能力有限，因此需要一种更鲁棒、适应性强的去噪方法。

Method: 提出Net2Net方法，将无监督的DIP与有监督的预训练模型DRUNet结合，利用未训练网络适应输入图像的独特噪声特征，同时借助预训练网络从大规模数据中学习到的表示能力，并通过正则化去噪（RED）框架进行联合优化。

Result: 在多个基准数据集上的实验表明，该方法在真实噪声去除任务中优于现有方法，尤其在训练数据有限的情况下仍能保持良好性能。

Conclusion: Net2Net通过融合未训练和预训练网络的优势，有效提升了去噪模型在复杂真实噪声下的泛化能力和实用性，为低数据条件下的图像去噪提供了新思路。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [17] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1是首个采用R1架构的多模态通用检索模型，通过引入信息压缩模块和新的训练范式，结合合成CoT数据激活阶段与课程奖励强化学习，在提升检索精度的同时显著提高效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 直接将DeepSeek-R1的方法应用于检索任务存在计算成本高和强化学习训练不稳定的问题，因此需要专门针对检索任务设计高效且稳定的多模态大模型。

Method: 提出Retrv-R1，包含信息压缩模块与细节检查机制以降低token消耗并保留关键信息；采用分阶段训练：先使用检索定制的合成思维链数据进行激活，再通过带课程奖励的强化学习优化模型。

Result: Retrv-R1在多个基准和任务上实现了最先进的性能，兼具高效率和强泛化能力，验证了其在多模态检索中的有效性。

Conclusion: Retrv-R1通过创新的信息压缩机制和分阶段训练策略，成功将R1类推理架构有效应用于多模态检索任务，解决了高计算成本和训练不稳定性问题，为多模态模型的高效推理提供了新思路。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [18] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 提出BCA+，一种无需训练的测试时自适应框架，通过动态缓存和贝叶斯推理统一提升视觉-语言模型在对象识别与检测中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法依赖反向传播计算成本高，或仅关注似然适应而忽略先验的重要性，导致在真实分布偏移下性能下降。

Method: 将适应过程建模为贝叶斯推理问题，引入动态缓存机制，存储并更新类别嵌入、空间尺度和基于历史预测的自适应类别先验，结合动态似然与先验进行预测融合。

Result: 在多个识别与检测基准上达到最先进性能，且无需训练和反向传播，具有高效性和实时部署优势。

Conclusion: BCA+通过融合动态缓存的似然与自适应先验，实现了对视觉-语言模型语义理解和上下文置信度的双重校正，显著提升了分布偏移下的鲁棒性。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [19] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 本文提出了一种用于脑肿瘤分类的层次化广义类别发现方法（HGCD-BT），结合对比学习与半监督层次聚类损失，在识别已知和未知肿瘤类型方面显著优于现有方法，尤其在不同成像模态下表现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分类方法受限于预定义类别，无法识别训练中未见的肿瘤类型；无监督和半监督方法难以融合标签先验知识并处理未见类别，因此需要一种能同时发现已知与未知类别的通用方法。

Method: 提出HGCD-BT，将层次聚类与对比学习相结合，引入新的半监督层次聚类损失，以利用标记数据中的先验知识并发现未标记数据中的新类别，适用于具有层级结构的脑肿瘤分类任务。

Result: 在OpenSRH数据集上，HGCD-BT在块级分类准确率上比当前最优GCD方法提升28%，并在Digital Brain Tumor Atlas的全切片图像分类任务中验证了其跨模态泛化能力。

Conclusion: HGCD-BT有效解决了脑肿瘤分类中已知与未知类别共存的问题，通过结合层次结构建模与对比学习，在多种成像模态下实现了优越的性能，具有临床应用潜力。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [20] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的关键帧采样模块AdaRD-Key，用于查询驱动的长视频理解。该方法通过结合相关性与多样性的统一目标函数，有效选择信息丰富且非冗余的关键帧，在长视频基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在处理长视频时通常采用均匀采样，容易忽略关键片段；而现有关键帧选择方法或因固定时间间隔限制错过精细动作，或忽视查询相关性，导致理解不准确。

Method: 提出AdaRD-Key，最大化一个融合查询条件相关性得分和基于行列式的多样性度量的统一目标（Relevance--Diversity Max-Volume, RD-MV）。引入轻量级的相关性感知门控机制，在相关性弱时自动切换为仅多样性模式，提升覆盖范围。整个方法无需训练，计算高效，可即插即用集成到现有视觉语言模型中。

Result: 在LongVideoBench和Video-MME等长视频理解基准上取得当前最优性能，尤其在长视频任务中表现突出，且具备实时处理能力。

Conclusion: AdaRD-Key通过平衡关键帧的相关性与多样性，显著提升了长视频理解的准确性与鲁棒性，是一种高效、通用、无需训练的即插即用解决方案。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [21] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 该论文研究了视觉语言模型（VLMs）在解决复杂横向思维挑战（如字谜）中的认知过程，提出一个包含221个字谜的标注数据集和评估框架，揭示了不同提示策略对模型推理质量和解题效果的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态任务中表现出色，但在字谜等需要复杂推理的任务上表现不佳，且其失败原因和推理过程尚不清楚，因此需要深入分析其认知机制。

Method: 构建了一个涵盖六类认知类型的221个字谜的标注数据集，设计了三种提示策略，并通过分离推理质量与答案正确性来评估VLM的推理过程。

Result: 发现模型在视觉组合方面表现较好，但在解释缺失信息和文化符号方面存在根本性缺陷；提示策略显著影响模型的推理方式和解题效果。

Conclusion: 推理质量是模型性能的核心组成部分，应将可解释性作为模型设计和评估的关键环节，而非事后补充。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [22] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 提出了一种合成文本移除基准数据集的方法，适用于非场景文本领域，解决了现有数据集中存在的真值缺陷和评估不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本移除数据集存在真值伪影、背景过于简单和评估指标不准确等问题，限制了跨域泛化和有效评估。

Method: 通过对象感知的文本放置和视觉-语言模型生成内容，在复杂背景下渲染文本，构建高质量、具有挑战性的文本移除基准数据集。

Result: 构建了一个高质量的文本移除数据集，具备干净的真值和多样化的复杂场景，支持更真实和全面的模型评估。

Conclusion: 该数据集有助于提升文本移除模型在不同领域的泛化能力，并推动文本移除任务的发展。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [23] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了一种基于表示对齐的检测器无关框架，通过引入模态令牌和多模态上下文注意力（MoCA）以及查询预训练方法QueryREPA，实现跨医学影像模态的鲁棒目标检测。


<details>
  <summary>Details</summary>
Motivation: 单一检测器在混合多种医学影像模态（如X光、CT、MRI）时性能下降，因不同模态间统计特性差异大且表示空间不一致。需要一种能统一表示空间的方法以提升多模态检测性能。

Method: 1. 设计轻量级文本衍生的模态令牌来编码成像模态；2. 提出多模态上下文注意力（MoCA），将模态信息通过自注意力机制注入DETR式对象查询中；3. 引入QueryREPA，采用类别平衡批次和对比学习目标进行短周期预训练，使查询表示与模态令牌对齐。

Result: 在多种医学影像模态联合训练下，该方法显著提升了平均精度（AP），且仅带来极小计算开销，无需修改检测器架构。

Conclusion: MoCA与QueryREPA结合可生成模态感知且类别忠实的查询，有效支持下游训练，为鲁棒的多模态医学目标检测提供了实用解决方案。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [24] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MaskCD的图像头掩码对比解码方法，用于缓解大视觉语言模型（LVLMs）中的幻觉问题。该方法通过掩码LVLM中的“图像头”来构建对比样本，从而提升对比解码效果，在多个基准上验证了其有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在多模态任务中表现出色，但存在生成内容与输入视觉或文本矛盾的幻觉问题。现有方法如对比解码和注意力操控在构建对比样本或稳定性方面存在不足，因此需要更鲁棒的方法。

Method: 提出图像头掩码对比解码（MaskCD），利用LVLM中的‘图像头’并对其进行掩码操作以构建对比样本，结合对比解码机制抑制幻觉生成。

Result: 在LLaVA-1.5-7b和Qwen-VL-7b模型上，于CHAIR、POPE、AMBER和MME等多个基准测试中验证了MaskCD能有效减轻幻觉现象，同时保持模型原有的通用能力。

Conclusion: MaskCD是一种有效且稳定的缓解LVLM幻觉问题的方法，通过掩码图像头构建对比样本，优于传统对比解码和注意力操控方法，具有良好的应用前景。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [25] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: 本文介绍了VERNIER，一个开源的相位处理软件，用于基于伪周期性图案实现快速可靠的姿态测量，具有抗噪声、离焦和遮挡的能力。


<details>
  <summary>Details</summary>
Motivation: 在微小尺度下进行姿态估计仍然是一项挑战，尤其是在需要纳米级分辨率和较大范围的应用中缺乏有效的解决方案。

Method: 采用基于相位的局部阈值算法对伪周期性图案进行相位处理，并结合不同的图案设计以满足多种应用需求。

Result: VERNIER软件在合成和实验图像上验证了其在噪声、离焦和遮挡条件下的鲁棒性，能够实现厘米级测量范围和纳米级分辨率。

Conclusion: VERNIER提供了一种高效可靠的姿态测量方案，适用于多种显微镜应用，并提供了根据性能需求选择图案设计和显微镜物镜的指导原则。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [26] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 本文提出Med-K2N，通过自适应加权和因果模态身份约束实现高质量的多模态医学图像合成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决跨模态医学图像生成中模态贡献异质性、融合质量控制和多输出模态一致性三个关键问题。

Method: 设计PreWeightNet、ThresholdNet和EffiWeightNet三个协作模块进行全局贡献评估、自适应滤波和有效权重计算，并提出因果模态身份模块（CMIM）保持生成图像与目标模态的一致性。

Result: 在多个基准上显著优于当前最先进的方法，实现了高质量的K到N医学图像生成。

Conclusion: Med-K2N通过学习模态-任务自适应权重和记忆有益融合模式，有效解决了多模态医学图像合成中的核心挑战。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [27] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态特征融合的集成学习框架ELMF4EggQ，仅利用鸡蛋的外部属性（图像、形状和重量）实现对鸡蛋等级和新鲜度的非侵入式质量评估，并发布了首个相关公开数据集。


<details>
  <summary>Details</summary>
Motivation: 准确、无损地评估鸡蛋质量对于食品安全、产品质量控制和禽类生产效率至关重要。传统方法依赖内部指标测量，具有破坏性且耗时，因此需要一种基于外部特征的非侵入式自动化评估方法。

Method: 构建包含186个褐壳鸡蛋的公开数据集，通过实验室专家评估确定鸡蛋等级和新鲜度；采用ResNet152、DenseNet169和ResNet152V2等预训练CNN提取图像特征，结合形状和重量等结构特征，使用PCA降维、SMOTE数据增强，并通过多种机器学习算法分类；最后采用集成投票机制融合最优分类器预测结果。

Result: 多模态集成方法在等级分类上达到86.57%的准确率，在新鲜度预测上达到70.83%，显著优于仅用图像或仅用结构特征的基线模型。

Conclusion: ELMF4EggQ是首个仅利用外部非侵入特征进行鸡蛋内部质量评估的机器学习框架，并发布了配套数据集，所有代码和数据均公开，推动了该领域的可重复研究与进一步发展。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [28] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 本文提出了一种名为\frameworkName{}的零样本图像描述生成框架，通过从图像中心范式转向基于图像块（patch）的细粒度描述方法，实现无需区域级监督即可对任意区域生成描述。该方法利用密集视觉特征（如DINO）提取语义，并在多个区域描述任务中达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本描述模型主要依赖全局图像表示，局限于整图描述，难以生成针对局部或非连续区域的细粒度描述。因此，需要一种更灵活、可扩展的框架来实现任意区域的描述生成。

Method: 将图像块作为基本描述单元，使用预训练视觉模型（如DINO）提取密集特征，并通过聚合图像块特征来生成任意区域的文本描述，无需区域-文本配对监督。同时分析了实现该框架的关键组件。

Result: \frameworkName{}在零样本密集描述、区域集描述和新提出的轨迹描述任务上优于现有基线和最先进模型，验证了基于图像块的语义表示在可扩展描述生成中的有效性。

Conclusion: 基于图像块的细粒度表示是实现灵活、可扩展零样本图像描述的关键，该框架为未来无需配对数据的区域级描述生成提供了新方向。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [29] [Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement](https://arxiv.org/abs/2205.03569)
*Bing Li,Jiaxin Chen,Dongming Zhang,Xiuguo Bao,Di Huang*

Main category: cs.CV

TL;DR: 提出了一种用于压缩视频动作识别的新型框架MEACI-Net，通过运动增强和跨模态交互模块有效提升了RGB和运动模态的融合性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有压缩视频动作识别中动态信息粗糙、噪声多以及RGB与运动模态融合不足的问题。

Method: 采用双流架构，其中运动流引入多尺度块与去噪模块以增强表示学习，并设计了选择性运动补充（SMC）和跨模态增强（CMA）模块来加强跨模态交互。

Result: 在UCF-101、HMDB-51和Kinetics-400数据集上的实验表明，该方法在性能和效率方面均优于现有方法。

Conclusion: MEACI-Net通过运动增强和注意力驱动的跨模态融合机制，在压缩视频动作识别任务中实现了更优的准确性和鲁棒性。

Abstract: Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.

</details>


### [30] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 本文研究了在语义分割中，经过微调的视觉基础模型是否能内在地区分分布内（ID）和分布外（OoD）区域，并提出一种无需训练、基于InternImage特征与K-Means聚类及置信度阈值的方法，在多个基准上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 检测语义分割中的未知物体对自动驾驶等安全关键应用至关重要，而当前大视觉基础模型在闭集任务中表现良好，但在开放集OoD检测方面的能力尚不明确，因此需要探索其在无监督条件下检测OoD区域的潜力。

Method: 提出一种无需训练的方法，利用InternImage骨干网络提取特征，结合K-Means聚类对特征进行分组，并在解码器原始logits上应用置信度阈值来识别OoD区域。

Result: 该方法在RoadAnomaly基准上达到50.02的平均精度，在ADE-OoD基准上达到48.77，优于多个有监督和无监督基线方法。

Conclusion: 研究表明，微调后的视觉基础模型具备一定的OoD检测能力，所提方法为低假设、无需额外数据的通用OoD语义分割提供了可行方向。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, including DINOv2, InternImage, and CLIP, have advanced
visual representation learning by providing rich features that generalize well
across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out-of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmentation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direction for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [31] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出HoloV，一种简单有效的即插即用型视觉token剪枝框架，通过自适应分配不同空间区域的剪枝预算，从全局视角保留视觉上下文信息，显著提升多模态大模型在高剪枝比下的效率与准确性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的视觉token剪枝方法倾向于保留语义相似的token，导致在高剪枝比例下性能显著下降。

Method: 提出HoloV框架，摒弃传统的注意力优先策略，转而从整体视角出发，自适应地在不同空间区域分配剪枝预算，确保保留的token能捕捉全局视觉上下文而非孤立的显著特征。

Result: 实验表明，HoloV在多种任务、MLLM架构和剪枝比例下均优于现有最先进方法。例如，在剪除88.9%视觉token后，LLaVA1.5仍保持原始性能的95.8%。

Conclusion: HoloV通过全局感知的剪枝策略有效缓解了表示崩溃问题，在高剪枝比下显著提升了多模态大模型推理的效率与准确性权衡，具有良好的通用性和实用性。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [32] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 提出了一种名为Confidence-Aware Weighting (CAW) 的方法，通过置信度感知损失和特征对齐正则化来提升视觉-语言模型在零样本场景下的对抗鲁棒性，在多个数据集上优于现有方法且内存占用更低。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（如CLIP）虽然具有良好的零样本泛化能力，但容易受到对抗攻击，缺乏鲁棒性。

Method: CAW包含两个部分：一是置信度感知损失，通过缩放干净样本与对抗样本预测之间的KL散度来关注不确定的对抗样本；二是特征对齐正则化，最小化冻结与微调图像编码器在对抗输入上的特征距离以保持语义一致性。

Result: 在TinyImageNet和另外14个数据集上的实验表明，CAW在AutoAttack等强攻击下优于PMG-AFT和TGA-ZSR等最新方法，同时内存使用更少，并兼顾了干净准确率和鲁棒性。

Conclusion: CAW能有效提升视觉-语言模型的零样本对抗鲁棒性，且不损害其泛化性能，是一种高效实用的防御方法。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [33] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 本研究探讨了大型视觉-语言模型（LVLMs）在整合超声图像与临床、人口统计学及生物标志物数据进行颈动脉斑块评估中的潜力，发现未经调优的LVLM在风险分类中表现不佳；通过LoRA对LLaVa-NeXT-Vicuna进行超声领域适配，并融合多模态表格数据，显著提升了卒中风险分层性能，达到与CNN基线模型相当的水平。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化疾病的风险评估需整合多种临床和影像信息，当前缺乏透明且可解释的方法，亟需可辅助临床决策的智能模型。

Method: 提出一种模拟真实诊断场景的问答式框架，比较多种开源LVLM（包括通用与医学微调模型），采用零样本实验；通过低秩适应（LoRA）将LLaVa-NeXT-Vicuna适配至超声领域，并融合文本形式的多模态表格数据以提升性能。

Result: 未经适配的LVLM难以准确识别影像模态与解剖结构，且在风险分类任务中普遍表现差；经LoRA适配后模型在卒中风险分层上显著提升，结合多模态数据进一步提高特异性和平衡准确率，性能媲美CNN基线模型。

Conclusion: LVLM在超声心血管风险预测中具有潜力但存在局限，多模态数据融合、模型校准与领域适配对临床转化至关重要。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [34] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 提出一种轻量化的特征解耦变分自编码器（FDA-VAE）用于多期相增强MRI图像合成，通过Flip分布对齐和Y形双向训练策略，有效分离共享与独立特征，在减少参数和推理时间的同时提升合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用深度自编码器生成器，参数效率低且缺乏可解释的训练策略，难以有效分离多期相CE-MRI中的共享与独立特征。

Method: 提出FDA-VAE模型，将输入和目标图像编码为关于标准正态分布对称的两个潜在分布，结合Y形双向训练策略实现特征解耦。

Result: 相比现有端到端方法，FDA-VAE显著减少了模型参数和推理时间，同时提升了图像合成质量。

Conclusion: FDA-VAE是一种高效、可解释的轻量级模型，适用于多期相CE-MRI图像合成，具备良好的实际应用潜力。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [35] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了LPG-Bench，一个用于评估长文本到图像生成的基准，以及一种新的零样本评估指标TIT，基于文本-图像-文本一致性，显著提升了与人类偏好的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在处理长而详细的提示时表现不佳，且现有评估指标与人类偏好一致性差，因此需要更有效的评估基准和指标。

Method: 构建包含200个长提示的LPG-Bench基准，生成2600张图像并进行人工标注；提出TIT框架，包括TIT-Score和TIT-Score-LLM，通过大模型对比原始提示与生成图像描述的一致性来评估生成质量。

Result: 实验表明，TIT指标与人类判断的一致性显著优于CLIP-score、LMM-score等基线方法，TIT-Score-LLM在配对准确率上绝对提升7.31%。

Conclusion: LPG-Bench和TIT为长提示文本到图像生成提供了有效的评估工具，有助于推动该领域的发展。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [36] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出了3DEditVerse数据集和3DEditFormer模型，用于解决3D编辑中的跨视角一致性、结构保真和精细控制难题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D编辑方法存在速度慢、几何失真或依赖人工精确掩码等问题，难以满足实际应用需求。

Method: 在数据方面，构建了包含11.6万训练对的大规模配对基准3DEditVerse；在模型方面，提出3DEditFormer，一种保持3D结构的条件Transformer，通过双引导注意力和时间自适应门控实现无需3D掩码的精准编辑。

Result: 实验表明该方法在定量和定性上均优于现有最先进方法，实现了更精确、一致且可扩展的3D编辑。

Conclusion: 3DEditFormer结合高质量数据集3DEditVerse，为实用化、可扩展的3D编辑设立了新标准。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [37] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 本文提出了一种云注入算法和一种轻量级的归一化差异指数（NDI）注入方法，以提升多云地区土地覆盖语义分割的性能，并验证了Sentinel-1雷达数据在云遮挡条件下与光学数据融合的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的Sentinel-2土地覆盖数据集多为无云图像，限制了热带多云地区的应用，因此需要评估云遮挡的影响并提升模型在有云条件下的性能。

Method: 提出云注入算法模拟真实云覆盖，并在解码器的最后层注入归一化差异指数（NDI），同时融合Sentinel-1雷达数据以弥补光学数据的缺失。

Result: 在DFC2020数据集上，NDI注入使U-Net和DeepLabV3在无云影像上的性能分别提升了1.99%和2.78%；在有云条件下，结合Sentinel-1数据显著提升了所有模型的性能。

Conclusion: NDI注入是一种计算代价低且有效的方法，能够保留关键空间特征，而融合Sentinel-1雷达数据可显著提升云遮挡场景下的土地覆盖分割性能，证明了雷达-光学数据融合在复杂大气条件下的优势。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [38] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文研究了几何锚定语义特征在辐射场蒸馏中的作用，提出了新框架SPINE用于无需初始猜测的辐射场反演。结果表明，尽管几何锚定特征包含更多几何细节，但在姿态估计等任务中表现不如纯视觉特征，说明当前几何锚定方法需进一步改进以提升预训练语义特征的通用性和性能。


<details>
  <summary>Details</summary>
Motivation: 探讨在辐射场蒸馏中引入几何锚定语义特征是否优于仅使用视觉特征，特别是在空间任务（如姿态估计）中的潜力和局限性。

Method: 提出SPINE框架，结合语义蒸馏进行粗略反演和基于光度优化的精细反演；比较几何锚定特征与纯视觉特征在几何感知、对象定位和辐射场反演中的表现。

Result: 几何锚定特征能提供更精细的结构细节，但在语义对象定位和姿态估计精度上未表现出优势，甚至导致精度下降。

Conclusion: 当前几何锚定语义特征虽具几何细节优势，但整体适用性不及纯视觉特征，未来需发展更有效的几何锚定策略以增强其下游任务表现。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [39] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: 本文提出了PocketSR，一种超轻量级的单步图像超分辨率模型，通过设计LiteED和在线退火剪枝方法，在保持高质量的同时显著提升了效率，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有的真实场景图像超分辨率方法计算成本高、延迟大，难以在边缘设备上部署，因此需要一种高效且高性能的解决方案。

Method: 提出LiteED作为SD中VAE的高效替代方案，并结合在线退火剪枝和多层特征蒸馏损失优化U-Net，实现高效的生成先验迁移。

Result: PocketSR仅用1.46亿参数即可在0.8秒内处理4K图像，速度显著快于先前方法，同时性能媲美最先进的单步甚至多步RealSR模型。

Conclusion: PocketSR在保持高保真度的同时极大提升了效率，为边缘设备上的真实场景超分辨率提供了实用且高效的解决方案。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [40] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 提出Mask2IV，一种无需密集掩码标注的交互中心视频生成框架，通过解耦两阶段流程实现高质量、可控的交互视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模复杂动态的人-物或机器人-物交互，且依赖难以获取的密集精确掩码标注。

Method: 采用解耦的两阶段框架：第一阶段预测主体和物体的运动轨迹，第二阶段基于轨迹生成视频；支持通过动作描述或空间线索进行控制。

Result: 在两个新构建的基准上实验表明，该方法在视觉真实感和可控性方面优于现有基线方法。

Conclusion: Mask2IV有效解决了密集掩码标注的需求问题，实现了灵活、直观且高质量的交互-centric视频生成，适用于机器人学习等场景。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [41] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 本文提出了MEve，一个用于评估多事件文本到视频生成的自建提示集，并系统研究了OpenSora和CogVideoX两类代表性模型，揭示了去噪早期阶段和分层模块对事件转换控制的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展至多事件生成时忽略了事件转换中的内在因素，缺乏对多事件提示何时何地控制视频生成中事件过渡的深入探究。

Method: 构建MEve提示集，对OpenSora和CogVideoX模型进行系统性实验，分析不同去噪步骤和模型层在事件转换中的作用。

Result: 实验证明在去噪过程的早期阶段以及特定的模块化层中进行干预，对实现多事件的时序连贯性和可控性至关重要。

Conclusion: 多事件T2V生成的关键在于早期去噪步骤和分层结构中的控制，为未来模型的多事件条件生成提供了方向。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [42] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: 提出了一种基于EfficientNetV2-S的可复现面部表情识别框架InsideOut，通过数据增强和类别不平衡优化，在FER2013数据集上取得了具有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在实际应用中面临遮挡、光照和姿态变化、类内差异小以及数据集不平衡等问题，尤其是少数情绪类别识别困难，现有方法在可复现性和效率方面仍有不足。

Method: 采用EfficientNetV2-S作为基础模型，结合迁移学习，对FER2013数据进行标准化处理，使用分层划分和强数据增强，并通过类别加权损失函数微调轻量级分类头以应对数据不平衡问题。

Result: 在FER2013数据集上达到62.8%的准确率和0.590的宏平均F1分数，表现出与传统CNN基线模型相当的性能。

Conclusion: 研究表明，高效的网络架构结合针对性的不平衡处理策略，能够在保证可复现性和透明性的同时，提供实用的面部表情识别解决方案。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [43] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 本文系统研究了不同设计选择对图像和视频生成中组合泛化能力的影响，发现训练目标的离散或连续分布特性以及训练过程中条件信息对组成部分概念的提供程度是两个关键因素，并提出通过结合连续JEPA目标来改进离散模型（如MaskGIT）的组合性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化能力对于视觉生成模型至关重要，但目前尚不完全清楚哪些机制能够促进或抑制这种能力。

Method: 通过受控实验，系统分析了训练目标的分布类型（离散或连续）和条件信息在训练中对组成概念的揭示程度这两个因素对组合泛化的影响，并提出了在MaskGIT中引入辅助的连续JEPA目标进行损失函数松弛的方法。

Result: 实验表明，训练目标的分布类型和条件信息的构成相关性显著影响组合泛化能力；引入连续JEPA目标可以有效提升MaskGIT等离散模型的组合生成性能。

Conclusion: 离散与连续训练目标的结合以及更充分的组成概念条件控制，有助于增强视觉生成模型的组合泛化能力。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [44] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 本文提出了一种在扩散模型的潜在空间中生成“不可学习”图像的新方法，通过轨迹偏移采样实现高视觉保真度的同时抵御下游模型的逆向攻击和个人化使用，有效提升了不可感知性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于文本到图像扩散模型在少量用户图像下即可实现高效个性化，引发了数据隐私和知识产权方面的担忧，因此需要一种能够防止未经授权使用的防御机制。

Method: 提出一种基于潜在空间的模型扰动策略，在去噪与反转过程中交替修改去噪轨迹的起始点，从而生成视觉上接近原图但难以被下游模型学习的图像。

Result: 在四个基准数据集上验证了该方法对最先进的逆向攻击具有更强的鲁棒性，不可感知性提升约8%-10%（PSNR、SSIM、FID等指标），鲁棒性平均提升约10%。

Conclusion: 该方法成功将不可学习性集成到潜在扩散模型框架中，提供了一种实用且不易察觉的防御手段，可有效保护敏感数据免受未经授权的模型适配。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [45] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: 提出GeoComplete，一种结合3D结构引导的双分支扩散框架，用于参考图像驱动的图像补全，显著提升几何准确性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏几何线索，导致在目标视图与参考图像差异大时生成内容错位或不合理。

Method: 通过投影点云注入几何信息，并采用目标感知掩码策略，结合双分支扩散架构和跨分支自注意力机制实现图像补全。

Result: 实验显示相比最先进方法PSNR提升17.1，在几何准确性和视觉质量上均表现更优。

Conclusion: GeoComplete通过显式引入3D结构信息和目标感知掩码，实现了更鲁棒且几何一致的图像补全。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [46] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本生成有声视频（T2SV）方法，通过分层视觉接地字幕（HVGC）和双塔扩散Transformer（BridgeDiT）实现音视频与文本的语义和时序对齐，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有T2SV方法因共享文本条件导致模态干扰，且跨模态交互机制不明确，限制了音视频同步生成的质量。

Method: 提出HVGC框架生成解耦的视频和音频字幕，并设计BridgeDiT模型，采用双交叉注意力（DCA）机制实现双向、对称的跨模态信息融合。

Result: 在三个基准数据集上实现了最先进的性能，人类评估和消融实验验证了方法的有效性。

Conclusion: HVGC与BridgeDiT有效解决了模态干扰和跨模态交互问题，为T2SV任务提供了新的解决方案。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [47] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: 提出HAVIR模型，通过分层处理脑活动来更准确地重建复杂视觉刺激。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建高度复杂的视觉刺激方面存在困难，主要由于自然场景的低层次特征异质性和高层次语义纠缠。

Method: 将视觉皮层分为两个层级区域，分别提取结构和语义特征：结构生成器从空间处理体素提取结构信息并转换为潜在扩散先验，语义提取器将语义处理体素转换为CLIP嵌入，并通过多功能扩散模型合成图像。

Result: 实验结果表明，HAVIR在复杂场景下显著提升了重建图像的结构和语义质量，优于现有模型。

Conclusion: HAVIR通过模拟视觉皮层的分层表征机制，有效解决了视觉重建中的特征分离与整合问题，推动了神经科学与计算机视觉的融合。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [48] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 本文提出了Markovian Reeb图，一种用于模拟时空轨迹的新框架，能够保持从基线数据中学到的生活模式（PoLs），在城市规划、流行病学和交通管理中具有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动性对于城市规划、流行病学和交通管理至关重要，现有方法难以同时捕捉个体与群体层面的移动规律并兼顾真实性和效率。

Method: 结合个体和群体层面的移动结构，提出基于概率拓扑模型的Markovian Reeb图框架，用于生成保留生活模式的未来轨迹。

Result: 在Atlanta和Berlin子集的Urban Anomalies数据集上评估，使用Jensen-Shannon散度指标，表明该方法在人口级和个体级指标上均表现出高保真度，且数据和计算效率高。

Conclusion: Markovian Reeb图是一种可扩展的轨迹模拟框架，能够在多样化城市环境中高效生成真实的人类移动轨迹。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [49] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: 本文提出了SpineMed，一个专为脊柱疾病AI辅助诊断设计的生态系统，包括大规模多模态数据集SpineMed-450k和临床导向评估框架SpineBench，显著提升了模型在椎体层级推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病影响广泛，但现有AI诊断受限于缺乏具备椎体层级标注的多模态数据集和临床可追溯的指令数据，亟需构建符合临床决策需求的数据与评估体系。

Method: 与执业脊柱外科医生共同设计SpineMed生态系统，构建包含45万条指令实例的SpineMed-450k数据集，采用临床医生参与的两阶段LLM生成方法（草稿与修订）进行数据标注，并开发SpineBench作为临床导向的评估基准。

Result: 在SpineBench上评估多个先进视觉-语言模型，发现其在细粒度椎体层级推理上存在系统性缺陷；而在SpineMed-450k上微调的模型在所有任务中均表现出显著提升，且经医生评估确认其输出具有诊断清晰性和临床实用性。

Conclusion: SpineMed填补了脊柱疾病AI诊断中高水平、多模态、可追溯数据的空白，为推动临床可信AI的发展提供了重要基础设施。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [50] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于多智能体的统一图像伪造检测与定位系统UniShield，能够跨多个领域（如图像篡改、文档篡改、DeepFake和AI生成图像）实现高效、可解释的检测，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有伪造图像检测方法局限于特定领域，跨域泛化能力差，且缺乏自适应的统一框架，难以应对多样化伪造技术的实际应用需求。

Method: 设计了一个包含感知智能体和检测智能体的多智能体系统：感知智能体动态分析图像特征并选择合适的检测模型，检测智能体集成多种专家检测器并生成可解释报告。

Result: 在多个伪造图像检测任务上达到最先进水平，优于现有的统一方法和领域专用检测器，展现出更强的实用性、适应性和可扩展性。

Conclusion: UniShield通过多智能体协同机制实现了跨域伪造图像的高效检测与定位，为构建通用、自适应的图像真实性验证系统提供了有效解决方案。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [51] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出一种结合动态体素提示生成和内容感知自适应裁剪的训练策略，以优化3D生物医学图像分割中用户交互下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型缺乏体素感知能力或交互功能有限，难以有效支持交互式3D生物医学图像分割。

Method: 采用动态体素提示生成与内容感知自适应裁剪相结合的训练策略，并基于nnInteractive模型的公开权重进行初始化，实现在单GPU上高效训练。

Result: 在‘基础模型用于交互式3D生物医学图像分割’竞赛中表现良好，平均最终Dice分数为0.6385，归一化表面距离为0.6614，Dice和NSD的曲线下面积分别为2.4799和2.5671。

Conclusion: 所提方法在保持计算效率的同时显著提升交互式3D分割性能，适用于资源受限环境下的实际应用。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [52] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: 本文提出了PQGAN，一种将乘积量化（PQ）引入VQGAN框架的量化图像自编码器，在重建性能上显著优于现有方法，并展示了其在预训练扩散模型中的高效集成能力。


<details>
  <summary>Details</summary>
Motivation: 尽管乘积量化（PQ）在可扩展向量编码中表现优异，但在高保真图像生成的潜在表示中应用有限，因此本文旨在探索PQ在该领域的潜力并改进现有量化方法的性能瓶颈。

Method: 将PQ整合到VQGAN的向量量化（VQ）框架中，通过分析码本大小、嵌入维度和子空间分解之间的关系，优化超参数选择，并实现与预训练扩散模型的无缝集成。

Result: PQGAN在PSNR上达到37dB（此前为27dB），FID、LPIPS和CMMD指标最多降低96%，且能加速生成过程或在不增加成本的情况下使输出分辨率翻倍。

Conclusion: PQGAN显著提升了图像重建质量与生成效率，证明了PQ作为图像合成中离散潜在表示的强有力扩展方案。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [53] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: 提出Memory Forcing框架，通过几何索引空间记忆和混合训练策略，在保持生成质量的同时实现长期空间一致性。


<details>
  <summary>Details</summary>
Motivation: 在有限计算资源下，现有自回归视频扩散模型在探索新场景和重访已探索区域时难以兼顾自然内容生成与空间一致性。

Method: 引入几何索引的空间记忆机制，结合混合训练（Hybrid Training）、链式前向训练（Chained Forward Training）、点到帧检索（Point-to-Frame Retrieval）和增量3D重建（Incremental 3D Reconstruction）方法。

Result: 实验表明，该方法在多种环境中实现了更优的长期空间一致性和生成质量，同时保持了对长序列的计算效率。

Conclusion: Memory Forcing有效平衡了空间记忆与时间记忆的使用，提升了视频扩散模型在世界建模和交互式场景生成中的长期一致性与生成性能。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [54] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: 本文提出了MonSTeR，首个能够联合建模动作、场景和文本的检索模型，通过统一的潜在空间实现跨模态对齐，并在多种任务中验证了其有效性与人类偏好的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏评估骨骼运动、意图和环境场景三者之间对齐关系的工具，且多模态模型未能充分捕捉高阶交互关系。

Method: MonSTeR利用单模态和跨模态表征构建统一的潜在空间，借鉴高阶关系建模方法，捕捉运动、场景和文本之间的复杂依赖关系，支持灵活而鲁棒的跨任务检索。

Result: MonSTeR在多个任务上优于仅依赖单模态表征的三模态模型；用户研究表明其检索分数与人类偏好高度一致；并在零样本场景物体放置和动作描述生成任务中展示了潜在空间的通用性。

Conclusion: MonSTeR有效实现了运动-场景-文本三者的语义对齐，提供了一种强大的跨模态检索框架，具有良好的应用潜力和可扩展性。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [55] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 提出一种无需训练、适用于多种网络架构和攻击类型的测试时防御机制，通过引入微小平移扰动并利用随机共振增强模型对对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法常依赖特征过滤或平滑，易导致信息损失；需要一种通用、无需训练且不依赖特定模型或攻击类型的防御机制。

Method: 在输入图像上引入微小的随机平移扰动，对齐变换后的特征嵌入并进行聚合，再映射回原始图像，通过闭式公式实现，无需额外模块或微调。

Result: 在图像分类、立体匹配和光流等任务中展现出先进的鲁棒性，分别恢复最多68.1%、71.9%和29.2%的准确率损失，首次实现了针对密集预测任务的通用测试时防御。

Conclusion: 该方法是训练免费、架构无关且攻击无关的，具有良好的通用性和实用性，为对抗攻击提供了一种高效、轻量的测试时防御方案。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [56] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: 本文提出了一种名为Mixer的新型随机神经网络，用于纹理表征学习，结合超球面随机嵌入和双分支学习模块，有效捕捉通道内和通道间关系，并通过新设计的优化问题提升表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有随机神经网络在纹理识别中虽表现良好，但主要关注跨信息预测，缺乏对整体架构的创新，因此需要一种更有效的网络结构来提升纹理表征能力。

Method: 提出Mixer模型，采用超球面随机嵌入和双分支学习模块，分别建模通道内和通道间关系，并设计新的优化目标以增强纹理表示的学习。

Result: 在多个具有不同特性和挑战的纯纹理基准上取得了优异结果，验证了方法的有效性和鲁棒性。

Conclusion: Mixer通过改进随机网络架构，显著提升了纹理表示学习性能，为该领域提供了新的思路和工具。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [57] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 本文提出了一种改进GUI定位任务的方法，通过引入RULER标记和交错多维相对位置编码（I-MRoPE），解决了现有视觉语言模型在高分辨率屏幕上坐标映射不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在将自然语言指令映射到像素坐标时，由于训练中未见的高分辨率屏幕导致patch到像素的映射不可靠，限制了GUI自动化性能。

Method: 提出两种创新方法：一是使用RULER标记作为显式坐标参考，使模型能像地图上的网格线一样调整位置；二是采用I-MRoPE增强空间编码，确保宽高维度的对称表示。

Result: 在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上的实验表明，该方法显著提升了定位精度，尤其在高分辨率界面上效果最明显。

Conclusion: 通过提供显式的空间引导而非依赖隐式学习，该方法实现了跨不同分辨率和平台更可靠的GUI自动化。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [58] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 提出了一种标签高效的多模态大模型适应框架LEAML，利用少量标注数据和大量未标注图像生成伪问答对，通过选择性更新关键神经元提升特定领域（如医学影像）的视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在通用视觉任务上表现良好，但在标注数据稀缺的专业领域（如医疗影像）的分布外任务上表现不佳，需要更高效的适应方法。

Method: 提出LEAML框架，使用经过字幕蒸馏正则化的QA生成器为未标注数据生成伪问答对，并仅选择性地更新与问答最相关的神经元，以实现高效的领域知识迁移。

Result: 在胃肠内镜和体育视觉问答任务上的实验表明，LEAML在极低监督条件下 consistently 优于标准微调方法。

Conclusion: LEAML通过结合伪标签生成和选择性神经元更新，有效提升了多模态大模型在标注数据稀缺场景下的领域适应能力。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [59] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 提出了一种无需训练的代理框架AMANDA，通过LLM代理实现医学知识增强，解决现有Med-MLLMs在低资源环境下因内在和外在推理瓶颈导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的医学多模态大语言模型在低资源场景下因无法充分提取医学图像细节和融合专业医学知识而存在推理能力瓶颈。

Method: AMANDA框架采用粗到细的问题分解进行内在医学知识增强，并通过生物医学知识图谱检索实现外在知识增强，结合LLM代理进行推理，无需训练。

Result: 在八个医学视觉问答基准上进行了广泛实验，AMANDA在零样本和少样本设置下均表现出显著性能提升。

Conclusion: AMANDA有效克服了Med-MLLMs的推理瓶颈，提升了其在低资源环境下的医学视觉问答能力，具备良好的应用潜力。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [60] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY是一个基于AI的临床辅助平台，结合有限状态机与大语言模型，用于患者分诊、转诊和病情评估，在真实大规模医疗IT系统中展现出优于人类的路由准确率和更短的咨询时间。


<details>
  <summary>Details</summary>
Motivation: 提升医疗资源分配效率，解决患者到专科医生转诊过程中的低效与不准确问题。

Method: 采用混合架构，结合有限状态机（FSM）进行结构化对话控制，以及基于大语言模型（LLM）的协作代理进行症状分析和转诊优先级判断，构建于模块化微服务框架之上。

Result: 在部署两个月内完成超过55,000次用户对话，其中2,500条经专家标注验证；结果显示CLARITY在首次转诊准确率上超过人类水平，且咨询时长缩短至人类的三分之一。

Conclusion: CLARITY在实际医疗环境中表现出高效、安全和可扩展性，显著优于人工转诊，具备广泛集成于现有医疗IT系统的潜力。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [61] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 本文提出了CASAL算法，通过将激活引导的优势直接嵌入模型权重中，有效减少大语言模型的幻觉问题，具备高效、轻量且在分布外数据和多模态场景下具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能力强，但容易产生幻觉，即给出错误答案而不承认无知。现有方法需在推理时实时干预，不够高效，因此需要一种更实用的解决方案。

Method: 提出对比激活引导的摊销学习（CASAL），仅训练单个Transformer层的子模块，利用可解释性指导模型学习何时回答、何时 abstain，将激活引导的效果固化到模型参数中。

Result: 在多个短问答基准上减少30%-40%的幻觉，计算效率提升30倍，数据效率提升20倍，并在OOD、文本-视觉模型及MoE架构中展现良好泛化性。

Conclusion: CASAL是首个在密集和MoE模型上均有效的基于引导的训练方法，推动了可解释性方法在实际生产系统中的应用。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [62] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: RA-FSM是一种基于GPT的模块化研究助手，通过有限状态机控制生成过程（相关性→置信度→知识），结合向量检索与确定性引用管道，提升文献合成的准确性与可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文献综合中易产生幻觉和误引，限制了其在专家工作流中的应用。因此需要一种更可靠、可解释的研究助手系统。

Method: 提出RA-FSM框架，采用有限状态机控制生成流程，集成向量检索和确定性引用机制；通过分级索引流程构建领域知识库，并在光子学领域实现系统并评估六类任务。

Result: 在盲测A/B评估中，领域专家更偏好RA-FSM，认为其边界条件处理更强、证据使用更可靠；相比Notebook LM，RA-FSM具有更高的覆盖性和新颖性，同时延迟和成本可控。

Conclusion: RA-FSM通过结构化控制和多源知识集成，提供了透明、可引用的回答，适用于高风险技术工作，且可推广至其他科学领域。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [63] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 提出一种混合架构，结合实时语音到语音模型和后端大语言模型，在保持低延迟的同时提升响应的准确性和语义理解。


<details>
  <summary>Details</summary>
Motivation: 实时语音到语音模型缺乏深层知识和语义理解，而级联文本大模型系统虽知识丰富但延迟高，影响自然交互流畅性。

Method: 通过S2S Transformer处理用户语音实现即时响应，同时将查询发送至后端大语言模型，实时注入其文本回复以引导语音生成。

Result: 在MT-Bench多轮问答语音版本上评估，系统显著优于基线S2S模型，响应正确性接近级联系统，且延迟与基线相当。

Conclusion: 该混合架构有效平衡了低延迟与知识丰富性，提升了实时语音对话系统的性能。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [64] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: SelfJudge提出了一种通过自监督训练judgе verifier的方法，以实现更广泛适用的LLM推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有judgе decoding方法依赖人工标注或具有可验证真实标签的任务，限制了在多样化NLP任务中的泛化能力。

Method: 通过目标模型的自我监督训练verifier，利用替换token后的响应是否保持原意来衡量语义一致性，从而自动训练judgе verifier。

Result: 实验表明，SelfJudge在多种NLP任务上实现了优于基线方法的推理-准确性权衡。

Conclusion: SelfJudge提供了一种可广泛应用于不同NLP任务的高效LLM推理加速方案。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [65] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 提出Cobweb框架，利用层次化原型树实现粗到细的文档检索，提升检索的可解释性、鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有神经文档检索方法多忽略语料库结构，仅在单一粒度上打分，导致结构利用不足且结果解释性差。

Method: 设计Cobweb框架，将句子嵌入组织成原型树，通过粗到细遍历进行检索；提出两种推理方法：广义最佳优先搜索和轻量级路径求和排序器。

Result: 在MS MARCO和QQP数据集上验证，使用BERT/T5等强编码器时性能媲美点积搜索，而在GPT-2等较弱嵌入下仍保持稳定，点积搜索则显著下降。

Conclusion: Cobweb框架在保持竞争力的同时，提升了对嵌入质量的鲁棒性、可扩展性和检索结果的可解释性。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [66] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 提出EntropyLong方法，利用预测不确定性验证长距离依赖质量，通过模型在环的验证机制构建高质量长上下文训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文语言模型的数据构造方法（如文本拼接或启发式方法）难以保证真正的长距离依赖，导致模型性能受限。

Method: 识别文档中的高熵位置，从大型语料库中检索语义相关上下文，并通过其是否降低预测熵来验证有效性，构建包含验证过上下文补充的训练样本。

Result: 在FineWebEdu和Cosmopedia上构建了128K长度序列的数据集，训练的模型在RULER和LongBenchv2基准上显著优于现有方法，尤其在需要远距离信息的任务上表现突出。

Conclusion: 基于熵的验证机制对长上下文训练至关重要，EntropyLong能有效提升模型对长距离依赖的学习能力与上下文理解性能。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [67] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 提出一种结合行为模拟器和语言模型提示的方法，生成具有一致性、事实性和自然性的对话数据，用于训练对话推荐系统。


<details>
  <summary>Details</summary>
Motivation: 由于公开的对话推荐系统（CRS）数据稀缺，难以微调语言模型；现有基于语言模型的用户模拟器常缺乏行为一致性。

Method: 结合行为模拟器与语言模型提示技术，生成符合用户潜在状态的自然对话，并构建包含偏好获取和示例批评的大规模开源CRS数据集。

Result: 人工评估结果显示生成的对话在一致性、事实性和自然性方面表现良好。

Conclusion: 该方法能有效生成高质量、行为一致的CRS对话数据，有助于缓解数据稀缺问题。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [68] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 本文提出了一种名为look-ahead Sync的新型神经语言隐写方法，解决了现有方法SyncPool因同步机制导致嵌入容量受限的问题，在保证可证明安全性的同时显著提升了信息嵌入率。


<details>
  <summary>Details</summary>
Motivation: 现有的神经语言隐写方法SyncPool由于采用粗粒度同步机制处理分词歧义，牺牲了大量嵌入容量，限制了实际应用中的信息承载能力。

Method: 该方法仅对真正不可区分的令牌序列进行最小化同步采样，而保留其他可区分路径以最大化嵌入容量，并通过理论证明确保安全性。

Result: 在英语（Llama 3）和中文（Qwen 2.5）基准上的实验表明，该方法在嵌入容量上显著优于SyncPool，英文提升超过160%，中文提升25%，尤其在候选池较大时表现更优。

Conclusion: look-ahead Sync在保持可证明安全性的前提下，大幅提升了语言隐写的嵌入容量，推动了高容量安全隐写技术的实用化进程。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [69] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: StepChain GraphRAG 是一种结合问题分解与广度优先搜索推理流程的检索增强生成框架，通过动态构建知识图谱和显式证据链，在多跳问答任务中实现了最先进的性能，并提升了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多跳问答系统在整合迭代推理步骤与外部知识检索方面仍存在挑战，影响了准确性和可解释性。

Method: 提出 StepChain GraphRAG 框架：首先建立语料库的全局索引；在推理时按需将检索到的段落动态解析为知识图谱，并将复杂问题分解为子问题；对每个子问题采用基于 BFS 的遍历方法沿相关边扩展，构建明确的证据链。

Result: 在 MuSiQue、2WikiMultiHopQA 和 HotpotQA 上均达到 SOTA 水平，平均提升 EM 2.57%、F1 2.13%，在 HotpotQA 上提升最大（+4.70% EM, +3.44% F1），并增强了推理过程的可解释性。

Conclusion: StepChain GraphRAG 有效提升了多跳问答的准确性与可解释性，未来工作需关注降低计算开销和缓解大模型幻觉以提高系统效率与可靠性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [70] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文介绍了两个公开可用的语义增强人类轨迹数据集及其构建流程，结合真实GPS轨迹、上下文信息、大语言模型生成的社交媒体文本，并支持语义推理与FAIR数据原则。


<details>
  <summary>Details</summary>
Motivation: 为了支持多模态和语义化的移动性分析，填补现有数据集在语义丰富性、文本模态和语义网兼容性方面的空白。

Method: 利用OpenStreetMap的GPS轨迹，通过开源可复现的流程添加停留点、移动段、兴趣点、交通方式推断、天气数据，并使用大语言模型生成逼真的社交媒体帖子，最终以表格和RDF格式发布。

Result: 发布了覆盖巴黎和纽约的两个大规模、结构不同的语义增强轨迹数据集，支持行为建模、移动预测、知识图谱构建和基于LLM的应用研究。

Conclusion: 这是首个将真实移动数据、结构化语义增强、大语言模型生成文本和语义网技术结合的可重用框架，推动了智能交通和城市计算领域的数据资源发展。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [71] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本研究开发并评估了一种基于大语言模型的检索增强生成（RAG）系统，用于查询英国NICE临床指南，显著提升了信息检索的准确性和生成答案的可信度。


<details>
  <summary>Details</summary>
Motivation: 由于NICE临床指南数量庞大、内容冗长，在时间受限的医疗环境中难以高效利用，因此需要一种能够快速精准响应自然语言查询的系统。

Method: 构建了一个基于混合嵌入机制的RAG系统，对来自300份指南的10,195个文本块进行索引，并在7,901个查询上评估其检索性能；在70个手动整理的问答对上评估生成阶段的表现。

Result: 检索阶段MRR达到0.814，首块召回率为81%，前十块召回率达99.1%；生成阶段RAG增强模型的答案忠实度提升64.7个百分点至99.5%，显著优于Meditron3-8B模型的43%，且上下文精确度均为1。

Conclusion: RAG是一种有效、可靠且可扩展的方法，能将生成式AI安全应用于医疗领域，实现对临床指南的成本效益型访问。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [72] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于表示及其梯度的新框架，用于诊断大语言模型中的有害行为，能够在激活空间中精确定位影响模型输出的训练样本和短语。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生有害内容、事实错误和社会偏见，现有归因方法因噪声大和计算复杂难以有效诊断其根本原因。

Method: 通过分析模型激活空间中的表示及其梯度，建立输出与训练数据之间的语义关联，实现对模型行为的细粒度归因。

Result: 在追踪有害内容、检测后门投毒和知识污染等任务中表现出色，支持样本级和token级的精细分析。

Conclusion: 该方法为理解和审计大语言模型风险提供了高效工具，有助于缓解模型的不良行为。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [73] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 本文提出了一个名为FormalML的Lean 4基准，用于评估大语言模型在数学证明中填补缺失步骤（即子目标补全）的能力，特别是在机器学习基础理论中的优化和概率不等式等领域。通过将过程性证明转换为声明性形式，提取了4937个不同难度的问题，强调了当前模型在准确性和效率上的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在形式化定理证明方面取得了进展，但其作为数学家实用助手、补全复杂证明中缺失步骤的能力仍缺乏探索。为此，作者提出子目标补全任务，以更贴近实际数学研究需求。

Method: 构建了一个名为FormalML的Lean 4基准，涵盖机器学习基础理论中的优化与概率不等式；使用翻译策略将过程性证明转为声明性形式，提取4937个子目标补全问题，并结合前提检索与复杂研究级上下文进行评估。

Result: 对当前最先进的定理证明器的评估表明，它们在准确性和效率方面仍存在显著局限，难以有效完成子目标补全任务。

Conclusion: FormalML是首个结合前提检索与研究级上下文的子目标补全基准，揭示了现有LLM在实际数学辅助任务中的不足，凸显了开发更强大、更高效的语言模型驱动定理证明器的必要性。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [74] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 本文介绍了首个库尔德语语义文本相似度（STS）数据集，包含10,000个句子对，并评估了多种模型在该数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言如库尔德语缺乏语义相似度研究资源，作者旨在填补这一空白。

Method: 构建了一个包含正式和非正式语域的库尔德语STS数据集，并使用Sentence-BERT、多语言BERT等模型进行基准测试。

Result: 模型在数据集上取得了具有竞争力的结果，但也揭示了库尔德语形态、拼写变异和语码混合带来的挑战。

Conclusion: 该数据集和基线模型为库尔德语语义研究和低资源NLP提供了可复现的评估工具和良好起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [75] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: 本文提出了CRACQ，一个针对机器生成文本的多维自动评估框架，涵盖连贯性、严谨性、适当性、完整性和质量五个维度，相较于单一评分和大模型直接评判，具有更强的稳定性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统自动评估方法多依赖单一总分，缺乏对文本多维度特征的细粒度分析，且大模型作为评判者存在不稳定和不可解释问题，因此需要一种更系统、可解释的评估框架。

Method: 借鉴基于特征的自动作文评分（AES）思路，设计包含五个评估维度的评分标准（rubric），结合语言学、语义和结构信号，对合成和真实申请材料进行训练与测试，并与大模型评判结果对比。

Result: 在500份合成资助提案上训练后，CRACQ在真实强弱案例中表现出比LLM-as-a-judge更稳定、更具可解释性的特质级判断，但仍在可靠性和领域覆盖范围方面存在挑战。

Conclusion: CRACQ提供了一种可解释、多维度的文本自动评估方法，适用于多种机器生成文本，在评估稳定性与细粒度分析方面优于直接使用大模型评判，未来需改进其可靠性与泛化能力。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [76] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 提出一种结合GRPO和DocLens的评估集成强化学习框架，用于长篇临床文本生成，直接优化事实准确性和完整性，无需单独训练奖励模型或依赖人工参考。


<details>
  <summary>Details</summary>
Motivation: 自动化临床文档需要与完整性、事实准确性等优先事项精确对齐，现有方法依赖人工参考或额外奖励模型，成本高且扩展性差。

Method: 采用Group Relative Policy Optimization (GRPO) 与 DocLens（基于对话的声明级评估器）结合的强化学习框架，通过确定性的、基于对话的奖励信号直接优化生成结果，并引入简单的奖励门控策略降低训练成本。

Result: 实验证明该方法提升了临床记录的质量，减少了遗漏和幻觉，在事实性、完整性和简洁性上获得GPT-5定性评估的更高偏好，同时降低了训练成本。

Conclusion: 该框架可扩展至实际应用场景，无需人工标注即可优化临床文本生成质量，并支持自定义目标（如指南依从性、计费偏好），改进效果在基础模型已良好对齐的情况下仍显著，表明其潜力被低估。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [77] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 本文研究了在基于计算论证的可解释性大语言模型（ArgLLMs）中集成不确定性量化（UQ）方法的效果，特别是在声明验证任务中的表现。实验结果表明，尽管简单，直接提示法在ArgLLMs中是一种有效的UQ策略，优于更复杂的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型技术的发展，确保其可靠性变得越来越重要。因此，研究如何在决策支持系统等关键应用中有效评估和利用大语言模型的不确定性成为了一个重要的课题。

Method: 通过将不同的大语言模型不确定性量化方法整合到ArgLLMs框架中，并在声明验证任务上进行实验比较，以评估这些UQ方法的有效性。

Result: 实验结果显示，直接提示作为一种UQ策略，在ArgLLMs中表现出色，甚至超过了更为复杂的UQ方法。

Conclusion: 直接提示是ArgLLMs中一种简单而高效的不确定性量化方法，对于处理复杂或争议性陈述尤其有用。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [78] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 研究探讨了通过提示词模拟大语言模型（LLM）早期知识截止的能力，发现虽然在直接查询时有效，但在涉及因果相关知识时难以实现“遗忘”，提示需更严格的评估设置。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在时间预测任务中可能依赖预训练数据中的记忆而非推理，存在数据污染风险，因此需要探究其是否能通过提示模拟知识截止以减少记忆影响。

Method: 构建三个评估数据集，分别测试LLM在提示下对直接事实知识、语义变化和因果相关知识的遗忘能力。

Result: 提示法在直接查询被遗忘信息时有一定效果，但在间接涉及或因果关联的查询中表现不佳，难以真正实现知识切除。

Conclusion: 当前提示-based unlearning 方法在模拟知识截止方面有限，尤其在处理隐含或因果知识时不足，需更严谨的评估框架用于时间预测任务。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [79] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: 本文提出了DRIFT方法，利用现实世界中丰富的用户不满信号进行偏好学习，在缺乏显式满意反馈的情况下实现高效的大模型后训练，显著提升了模型性能并保持了解答多样性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中显式用户满意度反馈稀少，而隐式不满信号丰富，但现有偏好学习方法依赖大量人工标注或假设充足正向反馈，难以有效利用此类数据。

Method: 提出DRIFT（Dissatisfaction-Refined Iterative Preference Training），以真实不满信号为锚点，从不断演化的策略中动态采样正面样本进行迭代训练。

Result: 在WildBench和AlpacaEval2等基准上，DRIFT在7B和14B模型上均显著优于基线方法，14B模型甚至超过GPT-4o-mini，同时保持生成解的多样性，避免模式坍缩。

Conclusion: DRIFT是一种高效且可扩展的现实世界大模型后训练方案，能充分利用最丰富且信息量大的不满信号，具备实际部署价值。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [80] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 本文提出了SIMPACT框架和BluePrint数据集，用于训练和评估基于大语言模型的社会媒体代理，通过行为建模和标准化数据支持可扩展、合乎伦理的社交媒体模拟研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏用于微调和评估大语言模型作为社会媒体代理的标准数据资源，且难以在保护隐私的前提下捕捉真实用户行为模式。

Method: 提出SIMPACT框架，以next-action prediction为任务，构建行为导向的社交媒体数据集；使用聚类方法将匿名用户聚合为行为一致的persona，并定义群体与集群层面的评估指标。

Result: 发布了基于Bluesky公开数据的大规模政治话语数据集BluePrint，包含12种社交互动类型及上下文关联信息，支持情境依赖的行为建模，并实现了行为保真度和风格真实性的量化评估。

Conclusion: SIMPACT为训练逼真的社会媒体代理提供了标准化的数据与评估基础，BluePrint可作为政治话语建模的基准，并为研究错误信息传播、极化等社会问题提供模板。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [81] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 提出一种基于动态专家聚类和结构化压缩的统一框架，有效解决了MoE大模型中的负载不平衡、参数冗余和通信开销三难问题，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: MoE大语言模型面临负载不平衡、参数冗余和通信开销的三重挑战，现有方法难以协同解决这些问题。

Method: 采用在线聚类方法，结合参数与激活相似性度量对专家进行动态重组；在每个聚类内，将专家权重分解为共享基矩阵和低秩残差适配器，并设计两阶段分层路由策略，结合异构精度存储和动态卸载机制。

Result: 在GLUE和WikiText-103上，模型性能与标准MoE相当，总参数减少约80%，吞吐量提高10%-20%，专家负载方差降低三倍以上，峰值内存接近密集模型水平。

Conclusion: 通过结构化重组，可在保持模型质量的同时大幅提升MoE模型的可扩展性、效率和内存效益，为高效MoE架构提供了新方向。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [82] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 本研究探讨了基于检索增强生成（RAG）的小型语言模型（SLMs）作为教育领域AI助教的潜力，发现经过优化提示和检索，SLMs在准确性与教学对齐方面可媲美大型语言模型（如GPT-4o），且具备更低能耗、可在本地设备实时运行，更具成本效益、隐私保护和环境友好优势。


<details>
  <summary>Details</summary>
Motivation: 探索在教育中可持续、高效且可扩展地应用生成式AI的方法，解决大型语言模型在计算资源、能耗和隐私方面的局限性。

Method: 采用检索增强生成（RAG）管道，对八个开源小型语言模型（如LLaMA 3.1、IBM Granite 3.3、Gemma 3）进行基准测试，并与GPT-4o对比，评估其在课程指导任务中的表现。

Result: 在适当提示和目标检索支持下，小型语言模型能够达到与大型语言模型相当的响应准确性和教学对齐水平，同时显著降低计算与能源消耗，支持在消费级硬件上实时运行。

Conclusion: 小型语言模型结合RAG技术是可持续、节能且隐私友好的AI教学助手解决方案，适合教育机构在不依赖云基础设施的情况下规模化部署个性化学习支持。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [83] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 提出mini-vec2vec，一种高效、稳定的线性方法，用于在无平行数据情况下对齐文本嵌入空间，显著提升效率并保持或超越原有vec2vec的性能。


<details>
  <summary>Details</summary>
Motivation: 原vec2vec方法虽能近似完美对齐文本嵌入空间，但计算成本高且不稳定，限制了其应用。

Method: 采用三阶段流程：伪平行嵌入向量的初步匹配、变换拟合和迭代优化，学习线性映射。

Result: mini-vec2vec在效率上比原方法高出几个数量级，结果相当甚至更优，且具有高鲁棒性和可解释性。

Conclusion: mini-vec2vec是一种高效、稳定、可解释的线性替代方案，便于扩展并有望在新领域中广泛应用。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [84] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMSQL是对WikiSQL的系统性修订，旨在为大语言模型时代提供一个干净、易用的Text-to-SQL基准数据集，解决了原始数据集中的多种错误，并支持直接生成和评估SQL查询。


<details>
  <summary>Details</summary>
Motivation: WikiSQL数据集存在诸如大小写不一致、数据类型不匹配、语法错误和未回答问题等结构性和标注问题，限制了其在现代大语言模型下的应用，因此需要一个更高质量、更适合当前模型需求的替代基准。

Method: 对WikiSQL中的错误进行分类，并开发自动化方法进行数据清洗和重新标注，最终构建出LLMSQL；新数据集提供清晰的自然语言问题和完整的纯文本SQL查询，适配现代生成式模型的需求。

Result: 构建了LLMSQL这一新的Text-to-SQL基准数据集，并通过多个主流大语言模型（如Gemma 3、LLaMA 3.2、Qwen 2.5等）验证了其可用性和改进效果。

Conclusion: LLMSQL作为一个专为大语言模型设计的高质量基准，能够有效支持自然语言到SQL的生成与评估，推动NL2SQL领域的进一步发展。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [85] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型在采用特定政治和文化视角时，如何评估政治言论中的冒犯性，发现具有推理能力的较大模型更能敏感捕捉意识形态和文化差异。


<details>
  <summary>Details</summary>
Motivation: 为了理解大型语言模型在不同政治和文化背景下判断言论冒犯性的能力及其偏差。

Method: 使用2020年美国大选推文的多语言子集（MD-Agreement数据集），评估多个LLM从不同政治立场（极右、保守、中间、进步）判断英语、波兰语和俄语推文是否冒犯的表现。

Result: 较大的具备推理能力的模型（如DeepSeek-R1、o4-mini）在识别意识形态和文化差异方面更一致且敏感，而较小模型则难以捕捉细微差别；推理能力提升了判断的个性化和可解释性。

Conclusion: 推理能力是提升LLM在跨语言、跨意识形态社会政治文本分类中表现的关键因素。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [86] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本文首次系统研究了端到端语音对话模型中的偏见问题，发现闭源模型整体偏见较低，而开源模型对年龄和性别更敏感，多轮对话可能加剧推荐任务中的跨群体差异。


<details>
  <summary>Details</summary>
Motivation: 语音对话模型中的副语言特征（如年龄、性别、口音）可能引发或加剧偏见，但相关研究尚不充分，亟需系统性评估以促进公平可靠的音频交互系统发展。

Method: 通过构建Group Unfairness Score (GUS) 和 similarity-based normalized statistics rate (SNSR) 指标，在多轮对话场景下评估包括Qwen2.5-Omni、GLM-4-Voice、GPT-4o Audio和Gemini-2.5-Flash在内的语音大模型的决策与推荐偏见。

Result: 闭源模型普遍偏见较低；开源模型对年龄和性别更敏感；推荐任务会放大跨群体差异；多轮对话中负面反馈可能导致偏见持续存在。

Conclusion: 语音对话模型中存在显著偏见，尤其在开源模型和推荐任务中更为突出，需针对性设计缓解策略，推动公平性提升。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [87] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本研究探讨了利用人工智能和大语言模型改善塞内加尔司法系统中法律文本获取的方法，成功提取并组织了大量法律条文，并构建了图数据库以可视化法律文本间的关联。


<details>
  <summary>Details</summary>
Motivation: 解决塞内加尔司法系统中法律文档提取与组织困难、司法信息获取不便的问题。

Method: 使用AI和大语言模型（如GPT-4o、GPT-4和Mistral-Large）进行三元组抽取，构建包含2,872个节点和10,774条关系的图数据库。

Result: 成功从各类法律文件中提取7,967条条款，重点来自《土地与公共财产法典》，并实现了法律知识的有效结构化与可视化。

Conclusion: 该框架有助于塞内加尔公民和法律从业者更有效地理解其权利与义务，提升了法律信息的可访问性与可用性。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [88] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 研究发现语言皮层中存在高度抽象、形式独立的意义表征，通过跨生成图像和句子释义的嵌入聚合可更准确预测神经反应。


<details>
  <summary>Details</summary>
Motivation: 探讨语言皮层中语义表征的抽象程度，检验其是否超越语言模型的表示能力。

Method: 利用视觉模型和语言模型的嵌入来建模句子在语言皮层中的神经响应，比较生成图像、释义及上下文增强后的嵌入对预测精度的影响。

Result: 聚合多个生成图像或释义的嵌入能提高预测准确性，加入隐含上下文细节后甚至超过原始句子的嵌入表现，表明语言系统具有比语言模型更丰富的语义表征。

Conclusion: 语言皮层中存在高度抽象且形式独立的意义表征，其语义表达比当前语言模型更为丰富和广泛。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [89] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: DiffuSpec是一种无需训练的框架，利用预训练的扩散语言模型（DLM）在单次前向传播中生成多令牌草案，结合因果一致性路径搜索（CPS）和自适应草案长度（ADL）控制器，实现最高3倍的实时时钟加速，显著提升推测解码效率。


<details>
  <summary>Details</summary>
Motivation: 由于自回归解码的串行特性导致延迟较高，现有推测解码方法在实际部署中受限于自回归草案模型的顺序生成，难以充分提升推理速度。因此需要一种更高效的非自回归草案生成方法。

Method: 提出DiffuSpec框架，使用预训练的扩散语言模型（DLM）进行单次前向传播生成多令牌草案，并引入因果一致性路径搜索（CPS）从双向条件生成的令牌格中提取符合自回归验证的左到右路径，同时设计自适应草案长度（ADL）控制器根据接受反馈动态调整草案长度。

Result: 在多个基准测试中，DiffuSpec实现了最高3倍的实时时钟加速，且保持与标准自回归验证器的兼容性，有效提升了推测解码的效率和灵活性。

Conclusion: DiffuSpec通过结合扩散语言模型与推测解码，提供了一种高效、无需训练的非自回归草案生成方案，为大规模语言模型的低延迟推理提供了新方向。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [90] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: 本文提出了Emission-GPT，一个针对大气排放领域的知识增强型大语言模型代理，旨在解决排放数据获取和理解困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的排放相关知识碎片化且专业化，非专家难以有效理解和使用排放数据，限制了研究和管理效率。

Method: 基于超过10,000份文档构建专业知识库，并结合提示工程与问题补全技术，实现面向领域的准确问答和自然语言交互式数据分析。

Result: 在广东省的案例研究中，Emission-GPT能够通过简单提示从原始数据中提取关键信息，如点源分布和行业趋势，并支持查询、可视化、源贡献分析及情景模拟下的排放因子推荐。

Conclusion: Emission-GPT具有模块化和可扩展架构，可自动化传统手动流程，为下一代排放清单构建和情景评估提供了基础工具。

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [91] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 本文研究了在大语言模型（LLM）群体中是否会出现类似“沉默螺旋”（SoS）的舆论动态，提出了一种评估框架，发现历史和角色信息共同作用时会引发多数主导和SoS模式。


<details>
  <summary>Details</summary>
Motivation: 由于SoS理论原本适用于人类社会，而LLM不具备心理机制，因此探究纯统计语言生成中是否会产生类似SoS的现象成为一个关键问题。

Method: 通过控制‘历史’和‘角色’信号的四种条件，使用Mann-Kendall、Spearman秩相关等趋势检验及峰度、四分位距等集中度量来评估意见动态。

Result: 实验显示，历史与角色共同存在时产生强烈多数主导并复现SoS模式；仅历史导致强锚定效应；仅角色促进多样化但不相关的观点；无历史锚定时SoS无法形成。

Conclusion: LLM群体中可出现SoS样态，其依赖于历史与角色的交互，提示需在AI系统设计中监控和缓解此类涌现性一致性风险。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [92] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: 本文提出ChunkLLM，一种轻量且可插拔的训练框架，通过QK Adapter和Chunk Adapter解决Transformer模型在长序列处理中的计算效率问题，在保持高性能的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: Transformer模型因自注意力机制的二次复杂度导致处理长序列时计算开销大，现有块选择与压缩方法存在语义不完整或训练-推理效率低的问题。

Method: 提出ChunkLLM框架，包含QK Adapter（用于特征压缩和块注意力获取）和Chunk Adapter（用于基于上下文语义检测块边界）；训练时仅更新适配器参数，并采用注意力蒸馏提升关键块召回率；推理时仅在检测到块边界时触发块选择。

Result: 在长短文本多个基准任务上实验表明，ChunkLLM在短文本任务性能相当，在长文本任务保持98.64%性能的同时保留48.58%的KV缓存，并在处理120K长文本时最高实现4.48倍加速。

Conclusion: ChunkLLM有效平衡了模型性能与计算效率，显著提升了长序列输入下的训练与推理效率，具备良好的实用性和扩展性。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [93] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 该研究通过多语言和多情境下的大模型回答，评估其在有争议的罗马尼亚历史问题上的偏见，发现模型的回答受提问方式影响，存在跨语言和格式的立场摇摆，且数值评分常与初始二元选择不一致。


<details>
  <summary>Details</summary>
Motivation: 认识到历史叙述常受国家文化和意识形态影响，而大语言模型可能因训练数据偏差传递非中立观点，因此需探究其在历史问题上的潜在偏见。

Method: 选取一组有争议的罗马尼亚历史问题，在不同语言和上下文中向多个大语言模型提问，分三阶段进行：先获取肯定性回答，再以数值评分形式重复提问，比较回答的一致性。

Result: 二元回答稳定性较高但不完美，且因语言而异；模型常在不同语言或格式间改变立场，数值评分常偏离初始二元选择，最一致的模型未必最准确或最中立。

Conclusion: 大语言模型在处理历史问题时表现出明显的不一致性与潜在偏见，其输出受语言、文化背景及提问方式的影响，提示用户需警惕其非中立性和情境依赖性。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [94] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 本文提出了实例级上下文学习（ILCL），通过引导探索和紧凑的TODO森林机制，自动构建高精度、可复用的上下文文档，显著提升LLM代理在复杂任务中的成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要依赖环境级手册和任务级指导，忽视了特定环境实例中的可验证、可复用事实（如实例中的对象位置、规则等），导致在复杂任务中表现不佳。作者认为缺乏实例级上下文是常见失败原因。

Method: 提出实例级上下文学习（ILCL）框架，采用紧凑的TODO森林进行动作优先级排序，并通过轻量级的‘计划-执行-提取’循环实现高效探索、验证和格式化实例级事实，生成可跨任务复用的高精度上下文文档。

Result: 在TextWorld、ALFWorld和Crafter三个基准上实验表明，该方法显著提升性能：例如，ReAct在TextWorld中的平均成功率从37%提升至95%，IGE从81%提升至95%，且提高了效率。

Conclusion: 通过将一次性探索转化为持久可复用的知识，该方法补充了现有的上下文类型，使LLM代理更加可靠和高效。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [95] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 本研究首次系统探讨了训练条件如何影响语言模型在参数化知识和上下文知识之间的仲裁策略，发现语料库中的事实重复、信息不一致和分布偏差等非理想特性有助于模型发展出更鲁棒的知识利用策略。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成虽广泛应用，但缺乏对训练过程中知识仲裁机制的系统理解，导致预训练模型可能出现不良仲裁行为，浪费大量计算资源。

Method: 通过在合成传记语料库上系统控制各种训练条件，训练基于Transformer的语言模型，并分析不同条件下模型对参数化知识和上下文知识的使用与仲裁行为。

Result: 实验表明，文档内事实的重复有助于提升模型的参数化和上下文知识能力；包含不一致信息或分布偏差的语料库能促使模型发展出更稳健的知识仲裁策略。

Conclusion: 语料库中的非理想特性（如信息不一致和分布偏差）并非应消除的噪声，而是促进模型学习鲁棒知识仲裁的重要因素，为高效预训练兼具参数与上下文知识整合能力的模型提供了实证指导。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [96] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 提出一种记忆增强的小型语言模型架构，通过从大型分层参数化记忆库中检索上下文相关记忆块来提升性能，显著减少模型参数需求。


<details>
  <summary>Details</summary>
Motivation: 传统大模型将所有世界知识存储在参数中，导致资源消耗大且不适用于边缘设备；仅部分知识在每条提示中被使用，因此需要更高效的机制。

Method: 引入分层参数化记忆库，在预训练和推理时动态检索并注入小块上下文相关记忆到小型语言模型中，使模型主干专注于通用知识和推理能力。

Result: 160M参数的模型结合18M参数的记忆块（来自4.6B记忆库）性能媲美两倍以上参数的传统模型；实验扩展至21B参数记忆库，验证了该方法在不同Transformer架构中的有效性。

Conclusion: 该记忆增强架构有效解耦知识存储与模型推理，降低对大规模参数的依赖，为边缘设备部署高效语言模型提供了可行方案。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [97] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 提出一种基于校准对数似然分数的高效方法，用于从多个不同大语言模型（LLMs）中选择最佳响应，在GSM8K、MMLU和ARC数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在资源受限场景下，如何从多个LLM生成的多样化响应中可靠地选择最优答案仍具挑战，现有方法依赖高成本验证器或多次采样，效率较低。

Method: 利用校准后的对数似然分数作为评分机制，结合多LLM系统的多样性优势，隐式利用各模型自身的知识与置信度进行响应选择。

Result: 在GSM8K、MMLU（6个子集）和ARC数据集上，相比辩论与非辩论设置，准确率分别提升约4%、3%和5%。

Conclusion: 该方法原理清晰、计算高效，显著提升了多LLM系统下的响应选择性能，适用于资源受限环境。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [98] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的路由框架，用于在检索增强生成中动态选择关系数据库或非结构化文档作为知识源，以提高领域特定问答系统的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统主要依赖非结构化文档，忽视了能提供精确、及时信息的关系数据库。在需要高准确性与实时性的领域（如金融、医疗）中，这一局限性尤为突出。因此，亟需一种能够有效利用并协同这两种知识源的方法。

Method: 通过系统分析发现查询类型与最优检索路径之间存在规律性，据此设计了一个包含路由代理、规则优化专家和路径级元缓存的规则驱动路由框架。路由代理根据显式规则评分并选择最佳增强路径；规则专家代理利用问答反馈持续优化规则；元缓存复用历史决策以降低延迟和成本。

Result: 在三个问答基准上的实验表明，该框架相比静态策略和学习型路由基线方法，在保持适中计算开销的同时，实现了更高的问答准确率。

Conclusion: 规则驱动的路由机制能有效整合数据库与文档的互补优势，提升领域特定问答系统的性能，兼具准确性、效率与可解释性。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [99] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出了KnowledgeSmith框架，用于系统理解大语言模型（LLM）的知识更新机制，将知识编辑与机器遗忘统一为约束优化问题，并通过自动数据生成器进行多层级、多规模的干预实验，揭示了知识传播、可塑性、一致性和鲁棒性等方面的深层洞见。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏充分、统一和大规模的评估，大语言模型的知识更新机制仍不清楚。例如，LLM在知识修改上是否像人类一样？随着训练数据增加，编辑与遗忘有何不同？这些问题促使作者构建一个统一的分析框架。

Method: 将知识编辑和机器遗忘建模为同一类约束优化问题，提出KnowledgeSmith框架，并设计了一个自动数据生成器，支持在多种图结构层次和数据规模下进行结构化干预，从而系统研究不同修改策略在模型知识中的传播方式。

Result: 实验揭示了LLM在不同知识层级上的更新行为与人类不同，存在一致性与容量之间的权衡，且知识传播具有复杂模式。此外，模型在可塑性、一致性和鲁棒性方面表现出细微差异。

Conclusion: KnowledgeSmith为理解LLM的知识更新机制提供了系统工具和实证基础，揭示了当前方法的局限性，并为设计更可靠、可扩展的知识更新策略提供了指导。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [100] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: 提出了一种在数据库层面关联结构化领域语句的系统框架，通过子字符串匹配检索相关领域语句，显著提升了自然语言转SQL的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准依赖不切实际的、针对特定查询的文本提示来表达领域知识，且大模型在不同数据库间的NL到SQL转换性能差异大，需更好理解领域表达与数据库模式的关系。

Method: 在数据库层面引入结构化领域语句，并采用基于子字符串匹配的方法检索与用户查询相关的领域语句，以增强语义理解。

Result: 在11个真实数据库模式和5个开源及专有大模型上验证表明，该方法比现有基于查询的文本提示更准确，且子字符串匹配的检索效果优于其他检索方式。

Conclusion: 数据库层面的结构化领域语句更实用且准确，结合子字符串匹配的检索策略能有效提升NL到SQL的转换性能。

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [101] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 通过感官提示（如“看”或“听”）可激活纯文本训练的大语言模型中与模态对应的隐含表征，使其在表示上更接近专用视觉和音频编码器。


<details>
  <summary>Details</summary>
Motivation: 探索纯文本训练的大型语言模型是否能通过显式感官提示激活其由语言中编码的多模态规律所形成的隐含结构。

Method: 使用感官提示来引导文本-only LLM，使其在预测下一个词时仿佛受到了潜在视觉或听觉证据的条件影响，并分析其表示变化。

Result: 轻量级的提示工程能够可靠地激活文本训练模型中适合特定模态的表示，提升了其与专业视觉和音频编码器的表示对齐程度。

Conclusion: 尽管LLM缺乏直接感知经验，但通过简单的提示工程即可唤醒其内部蕴含的多模态结构，表明语言中编码的跨模态规律足以支持一定程度的感知对齐。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [102] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 提出了一种基于PCFG的合成语言框架，研究小型语言模型在语法学习中的动态过程，揭示了Transformer并行学习子语法、难以处理深层递归结构等特点，并展示了预训练对小模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大模型在语言任务上表现优异，但其语法学习机制尚不清楚。为了理解语言模型如何获取语法知识，需要一个可控的环境来研究学习动态。

Method: 利用概率上下文无关文法（PCFG）生成合成语言，训练小型Transformer模型，通过控制文法复杂度、递归深度和子文法结构，分析其学习动态，并推导出训练损失和KL散度的递归公式。

Result: 发现Transformer与儿童不同，是并行减少所有子文法的损失；子文法预训练可改善小模型的最终损失，且预训练模型内部表示更贴近文法结构；模型在处理深层递归结构时表现困难，表明神经网络在表示层次化语法上的根本挑战。

Conclusion: 基于PCFG的合成语言为研究语言模型学习动态提供了一个灵活的测试平台，开启了理解神经网络语法学习的新研究方向。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [103] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 提出一种基于知识图谱的RAG评估方法，通过多跳推理和语义聚类提升评估的全面性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标难以有效捕捉现代LLM生成内容的关键特征，尤其是高流畅性和自然性下的语义准确性。

Method: 扩展RAGAS框架，引入基于知识图谱的评估范式，结合多跳推理与语义社区聚类，构建更全面的评分指标。

Result: 实验表明该方法在自动化指标与人工判断之间具有更高相关性，且对生成结果中的细微语义差异更敏感。

Conclusion: 基于知识图谱的评估方法能更深入地理解RAG系统性能，为未来RAG评估提供了有前景的方向。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [104] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: 研究了三种口语语言模型（SLM）中模态适配器（MA）的表示策略，发现使用Whisper编码器的模型通过英语为基础的中间语言表示语义，而未使用Whisper的模型则用英语词表达输入语音的音素信息。


<details>
  <summary>Details</summary>
Motivation: 理解模态适配器如何转换语音表示，以揭示不同SLM处理多语言语音输入的机制差异。

Method: 通过查找MA输出表示最接近的解码器语言模型标记，分析三种SLM中的MA表示策略。

Result: 发现两种MA表示策略：基于Whisper的模型使用英语中介语言表示语义；非Whisper模型（如Phi-4）则以英语词表达语音的音素信息。推测这与语音编码器是否具备翻译训练有关。

Conclusion: 模态适配器的表示策略取决于语音编码器的训练目标，仅用于语音识别或同时用于翻译会影响其生成语义还是音素表示。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [105] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了首个用于评估高亮解释（HEs）在上下文归因中有效性的黄金标准框架，并通过控制测试用例验证了四种HE方法在不同场景下的表现，发现现有方法在长上下文和位置偏差方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 语言模型如何利用上下文信息对用户而言仍不透明，缺乏对解释方法准确性的可靠评估，因此需要一个基于真实使用情况的评估框架。

Method: 构建了一个具有已知真实上下文使用的可控测试用例的黄金标准评估框架，评估了三种现有方法和一种基于机械可解释性的新方法MechLight，在四种上下文场景、四个数据集和五种语言模型上的表现。

Result: MechLight在所有上下文场景中表现最佳，但所有方法在处理较长上下文时性能下降，且表现出位置偏差。

Conclusion: 当前的高亮解释方法在准确性上存在根本性挑战，特别是在长上下文和位置偏差方面，需要新方法来实现可扩展且可靠的上下文使用解释。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [106] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 研究发现用户与LLM聊天机器人交互时的语言风格与人类代理不同，表现为语法流畅性、礼貌性和词汇多样性差异；训练数据中加入多样化语言风格可提升模型鲁棒性，而推理时消息重构效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索用户在与LLM聊天机器人和人类代理交互时沟通方式的差异，并提升LLM对部署后语言风格变化的适应能力。

Method: 分析用户与聊天机器人及人类代理交互的语言数据，比较其语法流畅性、礼貌性和词汇多样性；实验采用数据增强和推理时消息重构两种策略优化模型。

Result: 发现用户与LLM交互时语言风格显著不同；在多样化数据集上训练的模型性能优于单一风格数据集；推理时消息重构改善效果不明显。

Conclusion: 应使用包含多样化通信风格的数据训练LLM，以更好适应实际部署中的用户语言变化，从而提升人机交互体验。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [107] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: 提出了一种无需训练的多语言推理方法SoT，通过语言思维转换和结构化知识转换提升大模型在低资源语言下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的复杂推理能力难以迁移到非高资源语言，因语言资源限制导致多语言推理表现不佳。

Method: 提出Structured-of-Thought（SoT）方法，包含语言思维转换和结构化知识转换两个步骤，将语言特定语义转化为语言无关的结构化表示，并引导模型保持一致的推理路径。

Result: SoT在多个多语言推理基准上优于多种强基线方法，适用于不同大模型主干，且可与其他无需训练策略结合进一步提升性能。

Conclusion: SoT有效提升了大语言模型在多语言场景下的推理能力，为低资源语言的复杂推理任务提供了可行解决方案。

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [108] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 这是首篇关于多模态大语言模型（MLLM）自提升的综述，从数据收集、组织和模型优化三个角度系统梳理了现有方法，并讨论了常用评估方式、下游应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的自提升已取得进展，但在多模态领域的扩展仍具潜力，亟需系统性总结以推动发展。

Method: 从数据收集、数据组织和模型优化三个维度对MLLM自提升方法进行分类和综述，并总结常用评估手段与应用场景。

Result: 提供了MLLM自提升领域的全面概览，构建了系统的文献框架，归纳了当前的技术路径与实践方法。

Conclusion: 该领域尚处早期，未来需解决数据质量、跨模态一致性与评估标准等开放挑战。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [109] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文提出了一种理论上有根据的方法来量化上下文问答任务中的认知不确定性，通过分解交叉熵并利用理想模型近似真实分布，引入了上下文依赖、理解和诚实性三个特征来估计不确定性，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文问答在实际应用中很重要，但现有的不确定性量化研究主要集中在闭卷问答上，缺乏对上下文问答中不确定性的探索。

Method: 提出一种任务无关的、基于token级别的不确定性度量，通过分解交叉熵分离认知不确定性，并用理想化模型逼近真实分布；针对上下文QA任务，提取上下文依赖、理解和诚实性三个特征，使用少量标注样本结合集成方法构建不确定性评分。

Result: 在多个问答基准的分布内和分布外场景下实验表明，该方法显著优于当前最先进的有监督和无监督不确定性量化方法，PRR最高提升达13点，且推理开销极小。

Conclusion: 所提出的框架能有效量化上下文问答中的认知不确定性，通过语义特征差距解释不确定性，并在保持低计算成本的同时实现性能领先。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [110] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本研究首次采用生存分析方法评估大语言模型在多轮对话中的鲁棒性，发现渐进语义漂移反而显著降低失败风险，而突发的点对点漂移则加剧失败，挑战了对话系统需严格语义一致的传统假设。


<details>
  <summary>Details</summary>
Motivation: 现有评测框架多关注静态、单轮对话评估，难以捕捉实际多轮交互中随时间演化的对话退化现象，因此需要一种能建模时间动态失败过程的新方法来评估LLM的鲁棒性。

Method: 提出基于生存分析的框架，使用Cox比例风险模型、加速失效时间模型（AFT）和随机生存森林，对9个前沿大语言模型共36,951轮对话进行时间到事件分析，建模对话失败的时序动态。

Result: 发现突发的点对点语义漂移会显著增加对话失败风险，而渐进式累积漂移具有保护作用，可大幅降低失败风险并延长有效对话轮数；引入交互项的AFT模型表现最优，具备出色的区分度和校准能力。

Conclusion: 生存分析为评估大语言模型对话鲁棒性提供了有力范式，揭示了语义漂移类型对对话稳定性的关键影响，为构建更稳健的对话系统提供了新设计思路，并挑战了语义一致性必须始终维持的传统观点。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [111] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 本文提出了14个旅行领域的低资源NLP数据集，评估了大语言模型在真实场景中的表现，发现通用基准无法准确反映模型在低资源任务中的性能，且即使训练计算量大，现成的LLM在复杂领域任务中仍存在瓶颈，推理能力对小模型提升更显著。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准在低资源任务中提供的信息有限，难以有效开发针对这些领域的解决方案，因此需要更贴近真实场景的评估方式。

Method: 收集并整理了来自真实场景的14个旅行领域数据集，覆盖7种常见NLP任务，并在多个LLM上评估其准确性、扩展行为和推理能力。

Result: 实验表明，通用基准不足以反映低资源任务中的模型性能；现成的LLM在复杂领域任务中表现受限；推理机制对小型LLM的性能提升更为显著。

Conclusion: 针对低资源、领域特定任务需构建更专业的评估基准，推理优化尤其有助于提升小型模型的表现。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [112] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 提出了一种基于策略梯度的生成对抗网络PGMEL，用于多模态实体链接，通过生成高质量负样本提升表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索负样本选择对多模态实体链接中表示学习的影响，缺乏有效机制生成挑战性负样本。

Method: 在生成对抗框架下建模多模态实体链接任务，其中生成器负责生成高质量负样本，判别器执行度量学习任务，并采用策略梯度方法优化离散的生成过程。

Result: 在Wiki-MEL、Richpedia-MEL和WikiDiverse数据集上的实验表明，PGMEL能学习到更有意义的表示，并优于当前最先进的方法。

Conclusion: PGMEL通过对抗式生成负样本显著提升了多模态实体链接性能，验证了负样本质量在表示学习中的关键作用。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [113] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: 提出基于对比学习编码器的评估框架和新数据集IndiCASA，用于检测印度多维度社会偏见，发现现有大模型在残障相关偏见上尤为显著。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估方法难以捕捉印度多元文化背景下的细微刻板印象，需更细粒度的评估框架。

Method: 采用对比学习训练的编码器，通过嵌入相似性衡量细粒度偏见，并构建包含2,575个人工验证句子的IndiCASA数据集，覆盖种姓、性别、宗教、残疾和经济地位五个维度。

Result: 对多个开源大模型的评估显示，所有模型均存在一定程度的刻板偏见，其中残障相关偏见最持久，宗教偏见相对较低，可能得益于全球去偏努力。

Conclusion: 需针对特定文化背景开发更公平的模型，尤其应关注残障等易被忽视的偏见维度。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [114] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 提出一种基于自我意识的强化学习方法，通过大语言模型自主提出并尝试解决任务，在极小数据依赖下实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 减少强化学习训练中对大量标注数据的依赖，探索在极少数据条件下提升大语言模型推理能力的方法。

Method: 引入两种基于自我意识的机制：自我难度预测（评估任务难度并选择具挑战性但可解的任务）和自我极限突破（识别超出能力范围的任务并主动请求外部数据）。训练过程由模型交替提出任务并尝试解决。

Result: 在九个基准测试上实现了53.8%的相对性能提升，仅使用不到1.2%的额外数据。

Conclusion: 自我意识强化学习有效减少了数据依赖，展示了自演化智能体训练的潜力。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [115] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: 提出了一种名为XTRA的跨语言主题建模框架，结合词袋模型与多语言嵌入，通过表示对齐和主题对齐提升主题一致性、多样性和跨语言对齐质量。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题建模方法在主题连贯性和跨语言一致性方面表现不足，需提升主题质量和对齐效果。

Method: XTRA框架引入两种对齐机制：1）表示对齐，通过对比学习在共享语义空间中对齐文档-主题分布；2）主题对齐，将主题-词分布投影到同一空间以增强跨语言一致性。

Result: 在多语言语料库上的实验表明，XTRA在主题连贯性、多样性及对齐质量上显著优于强基线方法。

Conclusion: XTRA能有效学习出可解释且跨语言一致的主题，为跨语言主题建模提供了新思路。

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [116] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 本论文提出了一种可解释的个性评估框架SIMPA，利用从Reddit收集的大规模数据集MBTI9k和PANDORA解决NLP与个性心理学之间的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模标注数据集以及自然语言处理与个性心理学之间的脱节，当前自动个性评估模型的有效性和可解释性受限。

Method: 构建了两个来自Reddit的数据集（MBTI9k和PANDORA），并提出了SIMPA框架，通过语义匹配用户生成语句与标准化问卷项目进行个性评估。

Result: 实验证明人口统计学变量影响模型有效性，并显示SIMPA在保持高可解释性的同时，评估结果可与人工评估相媲美。

Conclusion: SIMPA是一种高效、可解释且模型无关的个性评估框架，具有扩展至其他复杂标签分类任务的潜力。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [117] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在物种分类任务中表现良好，但在保护状态推理方面严重不足，存在知识与推理脱节的问题，并偏向于 charismatic 物种，需结合人类专家进行负责任的应用。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在IUCN红色名录物种保护评估中的可靠性，特别是在分类、保护状态、分布和威胁四个核心方面的表现。

Method: 对五个领先的大型语言模型在21,955个物种的四项IUCN评估任务上进行系统验证，分析其准确率与偏差模式。

Result: 模型在分类任务上准确率达94.9%，但在保护状态评估上仅为27.2%，显示出显著的知识-推理鸿沟，并表现出对魅力脊椎动物的系统性偏见。

Conclusion: LLMs适用于信息检索，但不胜任需要判断的保护决策；应采用人类专家主导、LLMs辅助的混合模式以确保公平与准确性。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [118] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 本文提出了首个全面的Wordle约束满足问题（CSP）建模方法，引入了约束感知的信息熵计算和融合贝叶斯词频先验的概率CSP框架，在求解效率、成功率和抗噪性方面均优于传统方法，并验证了其跨语言适用性。


<details>
  <summary>Details</summary>
Motivation: 现有Wordle求解器多依赖信息论熵最大化或频率启发式方法，缺乏对约束的正式处理，导致在复杂和噪声环境下性能受限。因此，需要一种更系统、原则性的CSP方法来提升求解效率与鲁棒性。

Method: 提出了一种全新的CSP形式化建模方法，包括CSP-Aware Entropy（在约束传播后计算信息增益）和Probabilistic CSP框架（结合贝叶斯词频先验与逻辑约束），并在英语和西班牙语词汇库上进行评估，同时分析噪声影响与跨语言泛化能力。

Result: 在2,315个英文词上，CSP-Aware Entropy平均3.54次猜测成功率达99.9%，比前向检查快46%（12.9ms vs 23.7ms），且在10%噪声下仍保持显著优势；Probabilistic CSP在0-20%噪声下均实现100%成功率；在500个西班牙词上无需调参即达88%成功率，显示良好跨语言迁移性。

Conclusion: 基于形式化CSP的方法结合约束感知启发式和概率-逻辑融合框架，显著优于传统信息论和学习方法，为结构化谜题求解建立了新基准，具有良好的通用性与鲁棒性。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [119] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: 提出了一种轻量级的测试时自反思生成框架SRGen，通过动态熵阈值识别高不确定性token，并利用已生成上下文进行概率分布修正，显著提升大模型在数学推理等复杂任务中的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长链推理中因自回归生成的脆弱性易产生错误传播，现有自反思方法需重写完整输出或依赖昂贵训练，效率低且反应滞后，亟需一种高效、前瞻性的实时纠错机制。

Method: SRGen在生成过程中采用动态熵阈值检测高不确定性token，针对每个不确定token训练特定的校正向量，利用已有上下文进行自我反思式调整，优化token生成的概率分布，实现生成前的主动修正。

Result: 在多个LLM和数学推理基准（如AIME2024）上验证，SRGen显著提升性能：在DeepSeek-R1-Distill-Qwen-7B上Pass@1提升12.0%，Cons@5提升13.3%，并展现出与训练时和测试时其他技术的良好兼容性。

Conclusion: SRGen是一种即插即用、低开销、高兼容的测试时框架，通过在关键不确定点引入生成前的自反思机制，有效增强大模型推理的可靠性和鲁棒性。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [120] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: 提出了首个用于评估对话数据检索系统性能的综合测试集CDR，包含1.6k查询和9.1k对话，揭示了当前模型在对话检索上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的检索系统在处理对话数据以获取产品洞见方面缺乏统一、可靠的评估标准，且文档检索与对话数据检索之间存在明显差距。

Method: 构建了一个包含五个分析任务、1.6k查询和9.1k对话的基准数据集CDR，并评估了16种主流嵌入模型在该数据集上的表现，使用NDCG@10等指标进行衡量。

Result: 即使表现最好的模型在NDCG@10上也仅达到约0.51，显示出当前模型在处理对话数据时存在显著不足；同时识别出对话检索中的三大挑战：隐式状态识别、回合动态和上下文指代。

Conclusion: CDR为对话数据检索提供了可靠基准，揭示了现有技术的局限性，并通过提供查询模板和错误分析推动后续研究。

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [121] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: 提出TRACE框架，用于在大语言模型微调中实现对版权数据集使用的全黑盒检测，通过无失真水印和熵控评分机制，在不损害文本质量和任务性能的前提下实现高效、鲁棒的版权使用验证。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击和数据集推断方法通常需要访问内部信号或依赖手工提示与干净参考数据集，限制了实际应用；而现有水印技术可能损害文本质量或任务性能，因此需要一种实用且可靠的黑盒检测方法来防止大模型微调中对版权数据的未经授权使用。

Method: TRACE框架利用私钥引导生成无失真水印重写数据集，在检测阶段利用微调导致的‘放射性’效应，并引入熵门控机制，仅对高不确定性令牌进行评分，从而增强检测能力，实现全黑盒条件下的版权数据使用检测。

Result: 在多种数据集和模型族上，TRACE consistently 实现显著检测效果（p<0.05），常具有极强统计证据，支持多数据集归因，且在经历大规模非水印语料继续预训练后仍保持鲁棒性。

Conclusion: TRACE为大语言模型微调中版权数据集的使用提供了实用、可靠且高效的黑盒验证路径，兼顾文本质量、任务性能与检测强度。

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [122] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 该研究通过区分语音中的描述性语义和表达性语义，探索其与意图情绪和诱发情绪的关系，提升了语音情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）受限于语音中情感细微差别的复杂性，需更精确地区分语义与情感来源。

Method: 记录参与者观看情绪化电影片段后的叙述音频，结合意图情绪标签、自我报告情绪反应及效价/唤醒度评分，分析描述性与表达性语义的关联。

Result: 描述性语义与意图情绪一致，表达性语义与诱发情绪相关。

Conclusion: 研究结果有助于改进人机交互中的SER应用，并推动更具情境感知能力的AI系统发展。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [123] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: 本文系统比较了链式思维（CoT）与直接提示在语音到文本翻译（S2TT）中的表现，发现随着S2TT数据量增加，直接提示方法提升更稳定，可能在未来更大规模数据下更有效。


<details>
  <summary>Details</summary>
Motivation: 近年来S2TT研究多采用基于大模型的链式思维（CoT）提示方法，因其能利用丰富的ASR和T2TT数据提升性能；但尚不清楚当S2TT数据增多时，CoT是否仍优于直接提示。

Method: 通过将ASR语料的转录文本伪翻译成六种欧洲语言，构建不同规模的S2TT数据，训练基于大模型的S2TT系统，对比CoT与直接提示在不同数据量下的表现。

Result: 随着S2TT数据量增加，直接提示方法的性能提升比CoT更一致，在大规模数据下表现更优。

Conclusion: 尽管CoT在小数据场景下表现良好，但随着S2TT数据资源的增长，直接提示可能成为更有效的策略。

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [124] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: 本文提出了一种结合Llama 3.1和命名实体识别（NER）的语义相似性评分方法Llama-EntScore，用于比较放射科报告的初稿与终稿，以帮助初级医生提升诊断准确性，该方法在准确率上优于单独使用大语言模型或NER的方法。


<details>
  <summary>Details</summary>
Motivation: 初级放射科医生需要有效工具来识别初步报告与最终报告之间的语义差异，以提升临床知识和诊断能力，现有方法在提供准确反馈方面存在局限。

Method: 比较多种大语言模型（LLMs）在放射科报告对比中的表现，并评估基于命名实体识别（NER）的传统方法；提出Llama-EntScore方法，结合Llama 3.1和NER，通过可调节权重生成语义相似性评分及解释性反馈。

Result: Llama-EntScore方法在与放射科医生提供的真实评分对比中，达到67%的完全匹配准确率和93%的±1误差内准确率，优于单独使用LLM或NER的方法。

Conclusion: Llama-EntScore能更准确地评估放射科报告的语义差异，提供可解释的反馈，有助于医生培训和报告质量提升，是LLM与传统NLP方法融合的有效范例。

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [125] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 该论文研究了链式思维（CoT）提示在语音到文本翻译（S2TT）中的应用，发现其主要依赖转录文本而极少利用语音信号，表现类似于级联系统。通过归因分析、鲁棒性评估和对韵律的感知测试，作者指出CoT并未有效克服错误传播和缺乏声学线索利用的问题。引入直接S2TT数据或注入噪声转录可提升模型对语音的依赖和鲁棒性，表明需要更显式融合声学信息的新架构。


<details>
  <summary>Details</summary>
Motivation: 传统级联式S2TT系统存在错误传播和无法利用语调等声学线索的问题，而CoT提示被寄望于能同时利用语音和文本信息来缓解这些问题。本文旨在探究CoT是否真正实现了这一目标。

Method: 通过归因方法分析CoT模型对语音和文本的依赖程度，进行带噪声转录的鲁棒性评估，并测试模型对韵律信息的敏感性；同时尝试加入直接S2TT数据和噪声注入等训练干预手段以改进模型。

Result: 实验发现CoT模型行为与级联系统相似，主要依赖文本转录，几乎未利用语音信号；加入直接S2TT数据或噪声转录后，模型对语音的依赖性和鲁棒性有所提升。

Conclusion: 当前CoT提示并未有效融合语音与文本信息，未能充分发挥端到端S2TT的潜力；未来需设计能显式整合声学信息的新型架构。

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [126] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: 本文提出了一个细粒度、基于测验的评估框架SurveyBench，用于评估自动生成学术综述的质量，涵盖主题来源、多维度指标和双模式评估协议，结果表明现有方法与人类水平仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有的自动综述生成方法（LLM4Survey）输出质量不足，且缺乏严格、符合读者需求的基准来揭示其缺陷。

Method: 提出SurveyBench框架，包含来自arXiv论文和高质量综述的主题数据、多层次评估指标（结构、内容、非文本元素），以及基于内容和测验的双模式评估协议。

Result: 实验结果显示，现有LLM4Survey方法在内容评估中平均比人类低21%，验证了SurveyBench的有效性和挑战性。

Conclusion: SurveyBench能有效评估并揭示当前自动综述生成方法的不足，推动该领域向更高质量发展。

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [127] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文首次系统研究了大语言模型在多语言场景下的置信度校准问题，发现非英语语言的校准效果普遍较差，并揭示了英语中心化训练导致最终层表征偏差。通过分析发现，中间层能提供更可靠的校准信号，据此提出无需训练的LACE等方法，为构建更公平可信的多语言大模型提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言场景下的置信度校准问题尚未被充分研究，尤其是非英语语言的校准性能可能因英语中心化的训练而受损，影响模型的可靠性和公平性。

Method: 对六个模型家族、超过100种语言进行了大规模系统性研究，分析模型内部表示，特别是不同网络层的校准能力，并提出基于层集成的无需训练的校准方法LACE，自适应选择每种语言最优的层组合。

Result: 发现非英语语言普遍存在校准不佳的问题；最终层因英语偏见不适合作为校准依据；晚期中间层具有更稳定和准确的校准信号；LACE方法显著提升了多语言校准性能。

Conclusion: 英语中心化训练带来了多语言校准的隐性代价，仅依赖最终层会损害非英语语言的可靠性；通过利用中间层信号并结合语言自适应集成策略，可在不训练的情况下显著改善多语言置信度校准，推动更公平可信的全球化大模型发展。

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [128] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: 该论文提出了一种轻量级相似性度量方法来检测AI对人类文本的编辑程度，并训练了EditLens模型，实现了在区分人类、AI及混合写作方面的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要关注检测完全由AI生成的文本，而忽视了AI编辑文本的检测，这在实际应用中具有重要意义。

Method: 提出轻量级相似性度量指标，并以此作为中间监督信号训练EditLens回归模型，用于预测文本中AI编辑的程度。

Result: EditLens在二分类（F1=94.7%）和三分类（F1=90.4%）任务上达到最优性能，能够有效识别AI编辑文本及其编辑程度。

Conclusion: AI编辑的文本可以被有效检测，且编辑程度也可量化，这对作者归属、教育和政策制定具有重要影响。

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


### [129] [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
*Iñigo Parra*

Main category: cs.CL

TL;DR: 该研究验证了大语言模型隐藏状态与fMRI脑响应之间的相关性在多种潜在问题下的鲁棒性，确认并加强了先前研究的结论。


<details>
  <summary>Details</summary>
Motivation: 检验先前关于大语言模型与大脑表征相似性的研究结果是否受到维度灾难、相似性度量方法、训练数据和模型结构等因素的影响。

Method: 通过降维、使用新的相似性度量指标、比较不同训练条件下的模型以及分析位置编码的作用，系统地评估了模型与大脑表征相关性的稳健性。

Result: 发现先前结果在降维后依然存在，新相似性度量方法下得到验证，仅在训练于人类语言的模型中出现，并依赖于模型中的位置编码。

Conclusion: 大语言模型与大脑表征的相关性具有鲁棒性，且对模型训练和结构具有特异性，支持其生物学合理性和可解释性。

Abstract: Previous work has shown correlations between the hidden states of large
language models and fMRI brain responses, on language tasks. These correlations
have been taken as evidence of the representational similarity of these models
and brain states. This study tests whether these previous results are robust to
several possible concerns. Specifically this study shows: (i) that the previous
results are still found after dimensionality reduction, and thus are not
attributable to the curse of dimensionality; (ii) that previous results are
confirmed when using new measures of similarity; (iii) that correlations
between brain representations and those from models are specific to models
trained on human language; and (iv) that the results are dependent on the
presence of positional encoding in the models. These results confirm and
strengthen the results of previous research and contribute to the debate on the
biological plausibility and interpretability of state-of-the-art large language
models.

</details>


### [130] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的新型主题建模范式，将主题建模视为长文本生成任务，并通过零样本提示与传统神经主题模型（NTM）进行系统比较，探讨其在主题质量上的优劣。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，传统神经主题模型可能已过时，本文旨在探索利用LLM进行主题建模的新范式，以提升主题发现的质量和实用性。

Method: 将主题建模重构为长文本生成任务，采用零样本提示方法，通过采样数据子集、生成主题及代表性文本、关键词匹配进行文本分配，实现即插即用的LLM主题建模。

Result: 实验结果表明，在零样本设置下，基于LLM的方法在某些指标上可与或优于传统NTM，验证了其作为主题建模新范式的潜力。

Conclusion: LLM-based 长文本生成范式有望取代部分传统NTM，成为新时代主题建模的有效途径，支持‘多数NTM已过时’的观点。

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [131] [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
*Abteen Ebrahimi,Adam Wiemerslage,Katharina von der Wense*

Main category: cs.CL

TL;DR: 提出NN-Rank算法，利用多语言模型的隐藏表示和无标签目标语言数据进行跨语言迁移中的源语言排序，在POS和NER任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于词汇和语言特征的方法在跨语言迁移中表现受限，尤其在缺乏目标语言数据时效果不佳，需要一种更有效且鲁棒的源语言排序方法。

Method: 利用多语言预训练模型的隐藏层表示，结合少量无标签目标语言数据，通过最近邻机制计算源语言与目标语言的相似性，从而进行排序。

Result: 在51个源语言和多达72个目标语言上的实验表明，NN-Rank在域内数据下比现有方法平均提升最高达35.56 NDCG（POS）和18.14 NDCG（NER）；即使使用仅25个样本的少量数据，仍能达到使用全部数据92.8%的性能。

Conclusion: NN-Rank是一种高效且数据需求低的源语言排序方法，在有标注数据稀缺的场景下具有广泛应用潜力。

Abstract: We present NN-Rank, an algorithm for ranking source languages for
cross-lingual transfer, which leverages hidden representations from
multilingual models and unlabeled target-language data. We experiment with two
pretrained multilingual models and two tasks: part-of-speech tagging (POS) and
named entity recognition (NER). We consider 51 source languages and evaluate on
56 and 72 target languages for POS and NER, respectively. When using in-domain
data, NN-Rank beats state-of-the-art baselines that leverage lexical and
linguistic features, with average improvements of up to 35.56 NDCG for POS and
18.14 NDCG for NER. As prior approaches can fall back to language-level
features if target language data is not available, we show that NN-Rank remains
competitive using only the Bible, an out-of-domain corpus available for a large
number of languages. Ablations on the amount of unlabeled target data show
that, for subsets consisting of as few as 25 examples, NN-Rank produces
high-quality rankings which achieve 92.8% of the NDCG achieved using all
available target data for ranking.

</details>


### [132] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: FocusAgent是一种利用轻量级LLM检索器从可访问性树中提取与任务最相关行的方法，能在减少50%以上观测数据的同时保持性能，并提升对提示注入攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有网页代理在处理超长网页内容时面临上下文饱和、计算成本高和安全风险（如提示注入）等问题，且现有剪枝策略易丢弃关键信息或保留无关内容。

Method: 提出FocusAgent，使用轻量级LLM检索器，基于任务目标从可访问性树（AxTree）中提取最相关内容，过滤噪声和无关信息，实现高效推理并降低安全风险。

Result: 在WorkArena和WebArena基准上，FocusAgent在减少超过50%观测大小的情况下，性能与强基线相当；其变体显著降低了提示注入攻击的成功率，同时在无攻击环境下保持任务成功率。

Conclusion: 基于LLM的针对性检索是一种构建高效、有效且安全的网页代理的实用且鲁棒的策略。

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [133] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 提出Cache-to-Cache（C2C）新范式，实现大模型间基于KV-Cache的直接语义通信，提升准确率与推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统通过文本进行通信，导致语义信息损失和生成延迟，限制了性能提升。

Method: 利用神经网络投影并融合源模型与目标模型的KV-Cache，并引入可学习的门控机制选择受益于缓存通信的目标层，实现直接语义传递。

Result: 相比单个模型，C2C平均准确率提升8.5-10.5%；相比文本通信范式，准确率提高3.0-5.0%，且平均延迟降低2.0倍。

Conclusion: C2C为多LLM系统提供了一种高效、低延迟的语义通信新范式，验证了KV-Cache作为模型间通信媒介的潜力。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [134] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: 提出Self-Anchor方法，通过结构化推理路径和动态对齐注意力来提升大模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法在长推理链中难以保持对关键中间步骤和原始提示的关注，导致错误频发。

Method: 将推理过程分解为结构化计划，并自动对齐模型注意力到最相关的推理步骤上。

Result: 在六个基准上优于当前最优提示方法，显著缩小了非推理专用模型与专用推理模型之间的性能差距。

Conclusion: Self-Anchor无需重新训练即可提升大多数大语言模型的复杂推理能力，具有广泛适用潜力。

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [135] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型后训练中强化学习的兴起引发的奖励模型与评估指标之间的关系，指出这两个领域研究分离导致了术语冗余和重复问题。通过综述两者的研究现状，文章认为加强合作有助于解决诸如虚假相关性、奖励博弈和数据质量等问题，并提出了多个可改进的研究方向。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和评估指标在功能上相似，但目前两个领域的研究相对独立，造成了资源浪费和共同挑战的重复出现，因此需要推动两者的融合与协作。

Method: 通过对比分析奖励模型与评估指标的功能与挑战，进行广泛文献综述，并以具体任务表现为例展示评估指标在某些场景下的优势。

Result: 发现评估指标在特定任务上优于奖励模型，识别出包括虚假相关性、奖励博弈、数据质量提升和元评估在内的共性挑战，并提出跨领域合作的潜在研究方向。

Conclusion: 加强奖励模型与评估指标两个领域的协作，有助于克服当前面临的共同难题，推动AI模型评价体系的发展。

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [136] [Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model](https://arxiv.org/abs/2510.02390)
*Zilai Li*

Main category: cs.GR

TL;DR: 本文提出了一种无需训练的高效图像生成算法，能够在8步内生成高质量的512x512和1024x1024分辨率图像，并在FID指标上优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型生成图像需要较多迭代步骤，计算成本高；本文旨在通过分析扩散ODE/SDE的截断误差，设计一种无需额外训练即可实现快速高质量生成的方法。

Method: 基于对扩散常微分方程（ODE）和随机微分方程（SDE）截断误差的分析，提出一种训练免费的快速采样算法，支持灵活的引导尺度，在极少数步骤（如8步或5步）内完成高分辨率图像生成。

Result: 在COCO 2014、COCO 2017和LAION数据集上验证了该算法：8步生成时FID分别为15.7、22.35和17.52，优于DPM++ 2m和AMED-plugin等先进方法；5步生成性能与SDXL-turbo、Flash Diffusion等相当；6步可生成1024x1024图像，性能接近最新蒸馏模型。

Conclusion: 该算法是首个无需训练即可在8步内生成1024x1024图像并达到媲美蒸馏模型FID性能的方法，显著提升了扩散模型的推理效率。

Abstract: The diffusion model is a state-of-the-art generative model that generates an
image by applying a neural network iteratively. Moreover, this generation
process is regarded as an algorithm solving an ordinary differential equation
or a stochastic differential equation. Based on the analysis of the truncation
error of the diffusion ODE and SDE, our study proposes a training-free
algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight
steps, with flexible guidance scales. To the best of my knowledge, our
algorithm is the first one that samples a 1024 x 1024 resolution image in 8
steps with an FID performance comparable to that of the latest distillation
model, but without additional training. Meanwhile, our algorithm can also
generate a 512 x 512 image in 8 steps, and its FID performance is better than
the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We
validate our eight-step image generation algorithm using the COCO 2014, COCO
2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and
17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further,
it also outperforms the state-of-the-art AMED-plugin solver, whose FID
performance is 19.07, 25.50, and 18.06. We also apply the algorithm in
five-step inference without additional training, for which the best FID
performance in the datasets mentioned above is 19.18, 23.24, and 19.61,
respectively, and is comparable to the performance of the state-of-the-art AMED
Pulgin solver in eight steps, SDXL-turbo in four steps, and the
state-of-the-art diffusion distillation model Flash Diffusion in five steps. We
also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps,
whose FID performance only has a limited distance to the latest distillation
algorithm. The code is in repo:
https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need

</details>


### [137] [Visualizing Spatial Point Clouds: A Task-Oriented Taxonomy](https://arxiv.org/abs/2510.02651)
*Mahsa Partovi,Federico Iuricich*

Main category: cs.GR

TL;DR: 本文提出了一种针对3D点云数据可视化的分类法，系统地将四十年来的可视化设计选择与现代应用中的基本挑战联系起来，填补了可视化技术与分析目标之间缺乏系统映射的空白。


<details>
  <summary>Details</summary>
Motivation: 由于点云数据的稀疏性、密度变化和规模巨大，设计有效的可视化方法面临重大挑战，且现有研究缺乏将可视化技术与具体分析目标系统关联的方法。

Method: 通过分析空间点云可视化的设计空间，构建了一个分类体系，基于数据类型、用户目标和可视化技术对可视化策略进行组织。

Result: 提出了一个涵盖四十年可视化设计选择的分类法，明确了不同技术与应用场景之间的对应关系，为更高效、可解释和以用户为中心的可视化方法提供了框架基础。

Conclusion: 该分类框架有助于指导未来点云可视化的设计与评估，推动在自动驾驶、环境监测和灾害响应等领域的有效应用。

Abstract: The visualization of 3D point cloud data is essential in fields such as
autonomous navigation, environmental monitoring, and disaster response, where
tasks like object recognition, structural analysis, and spatiotemporal
exploration rely on clear and effective visual representation. Despite
advancements in AI-driven processing, visualization remains a critical tool for
interpreting complex spatial datasets. However, designing effective point cloud
visualizations presents significant challenges due to the sparsity, density
variations, and scale of the data. In this work, we analyze the design space of
spatial point cloud visualization, highlighting a gap in systematically mapping
visualization techniques to analytical objectives. We introduce a taxonomy that
categorizes four decades of visualization design choices, linking them to
fundamental challenges in modern applications. By structuring visualization
strategies based on data types, user objectives, and visualization techniques,
our framework provides a foundation for advancing more effective,
interpretable, and user-centered visualization techniques.

</details>


### [138] [GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting](https://arxiv.org/abs/2510.02884)
*Xinran Zhang,Hanqi Zhu,Yifan Duan,Yanyong Zhang*

Main category: cs.GR

TL;DR: GS-Share 是一种基于3D Gaussian Splatting的高保真、高效网络传输的3D地图共享系统，通过锚点建图、虚拟图像增强和增量更新实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的3D地图共享系统在保真度、连续更新和网络效率之间难以平衡，缺乏实用的解决方案。

Method: 提出GS-Share系统，采用基于锚点的全局建图、基于虚拟图像的地图增强和增量式地图更新方法，实现紧凑且高保真的地图表示与共享。

Result: 相比现有最先进方法，GS-Share在PSNR、LPIPS和Depth L1指标上分别提升11%、22%和74%，同时减少36%的地图传输开销。

Conclusion: GS-Share在保持高保真重建的同时显著提升了地图共享的效率和可扩展性，适用于自动驾驶和增强现实等实际应用场景。

Abstract: Constructing and sharing 3D maps is essential for many applications,
including autonomous driving and augmented reality. Recently, 3D Gaussian
splatting has emerged as a promising approach for accurate 3D reconstruction.
However, a practical map-sharing system that features high-fidelity, continuous
updates, and network efficiency remains elusive. To address these challenges,
we introduce GS-Share, a photorealistic map-sharing system with a compact
representation. The core of GS-Share includes anchor-based global map
construction, virtual-image-based map enhancement, and incremental map update.
We evaluate GS-Share against state-of-the-art methods, demonstrating that our
system achieves higher fidelity, particularly for extrapolated views, with
improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively.
Furthermore, GS-Share is significantly more compact, reducing map transmission
overhead by 36%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [139] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 本文研究了在多智能体系统中使用大语言模型（LLM）的提示方法，以提升协作行为和决策能力。作者改进了CoELA框架，并通过实验评估不同LLM和提示策略的组合效果，最佳组合使基于Gemma3的系统效率提升了22%。此外，集成语音功能实现了更自然的语音交互，增强了用户界面的互动性。


<details>
  <summary>Details</summary>
Motivation: 为了提升多智能体系统中基于大语言模型的协作性能，探索有效的提示工程方法，并增强人机交互体验。

Method: 改进CoELA框架，系统性地测试不同的大语言模型和提示工程策略，并集成语音交互功能。

Result: 最佳提示与模型组合使Gemma3系统的效率提升22%，语音集成改善了用户交互体验。

Conclusion: 提示优化显著提升多智能体协作性能，语音集成有助于更自然、高效的系统开发与演示。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [140] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: BrowserArena是一个用于评估在开放网络上运行的LLM网络代理的平台，通过用户提交任务、Arenastyle对比和逐级人工反馈，揭示了代理在验证码处理、弹窗清除和直接导航等方面的失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有的代理评估局限于沙盒环境或人为任务，缺乏对真实开放网络环境中代理性能的有效评估。

Method: 构建BrowserArena平台，收集用户任务，进行Arenastyle对抗性比较，并利用逐级人类反馈分析代理行为轨迹。

Result: 识别出三个常见失败模式：验证码解决、弹窗横幅清除和直接URL导航；发现不同语言模型在应对这些挑战时表现差异显著，例如o4-mini在绕过验证码方面策略更多样，而DeepSeek-R1常误导用户。

Conclusion: 当前网络代理在行为策略上具有多样性但依然脆弱，BrowserArena的方法为大规模评估和理解代理失败模式提供了有效途径。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [141] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文提出了RefineShot，一个经过系统优化和扩展的电影摄影理解基准，解决了ShotBench中选项设计模糊以及ShotVL模型在推理一致性和指令遵循上的不足，提升了评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的电影摄影理解基准ShotBench存在选项设计模糊，且当前最优模型ShotVL在推理一致性和指令遵循方面表现不佳，影响评估的公平性和可比性，亟需改进。

Method: 通过对ShotBench进行选项结构的系统重构，首次对ShotVL的推理行为进行深入分析，并引入联合评估任务准确率与核心能力的新评估协议。

Result: 构建了更可靠的RefineShot基准，在多项评测中揭示了现有模型的局限性，并为未来研究提供了更有效的评估手段。

Conclusion: RefineShot显著提升了电影摄影理解任务的评估质量，有助于推动该领域的公平比较和持续进步。

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [142] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出一种基于分布无关风险控制（DFRC）和动态早退预测的新方法，以防止大语言模型在上下文学习中因恶意示例而性能下降，同时保留对有益示例的效率增益。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能通过少量上下文示例学习新任务，但易受恶意或错误示例影响，存在安全隐患，需设计内建防御机制。

Method: 定义零样本性能为安全基准，结合分布无关风险控制（DFRC）与动态早退预测，通过忽略关注有害输入的后期注意力头来限制性能下降，并改进DFRC以区分有害与有益输入。

Result: 理论与实验结果表明，该方法能有效控制有害上下文示例带来的风险，同时在有益示例上实现显著的计算效率提升。

Conclusion: 所提方法在保障模型安全的同时，兼顾了性能与效率，为安全的上下文学习提供了可行方案。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [143] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 本文研究了大型多模态模型（LMMs）中支持空间关系推理的内部机制，发现一小部分注意力头通过“功能向量”传递空间关系表征，并可被提取和调整以提升零样本性能和类比推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LMM在少量示例下展现出强大的上下文学习能力，但其内部如何学习任务仍不明确，尤其是空间关系的理解机制缺乏透明性。

Method: 基于因果中介分析识别影响关系预测的关键注意力头，提取多模态功能向量，并在冻结模型参数的情况下对其进行微调；同时探索功能向量的线性组合用于解决类比问题。

Result: 成功识别出负责空间关系表征的注意力头，提取的功能向量提升了零样本准确率，微调后显著优于上下文学习基线，并能通过线性组合泛化到未见过的空间关系类比任务。

Conclusion: LMM内部存在局部化结构编码空间关系知识，这些结构可被系统提取与优化，增强了对模型模块性和关系推理控制的理解。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [144] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文提出了自主管理代理（Autonomous Manager Agent）的研究愿景，旨在协调动态人机团队中的多智能体工作流，形式化了工作流管理的挑战，并发布了MA-Gym框架以推动该领域的研究。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体AI在自动化单个任务方面取得了进展，但复杂多智能体工作流的管理仍具挑战性，尤其是在动态人机协作环境中缺乏有效的协调机制。

Method: 将工作流管理形式化为部分可观测随机博弈（Partially Observable Stochastic Game），提出四个基础挑战，并发布开源模拟与评估框架MA-Gym，基于GPT-5的管理代理在20个工作流中进行评估。

Result: 实验表明，当前的管理代理在目标完成、约束遵守和运行时间之间难以联合优化，凸显工作流管理仍是一个困难的开放问题。

Conclusion: 自主管理系统的实现需要解决组合推理、多目标优化、临时团队协调及合规治理等核心挑战，并需关注其组织与伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [145] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的多智能体系统，用于加速增材制造中的合金发现过程，通过调用热力学计算和工艺图生成等工具实现自主决策。


<details>
  <summary>Details</summary>
Motivation: 合金发现过程复杂，涉及多个专业知识领域，需要一种智能化方法来降低门槛并提升效率。

Method: 构建基于LLM的多智能体系统，利用MCP协议调用Thermo-Calc等工具进行性能计算和工艺分析，并根据工具返回结果动态调整任务路径。

Result: 该系统能够理解复杂用户指令，分析合金可打印性，并自主决策以优化合金设计流程。

Conclusion: LLM驱动的多智能体系统能有效自动化和加速增材制造中的合金发现，展示了其在材料研究中应用的潜力。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [146] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: 本研究提出了一种基于人工智能的新型对话系统Geolog-IA，用于回答关于厄瓜多尔中央大学地质学论文的问题，结合Llama 3.1和Gemini 2.5模型与RAG架构，有效减少幻觉和知识过时问题，系统在BLEU指标上达到0.87的平均分，并提供直观的网页界面，适用于教育、培训和科研支持。


<details>
  <summary>Details</summary>
Motivation: 为解决传统语言模型在专业领域中易产生幻觉和知识更新不及时的问题，提升地质学相关学术信息查询的准确性与交互性。

Method: 采用Llama 3.1和Gemini 2.5语言模型，结合检索增强生成（RAG）架构和SQLite数据库，构建一个面向地质学论文问答的对话系统。

Result: Geolog-IA在BLEU评分上取得0.87的平均成绩，表现出高一致性和响应准确性，并成功部署了用户友好的Web界面。

Conclusion: Geolog-IA能有效支持地质学领域的教育、研究与管理，具备推广至其他学科的潜力。

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [147] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 本文开发了一个用于集装箱堆放规划（CSPP）的Gym环境，并集成了单智能体和多智能体形式的起重机调度，评估了五种强化学习算法在不同复杂度场景下的性能，揭示了算法选择和问题建模的重要性，为海上物流提供了可复用的研究基础。


<details>
  <summary>Details</summary>
Motivation: 由于集装箱堆放规划（CSPP）高度复杂，传统上依赖人工经验，尽管已有研究将强化学习（RL）应用于该领域，但缺乏对不同算法的系统性基准比较。因此，本文旨在填补这一空白。

Method: 构建了一个捕捉CSPP核心特征的Gym环境，并扩展支持起重机调度的单智能体与多智能体建模；在此框架下评估了DQN、QR-DQN、A2C、PPO和TRPO五种强化学习算法在多种复杂场景中的表现。

Result: 实验结果表明，随着问题复杂度增加，不同算法之间出现显著性能差距，突显了算法选择和问题建模对CSPP的重要性。

Conclusion: 本文不仅为CSPP提供了多种强化学习方法的基准比较，还发布了一个可复用的Gym环境，支持未来在海运物流中的进一步研究与实际应用。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [148] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: 本文提出了AutoMaAS，一种基于神经架构搜索的自演化多智能体系统设计框架，能够根据性能-成本分析动态优化智能体配置，显著提升性能并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有自动化多智能体系统设计方法通常采用单一结构，无法根据查询复杂性和领域需求动态调整资源分配，缺乏灵活性和效率。

Method: 引入AutoMaAS框架，结合神经架构搜索与自动化机器学习技术，通过动态算子生命周期管理实现自动算子生成、融合与淘汰，支持实时参数调整、在线反馈优化和决策追溯。

Result: 在六个基准测试中，AutoMaAS相比现有最先进方法提升了1.0-7.1%的性能，同时降低了3-5%的推理成本，并展现出良好的跨数据集和大模型骨干的迁移能力。

Conclusion: AutoMaAS为大语言模型时代的多智能体系统自动化设计提供了新范式，兼顾高效性、适应性与可解释性。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [149] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 提出一种基于多模态大语言模型的框架，融合视觉感知、车辆定位与CAN总线数据，生成自然语言警报，提升电动车辆在城市交通中的安全性和可解释性，并支持智能电网下的车队协调与能量管理。


<details>
  <summary>Details</summary>
Motivation: 确保驾驶员、车辆与环境之间安全且可解释的交互是电动车辆融入智能电网的关键挑战。

Method: 结合YOLOv8、语义分割、地理编码定位和CAN总线遥测数据，构建一个多模态大语言模型框架，通过提示工程生成上下文相关的自然语言警报。

Result: 在真实城市道路数据上的案例研究表明，该框架能有效生成针对行人、骑行者和车辆接近等危险情境的预警信息。

Conclusion: 大语言模型有望作为电动出行的辅助工具，提升驾驶安全性，并支持智能电网下的可扩展车队协调、负荷预测与交通感知的能量规划。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [150] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 研究发现基础模型在跨模态冲突情境下表现不佳，主要由于跨模态注意力不平衡，即使增加数据规模也难以缓解；通过在训练中显式融合多模态信息可有效减轻该问题并提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究基础模型在多模态交互场景中的联合推理能力，尤其是在跨模态冲突下的表现，以理解其是否能有效整合不同模态信息。

Method: 设计跨模态冲突实验，分析模型在单模态与多模态、单语言与多语言情境下的识别能力，并通过注意力机制分析跨模态注意力不平衡问题；提出在训练实例中显式融合多模态信息的方法。

Result: 基础模型在单模态冲突中识别率达90%，但在跨模态情况下降至3%；存在显著的跨模态注意力不平衡现象；简单且可扩展的多模态融合训练方法能显著减少注意力不平衡并提升下游任务性能。

Conclusion: 当前基础模型在跨模态联合推理方面存在严重缺陷，需系统性设计训练策略（如显式多模态融合）来改善跨模态注意力分配，从而构建更可靠的基础模型。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [151] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 本文提出通过温度维度扩展（temperature scaling）来提升大语言模型推理能力，相较于单一温度的测试时扩展方法，在多个基准上平均提升7.3个点，且无需额外训练即可使基础模型达到强化学习微调模型的性能。


<details>
  <summary>Details</summary>
Motivation: 发现单一温度下增加推理路径数量（K）对准确率的提升存在瓶颈，且不同难度问题在不同温度下表现不同，说明单温度无法充分挖掘模型潜力。

Method: 引入温度维度扩展，探索多温度采样生成推理路径，并设计多温度投票方法以降低计算开销。

Result: 在Qwen3系列模型和五个推理基准上，温度扩展比单温度TTS平均提升7.3个百分点，使基础模型性能接近RL微调模型。

Conclusion: 温度扩展能有效拓展大模型的推理边界，揭示测试时扩展比以往认为的更具潜力，为释放基础模型能力提供简单有效的方法。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [152] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 本文提出了一种新的“可能性”概念，用于评估现实世界事件发生的可能性，基于事件的前提条件和约束条件的概率，适用于规划问题中的决策。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个更贴近人类推理方式的可能性模型，专注于现实世界事件的可能性评估，而非泛化的可能性定义。

Method: 采用多值逻辑连接词的Łukasiewicz解释，将事件的可能性计算为其前提成立且约束不成立的概率函数，并应用于规划问题中多个方案的选择。

Result: 构建了一个可用于路径规划等实际问题的可能性评估框架，并通过车辆路线规划示例进行了说明，展示了其在判断计划可行性方面的应用潜力。

Conclusion: 该可能性模型能更好地反映人类对计划可行性的自然推理过程，具有在规划及其他领域推广应用的前景。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [153] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为ARMS的自适应红队代理，用于系统化评估视觉-语言模型（VLM）的安全性，通过推理增强的多步编排自动优化多样化的攻击策略，在多种基准上实现了最先进的攻击成功率，并构建了包含3万余个实例的大规模多模态安全数据集ARMS-Bench，显著提升了VLM的安全对齐能力。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型（VLM）的广泛应用，其多模态接口带来了新的安全漏洞。现有红队测试方法受限于攻击模式狭窄或依赖人工设计，缺乏对现实世界中新兴漏洞的可扩展探索，因此需要一种自动化、系统化且高效的VLM安全评估框架。

Method: 提出ARMS，一个具备推理增强多步编排能力的自适应红队代理；设计11种新型多模态攻击策略和17种红队算法，通过模型上下文协议（MCP）集成；引入分层记忆结构与epsilon-greedy攻击探索算法以平衡攻击的多样性与有效性；基于生成结果构建大规模安全数据集ARMS-Bench。

Result: 在实例和策略基准测试中，ARMS平均攻击成功率比基线高出52.1%，在Claude-4-Sonnet上超过90%；生成的红队实例多样性显著更高，揭示了VLM中的新兴漏洞；基于ARMS-Bench进行安全微调可显著提升模型鲁棒性同时保持通用性能。

Conclusion: ARMS为VLM的安全评估提供了一个高效、可扩展的自动化框架，不仅能发现现有和新兴的多模态安全漏洞，还通过构建高质量安全数据集ARMS-Bench为后续安全对齐提供了实用工具和指导，推动了多模态模型安全性的发展。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [154] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 本文提出了一种以约束为中心的架构，利用大语言模型（LLM）实现生产调度中制造约束的自动化生成，通过分层结构设计和领域特定表示提升准确性和可靠性，并结合自适应算法适配具体制造场景，实验表明该方法显著优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 将制造需求转化为形式化约束是高级计划与排程（APS）系统的关键前提，但目前这一过程依赖人工且耗时耗力；尽管大语言模型（LLM）在自动化处理方面有潜力，但其在自然语言歧义、输出非确定性和领域知识不足方面存在挑战。

Method: 提出一种三层分层结构的约束中心架构，采用领域特定表示法来规范LLM的输出，并设计了自动化生产场景适配算法，以实现对不同制造配置的高效定制。

Result: 实验结果显示，该方法在约束生成任务中显著优于纯LLM方法，有效平衡了LLM的生成能力与制造业对可靠性的要求。

Conclusion: 所提出的架构能够可靠地实现制造约束的自动化生成，为LLM在工业调度系统中的安全、高效应用提供了可行路径。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [155] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了一种无需训练的节点级一致性验证框架NCV，用于提升大语言模型推理验证的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在验证大语言模型多步推理时存在误差定位不精确和令牌成本高的问题，难以有效评估整个推理链或依赖昂贵的多次采样。

Method: 将推理链分解为相互连接的验证节点，在每个节点上进行轻量级的二值一致性检查，从而实现对错误的精确定位并避免冗长生成。

Result: 实验显示，NCV在公共数据集上比基线方法F1分数提高10%到25%，且使用的令牌数量比传统CoT验证器少6到58倍。

Conclusion: NCV是一种高效、可解释且可扩展的大模型推理验证方案，显著降低了计算开销并提升了验证性能。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [156] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文提出了TRACE框架，用于多维度评估工具增强型LLM代理的推理轨迹，通过引入证据库实现对效率、幻觉和适应性等复杂行为的可扩展且低成本的评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试的评估方法多局限于答案匹配，难以全面评估代理在复杂任务中的推理过程，尤其在步骤增多时，缺乏对效率、幻觉和适应性等关键方面的有效评估手段。

Method: 提出TRACE框架，利用证据库存储推理过程中的知识，支持对代理推理轨迹的多维度分析；构建包含多样化错误轨迹的元评估数据集，并标注多方面性能分数以验证框架有效性。

Result: 实验表明，即使使用小型开源LLM，TRACE也能准确、可扩展且低成本地评估代理的复杂行为，并揭示了以往未报告的代理行为特征。

Conclusion: TRACE为工具增强型LLM代理提供了有效的轨迹评估方案，能够在无完整真值轨迹的情况下实现细粒度、多维度的性能评估，推动代理系统的可靠性和透明度提升。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [157] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 本文提出了机器学习中的“目标满足假设”（OSA），指出在现实条件下该假设必然失败，因技术误差和意图误设导致系统偏离预期目标，可能引发类似古德哈特法则的失控风险，因此需对通用人工智能的优化设置原则性限制。


<details>
  <summary>Details</summary>
Motivation: 作者旨在挑战机器学习中普遍却未经充分检验的假设——训练能产生满足指定目标函数的模型，并揭示忽略目标偏差所带来的严重后果，特别是在强优化压力下可能导致系统失控。

Method: 采用跨学习范式的分析框架，结合近来的数学成果，论证近似、估计与优化误差如何导致系统性偏离目标，并讨论开发者意图形式化过程中的固有局限性。

Result: 证明了在现实条件下OSA无法成立，目标偏差不可避免，且这些偏差在强优化下会演变为类似Goodhart法则的失效模式，而其临界点无法事先确定。

Conclusion: 为防止通用人工智能系统在持续优化中进入可预测且不可逆的失控状态，必须对其优化程度施加原则性限制。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [158] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: 提出BayesianRouter，一种结合离线学习与在线贝叶斯选择的混合奖励模型路由框架，有效提升大语言模型对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于单一奖励模型的强化学习对齐方法存在局限性，且现有路由方法面临冷启动和探索不足问题。

Method: 采用两阶段方法：离线阶段通过多任务路由器学习各奖励模型的可靠性；在线阶段使用贝叶斯Thompson采样进行查询级奖励模型选择，并以离线嵌入作为高斯先验动态更新后验。

Result: 在多个指令遵循和推理基准上，BayesianRouter均优于单个奖励模型、集成方法及现有路由方法。

Conclusion: BayesianRouter通过融合离线建模与在线自适应选择，显著提升了奖励模型路由的有效性和鲁棒性。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [159] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: MaskGRPO是首个能够在离散扩散模型中实现可扩展多模态强化学习的有效方法，通过改进重要性采样和rollout策略，提升了推理性能与生成质量。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在奖励优化方面面临挑战，传统强化学习方法如GRPO难以有效应用，尤其是在非自回归框架下重要性采样和rollout过程不可行。

Method: 提出MaskGRPO，建立离散扩散模型的理论基础，设计捕捉token波动的重要性估计器，并针对视觉序列定制rollout方法以生成多样化结果和稳定梯度。

Result: 在数学推理、代码生成和视觉生成等多个基准上，MaskGRPO实现了更稳定高效的更新，显著提升推理能力和生成质量。

Conclusion: MaskGRPO为离散扩散模型提供了系统性的策略优化框架，是首个适用于离散化视觉扩散的实际强化学习方法。

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [160] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: 本文探讨了可解释人工智能（XAI）方法中隐含的本体论和认识论假设，指出这些技术性选择背后存在深刻的哲学问题，忽视这些假设可能影响AI解释的有效性和适用性。


<details>
  <summary>Details</summary>
Motivation: 由于当前主流的深度学习模型缺乏可解释性，限制了其可信度与应用，XAI方法应运而生；但这些方法常基于技术人员的预设，忽略了‘解释’本身的哲学基础。

Method: 通过分析不同XAI方法中的本体论和认识论假设，揭示技术设计与哲学前提之间的联系，并讨论在不同应用场景中如何选择和调整合适的XAI方法。

Result: 发现XAI方法中的微小技术改动可能对应着对‘解释’本质的根本性不同假设，且忽略这些哲学基础可能导致解释无效或误读。

Conclusion: 选择XAI方法时必须考虑其背后的哲学范式，应根据具体应用领域审慎选择和调整解释方法，以确保解释的有效性和合理性。

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [161] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文提出了首个针对基于规则的智能环境的反事实解释的形式化方法和实现，并通过用户研究比较了反事实解释与传统因果解释的效果，发现用户偏好具有情境依赖性。


<details>
  <summary>Details</summary>
Motivation: 在基于规则的智能环境中，缺乏生成反事实解释的有效方法，而可解释性日益成为关键需求。

Method: 提出一种面向规则型智能环境的反事实解释形式化框架，并作为插件集成到现有解释引擎中；通过N=17的用户研究对比反事实与因果解释的使用效果。

Result: 用户更偏好因果解释的语言简洁性和在时间压力下的适用性，但在需采取行动解决问题时更倾向反事实解释的可操作性内容。

Conclusion: 本工作为智能环境提供了实用的新型解释框架，并通过实证结果指导不同情境下解释类型的选择。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [162] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: 该研究探讨了现代AI系统在不完整训练条件下的抽象推理能力，发现Transformer等模型在面对训练中未见的结构规则时性能显著下降，揭示了当前模型在真正抽象推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了检验AI系统是否具备真正的类比推理能力，而非依赖统计捷径，研究关注模型在训练中未包含结构规则情况下的泛化能力。

Method: 在I-RAVEN数据集上评估了序列到序列Transformer模型和基于视觉的CoPINet、Dual-Contrast Network等架构，通过故意省略部分结构规则进行不完整训练，测试模型对新规则的推理能力。

Result: 实验表明，尽管Transformer在熟悉规则上表现良好，但在面对新规则时准确率急剧下降；同时，token级准确率与完整答案准确率之间存在显著差距，反映出模型推理的不完整性。

Conclusion: 当前深度学习模型在抽象推理方面仍存在根本局限，需发展超越模式识别、具备更强泛化能力的新型架构。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [163] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体协作的可视化自动化系统CoDA，通过专业化LLM智能体实现元数据解析、任务规划、代码生成与自我反思，显著提升了复杂数据集下的可视化生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有可视化自动化系统在处理多文件复杂数据集和迭代优化时表现不佳，且难以应对数据复杂性、代码错误和可视化质量控制问题，亟需更鲁棒的解决方案。

Method: 将自然语言到可视化的任务重构为协作式多智能体问题，设计包含元数据分析师、任务规划师、代码生成器和自省模块的专用LLM代理系统，并形式化其工作流程。

Result: 实验表明CoDA在整体评分上显著优于基线模型，最高提升达41.5%，尤其在复杂数据集和质量驱动的迭代优化中表现突出。

Conclusion: 可视化自动化应转向集成化、协作式的智能体工作流，而非孤立的代码生成，CoDA展示了这一范式的有效性与前景。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [164] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 本文提出了Coevolutionary Continuous Discrete Diffusion (CCDD)，一种联合连续表示空间和离散标记空间的扩散语言模型，通过在联合空间中同时去噪，兼顾了表达能力和训练可行性，在真实任务的语言建模实验中表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管连续扩散模型理论上具有更强的表达能力，但在实践中表现不如离散模型，主要由于从连续空间解码到离散标记困难。本文旨在解决这一理论与实践之间的矛盾。

Method: 提出CCDD模型，定义在连续表示空间和离散标记空间并集上的联合多模态扩散过程，使用单一模型在联合空间中同时进行去噪，并设计了有效的架构及训练/采样技术。

Result: CCDD在多个真实世界语言建模任务中展现出强大的实证性能，优于现有方法。

Conclusion: 连续扩散语言模型无需局限于离散空间，CCDD通过融合连续与离散模态，在保持高表达性的同时提升了可训练性和生成质量，为扩散语言模型提供了新方向。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [165] [Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query Variants for Effective QPP](https://arxiv.org/abs/2510.02512)
*Fangzheng Tian,Debasis Ganguly,Craig Macdonald*

Main category: cs.IR

TL;DR: 本文提出了一种通过从训练集中检索查询变体（QVs）来提升查询性能预测（QPP）效果的方法，利用1-hop和2-hop检索提高召回率，在TREC DL'19和DL'20数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询扩展或非上下文嵌入生成查询变体的方法容易引入主题漂移和幻觉问题，影响QPP性能，因此需要更准确的QV获取方式。

Method: 从训练集（如MS MARCO）中检索目标查询的查询变体，先通过直接检索获得1-hop QVs，再利用其相关文档进行二次检索得到2-hop QVs，以提高信息需求相似查询的召回率。

Result: 在TREC DL'19和DL'20上的实验表明，所提方法结合神经排序模型（如MonoT5）使QPP性能相比现有最佳生成式QV方法提升约20%。

Conclusion: 通过多跳检索从训练集获取高质量查询变体能有效提升QPP的准确性，尤其适用于神经排序场景下的性能预测。

Abstract: Leveraging query variants (QVs), i.e., queries with potentially similar
information needs to the target query, has been shown to improve the
effectiveness of query performance prediction (QPP) approaches. Existing
QV-based QPP methods generate QVs facilitated by either query expansion or
non-contextual embeddings, which may introduce topical drifts and
hallucinations. In this paper, we propose a method that retrieves QVs from a
training set (e.g., MS MARCO) for a given target query of QPP. To achieve a
high recall in retrieving queries with the most similar information needs as
the target query from a training set, we extend the directly retrieved QVs
(1-hop QVs) by a second retrieval using their denoted relevant documents (which
yields 2-hop QVs). Our experiments, conducted on TREC DL'19 and DL'20, show
that the QPP methods with QVs retrieved by our method outperform the
best-performing existing generated-QV-based QPP approaches by as much as around
20\%, on neural ranking models like MonoT5.

</details>


### [166] [A Simple but Effective Elaborative Query Reformulation Approach for Natural Language Recommendation](https://arxiv.org/abs/2510.02656)
*Qianfeng Wen,Yifan Liu,Justin Cui,Joshua Zhang,Anton Korikov,George-Kirollos Saad,Scott Sanner*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的查询重构方法EQR，结合广度和深度生成信息丰富的子主题，以改进自然语言推荐系统对宽泛和间接意图查询的处理能力，并在旅行、酒店和餐厅领域构建了新的基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言推荐系统在处理表达宽泛或间接用户意图的复杂查询时表现不佳，且现有的查询重构方法通常只关注扩展查询子主题的广度或深化其含义，未能兼顾二者。

Method: 提出EQR（Elaborative Subtopic Query Reformulation），利用大语言模型同时扩展查询的子主题广度并进行语义深化，生成富含信息的重构查询。

Result: 在新构建的旅游、酒店和餐饮领域的自然语言推荐基准上，EQR在多个评估指标上显著优于当前最先进的查询重构方法。

Conclusion: 结合广度与深度的查询重构策略能有效提升自然语言推荐系统对复杂查询的理解能力，EQR是一种简单但高效的解决方案。

Abstract: Natural Language (NL) recommender systems aim to retrieve relevant items from
free-form user queries and item descriptions. Existing systems often rely on
dense retrieval (DR), which struggles to interpret challenging queries that
express broad (e.g., "cities for youth friendly activities") or indirect (e.g.,
"cities for a high school graduation trip") user intents. While query
reformulation (QR) has been widely adopted to improve such systems, existing QR
methods tend to focus only on expanding the range of query subtopics (breadth)
or elaborating on the potential meaning of a query (depth), but not both. In
this paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large
language model-based QR method that combines both breadth and depth by
generating potential query subtopics with information-rich elaborations. We
also introduce three new natural language recommendation benchmarks in travel,
hotel, and restaurant domains to establish evaluation of NL recommendation with
challenging queries. Experiments show EQR substantially outperforms
state-of-the-art QR methods in various evaluation metrics, highlighting that a
simple yet effective QR approach can significantly improve NL recommender
systems for queries with broad and indirect user intents.

</details>


### [167] [Less LLM, More Documents: Searching for Improved RAG](https://arxiv.org/abs/2510.02657)
*Jingjie Ning,Yibo Kong,Yunfan Long,Jamie Callan*

Main category: cs.IR

TL;DR: 扩大检索器的语料库规模可以有效增强检索增强生成（RAG）系统的性能，减少对大规模语言模型的依赖，尤其对中等规模生成器效果最显著。


<details>
  <summary>Details</summary>
Motivation: 探索在不增加语言模型规模的前提下，通过扩大检索器语料库来提升RAG系统性能的可能性，以降低成本并提高可部署性。

Method: 通过实验分析不同规模的生成器与不同大小的检索语料库组合下的RAG性能，评估语料库扩展对准确率的影响，并分析答案覆盖度和利用效率的变化。

Result: 语料库扩展能持续增强RAG性能，在许多情况下可替代增大模型规模；中小模型配合大语料库常优于大模型配小语料库；中等模型收益最大，而极小或极大模型受益较少；性能提升主要源于答案相关段落的覆盖率增加，利用率变化不大。

Conclusion: 建立了一种有原则的语料库与生成器之间的权衡关系：投资更大语料库是提升RAG性能的有效途径，效果常可媲美甚至替代扩大语言模型本身。

Abstract: Retrieval-Augmented Generation (RAG) couples document retrieval with large
language models (LLMs). While scaling generators improves accuracy, it also
raises cost and limits deployability. We explore an orthogonal axis: enlarging
the retriever's corpus to reduce reliance on large LLMs. Experimental results
show that corpus scaling consistently strengthens RAG and can often serve as a
substitute for increasing model size, though with diminishing returns at larger
scales. Small- and mid-sized generators paired with larger corpora often rival
much larger models with smaller corpora; mid-sized models tend to gain the
most, while tiny and large models benefit less. Our analysis shows that
improvements arise primarily from increased coverage of answer-bearing
passages, while utilization efficiency remains largely unchanged. These
findings establish a principled corpus-generator trade-off: investing in larger
corpora offers an effective path to stronger RAG, often comparable to enlarging
the LLM itself.

</details>


### [168] [AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems](https://arxiv.org/abs/2510.02668)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.IR

TL;DR: 本文提出了AgenticRAG，一种结合工具增强型基础模型与检索增强生成的零样本可解释推荐框架，通过外部工具调用、知识检索和思维链推理实现透明决策，在多个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基础模型在推荐系统中的应用受限于推理不透明和知识约束，缺乏无需任务特定训练即可实现可解释推荐的有效框架。

Method: 提出AgenticRAG框架，整合外部工具调用、知识检索和思维链推理，构建自主推荐代理，实现零样本、可解释的推荐生成。

Result: 在Amazon Electronics、MovieLens-1M和Yelp三个真实数据集上，NDCG@10分别提升了0.4%、0.8%和1.6%，且具备良好可解释性和计算效率。

Conclusion: AgenticRAG能有效提升推荐系统的性能与可解释性，同时保持计算高效，为零样本可解释推荐提供了新思路。

Abstract: Foundation models have revolutionized artificial intelligence, yet their
application in recommender systems remains limited by reasoning opacity and
knowledge constraints. This paper introduces AgenticRAG, a novel framework that
combines tool-augmented foundation models with retrieval-augmented generation
for zero-shot explainable recommendations. Our approach integrates external
tool invocation, knowledge retrieval, and chain-of-thought reasoning to create
autonomous recommendation agents capable of transparent decision-making without
task-specific training. Experimental results on three real-world datasets
demonstrate that AgenticRAG achieves consistent improvements over
state-of-the-art baselines, with NDCG@10 improvements of 0.4\% on Amazon
Electronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The framework
exhibits superior explainability while maintaining computational efficiency
comparable to traditional methods.

</details>


### [169] [OpenZL: A Graph-Based Model for Compression](https://arxiv.org/abs/2510.03203)
*Yann Collet,Nick Terrell,W. Felix Handte,Danielle Rozenblit,Victor Zhang,Kevin Zhang,Yaelle Goldschlag,Jennifer Lee,Daniel Riegel,Stan Angelov,Nadav Rotem*

Main category: cs.IR

TL;DR: 本文提出了一种基于“图模型”的新型压缩框架OpenZL，通过将压缩表示为模块化编解码器的有向无环图，实现高效、可复用、自描述的压缩格式，在保持高压缩比和高速度的同时，显著提升了开发效率和部署便捷性。


<details>
  <summary>Details</summary>
Motivation: 通用无损压缩研究往往以资源消耗为代价提升压缩率，难以在生产环境中应用；而专用压缩器虽高效但通用性差、维护困难，亟需一种兼具高性能与高可维护性的新压缩范式。

Method: 提出“图模型”压缩理论框架，将压缩过程建模为模块化编解码器组成的有向无环图，并基于此设计OpenZL系统，支持生成自描述的压缩格式和通用解码器，同时构建标准化组件库以提升安全性与可维护性。

Result: 实验表明，OpenZL在多种真实数据集上优于最先进的通用压缩器，兼具更高的压缩比和速度；Meta内部部署显示压缩效果提升且开发周期从数月缩短至数天。

Conclusion: OpenZL通过图模型实现了高性能、易扩展、易维护的压缩方案，为现代数据密集型应用提供了一种更实用、可扩展且可持续的压缩解决方案。

Abstract: Research in general-purpose lossless compression over the last decade has
largely found improvements in compression ratio that come at great cost to
resource utilization and processing throughput. However, most production
workloads require high throughput and low resource utilization, so most
research systems have seen little adoption. Instead, real world improvements in
compression are increasingly often realized by building application-specific
compressors which can exploit knowledge about the structure and semantics of
the data being compressed. These systems easily outperform even the best
generic compressors, but application-specific compression schemes are not
without drawbacks. They are inherently limited in applicability and are
difficult to maintain and deploy.
  We show that these challenges can be overcome with a new way of thinking
about compression. We propose the ``graph model'' of compression, a new
theoretical framework for representing compression as a directed acyclic graph
of modular codecs. This motivates OpenZL, an implementation of this model that
compresses data into a self-describing wire format, any configuration of which
can be decompressed by a universal decoder. OpenZL's design enables rapid
development of tailored compressors with minimal code, its universal decoder
eliminates deployment lag, and its investment in a well-vetted standard
component library minimizes security risks. Experimental results demonstrate
that OpenZL achieves superior compression ratios and speeds compared to
state-of-the-art general-purpose compressors on a variety of real-world
datasets. Internal deployments at Meta have also shown consistent improvements
in size and/or speed, with development timelines reduced from months to days.
OpenZL thus represents an advance in practical, scalable, and maintainable data
compression for modern data-intensive applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [170] [ERUPT: An Open Toolkit for Interfacing with Robot Motion Planners in Extended Reality](https://arxiv.org/abs/2510.02464)
*Isaac Ngui,Courtney McBeth,André Santos,Grace He,Katherine J. Mimnaugh,James D. Motes,Luciano Soares,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本文提出了ERUPT，一个用于交互式运动规划的扩展现实（XR）系统，支持用户在三维沉浸式环境中动态配置环境并规划机器人路径，集成MoveIt框架，可在虚拟或增强现实中可视化机器人运动，并部署到真实机器人。


<details>
  <summary>Details</summary>
Motivation: 传统基于二维屏幕和鼠标键盘的机器人路径规划方式缺乏空间直观性和自然交互能力，限制了规划效率和准确性。

Method: 开发了一个名为ERUPT的扩展现实系统，结合虚拟现实与增强现实技术，集成MoveIt运动规划框架，提供多种交互方式，使用户能在三维环境中直接操作物体和机器人。

Result: 用户能够在沉浸式XR环境中创建和修改环境、发送运动规划请求、可视化机器人路径，并在无碰撞风险的虚拟空间中验证行为，最终将路径部署到物理机器人。

Conclusion: ERUPT提升了机器人运动规划的空间理解与交互自然性，有效支持从虚拟规划到现实执行的无缝过渡。

Abstract: We propose the Extended Reality Universal Planning Toolkit (ERUPT), an
extended reality (XR) system for interactive motion planning. Our system allows
users to create and dynamically reconfigure environments while they plan robot
paths. In immersive three-dimensional XR environments, users gain a greater
spatial understanding. XR also unlocks a broader range of natural interaction
capabilities, allowing users to grab and adjust objects in the environment
similarly to the real world, rather than using a mouse and keyboard with the
scene projected onto a two-dimensional computer screen. Our system integrates
with MoveIt, a manipulation planning framework, allowing users to send motion
planning requests and visualize the resulting robot paths in virtual or
augmented reality. We provide a broad range of interaction modalities, allowing
users to modify objects in the environment and interact with a virtual robot.
Our system allows operators to visualize robot motions, ensuring desired
behavior as it moves throughout the environment, without risk of collisions
within a virtual space, and to then deploy planned paths on physical robots in
the real world.

</details>


### [171] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为SIMSplat的预测性驾驶场景编辑器，结合语言对齐的高斯点阵化技术，支持通过自然语言指令对驾驶场景进行直观、精细且可预测的编辑。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶模拟框架在生成逼真场景时效率低下，编辑能力有限，难以满足复杂自动驾驶测试需求。

Method: 利用语言与高斯重建场景的对齐实现自然语言控制，支持道路对象的查询与操作，并结合多智能体运动预测进行路径优化，实现车辆与行人的轨迹修改和新增对象的合理融入。

Result: 在Waymo数据集上的实验表明，SIMSplat具备广泛的场景适应性和强大的编辑能力，能生成具有真实交互的复杂驾驶场景。

Conclusion: SIMSplat为自动驾驶仿真提供了一种高效、灵活且语义可控的场景编辑方案，显著提升了生成场景的真实性和实用性。

Abstract: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>


### [172] [U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation](https://arxiv.org/abs/2510.02526)
*Anamika J H,Anujith Muraleedharan*

Main category: cs.RO

TL;DR: 本文提出U-LAG，一种在执行过程中动态重定向目标的模块化方法，核心是UAR-PF算法，能在感知延迟和噪声下基于不确定性估计调整操作目标，提升机器人在动态环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人在动态环境中面临感知延迟、噪声和状态过时的问题，传统控制方法难以实时适应变化，因此需要一种能在执行中灵活调整目标的机制。

Method: 提出U-LAG框架和UAR-PF算法，通过维护物体位姿的概率分布，在感知滞后下选择能最大化预期进展的目标；在PyBullet/PandaGym中构建Shift x Lag压力测试，模拟物体突变和感知延迟。

Result: 在0-10 cm位移和0-400 ms延迟下，UAR-PF相比无重定向基线表现更优，任务成功率更高，末端执行器移动更少，中断更少，且性能随简单安全措施进一步提升。

Conclusion: UAR-PF作为一种不确定性感知的重定向方法，有效提升了机器人在感知受限动态环境中的操作鲁棒性，同时U-LAG的模块化设计便于集成，Shift x Lag基准为后续研究提供了可复现的测试平台。

Abstract: Robots manipulating in changing environments must act on percepts that are
late, noisy, or stale. We present U-LAG, a mid-execution goal-retargeting layer
that leaves the low-level controller unchanged while re-aiming task goals
(pre-contact, contact, post) as new observations arrive. Unlike motion
retargeting or generic visual servoing, U-LAG treats in-flight goal re-aiming
as a first-class, pluggable module between perception and control. Our main
technical contribution is UAR-PF, an uncertainty-aware retargeter that
maintains a distribution over object pose under sensing lag and selects goals
that maximize expected progress. We instantiate a reproducible Shift x Lag
stress test in PyBullet/PandaGym for pick, push, stacking, and peg insertion,
where the object undergoes abrupt in-plane shifts while synthetic perception
lag is injected during approach. Across 0-10 cm shifts and 0-400 ms lags,
UAR-PF and ICP degrade gracefully relative to a no-retarget baseline, achieving
higher success with modest end-effector travel and fewer aborts; simple
operational safeguards further improve stability. Contributions: (1) UAR-PF for
lag-adaptive, uncertainty-aware goal retargeting; (2) a pluggable retargeting
interface; and (3) a reproducible Shift x Lag benchmark with evaluation on
pick, push, stacking, and peg insertion.

</details>


### [173] [A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models](https://arxiv.org/abs/2510.02538)
*Yilin Wang,Shangzhe Li,Haoyi Niu,Zhiao Huang,Weitong Zhang,Hao Su*

Main category: cs.RO

TL;DR: 提出一种基于世界模型的sim-to-real框架，通过在线模仿预训练和离线微调相结合的方法，利用机器人模拟器解决真实世界专家数据有限下的模仿学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有离线模仿学习方法在真实世界专家数据有限的情况下常面临数据覆盖不足和性能严重下降的问题。

Method: 采用基于世界模型的sim-to-real框架，结合在线模仿预训练与离线微调，利用模拟环境中的在线交互来增强数据覆盖。

Result: 在sim-to-sim和sim-to-real迁移中，成功率分别比现有离线模仿学习基线提高了至少31.7%和23.3%。

Conclusion: 该方法有效缓解了离线方法的数据覆盖限制，提升了鲁棒性、泛化能力和跨域迁移性能。

Abstract: We are interested in solving the problem of imitation learning with a limited
amount of real-world expert data. Existing offline imitation methods often
struggle with poor data coverage and severe performance degradation. We propose
a solution that leverages robot simulators to achieve online imitation
learning. Our sim-to-real framework is based on world models and combines
online imitation pretraining with offline finetuning. By leveraging online
interactions, our approach alleviates the data coverage limitations of offline
methods, leading to improved robustness and reduced performance degradation
during finetuning. It also enhances generalization during domain transfer. Our
empirical results demonstrate its effectiveness, improving success rates by at
least 31.7% in sim-to-sim transfer and 23.3% in sim-to-real transfer over
existing offline imitation learning baselines.

</details>


### [174] [Efficient Optimal Path Planning in Dynamic Environments Using Koopman MPC](https://arxiv.org/abs/2510.02584)
*Mohammad Abtahi,Navid Mojahed,Shima Nazari*

Main category: cs.RO

TL;DR: 提出一种基于Koopman算子理论的数据驱动模型预测控制框架，用于移动机器人在动态环境中的导航，通过双线性Koopman模型实现非线性路径规划问题的高效求解，计算速度比非线性MPC快320倍。


<details>
  <summary>Details</summary>
Motivation: 传统Koopman方法仅线性化系统动力学，难以处理包含非线性动力学和避障约束的路径规划问题，因此需要一种能全局线性表示最优路径规划的方法。

Method: 采用扩展动态模态分解（EDMD）从输入-状态数据中识别线性和双线性Koopman实现，并在提升空间中构建包含移动障碍物的二次规划路径规划问题，结合MPC框架求解最优控制。

Result: 双线性Koopman模型能准确捕捉非线性状态-输入耦合和避障所需的二次项，而线性模型则不能；所提方法在计算速度上比非线性MPC快320倍。

Conclusion: 双线性Koopman实现可有效线性化高度非线性的最优控制问题，在满足非线性约束的同时实现类似线性问题的计算效率，具有显著应用潜力。

Abstract: This paper presents a data-driven model predictive control framework for
mobile robots navigating in dynamic environments, leveraging Koopman operator
theory. Unlike the conventional Koopman-based approaches that focus on the
linearization of system dynamics only, our work focuses on finding a global
linear representation for the optimal path planning problem that includes both
the nonlinear robot dynamics and collision-avoidance constraints. We deploy
extended dynamic mode decomposition to identify linear and bilinear Koopman
realizations from input-state data. Our open-loop analysis demonstrates that
only the bilinear Koopman model can accurately capture nonlinear state-input
couplings and quadratic terms essential for collision avoidance, whereas linear
realizations fail to do so. We formulate a quadratic program for the robot path
planning in the presence of moving obstacles in the lifted space and determine
the optimal robot action in an MPC framework. Our approach is capable of
finding the safe optimal action 320 times faster than a nonlinear MPC
counterpart that solves the path planning problem in the original state space.
Our work highlights the potential of bilinear Koopman realizations for
linearization of highly nonlinear optimal control problems subject to nonlinear
state and input constraints to achieve computational efficiency similar to
linear problems.

</details>


### [175] [SubSense: VR-Haptic and Motor Feedback for Immersive Control in Subsea Telerobotics](https://arxiv.org/abs/2510.02594)
*Ruo Chen,David Blow,Adnan Abdullah,Md Jahidul Islam*

Main category: cs.RO

TL;DR: 提出了一种名为SubSense的VR-触觉框架，通过结合非侵入式反馈接口和虚拟现实技术，提升水下ROV遥操作的沉浸感与情境感知能力。


<details>
  <summary>Details</summary>
Motivation: 传统ROV遥操作依赖低分辨率2D摄像头，缺乏感官反馈，导致复杂水下环境中情境感知不足。

Method: 开发了SubSense框架，将1-DOF机械臂与操作员手套配对，提供触觉反馈和抓取状态，并集成端到端软件通过VR平台显示沉浸式视角。

Result: 实验和用户研究表明，该系统在精细操作任务中显著优于传统界面，提升了情境感知和任务表现。

Conclusion: 多感官反馈结合沉浸式虚拟环境可显著改善远程操作体验，使ROV操作更直观、易用。

Abstract: This paper investigates the integration of haptic feedback and virtual
reality (VR) control interfaces to enhance teleoperation and telemanipulation
of underwater ROVs (remotely operated vehicles). Traditional ROV teleoperation
relies on low-resolution 2D camera feeds and lacks immersive and sensory
feedback, which diminishes situational awareness in complex subsea
environments. We propose SubSense -- a novel VR-Haptic framework incorporating
a non-invasive feedback interface to an otherwise 1-DOF (degree of freedom)
manipulator, which is paired with the teleoperator's glove to provide haptic
feedback and grasp status. Additionally, our framework integrates end-to-end
software for managing control inputs and displaying immersive camera views
through a VR platform. We validate the system through comprehensive experiments
and user studies, demonstrating its effectiveness over conventional
teleoperation interfaces, particularly for delicate manipulation tasks. Our
results highlight the potential of multisensory feedback in immersive virtual
environments to significantly improve remote situational awareness and mission
performance, offering more intuitive and accessible ROV operations in the
field.

</details>


### [176] [UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies](https://arxiv.org/abs/2510.02614)
*Harsh Gupta,Xiaofeng Guo,Huy Ha,Chuer Pan,Muqing Cao,Dongjae Lee,Sebastian Sherer,Shuran Song,Guanya Shi*

Main category: cs.RO

TL;DR: 本文提出了UMI-on-Air框架，利用手持抓取器收集的人类演示训练可泛化的视觉运动策略，并通过引入具身感知扩散策略（EADP）实现对不同机器人形态的适应性部署，尤其在空中操作任务中表现出更高的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 将通用操作策略迁移到受约束的机器人平台（如空中机械臂）时，常因控制与动力学不匹配导致性能下降，因此需要一种能适应不同具身特性的部署方法。

Method: 提出Embodiment-Aware Diffusion Policy（EADP），在推理时结合高层UMI策略与低层具身特定控制器，并在扩散采样过程中引入控制器跟踪代价的梯度反馈，引导生成符合目标平台动力学的轨迹。

Result: 在多个长视野、高精度的空中操作任务中验证了方法的有效性，相比无引导的扩散模型基线，提升了成功率、执行效率和抗干扰能力，并成功部署于未见过的环境中。

Conclusion: UMI-on-Air提供了一种即插即用的具身感知策略部署方案，支持利用野外收集的人类演示将可泛化操作技能扩展到多样且高度受限的机器人平台上。

Abstract: We introduce UMI-on-Air, a framework for embodiment-aware deployment of
embodiment-agnostic manipulation policies. Our approach leverages diverse,
unconstrained human demonstrations collected with a handheld gripper (UMI) to
train generalizable visuomotor policies. A central challenge in transferring
these policies to constrained robotic embodiments-such as aerial
manipulators-is the mismatch in control and robot dynamics, which often leads
to out-of-distribution behaviors and poor execution. To address this, we
propose Embodiment-Aware Diffusion Policy (EADP), which couples a high-level
UMI policy with a low-level embodiment-specific controller at inference time.
By integrating gradient feedback from the controller's tracking cost into the
diffusion sampling process, our method steers trajectory generation towards
dynamically feasible modes tailored to the deployment embodiment. This enables
plug-and-play, embodiment-aware trajectory adaptation at test time. We validate
our approach on multiple long-horizon and high-precision aerial manipulation
tasks, showing improved success rates, efficiency, and robustness under
disturbances compared to unguided diffusion baselines. Finally, we demonstrate
deployment in previously unseen environments, using UMI demonstrations
collected in the wild, highlighting a practical pathway for scaling
generalizable manipulation skills across diverse-and even highly
constrained-embodiments. All code, data, and checkpoints will be publicly
released after acceptance. Result videos can be found at umi-on-air.github.io.

</details>


### [177] [RSV-SLAM: Toward Real-Time Semantic Visual SLAM in Indoor Dynamic Environments](https://arxiv.org/abs/2510.02616)
*Mobin Habibpour,Alireza Nemati,Ali Meghdari,Alireza Taheri,Shima Nazari*

Main category: cs.RO

TL;DR: 提出了一种面向动态环境的实时语义RGBD SLAM方法，结合深度学习语义信息与扩展卡尔曼滤波，有效抑制动态物体影响，并在TUM数据集上实现了具有竞争力的定位精度和约22 fps的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉SLAM方法多假设环境静态，在动态环境中表现不佳，尤其在社交机器人等存在移动物体的场景中难以保持鲁棒的相机跟踪和地图构建。

Method: 提出一种基于语义RGBD的SLAM框架，引入深度学习进行语义分割，并结合扩展卡尔曼滤波识别暂时静止的动态物体；使用生成网络修复被动态物体遮挡的图像区域，维护静态地图以提升跟踪鲁棒性。系统在ROS平台上实现，模块化设计支持实时运行。

Result: 在TUM动态数据集上测试表明，该方法在接近实时（约22 fps）运行的同时，定位误差具有与当前最先进方法相媲美的竞争力。

Conclusion: 通过融合语义信息与滤波优化，所提出的SLAM系统能有效应对动态环境挑战，兼顾精度与效率，适用于社交机器人等实际应用场景。

Abstract: Simultaneous Localization and Mapping (SLAM) plays an important role in many
robotics fields, including social robots. Many of the available visual SLAM
methods are based on the assumption of a static world and struggle in dynamic
environments. In the current study, we introduce a real-time semantic RGBD SLAM
approach designed specifically for dynamic environments. Our proposed system
can effectively detect moving objects and maintain a static map to ensure
robust camera tracking. The key innovation of our approach is the incorporation
of deep learning-based semantic information into SLAM systems to mitigate the
impact of dynamic objects. Additionally, we enhance the semantic segmentation
process by integrating an Extended Kalman filter to identify dynamic objects
that may be temporarily idle. We have also implemented a generative network to
fill in the missing regions of input images belonging to dynamic objects. This
highly modular framework has been implemented on the ROS platform and can
achieve around 22 fps on a GTX1080. Benchmarking the developed pipeline on
dynamic sequences from the TUM dataset suggests that the proposed approach
delivers competitive localization error in comparison with the state-of-the-art
methods, all while operating in near real-time. The source code is publicly
available.

</details>


### [178] [Reachable Predictive Control: A Novel Control Algorithm for Nonlinear Systems with Unknown Dynamics and its Practical Applications](https://arxiv.org/abs/2510.02623)
*Taha Shafa,Yiming Meng,Melkior Ornik*

Main category: cs.RO

TL;DR: 提出一种无需先验系统动力学知识即可驱动系统沿分段线性轨迹运行的算法，通过局部学习动态并计算可达状态集来实现鲁棒控制。


<details>
  <summary>Details</summary>
Motivation: 针对系统动力学突变的临界故障场景，需在未知动力学情况下仍能跟踪路径。

Method: 通过施加小扰动局部学习当前状态附近的系统动力学，结合最大增长率边界计算可证明的可达状态集，并综合控制动作引导系统至可达状态。

Result: 证明了一组由可达状态组成的航路点可以在未知系统动力学的情况下被跟踪。

Conclusion: 所提算法能够在不依赖系统先验动力学信息的前提下，实现对分段线性轨迹的有效跟踪，具有较强的鲁棒性。

Abstract: This paper proposes an algorithm capable of driving a system to follow a
piecewise linear trajectory without prior knowledge of the system dynamics.
Motivated by a critical failure scenario in which a system can experience an
abrupt change in its dynamics, we demonstrate that it is possible to follow a
set of waypoints comprised of states analytically proven to be reachable
despite not knowing the system dynamics. The proposed algorithm first applies
small perturbations to locally learn the system dynamics around the current
state, then computes the set of states that are provably reachable using the
locally learned dynamics and their corresponding maximum growth-rate bounds,
and finally synthesizes a control action that navigates the system to a
guaranteed reachable state.

</details>


### [179] [Multi-robot Rigid Formation Navigation via Synchronous Motion and Discrete-time Communication-Control Optimization](https://arxiv.org/abs/2510.02624)
*Qun Yang,Soung Chang Liew*

Main category: cs.RO

TL;DR: 提出了一种名为“hold-and-hit”的通信-控制框架，结合周期内优化方法，实现多机器人刚性编队在复杂曲线路径上的精确导航，适用于ROS平台并在仿真和真实实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏可在微处理器平台上通过无线网络实现的多机器人编队导航综合解决方案，尤其针对需沿复杂曲线路径移动的编队。

Method: 设计了“hold-and-hit”通信-控制框架，结合离散时间周期内的优化策略，在ROS平台上运行，以应对无线延迟和丢包，并满足非完整运动约束下的轨迹跟踪需求。

Result: 仿真结果显示该方法在S形路径上优于两种现有方法；真实实验中，四机器人方阵在0.1m/s速度下保持±0.069m距离误差和±19.15°角度误差。

Conclusion: 所提框架能有效实现刚性编队在复杂路径下的鲁棒、精确导航，适用于资源受限的嵌入式系统，具有实际应用价值。

Abstract: Rigid-formation navigation of multiple robots is essential for applications
such as cooperative transportation. This process involves a team of
collaborative robots maintaining a predefined geometric configuration, such as
a square, while in motion. For untethered collaborative motion, inter-robot
communication must be conducted through a wireless network. Notably, few
existing works offer a comprehensive solution for multi-robot formation
navigation executable on microprocessor platforms via wireless networks,
particularly for formations that must traverse complex curvilinear paths. To
address this gap, we introduce a novel "hold-and-hit" communication-control
framework designed to work seamlessly with the widely-used Robotic Operating
System (ROS) platform. The hold-and-hit framework synchronizes robot movements
in a manner robust against wireless network delays and packet loss. It operates
over discrete-time communication-control cycles, making it suitable for
implementation on contemporary microprocessors. Complementary to hold-and-hit,
we propose an intra-cycle optimization approach that enables rigid formations
to closely follow desired curvilinear paths, even under the nonholonomic
movement constraints inherent to most vehicular robots. The combination of
hold-and-hit and intra-cycle optimization ensures precise and reliable
navigation even in challenging scenarios. Simulations in a virtual environment
demonstrate the superiority of our method in maintaining a four-robot square
formation along an S-shaped path, outperforming two existing approaches.
Furthermore, real-world experiments validate the effectiveness of our
framework: the robots maintained an inter-distance error within $\pm 0.069m$
and an inter-angular orientation error within $\pm19.15^{\circ}$ while
navigating along an S-shaped path at a fixed linear velocity of $0.1 m/s$.

</details>


### [180] [A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios](https://arxiv.org/abs/2510.02627)
*Ruining Yang,Yi Xu,Yixiao Chen,Yun Fu,Lili Su*

Main category: cs.RO

TL;DR: 提出了一种新的轨迹生成框架，用于增强自动驾驶中高密度场景和罕见行为的覆盖，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测数据集存在长尾分布问题，缺乏高密度场景和关键安全行为（如变道、超车、转弯）的代表性，导致模型评估过于乐观且泛化能力差。

Method: 将连续道路环境转化为结构化网格表示，结合基于规则的决策触发、Frenet坐标系下的轨迹平滑和动态可行性约束，实现行为感知的轨迹生成。

Result: 在Argoverse 1和Argoverse 2数据集上实验表明，该方法显著提高了智能体密度和行为多样性，同时保持运动真实性和场景安全性，并提升了下游预测模型在高密度场景中的性能。

Conclusion: 所提出的框架有效缓解了自动驾驶轨迹预测中数据分布不均的问题，通过合成高质量、多样化的高密度交互场景，增强了模型的实用性和鲁棒性。

Abstract: Accurate trajectory prediction is fundamental to autonomous driving, as it
underpins safe motion planning and collision avoidance in complex environments.
However, existing benchmark datasets suffer from a pronounced long-tail
distribution problem, with most samples drawn from low-density scenarios and
simple straight-driving behaviors. This underrepresentation of high-density
scenarios and safety critical maneuvers such as lane changes, overtaking and
turning is an obstacle to model generalization and leads to overly optimistic
evaluations. To address these challenges, we propose a novel trajectory
generation framework that simultaneously enhances scenarios density and
enriches behavioral diversity. Specifically, our approach converts continuous
road environments into a structured grid representation that supports
fine-grained path planning, explicit conflict detection, and multi-agent
coordination. Built upon this representation, we introduce behavior-aware
generation mechanisms that combine rule-based decision triggers with
Frenet-based trajectory smoothing and dynamic feasibility constraints. This
design allows us to synthesize realistic high-density scenarios and rare
behaviors with complex interactions that are often missing in real data.
Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets
demonstrate that our method significantly improves both agent density and
behavior diversity, while preserving motion realism and scenario-level safety.
Our synthetic data also benefits downstream trajectory prediction models and
enhances performance in challenging high-density scenarios.

</details>


### [181] [A $1000\times$ Faster LLM-enhanced Algorithm For Path Planning in Large-scale Grid Maps](https://arxiv.org/abs/2510.02716)
*Junlin Zeng,Xin Zhang,Xiang Zhao,Yan Pan*

Main category: cs.RO

TL;DR: 本文提出了一种改进的LLM增强型A*算法iLLM-A*，通过优化A*、增量学习生成高质量航点及合理选择航点机制，在大规模网格地图路径规划中实现了显著加速（平均超过1000倍）、降低内存消耗（最多58.6%）并缩短路径长度。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划方法在处理大规模网格地图时存在搜索时间长、内存消耗高问题，而当前基于大语言模型的方法存在空间幻觉和规划性能差的问题，特别是LLM-A*在大规模地图上计算时间仍然较高。

Method: 设计了iLLM-A*算法，包含三个机制：A*算法优化、用于生成高质量航点的LLM增量学习方法、以及为A*路径规划选择合适航点的策略。

Result: 在多种网格地图上的实验表明，相比LLM-A*，iLLM-A*平均实现超过1000倍的速度提升，极端情况下达2349.5倍；最多节省58.6%内存；路径更短且长度标准差更低。

Conclusion: iLLM-A*有效解决了LLM-A*在大规模地图中的性能瓶颈，显著提升了路径规划效率与质量，适用于大规模网格地图的高效路径规划。

Abstract: Path planning in grid maps, arising from various applications, has garnered
significant attention. Existing methods, such as A*, Dijkstra, and their
variants, work well for small-scale maps but fail to address large-scale ones
due to high search time and memory consumption. Recently, Large Language Models
(LLMs) have shown remarkable performance in path planning but still suffer from
spatial illusion and poor planning performance. Among all the works, LLM-A*
\cite{meng2024llm} leverages LLM to generate a series of waypoints and then
uses A* to plan the paths between the neighboring waypoints. In this way, the
complete path is constructed. However, LLM-A* still suffers from high
computational time for large-scale maps. To fill this gap, we conducted a deep
investigation into LLM-A* and found its bottleneck, resulting in limited
performance. Accordingly, we design an innovative LLM-enhanced algorithm, abbr.
as iLLM-A*. iLLM-A* includes 3 carefully designed mechanisms, including the
optimization of A*, an incremental learning method for LLM to generate
high-quality waypoints, and the selection of the appropriate waypoints for A*
for path planning. Finally, a comprehensive evaluation on various grid maps
shows that, compared with LLM-A*, iLLM-A* \textbf{1) achieves more than
$1000\times$ speedup on average, and up to $2349.5\times$ speedup in the
extreme case, 2) saves up to $58.6\%$ of the memory cost, 3) achieves both
obviously shorter path length and lower path length standard deviation.}

</details>


### [182] [Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4](https://arxiv.org/abs/2510.02728)
*Lingfeng Zhang,Erjia Xiao,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Wenbo Ding,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 提出了一种两阶段检索优化方法CGRS，用于自然语言引导的跨模态无人机导航图像检索，在RoboSense 2025挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度语义匹配方面存在不足，尤其是在复杂航拍场景中难以实现文本查询与视觉内容的精确对齐。

Method: 首先使用基线模型获取前20个候选图像的粗排序，然后利用视觉语言模型（VLM）生成这些图像的详细描述，并通过多模态相似性计算框架进行精排序。

Result: 相比基线方法在Recall@1、Recall@5和Recall@10上均提升5%，并在挑战赛中获得TOP-2成绩。

Conclusion: 所提出的语义细化策略能有效提升跨模态检索精度，具有实际应用价值。

Abstract: Cross-modal drone navigation remains a challenging task in robotics,
requiring efficient retrieval of relevant images from large-scale databases
based on natural language descriptions. The RoboSense 2025 Track 4 challenge
addresses this challenge, focusing on robust, natural language-guided
cross-view image retrieval across multiple platforms (drones, satellites, and
ground cameras). Current baseline methods, while effective for initial
retrieval, often struggle to achieve fine-grained semantic matching between
text queries and visual content, especially in complex aerial scenes. To
address this challenge, we propose a two-stage retrieval refinement method:
Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse
ranking through intelligent reranking. Our method first leverages a baseline
model to obtain an initial coarse ranking of the top 20 most relevant images
for each query. We then use Vision-Language-Model (VLM) to generate detailed
captions for these candidate images, capturing rich semantic descriptions of
their visual content. These generated captions are then used in a multimodal
similarity computation framework to perform fine-grained reranking of the
original text query, effectively building a semantic bridge between the visual
content and natural language descriptions. Our approach significantly improves
upon the baseline, achieving a consistent 5\% improvement across all key
metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the
challenge, demonstrating the practical value of our semantic refinement
strategy in real-world robotic navigation scenarios.

</details>


### [183] [Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data](https://arxiv.org/abs/2510.02738)
*Tianyu Li,Yihan Li,Zizhe Zhang,Nadia Figueroa*

Main category: cs.RO

TL;DR: 本文提出了一种在模拟环境中生成力感知数据的框架，并结合柔顺策略提升了基于视觉的模仿学习在接触密集型机器人操作任务中的性能，通过真实机器人实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 接触密集型任务对机器人的柔顺性和力控制有高要求，但大多数视觉运动策略忽略了力信息，导致在实际交互中表现不佳。因此需要引入力信息并解决真实场景中数据稀缺的问题。

Method: 提出一个仿真中生成力感知数据的框架，仅需单个人类演示即可实例化数据，并结合柔顺控制策略训练视觉运动策略，通过Sim2Real迁移应用于真实机器人。

Result: 在非抓取式块体翻转和双臂物体移动等真实机器人任务中，所提方法实现了稳定的接触维持和对新环境的适应能力。

Conclusion: 结合力信息与柔顺策略可有效提升视觉运动策略在接触密集任务中的鲁棒性和泛化能力，且通过少量真实演示驱动的仿真数据生成能成功跨越Sim2Real鸿沟。

Abstract: While visuomotor policy has made advancements in recent years, contact-rich
tasks still remain a challenge. Robotic manipulation tasks that require
continuous contact demand explicit handling of compliance and force. However,
most visuomotor policies ignore compliance, overlooking the importance of
physical interaction with the real world, often leading to excessive contact
forces or fragile behavior under uncertainty. Introducing force information
into vision-based imitation learning could help improve awareness of contacts,
but could also require a lot of data to perform well. One remedy for data
scarcity is to generate data in simulation, yet computationally taxing
processes are required to generate data good enough not to suffer from the
Sim2Real gap. In this work, we introduce a framework for generating
force-informed data in simulation, instantiated by a single human
demonstration, and show how coupling with a compliant policy improves the
performance of a visuomotor policy learned from synthetic data. We validate our
approach on real-robot tasks, including non-prehensile block flipping and a
bi-manual object moving, where the learned policy exhibits reliable contact
maintenance and adaptation to novel conditions. Project Website:
https://flow-with-the-force-field.github.io/webpage/

</details>


### [184] [Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving](https://arxiv.org/abs/2510.02803)
*Yifan Liao,Zhen Sun,Xiaoyun Qiu,Zixiao Zhao,Wenbing Tang,Xinlei He,Xinhu Zheng,Tianwei Zhang,Xinyi Huang,Xingshuo Han*

Main category: cs.RO

TL;DR: 本文首次系统研究了视觉语言模型（VLMs）在施工区域轨迹规划中的应用，发现主流VLMs在68%的情况下无法生成正确轨迹。作者识别出8种常见失败模式，并提出REACT-Drive框架，结合检索增强生成（RAG）提升性能，在ROADWork数据集上显著降低误差且推理速度快，实车实验验证了其实际可用性。


<details>
  <summary>Details</summary>
Motivation: 施工区域具有不规则布局、临时交通控制和动态几何结构，对VLM的轨迹规划能力构成挑战，而现有研究尚未探索该问题。

Method: 通过子图挖掘和聚类分析识别VLM在施工区轨迹规划中的失败模式，并结合人类验证确认；提出REACT-Drive框架，利用VLM将历史失败案例转化为约束规则和可执行代码，结合RAG检索相似模式以指导新场景下的轨迹生成。

Result: 在ROADWork数据集上，相比基线VLM，REACT-Drive使用Qwen2.5-VL评估时平均位移误差减少约3倍，推理时间最低（0.58秒），优于微调等方法（17.90秒）；在真实车辆的15个施工场景中验证了实用性。

Conclusion: REACT-Drive有效提升了VLM在复杂施工区域的轨迹规划准确性与效率，具备良好的现实部署潜力。

Abstract: Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in $68.0%$ of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of $8$ common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around $3\times$ in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time ($0.58$s) compared with other methods such as fine-tuning
($17.90$s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.

</details>


### [185] [Assist-as-needed Control for FES in Foot Drop Management](https://arxiv.org/abs/2510.02808)
*Andreas Christou,Elliot Lister,Georgia Andreopoulou,Don Mahad,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 提出一种基于实时足趾 Clearance 的闭环FES控制器，用于动态调节刺激强度，提供“按需辅助”，在不同步行条件下维持足够的足趾 Clearance，同时使用显著较低的刺激强度。


<details>
  <summary>Details</summary>
Motivation: 传统的开环FES控制器固定刺激强度，可能导致肌肉疲劳、不适或足背屈不足，增加跌倒风险，因此需要一种能根据实际需求动态调整刺激强度的闭环控制系统。

Method: 开发了一种新型闭环FES控制器，根据实时足趾离地高度动态调整刺激强度，并在健康受试者中模拟足下垂，比较闭环与传统开环控制器在不同行走速度和坡度下的表现。

Result: 闭环控制器在维持足够足趾 Clearance 的同时，显著降低了刺激强度，且未显著影响髋、膝、踝关节角度。

Conclusion: 所提出的闭环FES控制器不仅能达到传统系统相当的效果，还有望减少肌肉疲劳，提升长期使用的舒适性和依从性。

Abstract: Foot drop is commonly managed using Functional Electrical Stimulation (FES),
typically delivered via open-loop controllers with fixed stimulation
intensities. While users may manually adjust the intensity through external
controls, this approach risks overstimulation, leading to muscle fatigue and
discomfort, or understimulation, which compromises dorsiflexion and increases
fall risk. In this study, we propose a novel closed-loop FES controller that
dynamically adjusts the stimulation intensity based on real-time toe clearance,
providing "assistance as needed". We evaluate this system by inducing foot drop
in healthy participants and comparing the effects of the closed-loop controller
with a traditional open-loop controller across various walking conditions,
including different speeds and surface inclinations. Kinematic data reveal that
our closed-loop controller maintains adequate toe clearance without
significantly affecting the joint angles of the hips, the knees, and the
ankles, and while using significantly lower stimulation intensities compared to
the open-loop controller. These findings suggest that the proposed method not
only matches the effectiveness of existing systems but also offers the
potential for reduced muscle fatigue and improved long-term user comfort and
adherence.

</details>


### [186] [Action Deviation-Aware Inference for Low-Latency Wireless Robots](https://arxiv.org/abs/2510.02851)
*Jeyoung Park,Yeonsub Lim,Seungeun Oh,Jihong Park,Jinho Choi,Seong-Lyun Kim*

Main category: cs.RO

TL;DR: 提出了一种动作偏差感知的混合推理方法，通过预测动作是否需要验证和修正来减少通信和计算开销，在保持高任务成功率的同时显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 在6G支持的分布式机器学习中，行为克隆策略无法并行处理多个草案的验证与修正，导致延迟敏感型AI应用（如自动驾驶和机器人操作）效率低下。

Method: 引入动作偏差作为衡量标准，草案模型据此判断动作是否需目标模型验证；结合路径偏差阈值动态决定是否跳过服务器通信与计算，实现选择性跳过机制。

Result: 相比无跳过的混合推理，上行传输和服务器操作减少40%，端到端延迟降低33.32%，任务成功率可达仅使用目标模型推理的97.03%。

Conclusion: 动作偏差感知的混合推理有效平衡了通信效率与推理性能，适用于低延迟、高可靠性的分布式AI应用场景。

Abstract: To support latency-sensitive AI applications ranging from autonomous driving
to industrial robot manipulation, 6G envisions distributed ML, connecting
distributed computational resources in edge and cloud over hyper-reliable
low-latency communication (HRLLC). In this setting, speculative decoding can
facilitate collaborative inference of models distributively deployed: an
on-device draft model locally generates drafts and a remote server-based target
model verifies and corrects them, resulting lower latency. However, unlike
autoregressive text generation, behavior cloning policies, typically used for
embodied AI applications like robot manipulation and autonomous driving, cannot
parallelize verification and correction for multiple drafts as each action
depends on observation which needs to be updated by a previous action. To this
end, we propose Action Deviation-Aware Hybrid Inference, wherein the draft
model estimates an action's need for verification and correction by the target
model and selectively skips communication and computation for server
operations. Action deviation shows a strong correlation with action's rejection
probability by the target model, enabling selective skipping. We derive the
path deviation threshold that balances the transmission rate and the inference
performance, and we empirically show that action deviation-aware hybrid
inference reduces uplink transmission and server operation by 40%, while
lowering end-to-end latency by 33.32% relative to hybrid inference without
skipping and achieving task success rate up to 97.03% of that of target model
only inference.

</details>


### [187] [Novel UWB Synthetic Aperture Radar Imaging for Mobile Robot Mapping](https://arxiv.org/abs/2510.02874)
*Charith Premachandra,U-Xuan Tan*

Main category: cs.RO

TL;DR: 本文提出了一种基于超宽带（UWB）雷达合成孔径雷达（SAR）成像的移动机器人环境建图方法，能够在恶劣环境下实现高分辨率环境感知和回环检测。


<details>
  <summary>Details</summary>
Motivation: 传统外部传感器（如LiDAR和摄像头）在低能见度条件下感知能力受限，需要一种能在尘埃、烟雾和雨中有效工作的替代方案。

Method: 利用移动机器人运动合成大孔径，生成高分辨率的UWB SAR图像，并采用经典特征检测器（SIFT、SURF、BRISK、AKAZE、ORB）进行回环检测。

Result: 实验模拟了恶劣环境条件，结果表明UWB SAR成像能够有效支持高分辨率环境建图和可靠的回环检测。

Conclusion: UWB SAR成像为提升机器人在复杂环境下的感知鲁棒性和可靠性提供了可行且有效的解决方案。

Abstract: Traditional exteroceptive sensors in mobile robots, such as LiDARs and
cameras often struggle to perceive the environment in poor visibility
conditions. Recently, radar technologies, such as ultra-wideband (UWB) have
emerged as potential alternatives due to their ability to see through adverse
environmental conditions (e.g. dust, smoke and rain). However, due to the small
apertures with low directivity, the UWB radars cannot reconstruct a detailed
image of its field of view (FOV) using a single scan. Hence, a virtual large
aperture is synthesized by moving the radar along a mobile robot path. The
resulting synthetic aperture radar (SAR) image is a high-definition
representation of the surrounding environment. Hence, this paper proposes a
pipeline for mobile robots to incorporate UWB radar-based SAR imaging to map an
unknown environment. Finally, we evaluated the performance of classical feature
detectors: SIFT, SURF, BRISK, AKAZE and ORB to identify loop closures using UWB
SAR images. The experiments were conducted emulating adverse environmental
conditions. The results demonstrate the viability and effectiveness of UWB SAR
imaging for high-resolution environmental mapping and loop closure detection
toward more robust and reliable robotic perception systems.

</details>


### [188] [Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots](https://arxiv.org/abs/2510.02885)
*Faduo Liang,Yunfeng Yang,Shi-Lu Dai*

Main category: cs.RO

TL;DR: 提出了一种结合动态障碍物跟踪与映射、卡尔曼滤波预测和控制屏障函数的实时运动规划算法，用于提升自主移动机器人在复杂环境中的安全导航性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高自主移动机器人在存在动态障碍物的复杂环境中进行安全关键型导航的安全性和鲁棒性，需要一种能够实时预测障碍物行为并有效规避的运动规划方法。

Method: 该算法通过将点云分为动态和静态部分，利用卡尔曼滤波估计和预测动态障碍物的运动状态，并构建前向时域地图（FTD）。结合控制屏障函数（CBF）与非线性模型预测控制，基于FTD地图中的碰撞风险点生成CBF约束，实现对静态和动态障碍物的避障。

Result: 仿真和真实场景实验表明，该算法在复杂环境中具有良好的避障效果，在安全性与鲁棒性方面优于两种基线方法。

Conclusion: 所提出的运动规划算法能有效整合动态环境预测与安全控制机制，显著提升自主机器人在动态环境下的安全导航能力，具备实际应用潜力。

Abstract: In this work, we propose a novel motion planning algorithm to facilitate
safety-critical navigation for autonomous mobile robots. The proposed algorithm
integrates a real-time dynamic obstacle tracking and mapping system that
categorizes point clouds into dynamic and static components. For dynamic point
clouds, the Kalman filter is employed to estimate and predict their motion
states. Based on these predictions, we extrapolate the future states of dynamic
point clouds, which are subsequently merged with static point clouds to
construct the forward-time-domain (FTD) map. By combining control barrier
functions (CBFs) with nonlinear model predictive control, the proposed
algorithm enables the robot to effectively avoid both static and dynamic
obstacles. The CBF constraints are formulated based on risk points identified
through collision detection between the predicted future states and the FTD
map. Experimental results from both simulated and real-world scenarios
demonstrate the efficacy of the proposed algorithm in complex environments. In
simulation experiments, the proposed algorithm is compared with two baseline
approaches, showing superior performance in terms of safety and robustness in
obstacle avoidance. The source code is released for the reference of the
robotics community.

</details>


### [189] [Metrics vs Surveys: Can Quantitative Measures Replace Human Surveys in Social Robot Navigation? A Correlation Analysis](https://arxiv.org/abs/2510.02941)
*Stefano Trepella,Mauro Martini,Noé Pérez-Higueras,Andrea Ostuni,Fernando Caballero,Luis Merino,Marcello Chiaberge*

Main category: cs.RO

TL;DR: 该研究探讨了数值化社交导航指标与人类主观评估之间的关系，发现现有指标虽能部分反映机器人导航行为，但不足以充分捕捉主观感受，需开发新指标。


<details>
  <summary>Details</summary>
Motivation: 为了减少对耗时耗力的人类调查的依赖，探索能否用易于计算的数值指标来有效衡量社交导航性能。

Method: 通过分析数值社交导航指标与人类中心评估结果之间的相关性，检验现有量化指标是否与人类感知一致。

Result: 当前的数值指标能够捕捉机器人导航行为的某些方面，但重要的主观因素（如舒适性、可理解性）仍未能被充分表示。

Conclusion: 现有的数值指标尚不足够作为标准化评估工具，需要设计新的、更能反映人类感知的社交导航度量标准。

Abstract: Social, also called human-aware, navigation is a key challenge for the
integration of mobile robots into human environments. The evaluation of such
systems is complex, as factors such as comfort, safety, and legibility must be
considered. Human-centered assessments, typically conducted through surveys,
provide reliable insights but are costly, resource-intensive, and difficult to
reproduce or compare across systems. Alternatively, numerical social navigation
metrics are easy to compute and facilitate comparisons, yet the community lacks
consensus on a standard set of metrics.
  This work explores the relationship between numerical metrics and
human-centered evaluations to identify potential correlations. If specific
quantitative measures align with human perceptions, they could serve as
standardized evaluation tools, reducing the dependency on surveys. Our results
indicate that while current metrics capture some aspects of robot navigation
behavior, important subjective factors remain insufficiently represented and
new metrics are necessary.

</details>


### [190] [Single-Rod Brachiation Robot: Mechatronic Control Design and Validation of Prejump Phases](https://arxiv.org/abs/2510.02946)
*Juraj Lieskovský,Hijiri Akahane,Aoto Osawa,Jaroslav Bušek,Ikuo Mizuuchi,Tomáš Vyhlídal*

Main category: cs.RO

TL;DR: 提出了一种极简构型的摆荡机器人，通过质心重定位实现摆动和旋转运动，并设计了两种控制策略进行仿真验证，最终在低成本控制系统上实现了连续控制策略的实验验证。


<details>
  <summary>Details</summary>
Motivation: 设计一种结构简单的摆荡机器人，模仿臂行运动，以实现高效、低成本的移动机制。

Method: 采用单刚性杆与两端抓手机构构成机器人，利用曲柄滑块机构调节质心位置，并基于非线性模型提出 bang-bang 和连续输入-输出线性化两种控制策略。

Result: 两种控制策略在仿真中得到验证和比较，连续控制策略在基于STM32的低本控系统上成功实现，并完成了摆动和旋转阶段的实验验证。

Conclusion: 所提出的连续控制策略能有效处理执行器扭矩限制和几何约束，适用于实际硬件系统，且具备能量积累能力，提升了后续跳跃阶段的性能。

Abstract: A complete mechatronic design of a minimal configuration brachiation robot is
presented. The robot consists of a single rigid rod with gripper mechanisms
attached to both ends. The grippers are used to hang the robot on a horizontal
bar on which it swings or rotates. The motion is imposed by repositioning the
robot's center of mass, which is performed using a crank-slide mechanism. Based
on a non-linear model, an optimal control strategy is proposed, for
repositioning the center of mass in a bang-bang manner. Consequently, utilizing
the concept of input-output linearization, a continuous control strategy is
proposed that takes into account the limited torque of the crank-slide
mechanism and its geometry. An increased attention is paid to energy
accumulation towards the subsequent jump stage of the brachiation. These two
strategies are validated and compared in simulations. The continuous control
strategy is then also implemented within a low-cost STM32-based control system,
and both the swing and rotation stages of the brachiation motion are
experimentally validated.

</details>


### [191] [YawSitter: Modeling and Controlling a Tail-Sitter UAV with Enhanced Yaw Control](https://arxiv.org/abs/2510.02968)
*Amir Habel,Fawad Mehboob,Jeffrin Sam,Clement Fortin,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种针对尾座式无人机在悬停模式下实现精确侧向运动建模与解耦控制的新方法，通过引入基于差动螺旋桨滑流效应的侧滑力模型，增强了偏航控制能力，并实现了无滚转耦合的侧向位置控制。


<details>
  <summary>Details</summary>
Motivation: 尾座式无人机由于复杂的气动耦合和缺乏明确的侧向动力学，在悬停时难以实现精确的侧向运动控制。本文旨在解决这一挑战，提升其机动性和控制精度。

Method: 提出一种基于差动推力下螺旋桨滑流对机身作用的侧滑力模型，采用YXZ欧拉旋转序列进行姿态建模，准确包含重力分量并直接在体坐标系y轴上控制偏航，避免奇异问题。

Result: 在Unity仿真环境中进行了轨迹跟踪验证，矩形和圆形路径测试显示悬停模式下位置误差小，最大偏航偏差不超过5.688度，表现出良好的稳定性和控制性能。

Conclusion: 所提出的侧向力生成模型和控制框架有效解决了尾座式无人机在悬停时的侧向控制耦合问题，为高机动性、可悬停的尾座式无人机发展提供了可行方案。

Abstract: Achieving precise lateral motion modeling and decoupled control in hover
remains a significant challenge for tail-sitter Unmanned Aerial Vehicles
(UAVs), primarily due to complex aerodynamic couplings and the absence of
welldefined lateral dynamics. This paper presents a novel modeling and control
strategy that enhances yaw authority and lateral motion by introducing a
sideslip force model derived from differential propeller slipstream effects
acting on the fuselage under differential thrust. The resulting lateral force
along the body y-axis enables yaw-based lateral position control without
inducing roll coupling. The control framework employs a YXZ Euler rotation
formulation to accurately represent attitude and incorporate gravitational
components while directly controlling yaw in the yaxis, thereby improving
lateral dynamic behavior and avoiding singularities. The proposed approach is
validated through trajectory-tracking simulations conducted in a Unity-based
environment. Tests on both rectangular and circular paths in hover mode
demonstrate stable performance, with low mean absolute position errors and yaw
deviations constrained within 5.688 degrees. These results confirm the
effectiveness of the proposed lateral force generation model and provide a
foundation for the development of agile, hover-capable tail-sitter UAVs.

</details>


### [192] [AI-Enhanced Kinematic Modeling of Flexible Manipulators Using Multi-IMU Sensor Fusion](https://arxiv.org/abs/2510.02975)
*Amir Hossein Barjini,Jouni Mattila*

Main category: cs.RO

TL;DR: 提出了一种基于多IMU的柔性机械臂运动学估计框架，结合互补滤波、粒子群优化和径向基神经网络，有效提高了位置和姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在估计柔性机械臂位姿时存在噪声大、延迟高等问题，难以满足高精度控制需求。

Method: 将柔性连杆建模为多个刚性段，利用低成本IMU测量加速度和角速度；采用互补滤波融合数据，并用PSO优化滤波参数；通过RBFNN补偿残差误差。

Result: 实验结果表明该方法具有高精度，y、z方向位置RMSE分别为0.00021 m和0.00041 m，角度θ的RMSE为0.00024 rad。

Conclusion: 所提多IMU融合框架结合智能优化与学习方法，显著提升了柔性机械臂在垂直运动中的位姿估计性能，适用于高精度应用场景。

Abstract: This paper presents a novel framework for estimating the position and
orientation of flexible manipulators undergoing vertical motion using multiple
inertial measurement units (IMUs), optimized and calibrated with ground truth
data. The flexible links are modeled as a series of rigid segments, with joint
angles estimated from accelerometer and gyroscope measurements acquired by
cost-effective IMUs. A complementary filter is employed to fuse the
measurements, with its parameters optimized through particle swarm optimization
(PSO) to mitigate noise and delay. To further improve estimation accuracy,
residual errors in position and orientation are compensated using radial basis
function neural networks (RBFNN). Experimental results validate the
effectiveness of the proposed intelligent multi-IMU kinematic estimation
method, achieving root mean square errors (RMSE) of 0.00021~m, 0.00041~m, and
0.00024~rad for $y$, $z$, and $\theta$, respectively.

</details>


### [193] [Real-Time Nonlinear Model Predictive Control of Heavy-Duty Skid-Steered Mobile Platform for Trajectory Tracking Tasks](https://arxiv.org/abs/2510.02976)
*Alvaro Paz,Pauli Mustalahti,Mohammad Dastranj,Jouni Mattila*

Main category: cs.RO

TL;DR: 提出了一种用于重载滑移转向移动平台轨迹跟踪的实时最优控制框架，采用多射击非线性模型预测控制方法，结合多种传感器数据，实现了高精度实时控制，在速度和精度方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了在存在不确定性和干扰的情况下确保动态系统的安全性和稳定性能，需要高精度的实时控制器。

Method: 采用多射击非线性模型预测控制（NMPC）框架，并融合多种传感器数据以提升实时性和准确性。

Result: 控制器在不同轨迹跟踪任务中表现出色，相较于现有方法在速度和精度上均有显著提升。

Conclusion: 所提出的控制框架能有效提高滑移转向移动平台的轨迹跟踪性能，具有较强的鲁棒性和实时性，优于现有NMPC方法。

Abstract: This paper presents a framework for real-time optimal controlling of a
heavy-duty skid-steered mobile platform for trajectory tracking. The importance
of accurate real-time performance of the controller lies in safety
considerations of situations where the dynamic system under control is affected
by uncertainties and disturbances, and the controller should compensate for
such phenomena in order to provide stable performance. A multiple-shooting
nonlinear model-predictive control framework is proposed in this paper. This
framework benefits from suitable algorithm along with readings from various
sensors for genuine real-time performance with extremely high accuracy. The
controller is then tested for tracking different trajectories where it
demonstrates highly desirable performance in terms of both speed and accuracy.
This controller shows remarkable improvement when compared to existing
nonlinear model-predictive controllers in the literature that were implemented
on skid-steered mobile platforms.

</details>


### [194] [3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning](https://arxiv.org/abs/2510.03011)
*Chenyuan Chen,Haoran Ding,Ran Ding,Tianyu Liu,Zewen He,Anqing Duan,Dezhen Song,Xiaodan Liang,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的端到端框架，用于生成长且平滑的轨迹，以实现工业任务（如抛光、喷涂）中的高表面覆盖率。相比传统方法，该方法更具表达力和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹生成方法受限于预定义函数形式，难以应对复杂多样的任务，且泛化能力差，需要手动调整或大量参数调优。因此需要更强大的生成模型。

Method: 采用扩散模型，通过迭代去噪并结合精心设计的噪声调度和条件机制，生成连续、平滑且适应任务上下文的轨迹。

Result: 实验表明，该方法在点对点Chamfer距离上平均提升98.2%，平滑性提升97.0%，表面覆盖率提高61%，并能泛化到未见形状。

Conclusion: 扩散模型为工业表面处理任务提供了一种统一的端到端轨迹学习方案，无需针对特定类别设计模型，具有优异的性能和泛化能力。

Abstract: Diffusion models, as a class of deep generative models, have recently emerged
as powerful tools for robot skills by enabling stable training with reliable
convergence. In this paper, we present an end-to-end framework for generating
long, smooth trajectories that explicitly target high surface coverage across
various industrial tasks, including polishing, robotic painting, and spray
coating. The conventional methods are always fundamentally constrained by their
predefined functional forms, which limit the shapes of the trajectories they
can represent and make it difficult to handle complex and diverse tasks.
Moreover, their generalization is poor, often requiring manual redesign or
extensive parameter tuning when applied to new scenarios. These limitations
highlight the need for more expressive generative models, making
diffusion-based approaches a compelling choice for trajectory generation. By
iteratively denoising trajectories with carefully learned noise schedules and
conditioning mechanisms, diffusion models not only ensure smooth and consistent
motion but also flexibly adapt to the task context. In experiments, our method
improves trajectory continuity, maintains high coverage, and generalizes to
unseen shapes, paving the way for unified end-to-end trajectory learning across
industrial surface-processing tasks without category-specific models. On
average, our approach improves Point-wise Chamfer Distance by 98.2\% and
smoothness by 97.0\%, while increasing surface coverage by 61\% compared to
prior methods. The link to our code can be found
\href{https://anonymous.4open.science/r/spraydiffusion_ral-2FCE/README.md}{here}.

</details>


### [195] [HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton](https://arxiv.org/abs/2510.03022)
*Rui Zhong,Yizhe Sun,Junjie Wen,Jinming Li,Chuang Cheng,Wei Dai,Zhiwen Zeng,Huimin Lu,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: 提出HumanoidExo系统，通过迁移人类动作生成全身人形机器人数据，有效缓解真实数据稀缺问题，提升策略在动态环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 获取大规模、多样化的人形机器人策略学习数据成本高且困难，真实世界数据稀缺成为瓶颈。

Method: 开发HumanoidExo系统，将人类运动迁移到全身人形机器人，缩小人与机器人之间的形态差异，高效生成高质量训练数据。

Result: 在三个真实任务中验证：桌面操作、站-蹲结合操作、全身操作；仅用5次真实演示即可学习复杂控制，仅凭HumanoidExo数据可学会行走等新技能。

Conclusion: HumanoidExo显著增强人形机器人策略的泛化能力和学习效率，是真实机器人数据的重要补充。

Abstract: A significant bottleneck in humanoid policy learning is the acquisition of
large-scale, diverse datasets, as collecting reliable real-world data remains
both difficult and cost-prohibitive. To address this limitation, we introduce
HumanoidExo, a novel system that transfers human motion to whole-body humanoid
data. HumanoidExo offers a high-efficiency solution that minimizes the
embodiment gap between the human demonstrator and the robot, thereby tackling
the scarcity of whole-body humanoid data. By facilitating the collection of
more voluminous and diverse datasets, our approach significantly enhances the
performance of humanoid robots in dynamic, real-world scenarios. We evaluated
our method across three challenging real-world tasks: table-top manipulation,
manipulation integrated with stand-squat motions, and whole-body manipulation.
Our results empirically demonstrate that HumanoidExo is a crucial addition to
real-robot data, as it enables the humanoid policy to generalize to novel
environments, learn complex whole-body control from only five real-robot
demonstrations, and even acquire new skills (i.e., walking) solely from
HumanoidExo data.

</details>


### [196] [Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics](https://arxiv.org/abs/2510.03031)
*Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.RO

TL;DR: 本文提出了一种基于动态图（MoDs）的长期人类运动预测（LHMP）框架，通过引入时间条件动态图提升预测精度，在真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高在人机共存环境中自主机器人和车辆的安全与效率，需要准确且长期的人类运动预测。

Method: 利用编码空间或时空运动模式的动态图（MoDs）作为环境特征，构建MoD支持的LHMP框架，并提出一种排序方法以输出最可能的轨迹；同时引入时间条件MoD以捕捉不同时段的运动模式变化。

Result: 在两个真实世界数据集上的实验表明，MoD-LHMP比纯学习方法平均位移误差降低最多达50%，其中时间条件变体整体精度最高。

Conclusion: MoD-LHMP框架有效提升了长期人类运动预测的准确性，尤其在考虑时间变化因素后性能更优，具有良好的机器人应用前景。

Abstract: Long-term human motion prediction (LHMP) is important for the safe and
efficient operation of autonomous robots and vehicles in environments shared
with humans. Accurate predictions are important for applications including
motion planning, tracking, human-robot interaction, and safety monitoring. In
this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or
spatio-temporal motion patterns as environment features, to achieve LHMP for
horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that
supports various types of MoDs and includes a ranking method to output the most
likely predicted trajectory, improving practical utility in robotics. Further,
a time-conditioned MoD is introduced to capture motion patterns that vary
across different times of day. We evaluate MoD-LHMP instantiated with three
types of MoDs. Experiments on two real-world datasets show that MoD-informed
method outperforms learning-based ones, with up to 50\% improvement in average
displacement error, and the time-conditioned variant achieves the highest
accuracy overall. Project code is available at
https://github.com/test-bai-cpu/LHMP-with-MoDs.git

</details>


### [197] [Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot](https://arxiv.org/abs/2510.03081)
*Guiliang Liu,Bo Yue,Yi Jin Kim,Kui Jia*

Main category: cs.RO

TL;DR: 本文主张在人形机器人设计中采用控制策略与物理结构协同进化的共设计机制，以提升其在多样化现实环境中的适应性和智能性。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注固定结构下的控制策略优化，而忽视了形态与控制的协同演化，限制了机器人在复杂环境中的适应能力。

Method: 提出基于战略探索、仿真到现实迁移（Sim2Real）和元策略学习的共设计方法，并从方法论、应用导向和社区角度论证其必要性。

Result: 系统分析了共设计在人形机器人中的潜力与挑战，提出了短期创新到长期目标的开放性研究问题。

Conclusion: 共设计是实现真正具身智能的关键，应成为下一代智能人形机器人发展的核心方向。

Abstract: Humanoid robots, as general-purpose physical agents, must integrate both
intelligent control and adaptive morphology to operate effectively in diverse
real-world environments. While recent research has focused primarily on
optimizing control policies for fixed robot structures, this position paper
argues for evolving both control strategies and humanoid robots' physical
structure under a co-design mechanism. Inspired by biological evolution, this
approach enables robots to iteratively adapt both their form and behavior to
optimize performance within task-specific and resource-constrained contexts.
Despite its promise, co-design in humanoid robotics remains a relatively
underexplored domain, raising fundamental questions about its feasibility and
necessity in achieving true embodied intelligence. To address these challenges,
we propose practical co-design methodologies grounded in strategic exploration,
Sim2Real transfer, and meta-policy learning. We further argue for the essential
role of co-design by analyzing it from methodological, application-driven, and
community-oriented perspectives. Striving to guide and inspire future studies,
we present open research questions, spanning from short-term innovations to
long-term goals. This work positions co-design as a cornerstone for developing
the next generation of intelligent and adaptable humanoid agents.

</details>


### [198] [Whisker-based Tactile Flight for Tiny Drones](https://arxiv.org/abs/2510.03119)
*Chaoxiang Ye,Guido de Croon,Salua Hamaza*

Main category: cs.RO

TL;DR: 提出了一种受生物启发的轻量级触觉感知系统，使微型无人机能够在完全黑暗或复杂环境中通过触须感知障碍物并自主飞行。


<details>
  <summary>Details</summary>
Motivation: 微型无人机因尺寸限制在低光照、烟雾、灰尘或多反射环境中难以使用传统传感器进行导航，需要一种不依赖视觉的新型感知方式。

Method: 设计了一种仅3.2克的基于触须的触觉传感装置，采用气压计原理检测障碍物，并开发了可抑制噪声和漂移的触觉深度估计算法，在192KB RAM的微控制器上实现全机载运行。

Result: 系统可在完全黑暗条件下实现亚6毫米精度的障碍物感知，支持无人机沿软硬表面贴壁飞行和探索狭小空间，在仿真和真实环境中均验证了自主触觉飞行能力。

Conclusion: 该生物启发式方法重新定义了无视觉导航，为微型无人机在极端环境中的应用开辟了新可能。

Abstract: Tiny flying robots hold great potential for search-and-rescue, safety
inspections, and environmental monitoring, but their small size limits
conventional sensing-especially with poor-lighting, smoke, dust or reflective
obstacles. Inspired by nature, we propose a lightweight, 3.2-gram,
whisker-based tactile sensing apparatus for tiny drones, enabling them to
navigate and explore through gentle physical interaction. Just as rats and
moles use whiskers to perceive surroundings, our system equips drones with
tactile perception in flight, allowing obstacle sensing even in pitch-dark
conditions. The apparatus uses barometer-based whisker sensors to detect
obstacle locations while minimising destabilisation. To address sensor noise
and drift, we develop a tactile depth estimation method achieving sub-6 mm
accuracy. This enables drones to navigate, contour obstacles, and explore
confined spaces solely through touch-even in total darkness along both soft and
rigid surfaces. Running fully onboard a 192-KB RAM microcontroller, the system
supports autonomous tactile flight and is validated in both simulation and
real-world tests. Our bio-inspired approach redefines vision-free navigation,
opening new possibilities for micro aerial vehicles in extreme environments.

</details>


### [199] [Learning Stability Certificate for Robotics in Real-World Environments](https://arxiv.org/abs/2510.03123)
*Zhe Shen*

Main category: cs.RO

TL;DR: 提出一种从轨迹数据中直接学习Lyapunov函数的新框架，无需系统模型即可为自主系统提供数据驱动的稳定性保证。


<details>
  <summary>Details</summary>
Motivation: 传统稳定性证书依赖系统动力学的显式知识，难以应用于复杂或未知系统，尤其在控制算法不透明或数据含噪声时面临挑战。

Method: 通过神经网络参数化Lyapunov候选函数，并利用Cholesky分解确保正定性；允许稳定性条件的可控违反以应对真实噪声数据。

Result: 该框架能从数据中自动判断系统稳定性，即使在无内部控制算法访问权限的情况下也能实现高置信度的稳定性认证。

Conclusion: 所提方法为动态真实环境中的机器人系统提供了一种鲁棒、可扩展的数据驱动稳定性验证手段，且已开源工具支持实际应用。

Abstract: Stability certificates play a critical role in ensuring the safety and
reliability of robotic systems. However, deriving these certificates for
complex, unknown systems has traditionally required explicit knowledge of
system dynamics, often making it a daunting task. This work introduces a novel
framework that learns a Lyapunov function directly from trajectory data,
enabling the certification of stability for autonomous systems without needing
detailed system models. By parameterizing the Lyapunov candidate using a neural
network and ensuring positive definiteness through Cholesky factorization, our
approach automatically identifies whether the system is stable under the given
trajectory. To address the challenges posed by noisy, real-world data, we allow
for controlled violations of the stability condition, focusing on maintaining
high confidence in the stability certification process. Our results demonstrate
that this framework can provide data-driven stability guarantees, offering a
robust method for certifying the safety of robotic systems in dynamic,
real-world environments. This approach works without access to the internal
control algorithms, making it applicable even in situations where system
behavior is opaque or proprietary. The tool for learning the stability proof is
open-sourced by this research: https://github.com/HansOersted/stability.

</details>


### [200] [MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning](https://arxiv.org/abs/2510.03142)
*Tianyu Xu,Jiawei Chen,Jiazhao Zhang,Wenyao Zhang,Zekun Qi,Minghan Li,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 提出一种基于视觉-语言-动作（VLA）模型的视觉导航方法MM-Nav，通过多视角视觉输入和从强化学习专家生成的合成数据进行师生训练，实现了在复杂环境中的强泛化能力，并在仿真和真实世界实验中超越教师模型。


<details>
  <summary>Details</summary>
Motivation: 视觉导航难以像LiDAR或深度图那样显式建模光学信息，需要智能模型和大规模数据；现有方法在多样化导航能力上受限。

Method: 构建多视图VLA模型MM-Nav，结合预训练大语言模型与视觉基础模型；利用具有深度信息的RL专家在定制环境中生成涵盖到达、穿行和避障能力的大规模专家数据；采用动态平衡训练策略进行在线迭代训练。

Result: 在仿真环境中展现出强大的泛化能力，学生模型性能超过各单一RL教师；在真实世界实验中验证了方法的有效性。

Conclusion: 通过融合多能力专家数据与VLA架构，MM-Nav实现了优于教师模型的导航性能，证明了多能力集成与师生框架在视觉导航中的潜力。

Abstract: Visual navigation policy is widely regarded as a promising direction, as it
mimics humans by using egocentric visual observations for navigation. However,
optical information of visual observations is difficult to be explicitly
modeled like LiDAR point clouds or depth maps, which subsequently requires
intelligent models and large-scale data. To this end, we propose to leverage
the intelligence of the Vision-Language-Action (VLA) model to learn diverse
navigation capabilities from synthetic expert data in a teacher-student manner.
Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360
observations) based on pretrained large language models and visual foundation
models. For large-scale navigation data, we collect expert data from three
reinforcement learning (RL) experts trained with privileged depth information
in three challenging tailor-made environments for different navigation
capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA
model using data collected online from RL experts, where the training ratio is
dynamically balanced based on performance on individual capabilities. Through
extensive experiments in synthetic environments, we demonstrate that our model
achieves strong generalization capability. Moreover, we find that our student
VLA model outperforms the RL teachers, demonstrating the synergistic effect of
integrating multiple capabilities. Extensive real-world experiments further
confirm the effectiveness of our method.

</details>


### [201] [Optimal Smooth Coverage Trajectory Planning for Quadrotors in Cluttered Environment](https://arxiv.org/abs/2510.03169)
*Duanjiao Li,Yun Chen,Ying Zhang,Junwen Yao,Dongyue Huang,Jianguo Zhang,Ning Ding*

Main category: cs.RO

TL;DR: 提出了一种用于无人机在复杂环境中覆盖路径规划的两阶段优化算法，通过遗传算法求解TSP问题并优化轨迹平滑性、时间和避障。


<details>
  <summary>Details</summary>
Motivation: 针对电力系统中无人机在复杂环境下的覆盖路径规划需求，提高覆盖效率和飞行安全性。

Method: 前端使用遗传算法求解兴趣点的旅行商问题，生成访问序列；后端将其转化为非线性最小二乘问题，优化轨迹平滑性、时间消耗和避障。

Result: 数值仿真验证了算法的有效性，能够生成满足约束的平滑覆盖轨迹。

Conclusion: 该算法能有效提升无人机在复杂电网场景中的覆盖能力和运行安全性。

Abstract: For typical applications of UAVs in power grid scenarios, we construct the
problem as planning UAV trajectories for coverage in cluttered environments. In
this paper, we propose an optimal smooth coverage trajectory planning
algorithm. The algorithm consists of two stages. In the front-end, a Genetic
Algorithm (GA) is employed to solve the Traveling Salesman Problem (TSP) for
Points of Interest (POIs), generating an initial sequence of optimized visiting
points. In the back-end, the sequence is further optimized by considering
trajectory smoothness, time consumption, and obstacle avoidance. This is
formulated as a nonlinear least squares problem and solved to produce a smooth
coverage trajectory that satisfies these constraints. Numerical simulations
validate the effectiveness of the proposed algorithm, ensuring UAVs can
smoothly cover all POIs in cluttered environments.

</details>


### [202] [Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning](https://arxiv.org/abs/2510.03182)
*Yilun Hao,Yongchao Chen,Chuchu Fan,Yang Zhang*

Main category: cs.RO

TL;DR: 提出VLMFP框架，利用双视觉语言模型（SimVLM和GenVLM）自主生成PDDL问题和领域文件，实现精确的视觉规划，并在多种场景下展现良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法中视觉语言模型难以准确生成PDDL领域文件，依赖人工定义或环境反馈，限制了自主视觉规划的通用性和实用性。

Method: 设计双VLM协同框架：SimVLM根据规则描述模拟动作结果，GenVLM通过比较PDDL执行与SimVLM模拟结果来迭代生成和优化PDDL文件。

Result: 在6个网格世界任务中验证，SimVLM对已见/未见外观场景的动作序列模拟准确率分别为85.5%和87.8%；VLMFP生成的PDDL可为未见实例生成70.0%和54.1%的有效计划。

Conclusion: VLMFP能自主生成高质量PDDL文件，支持跨实例、外观和规则的泛化，推动无需人工干预的视觉规划发展。

Abstract: Vision Language Models (VLMs) show strong potential for visual planning but
struggle with precise spatial and long-horizon reasoning. In contrast, Planning
Domain Definition Language (PDDL) planners excel at long-horizon formal
planning, but cannot interpret visual inputs. Recent works combine these
complementary advantages by enabling VLMs to turn visual planning problems into
PDDL files for formal planning. However, while VLMs can generate PDDL problem
files satisfactorily, they struggle to accurately generate the PDDL domain
files, which describe all the planning rules. As a result, prior methods rely
on human experts to predefine domain files or on constant environment access
for refinement. We propose VLMFP, a Dual-VLM-guided framework that can
autonomously generate both PDDL problem and domain files for formal visual
planning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A
SimVLM that simulates action consequences based on input rule descriptions, and
a GenVLM that generates and iteratively refines PDDL files by comparing the
PDDL and SimVLM execution results. VLMFP unleashes multiple levels of
generalizability: The same generated PDDL domain file works for all the
different instances under the same problem, and VLMs generalize to different
problems with varied appearances and rules. We evaluate VLMFP with 6 grid-world
domains and test its generalization to unseen instances, appearance, and game
rules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,
simulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal
reaching for seen and unseen appearances, respectively. With the guidance of
SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for
unseen instances in seen and unseen appearances, respectively. Project page:
https://sites.google.com/view/vlmfp.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [203] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了一种用于极端值预测的数据增强框架，结合GAN和SMOTE等方法，利用Conv-LSTM和BD-LSTM进行多步预测，实验表明SMOTE策略在短期和长期预测中表现更优。


<details>
  <summary>Details</summary>
Motivation: 极端值预测在金融、气候变化等领域具有挑战性，现有数据增强方法在深度学习中的应用尚不充分，因此需要探索适用于极端值预测的增强策略。

Method: 提出基于相关函数的新型数据增强策略，结合GAN和SMOTE对极端值进行增强，并采用Conv-LSTM和BD-LSTM进行多步预测，评估不同模型在整体及极端区域的预测精度与计算效率。

Result: SMOTE策略在短期和长期预测中均表现出更好的适应性和性能提升；Conv-LSTM在周期性稳定数据中表现优异，而BD-LSTM在混沌或非平稳序列中效果更佳。

Conclusion: SMOTE相较于GAN更适合极端值预测中的数据增强，且两种深度学习模型各有优势，应根据数据特性选择合适组合。

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [204] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: 本文提出了OpenTSLM，一种将时间序列作为原生模态融入预训练大语言模型的框架，以解决LLMs在处理医学时间序列数据上的局限。提出了两种架构：OpenTSLM-SoftPrompt和OpenTSLM-Flamingo，并在多个文本-时间序列推理任务上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多模态数据处理中表现出色，但在医学领域对时间序列数据的处理能力有限。为填补这一空白，需开发能有效融合并推理时间序列的新模型。

Method: 提出OpenTSLM框架，包含两种架构：1) OpenTSLM-SoftPrompt：通过软提示将可学习的时间序列标记与文本标记拼接；2) OpenTSLM-Flamingo：利用交叉注意力机制融合时间序列与文本。在HAR-CoT、Sleep-CoT和ECG-QA-CoT三个新构建的数据集上进行链式思维（CoT）推理任务评估。

Result: OpenTSLM在各项任务中均优于基线模型，睡眠分期F1达69.9，人类活动识别（HAR）达65.4，显著高于仅微调文本模型的9.05和52.2。即使是1B参数的OpenTSLM也超越GPT-4o。OpenTSLM-Flamingo在长序列上表现更优且显存稳定，而SoftPrompt随序列增长显存需求指数上升（如ECG-QA训练时达110GB vs 40GB）。临床专家评估显示模型具备强推理能力。

Conclusion: OpenTSLM成功将时间序列整合进大语言模型，实现了高效的跨模态推理，尤其适用于医疗场景中的长时间序列分析，且OpenTSLM-Flamingo在性能与效率间取得更好平衡，具有更强实用性。

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [205] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: RainSeer是一种结构感知的降雨场重建框架，利用雷达反射率作为物理基础的结构先验，通过物理信息驱动的两阶段架构，提升高分辨率降雨场重建的精度和结构保真度。


<details>
  <summary>Details</summary>
Motivation: 现有降雨插值方法常过度平滑关键结构，难以捕捉剧烈变化和局部极端降雨，影响洪水预报和水文建模的准确性。

Method: 提出RainSeer框架，包含结构到点映射器（Structure-to-Point Mapper）进行雷达结构与地面降雨的空间对齐，以及地理感知降雨解码器（Geo-Aware Rain Decoder）通过因果时空注意力机制模拟水成物下落过程中的物理变化。

Result: 在韩国RAIN-F和法国MeteoNet数据集上实验显示，相比现有最先进方法，MAE降低超过13.31%，显著提升重建降雨场的结构保真度。

Conclusion: RainSeer通过引入物理引导的结构先验和两阶段解码机制，有效解决了从雷达观测到地面降雨的映射难题，为高分辨率降雨重建提供了新思路。

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [206] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: 本文提出了Advisor Models，一种通过强化学习训练的轻量级参数化策略，用于在上下文中动态向黑盒模型提供自然语言指导，从而实现对不同输入、用户和环境的自适应优化。


<details>
  <summary>Details</summary>
Motivation: 现有的静态提示优化方法无法根据不同的输入或环境进行调整，限制了黑盒模型的个性化与适应性。因此需要一种能够动态优化提示的方法。

Method: 提出Advisor Models，使用强化学习训练一个小型策略模型，作为输入与黑盒模型之间的中介，根据环境反馈的奖励信号为每个实例生成动态的自然语言指导指令。

Result: 在多个涉及推理和个性化的领域中，Advisor Models优于静态提示优化方法，能够发现环境动态并提升下游任务性能；同时具备跨模型迁移能力和对分布外输入的鲁棒性。

Conclusion: Advisor Models为黑盒系统提供了一种可学习的接口，代表了通过动态优化实现个性化和环境适应性AI的一个有前景的方向。

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [207] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: 提出基于市场机制的训练数据选择方法，通过成本函数预测市场（LMSR）统一异构信号，实现高效、稳定的多信号数据筛选。


<details>
  <summary>Details</summary>
Motivation: 传统方法中示例效用信号（如不确定性、稀有性、多样性等）异质且权重设置随意，难以有效整合，导致数据选择效果不稳定。

Method: 设计一个基于市场的选择器，将不同信号作为交易者，通过LMSR机制为每个样本定价，引入单一流动性参数控制集中度，并采用主题归一化提升校准稳定性；使用价格每token规则处理预算，结合轻量级多样性模块增强覆盖。

Result: 在GSM8K上以60k token预算达到与强单信号基线相当的性能，同时降低种子方差和选择开销（<0.1 GPU小时）；在AGNews（保留5-25%数据）上实现竞争性准确率，并提升类别平衡性和稳定性。

Conclusion: 该框架在固定计算下统一了多信号数据整理，适用于提示级推理与分类任务，具有良好的可解释性和实用性。

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [208] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: 本文研究了动态后训练量化（PTQ）可能导致的极端性能下降问题，通过知识蒸馏与强化学习构建网络与比特宽度策略对，揭示了某些情况下准确率可能下降10-65%，远高于鲁棒情况下的<2%下降，强调在安全关键场景中需谨慎部署PTQ。


<details>
  <summary>Details</summary>
Motivation: 由于PTQ在不同输入分布下可能导致显著性能下降，尤其在安全关键场景中，亟需探究其最坏情况下的失效机制及成因。

Method: 提出一种结合知识蒸馏和强化学习的方法，学习网络结构与比特宽度策略的组合，以分析量化导致灾难性失败的最坏情况。

Result: 发现了‘有害’的网络-策略对，在多个实例中准确率下降10-65%，而鲁棒对仅下降<2%；并通过实验识别出最脆弱的关键点。

Conclusion: PTQ可能引发严重性能退化，存在显著风险，实际部署需谨慎；呼吁未来研究加强对深度模型鲁棒性和安全性的评估。

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [209] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: SAGE是一种流式数据子集选择方法，通过维护梯度几何的紧凑Frequent Directions草图，在低内存下优先选择与共识方向对齐的样本，实现高效神经网络训练。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在大数据集上训练计算和能耗高，需要更高效的数据子集选择方法以减少资源消耗。

Method: 提出SAGE方法，使用O(ℓD)内存维护梯度几何的Frequent Directions草图，通过共识方向对齐评分机制选择重要样本，避免显式存储梯度和成对相似性计算，实现两遍、GPU友好的训练流程。

Result: 在多个基准测试中，SAGE在较小保留率下保持了与全数据训练及最新子集选择基线相当的准确率，同时显著降低端到端计算量和峰值内存。

Conclusion: SAGE提供了一种实用的、常数内存的高效训练方案，可与模型剪枝和压缩技术结合，适用于大规模神经网络训练。

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [210] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: 本研究提出了一种基于不确定性的模型选择策略，利用TabPFN模型在siRNA敲低效率预测任务中实现了优于现有最先进方法的性能，并发现模型预测的四分位距（IQR）与其真实误差呈负相关，通过选择低IQR模型构建集成可显著提升预测效果。


<details>
  <summary>Details</summary>
Motivation: In-context学习模型（如TabPFN）在生物分子功效预测中具有潜力，但其性能对上下文高度敏感，如何在无真实标签的情况下选择最优模型进行集成仍是一个开放问题。

Method: 采用不确定性引导的模型选择策略，使用TabPFN模型结合简单的序列特征进行siRNA敲低效率预测，并以模型预测的四分位距（IQR）作为不确定性度量，选取IQR最低的模型构建集成。

Result: 在siRNA敲低效率预测任务中，仅用序列特征的TabPFN模型超过了专用的最先进预测器；模型的IQR与真实预测误差呈负相关；基于低IQR选择的集成模型优于朴素集成或单一模型。

Conclusion: 模型不确定性（以IQR衡量）是一种有效的无标签启发式指标，可用于优化生物分子功效预测中的模型选择和集成。

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [211] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: LiteSpark是一种针对Transformer模型注意力和MLP层的新型预训练框架，显著提升了训练吞吐量并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练耗时长、能耗高，亟需提升计算效率和能源利用率。

Method: 通过结合架构改进和算法优化，针对性地优化Transformer的注意力和MLP层，提高模型FLOPs利用率（MFU），同时保持与标准实现的兼容性。

Result: 在3B和30B参数的Llama模型上使用SlimPajama-627B数据集进行测试，训练吞吐量提升2-6倍，能耗降低55%-83%，且适用于多种硬件和模型架构。

Conclusion: LiteSpark有效解决了大模型训练中的效率瓶颈，具有广泛的适用性和实用价值，可扩展至微调等后续训练阶段。

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [212] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 提出了一种名为Action-Controllable Factorization (ACF)的对比学习方法，用于从高维观测中发现可独立控制的潜在变量，从而在无需先验知识的情况下恢复出可解释的状态因子结构。


<details>
  <summary>Details</summary>
Motivation: 传统基于因子化马尔可夫决策过程的方法虽然样本效率高，但需要已知的因子结构；而深度强化学习虽能处理高维输入，却无法利用因子结构。因此，需要一种能够自动从高维观测中发现可控因子表示的方法。

Method: 提出ACF（Action-Controllable Factorization），通过对比学习识别出动作可以分别影响的独立可控制潜在变量。利用动作通常只影响部分状态变量的稀疏性特性，将受动作影响和环境动态演化的变量区分开来进行对比训练。

Result: 在Taxi、FourRooms和MiniGrid-DoorKey三个具有已知因子结构的基准任务上，ACF能直接从像素输入中恢复出真实的可控因子，并且持续优于基线解耦算法。

Conclusion: ACF有效解决了高维观测下因子化结构的学习问题，实现了对可控制状态因子的自动发现，为结合因子化MDP的样本效率与深度RL的表征能力提供了可行路径。

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [213] [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)
*Tianqi Liu,Kairui Fu,Shengyu Zhang,Wenyan Fan,Zhaocheng Du,Jieming Zhu,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: 提出CHORD框架，通过设备-云协作实现个性化混合精度量化，提升移动端序列推荐的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 将重排序模型部署到移动设备上面临资源异构和用户兴趣个性化的问题，现有量化方法忽视设备端用户兴趣，影响推荐准确性。

Method: 提出CHORD框架，采用通道级混合精度量化，通过云端辅助超网络识别用户特定关键参数，结合多粒度（层、滤波器、元素级）参数敏感性分析，实现个性化量化策略；在设备端进行动态模型自适应和加速推理，无需反向传播。

Result: 在三个真实数据集和两种主流骨干模型（SASRec、Caser）上的实验表明，CHORD在保持高推荐准确性的同时，显著提升效率和适应性，并通过仅用2比特每通道编码量化策略大幅降低通信开销。

Conclusion: CHORD实现了个性化与资源自适应的平衡，为移动端序列推荐提供了一种高效、低通信成本的混合精度量化部署方案。

Abstract: With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

</details>


### [214] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: 本文研究了将鲁棒的模型无关有界极值搜索（ES）反馈控制与深度强化学习（DRL）结合，以提升非线性时变系统中DRL控制器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DRL在处理多参数系统时具有潜力，但在系统模型快速变化时性能急剧下降；而传统ES方法虽能应对未知控制方向的时变系统，但收敛速度慢且易陷入局部极小。因此，需要一种结合两者优势的混合控制策略。

Method: 提出一种结合深度强化学习（DRL）和有界极值搜索（ES）的混合控制器，利用DRL从历史数据中快速学习控制策略，同时通过有界ES增强对系统时变特性的鲁棒性。

Result: 数值实验表明，该混合控制器在一般时变系统及洛斯阿拉莫斯中子科学中心直线粒子加速器的低能束流输运段自动调谐任务中，性能优于单独使用DRL或ES的方法。

Conclusion: DRL与有界ES相结合能够实现快速响应与强鲁棒性的平衡，显著提升复杂时变系统下的控制性能。

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [215] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 本文提出监督微调（SFT）不仅是一种策略模仿，本质上等同于逆强化学习，能够隐式学习到细粒度的token级奖励模型，并可通过恢复该奖励信号进一步提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 传统上将SFT视为简单的模仿学习，忽视了其潜在的奖励学习能力；作者旨在揭示SFT与逆强化学习之间的理论联系，并挖掘其隐含的奖励信号。

Method: 证明SFT目标是逆Q学习的一个特例，提出从SFT模型中提取基线相对的密集奖励函数的方法，并基于此设计Dense-Path REINFORCE算法进行策略优化。

Result: 成功从SFT模型中恢复出token级别的密集奖励信号，并在指令跟随基准上通过强化学习显著优于原始SFT模型。

Conclusion: SFT不仅是策略模仿，更是一种强大的奖励学习机制，为利用专家示范开辟了新途径。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [216] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出了一种基于残差学习的框架，通过在多个交叉阵列上顺序学习来补偿低精度权重更新带来的残差误差，从而实现在有限状态存储设备上的高效片上训练。


<details>
  <summary>Details</summary>
Motivation: 现有忆阻器设备通常只有约4比特的分辨率，导致训练精度显著下降，难以实现高效的内存中训练。

Method: 采用残差学习框架，将模型参数分布在多个交叉阵列上，逐级补偿低精度更新带来的误差，并进行理论分析和实验验证。

Result: 在标准图像分类任务上，该方法在有限状态设置下优于现有的先进模拟训练策略，且硬件开销适中。理论分析表明优化间隙随阵列数量增加而缩小，具有线性收敛速率。

Conclusion: 所提出的残差学习框架有效解决了低精度忆阻器件在片上训练中的精度退化问题，为高能效、低成本的模拟内存计算提供了可行方案。

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [217] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的图生成框架SFMG，通过谱几何与流匹配结合，在保持图的几何结构和全局特性的同时实现高效、可扩展的图生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只对齐目标图的谱或度分布，忽略了特征向量诱导的几何结构和图的全局结构。

Method: 利用谱特征映射将输入和目标图嵌入到连续黎曼流形中，并在这些嵌入之间定义测地流，沿流匹配分布以生成图。

Result: SFMG在图元、度和谱指标上达到最先进水平，相比扩散模型最高速度提升30倍，并能泛化到未见的图规模。

Conclusion: SFMG通过融合谱几何与流匹配，为图合成提供了兼顾几何结构、灵活性和效率的新方法。

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [218] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: 提出基于Inter-Animal Transform Class (IATC) 的神经网络模型与大脑响应比较方法，实现高预测性和机制特异性，支持TDANNs作为视觉系统模型。


<details>
  <summary>Details</summary>
Motivation: 缺乏一致的模型激活与大脑响应比较方法，需解决模型预测性与机制准确性之间的权衡问题。

Method: 基于IATC框架，在模拟网络、小鼠和人类群体中识别跨个体神经响应映射函数，双向比较模型与大脑活动。

Result: IATC能解析神经机制细节（如非线性激活函数），在保持高预测精度的同时区分不同脑区响应模式，验证TDANNs对视觉系统的建模优势。

Conclusion: IATC提供了一种原则性的模型-大脑比较方法，证明高预测性与机制准确性可兼得，推动深度学习模型在神经科学中的应用。

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [219] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的GRU自编码器（AttentiveGRUAE），用于从纵向可穿戴数据中进行时间聚类和结果预测，在抑郁分类和行为亚型识别上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了有效利用纵向可穿戴数据中的时序行为特征，实现对心理健康状态（如抑郁症）的准确预测，并发现潜在的行为亚型。

Method: 设计了一个结合序列重构、二分类预测和基于GMM软聚类的多任务GRU自编码器模型，引入注意力机制以增强对关键时间窗口的捕捉能力。

Result: 在两个队列中均表现出优于基线模型的聚类质量和抑郁分类性能（AUC最高达0.74），且外部验证显示聚类结果稳定可重复，并通过注意力可视化揭示了与睡眠规律变化相关的临床可解释模式。

Conclusion: AttentiveGRUAE能有效学习可穿戴设备采集的纵向行为数据的紧凑表示，支持高质聚类与抑郁预测，具备临床应用潜力。

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [220] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: 本文提出了一种新的图神经网络方法HOD-GNN，利用高阶节点导数提升GNN的表达能力，理论分析表明其表达力与WL层次对齐，并在多个图学习基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNN）取得了显著进展，但其表达能力有限仍是根本挑战。现有研究多集中在设计更复杂的架构以提升表达力，而忽略了节点特征导数在增强表达力方面的潜力。

Method: 提出High-Order Derivative GNN（HOD-GNN），通过利用基础模型的高阶节点导数来增强消息传递神经网络（MPNN）的表达能力。这些导数生成结构感知的节点嵌入，并由第二个GNN处理，形成端到端可训练架构。同时开发了一种利用图稀疏性和并行性的消息传递算法来高效计算高阶导数。

Result: 理论分析表明HOD-GNN的表达能力与WL层次一致，并揭示了其与子图GNN和主流结构编码方案之间的深层联系。实验结果表明，HOD-GNN在多个流行图学习基准任务中表现优异。

Conclusion: 高阶导数为提升GNN表达能力提供了自然且有效的新途径，HOD-GNN不仅理论上具有强表达力，而且在实际任务中也具备良好性能。

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [221] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: 本文综述了地理空间机器学习（GeoML）软件库的发展现状，介绍了主流工具及其功能，并通过作物分类案例展示了实际应用，同时讨论了软件设计最佳实践和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据快速增长，但专用GeoML软件库发展滞后，亟需系统性梳理现有工具以支持研究与应用。

Method: 对现有GeoML库进行综合性分析，介绍TorchGeo、eo-learn和Raster Vision等库的架构与功能，并结合作物类型映射案例展示其应用。

Result: 总结了GeoML库的核心功能、数据预处理方法、基准测试及预训练模型使用，提出了软件设计、许可和测试的最佳实践。

Conclusion: 为从业者、开发者和研究人员提供了导航和贡献于快速发展的GeoML生态系统的指南，强调了基础模型和开源治理的未来方向。

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [222] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 提出MINTO方法，通过结合目标网络和在线网络的最小估计值来实现快速且稳定的值函数学习。


<details>
  <summary>Details</summary>
Motivation: 解决使用目标网络导致的学习缓慢问题，同时避免直接使用在线网络带来的不稳定学习。

Method: 引入一种新更新规则，计算目标时采用目标网络和在线网络估计值的最小值（MINimum esTimatE, MINTO）。

Result: 在多种基准测试中验证了MINTO的有效性，包括在线与离线强化学习、离散与连续动作空间，均表现出一致的性能提升。

Conclusion: MINTO能够在几乎无额外成本的情况下广泛集成到基于值和演员-评论家算法中，具有良好的通用性和实用性。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [223] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: 本研究通过在全美范围内使用基于质量守恒感知机（MCP）的可解释性水文模型，结合机器学习与物理机制，展示了在不同水文条件下选择适当复杂度模型的重要性，并证明其性能可媲美LSTM等纯数据驱动模型。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在大样本水文建模中广泛应用，但其预测改进往往缺乏对物理机制的深入理解。本研究旨在通过物理可解释的模型提升预测的可靠性与机制理解。

Method: 采用基于质量守恒感知机（MCP）的可解释性集水区尺度模型，在CONUS范围内进行大样本实验，利用雪 regime、森林覆盖和气候区等属性进行结果评估，并与LSTM等数据驱动模型进行基准比较。

Result: 研究表明，根据水文过程主导性的变化选择适当复杂度的模型架构至关重要；MCP-based模型性能可与LSTM相媲美，且具有更好的物理可解释性。

Conclusion: 理论引导、物理基础的建模范式在大样本水文学中具有巨大潜力，有助于构建简洁、可解释且能编码时空变化过程主导性的未来通用模型。

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [224] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络互信息估计的监督特征选择新方法MINERVA，能有效捕捉复杂特征-目标关系。


<details>
  <summary>Details</summary>
Motivation: 现有特征过滤方法依赖于成对统计依赖性度量，难以捕获高阶特征交互作用对目标的影响。

Method: 使用神经网络参数化互信息估计，并设计带有稀疏性正则项的损失函数，在两阶段框架中解耦表示学习与特征选择。

Result: 在合成数据和真实欺诈数据集上的实验表明，该方法能有效识别复杂依赖结构并实现精确特征选择。

Conclusion: MINERVA能够更好地建模高阶特征交互，提升特征选择的准确性和泛化能力。

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [225] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 提出TabImpute，一种基于预训练Transformer的零样本快速准确数据填补方法，无需微调或超参数调节。


<details>
  <summary>Details</summary>
Motivation: 由于现有填补方法在不同领域性能差异大且依赖超参数调优，缺乏通用默认方法。

Method: 基于TabPFN构建TabImpute，采用逐项特征化、合成数据生成流程和MissBench基准进行训练与评估。

Result: 实现比原方法快100倍，且在42个数据集和13种缺失模式下优于11种现有方法。

Conclusion: TabImpute实现了高效、通用的零样本缺失数据填补，具有强鲁棒性和实用性。

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [226] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为HyperAdaLoRA的新框架，通过引入基于注意力机制的超网络来加速AdaLoRA的收敛速度，同时保持性能，并实现动态秩分配。


<details>
  <summary>Details</summary>
Motivation: LoRA假设每个增量矩阵具有统一的秩，未考虑不同模块和层中权重矩阵的重要性差异；AdaLoRA虽能动态分配秩，但存在收敛慢和计算开销高的问题。

Method: 提出HyperAdaLoRA框架，利用基于注意力机制的超网络动态生成SVD参数（P, Λ, Q），并通过修剪超网络输出的奇异值实现动态秩分配，从而加速训练过程。

Result: 在多个数据集和模型上的实验表明，HyperAdaLoRA在不牺牲性能的前提下显著加快了收敛速度，并验证了其在其他LoRA-based方法中的广泛适用性。

Conclusion: HyperAdaLoRA有效解决了AdaLoRA收敛慢和计算开销高的问题，通过超网络和动态秩分配实现了高效、快速的参数高效微调。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [227] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: 本研究提出了一种基于对抗自编码器（AAE）和Wasserstein距离的框架，用于优化车载检测系统以提高桥梁损伤检测灵敏度，首次实现了对巡检车辆设计的系统性优化。


<details>
  <summary>Details</summary>
Motivation: 车载检测方法受车辆自身动力特性影响较大，限制了损伤识别效果，因此需要优化检测车辆的设计以提升其对桥梁损伤的敏感性。

Method: 采用基于对抗自编码器（AAE）的无监督深度学习方法重建加速度响应的频域表示，并通过最小化健康与受损桥梁状态下损伤指数分布之间的Wasserstein距离，优化两轴车辆轮胎悬架系统的质量和刚度；使用Kriging代理模型高效逼近目标函数并寻找最优参数配置。

Result: 频率比（车辆与桥梁一阶固有频率之比）在0.3至0.7之间的车辆检测效果最佳，接近共振的车辆性能较差；较轻的车辆需要更低的固有频率才能实现最优检测。

Conclusion: 通过优化检测车辆的动力学参数可显著提升驱动式检测的损伤识别能力，本文首次提出了针对驱动式感知专门设计的检测车辆优化框架。

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency-domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [228] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: 本文提出了TutorBench，一个用于评估大语言模型在高中和AP课程辅导能力的基准数据集，包含1490个由专家策划的样本，涵盖三种常见辅导任务，并采用基于LLM裁判和特定评分标准的细粒度自动评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着学生越来越多地使用大语言模型作为学习辅助工具，亟需构建能够理解学生核心需求、具备适应性、个性化指导能力和准确性的AI辅导系统，因此需要一个专门评估这些核心辅导技能的基准。

Method: 构建了一个包含1490个样本的数据集TutorBench，覆盖生成适应性解释、提供可操作反馈和有效提示以促进主动学习三类任务，每个样本配有特定评分标准，并采用基于LLM裁判的细粒度自动评估方法对16个前沿大模型进行评测。

Result: 评估结果显示，所有前沿大模型在TutorBench上的得分均未超过56%，在指导、诊断和支持学生方面的技能通过率低于60%；不同模型家族表现各异，Claude系列在促进主动学习方面表现最佳，但在其他两项任务上落后。

Conclusion: 当前的大语言模型在全面掌握有效辅导所需的核心技能方面仍有显著不足，TutorBench作为一个未饱和的综合基准，可为下一代AI导师系统的开发提供方向指引。

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [229] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: 本文证明了在置换等变学习规则下，神经元训练过程诱导出双利普希茨映射，并揭示了学习率在拓扑临界点上下对神经元拓扑结构的不同影响，提出了深度学习中普遍适用的两阶段学习动态理论。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降过程中神经网络的学习动态，特别是学习率对神经元分布拓扑结构的影响，以及如何在不同学习率下保持或简化模型表达能力。

Method: 通过分析置换等变学习规则（如SGD、Adam）下的训练过程，建立神经元之间的双利普希茨映射，并研究学习率超过拓扑临界点时的拓扑简化现象。

Result: 发现低于临界学习率η*时，训练过程保持神经元拓扑结构；高于η*时则允许拓扑简化，降低模型表达力；结合‘稳定性边缘’现象提出两阶段学习动态：先是在拓扑约束下的平滑优化，后是通过剧烈拓扑简化进行学习。

Conclusion: 该理论独立于具体架构和损失函数，为深度学习提供了可普遍应用的拓扑分析方法，揭示了学习率在调控模型表达性和训练动态中的关键作用。

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [230] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数集中现象的FP8低精度浮点格式ECF8，实现了生成式AI模型的无损压缩与高效计算，理论压缩极限接近FP4.67，在高达671B参数的模型上实现了最高26.9%的内存节省和177.1%的吞吐加速。


<details>
  <summary>Details</summary>
Motivation: 大规模生成式AI模型对高效部署提出了严苛要求，传统低精度计算方法存在数值不稳定或去量化开销等问题，亟需一种兼具稳定性、高效性和无损性的新型浮点格式。

Method: 通过理论与实证分析发现GenAI模型权重中存在指数集中现象，源于SGD诱导的α-稳定分布，并推导出指数熵的紧界；基于此提出ECF8，采用熵感知编码和GPU优化解码的无损压缩框架。

Result: 在LLM和DiT等大模型上验证了ECF8的有效性，最高实现26.9%内存节省和177.1%吞吐提升，且计算完全无损，模型输出无偏差。

Conclusion: 指数集中是训练后模型的一种统计规律，可作为设计FP8时代无损低精度浮点格式的理论基础，ECF8为大模型高效部署提供了可行路径。

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [231] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: 本文将神经算子分为两类：空间域模型和函数域模型，探讨其在物理规律驱动下的动力学学习机制，提出可解释性方法的局限性，展示双空间多尺度模型的优越性能，并强调构建融合已知物理原理的框架对提升泛化能力和发现隐含物理现象的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管神经算子在学习函数空间映射方面取得成功，但其学习机制仍缺乏深入理解，尤其是如何遵循物理原理进行数据驱动建模。

Method: 通过将神经算子分为空间域和功能域两类，从不同视角分析其学习机制，提出解释预测过程的方法，并设计双空间多尺度模型验证性能。

Result: 发现神经算子能从数据中学习隐含物理模式；提出的方法在特定情况下有效，但通用可解释方法仍欠缺；双空间多尺度模型达到SOTA性能。

Conclusion: 需要更通用的解释方法和基于物理原则的框架来提升神经算子的可解释性、泛化能力及对复杂物理系统的建模潜力。

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [232] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: EvoSpeak是一个结合遗传编程（GP）和大语言模型（LLM）的新框架，旨在提升启发式算法在动态大规模优化问题中的效率、可解释性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统GP生成的启发式规则在复杂场景下往往过于复杂，导致难以解释、收敛慢且难以跨任务迁移。

Method: EvoSpeak利用LLM从高质量GP启发式中提取知识，用于生成预热种群、翻译自然语言解释，并实现跨任务的知识迁移和偏好感知启发式生成。

Result: 在动态柔性作业车间调度问题上的实验表明，EvoSpeak能生成更优的启发式规则，加快进化速度，并提供人类可读的解释报告。

Conclusion: 通过融合GP的符号推理与LLM的生成能力，EvoSpeak推动了智能、透明且用户对齐的启发式方法的发展。

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [233] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 本文提出了GRAFT和P-GRAFT方法，将基于拒绝采样的微调统一到PPO框架下，并通过重塑奖励隐式执行策略梯度优化，同时引入逆噪声校正来提升生成效果，在文本到图像、分子生成等多个任务上优于传统策略梯度方法。


<details>
  <summary>Details</summary>
Motivation: 预训练的扩散模型虽能有效建模数据分布，但难以直接适应下游应用中的奖励函数；现有策略梯度方法因边缘似然不可计算而不适用于扩散模型，因此需要新的微调框架。

Method: 提出并统一RAFT的变体为GRAFT，证明其等价于使用重塑奖励的PPO；进一步设计P-GRAFT以在中间噪声水平进行分布调控，并引入逆噪声校正技术来改进流模型。

Result: 在文本到图像生成（如Stable Diffusion 2）、布局生成、分子生成和无条件图像生成任务中验证了方法的有效性；相比基线模型，T2I任务中VQAScore相对提升8.81%，且在更低FLOPs下改善了FID分数。

Conclusion: P-GRAFT通过在中间噪声层次调节分布实现了更有效的微调，结合逆噪声校正可在无需显式奖励的情况下提升生成质量，为扩散模型的对齐提供了一种高效统一的框架。

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [234] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: 提出了一种名为RAMAC的风险感知多模态演员-评论家框架，结合生成式演员与分布鲁棒性评论家，在离线强化学习中实现高回报同时降低下尾风险。


<details>
  <summary>Details</summary>
Motivation: 在无法在线收集数据的安全关键领域，现有风险规避的离线强化学习方法往往牺牲性能或限制策略表达能力，需要一种既能保证安全又能保持高性能的框架。

Method: 引入RAMAC框架，将表达能力强的生成式演员（如扩散模型和流匹配模型）与分布式评论家结合，通过生成路径优化包含分布风险和行为克隆损失的复合目标函数。

Result: 在多个Stochastic-D4RL任务上验证了RAMAC的有效性，显著提升了CVaR₀.₁指标，同时保持了较高的整体回报。

Conclusion: RAMAC成功实现了在复杂多模态场景下的风险敏感学习，平衡了安全性与策略性能，为风险规避的离线强化学习提供了新思路。

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [235] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: 提出了一种基于时空Transformer的轻量级入侵检测系统TSLT-Net，用于无人机网络，具有高准确率、低资源消耗，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有入侵检测机制在动态和资源受限的无人机网络中缺乏适应性、效率和通用性。

Method: 设计了基于自注意力机制的TSLT-Net，统一建模网络流量的时序模式与空间依赖，并支持多分类攻击识别与二分类异常检测。

Result: 在ISOT无人机异常检测数据集上实现99.99%的多分类准确率和100%的二分类检测准确率，模型仅占0.04 MB内存，含9722个可训练参数。

Conclusion: TSLT-Net是一种高效、可扩展的实时无人机网络安全解决方案，特别适合任务关键型无人机系统的边缘部署。

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [236] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: 提出了一种基于双注意力机制的深度学习框架CST AFNet，用于物联网环境下的鲁棒入侵检测，在Edge IIoTset数据集上实现了99.97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展带来了复杂的安全挑战，现有方法难以应对异构、资源受限和分布式的环境，需要更高效的入侵检测模型。

Method: 结合多尺度CNN进行空间特征提取，BiGRU捕捉时间依赖性，并引入通道和时间双注意力机制以增强对关键模式的关注。

Result: 在Edge IIoTset数据集上，模型对15类攻击和正常流量的分类准确率达到99.97%，宏平均精确率、召回率和F1分数均超过99.3%。

Conclusion: CST AFNet是一种高效、可扩展的实时物联网威胁检测方案，适用于复杂的IoT和IIoT环境，有助于构建更安全智能的网络物理系统。

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [237] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 提出了一种新理论和工具，用于理解大规模模型超参数损失面的渐近特性，基于随机搜索发现的新分布可提取关键特征并推导出随机搜索的渐近定律。


<details>
  <summary>Details</summary>
Motivation: 现代模型规模大，难以进行广泛的超参数搜索，因此需要理解超参数损失面以设计跨尺度有效的训练策略，但目前缺乏相关分析工具。

Method: 提出一种基于随机搜索的新技术，发现在接近最优解时损失面呈现简单结构，并识别出有效维度和最佳损失等基本特征；通过分析随机搜索最佳结果的分布来刻画这些特征。

Result: 发现了随机搜索在渐近区域中的新分布规律，其参数对应损失面的关键特征；由此推导出随机搜索的渐近收敛定律，并实现了对最佳性能的置信区间估计和有效超参数数量的判定。

Conclusion: 该理论为理解超参数优化提供了新工具，能够分析和外推随机搜索的表现，有助于设计更高效的超参数优化方法。

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [238] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 本文提出了深度时间序列预测中的“精度定律”，揭示了最小预测误差与窗口内序列模式复杂度之间的指数关系，为评估预测性能上限提供了新方法，并指导识别饱和任务和设计大模型训练策略。


<details>
  <summary>Details</summary>
Motivation: 由于时间序列预测存在固有的不确定性，其性能存在非零下限，导致研究人员在标准基准上难以判断改进的有效性。因此，需要一种新的方法来估计深度时间序列预测的性能上限，以明确研究方向。

Method: 基于超过2800个新训练的深度预测模型进行严格的统计检验，分析窗口级序列特性对预测性能的影响，提出“精度定律”来刻画最小预测误差与模式复杂度的关系。

Result: 发现了深度模型的最小预测误差与窗口级序列模式复杂度之间存在显著的指数关系（即精度定律），并利用该定律识别出常用基准中的饱和任务，提出有效的大型时间序列模型训练策略。

Conclusion: 精度定律为深度时间序列预测提供了性能上限的估计方法，有助于避免在已饱和的任务上过度优化，为未来研究提供了理论依据和实践指导。

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [239] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 本文提出了一种受生物启发的生成模型，基于几何布朗运动和对数正态分布，采用乘性更新规则，通过反向随机微分方程离散化推导出与Dale定律一致的指数梯度下降，并提出了适用于非负数据的乘性去噪得分匹配框架，在MNIST等数据集上验证了其生成能力。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降在生物学系统中与实际学习机制不一致，而Dale定律表明突触的抑制和兴奋作用不会在学习过程中互换，因此需要构建符合生物学习机制的优化方法。

Method: 基于几何布朗运动对应的反向时间随机微分方程进行离散化，得到乘性更新规则；提出新的乘性去噪得分匹配框架，适用于对数正态分布的非负数据。

Result: 推导出的更新规则与基于Dale定律的指数梯度下降采样等价；所提得分匹配方法自然适用于对数正态数据；在MNIST、Fashion MNIST和Kuzushiji数据集上实现了有效的样本生成。

Conclusion: 这是首个基于几何布朗运动、采用乘性更新的生物启发式生成模型，为生物可解释的机器学习优化与生成建模提供了新方向。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [240] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒属性图聚类方法RAGC，结合混合协同增强（HCA）和对比样本自适应差异感知（CSADA），通过节点和边级别的嵌入增强及差异化的对比学习策略，提升了表示学习的判别能力，在六个基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对比属性图聚类方法主要关注节点级增强，忽略边级增强及其与节点级增强的交互，并且对所有对比样本对一视同仁，限制了模型的判别能力。

Method: 提出RAGC模型，包含两个核心模块：1）HCA：同时进行节点级和边级嵌入表示与增强，并相互引导；2）CSADA：利用高置信度伪标签设计自适应权重调制函数，区分难易正负样本对。

Result: 在六个基准数据集上的图聚类实验表明，RAGC优于多种先进的对比属性图聚类方法。

Conclusion: RAGC通过联合节点与边级别增强以及自适应区分对比样本，有效提升了属性图聚类的性能，展现出更强的表示学习判别能力。

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [241] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: 本文提出了TokenFlow，一种通过抢占式请求调度和主动KV缓存管理来提升实时大语言模型（LLM）服务中文本流性能的新系统。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM服务系统由于非抢占式请求调度和反应式内存管理导致灵活性差，在请求突发时资源利用率低、并行处理能力弱，难以平衡响应速度与生成稳定性。

Method: TokenFlow采用基于实时token缓冲区占用率和消耗速率的动态请求优先级调度，并在后台主动进行GPU与CPU间KV缓存转移，同时重叠I/O与计算以减少抢占开销。

Result: 在Llama3-8B和Qwen2.5-32B模型上的实验表明，TokenFlow在不降低整体吞吐的情况下，有效吞吐最高提升82.5%，P99首次响应时间（TTFT）最多降低80.2%。

Conclusion: TokenFlow通过抢占式调度和主动缓存管理显著提升了LLM流式服务的响应性和资源利用率，适用于高并发实时场景。

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [242] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: 提出了一种名为SIT-FUSE的自监督学习框架，利用多传感器卫星数据（包括反射率和太阳诱导荧光）实现有害藻华（HAB）严重程度和种类的检测与制图，无需依赖标注数据，且在多个区域验证表现出与实地测量高度一致的结果。


<details>
  <summary>Details</summary>
Motivation: 传统HAB监测方法依赖大量标注数据，难以扩展；而现实环境中标签稀缺，亟需一种可扩展、无需人工标注的监测方法。

Method: 融合多种卫星传感器的反射率数据与TROPOMI的太阳诱导荧光（SIF），采用自监督表征学习和分层深度聚类，对浮游植物浓度和种类进行分类，并通过墨西哥湾和南加州的实地数据进行验证。

Result: SIT-FUSE在2018–2025年期间对总浮游植物、Karenia brevis、Alexandrium spp. 和 Pseudo-nitzschia spp. 的检测结果与实地观测数据表现出强一致性，能够生成可解释的HAB严重程度和种类分布图。

Conclusion: 该框架实现了在标签稀缺环境下的可扩展HAB监测，推动了自监督学习在全局水生生物地球化学中的应用，为未来业务化运行提供了可行路径。

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [243] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: 研究发现神经网络学习中可能存在非梯度的“旋度”成分，这些成分在某些架构下可促进学习，挑战了传统的梯度下降范式。


<details>
  <summary>Details</summary>
Motivation: 探索生物神经网络是否使用类似人工神经网络的梯度下降学习机制，并检验非梯度成分（如旋度）是否也能有效优化损失函数。

Method: 在可解析的学生-教师框架中分析前馈网络，通过引入具有规则翻转可塑性的神经元系统地引入非梯度动力学。

Result: 小旋度项保持解流形稳定，类似梯度下降；强旋度项可能引发混沌或加速学习，帮助逃离鞍点。特定网络结构能支持由多样化学习规则驱动的鲁棒学习。

Conclusion: 非梯度‘旋度’成分可在某些神经网络架构中支持高效学习，为神经网络学习理论提供了重要补充和反例。

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [244] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 该研究探讨了在推理时通过简单的激活编辑（如模型abliteration）对开放权重大语言模型进行修改后，常见的安全干预措施（如拒绝训练或元标签训练）是否仍然有效。作者在SmolLM2-1.7B的安全预训练检查点及多个开源基线上进行了系统评估，分析了拒绝行为的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着开放权重语言模型在推理时可通过激活编辑轻松修改，其安全性可能被绕过。因此，需要评估现有安全机制在面对此类轻量级干预时的鲁棒性，以确保模型在实际应用中的安全性。

Method: 采用一种名为abliteration的轻量级投影技术，去除与拒绝相关的敏感方向，并在20个模型（包括原始和abliterated版本）上对100个有害与无害平衡的提示进行测试。使用多个自动判别器将响应分类为“拒绝”或“非拒绝”，并在人工标注子集上验证判别器准确性。同时探测模型能否识别自身输出中的拒绝行为。

Result: 研究发现部分安全组件在abliteration下仍具鲁棒性，且不同判别器的选择显著影响评估结果。此外，某些检查点阶段的安全预训练成分对拒绝行为更具抵抗力。模型在自我识别拒绝方面表现有限。

Conclusion: 安全干预的效果在推理时编辑下存在差异，需结合检查点级分析和多判别器评估来全面衡量鲁棒性。研究提出了将推理时编辑纳入安全评估的实用协议，强调了持续监控和综合评估的重要性。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [245] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: 本文研究了梯度下降在深度ReLU网络中的泛化性能，通过权衡优化误差和泛化误差，建立了具有多项式深度依赖的最优泛化率。


<details>
  <summary>Details</summary>
Motivation: 现有研究在深度神经网络的泛化性能上要么得到次优速率，要么对激活函数平滑性有要求且对网络深度呈指数依赖，因此需要探索梯度下降在非平滑激活函数（如ReLU）下的最优泛化能力。

Method: 通过假设数据在NTK框架下以边界γ可分，结合对激活模式在参考模型附近的精细控制，推导出更紧致的Rademacher复杂度上界，从而分析梯度下降的泛化误差。

Result: 证明了深度ReLU网络在梯度下降下的超额风险速率为\(\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))\)，与核方法中最优SVM型速率一致，仅有多项式深度因子差异。

Conclusion: 梯度下降在深度ReLU网络中可以实现接近核方法最优的泛化速率，且对网络深度的依赖为多项式级别，优于以往的指数依赖结果。

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [246] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: OptunaHub是一个集中化黑盒优化方法和基准的社区平台，旨在促进跨领域研究和应用。


<details>
  <summary>Details</summary>
Motivation: 黑盒优化在多个领域中推动进展，但研究工作往往分散，缺乏统一平台。

Method: 提供统一的Python API、贡献者包注册表和网页界面，集成到Optuna组织的开源仓库中。

Result: 实现了方法的集中化和可搜索性，支持跨领域研究和社区贡献循环。

Conclusion: OptunaHub有助于建立黑盒优化领域的协作生态，推动AutoML和材料信息学等领域的共同发展。

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [247] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文提出了一种改进在线共形预测（OCP）中阈值更新步骤的方法，通过引入衡量预测区间相关性的函数，替代传统的二元判断，从而获得更窄且保持覆盖率的有效预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有OCP方法在更新阈值时仅考虑预测区间是否覆盖真实值，忽略了区间的相关性信息，导致可能产生过宽的预测区间。本文旨在利用这一被忽视的信息来提升预测性能。

Method: 提出将传统二元的（内部/外部）评估替换为一类能量化预测区间与真实值之间相关性的新函数，并将其用于阈值的动态更新过程中。

Result: 实验结果表明，所提方法在多个真实数据集上相比现有OCP方法能生成更窄的预测区间，同时保持良好的覆盖率。

Conclusion: 通过在阈值更新中引入相关性度量，可以有效提升OCP方法的预测效率，在不牺牲覆盖率的前提下获得更精确的不确定性估计。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [248] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: 本文提出了CLEAR方法，首次对Transformer架构的推理能耗进行细粒度实证分析，揭示注意力模块每FLOP能耗显著更高，表明FLOP不能准确反映真实能耗，为构建能效更高的模型提供了组件级优化基础。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注训练能耗且仅报告模型级粗粒度指标，缺乏对推理阶段组件级能耗的精细测量，导致能效未被充分重视。因此需要一种新方法来量化各组件的能耗以指导能效优化。

Method: 提出CLEAR（Component-Level Energy Assessment via Repeated sampling）方法，通过重复采样解决微秒级组件执行时间与毫秒级能耗监测之间的时间不匹配问题，并在15个涵盖四种架构的模型上进行细粒度能耗评估，保持组件级能耗方差低于9.5%，覆盖超90%总能耗。

Result: 发现注意力块每FLOP的能耗显著高于其他组件，说明能耗与FLOP数不成正比；FLOP作为计算量指标无法准确反映实际能耗；成功建立了组件级能耗基线。

Conclusion: FLOPs不足以衡量组件级能效，必须转向细粒度的组件级分析；CLEAR方法为未来设计更节能的Transformer模型提供了关键洞察和优化方向。

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [249] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: 提出了一种Group DRO的分层扩展方法，以同时应对组间和组内分布偏移，在标准和新设计的基准上均表现出更强的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Group DRO虽能处理组间偏移，但对样本较少的少数群体中的组内分布偏移仍脆弱，需更全面的不确定性建模。

Method: 提出一种分层的Group DRO扩展方法，扩大模糊集以同时建模组间和组内分布不确定性，并设计新的基准来模拟现实中的少数群体分布偏移。

Result: 在新基准和标准基准上均显著优于现有方法，尤其在组内分布偏移下保持鲁棒性，而现有方法在此情况下失效。

Conclusion: 通过扩展模糊集以涵盖多层次分布不确定性，可有效提升模型在复杂分布偏移下的鲁棒性，应重视对组内分布偏移的建模。

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [250] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: 本文提出了一种通用框架，用于将随机学习算法适应于在线学习的随机顺序模型，并证明了在预测延迟、带约束的在线学习和切换成本下的赌博机等问题中可获得改进的遗憾界；此外，在线分类的学习能力在随机顺序下由VC维而非Littlestone维刻画。


<details>
  <summary>Details</summary>
Motivation: 随机顺序模型中的非平稳性可能损害随机学习算法的性能，而现有方法在随机顺序和对抗模型之间存在差距，因此需要一种能同时保持良好遗憾保证的自适应方法。

Method: 提出一个通用模板，将适用于随机模型的简单无遗憾算法调整以适应随机顺序模型，并通过理论分析验证其在多种场景下的有效性。

Result: 成功恢复了在延迟预测、带约束在线学习和带切换成本的赌博机问题中的改进遗憾界，并证明在线分类在随机顺序下的可学习性由VC维决定。

Conclusion: 该通用模板有效桥接了随机与随机顺序模型之间的差距，表明在随机顺序下，VC维是在线分类可学习性的关键指标，区别于对抗模型中的Littlestone维。

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [251] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: 本文提出了一种名为FlexiQ的自适应混合精度量化方法，用于计算机视觉模型，能够在保持推理精度的同时动态调整低比特通道比例，有效应对实时工作负载波动，并在定制NPU和GPU上实现了高效准确的推理。


<details>
  <summary>Details</summary>
Motivation: 神经网络在NPU和GPU等硬件加速器上运行时面临资源扩展难、难以应对实时工作负载波动的问题，因此需要一种灵活且高效的量化方案来提升资源利用率和推理性能。

Method: FlexiQ选择性地对值范围较小的特征通道应用低位宽计算，采用高效的位降低方法减少量化误差，并实时调整低比特宽度通道的比例，以适应变化的推理负载。

Result: 在11个卷积和Transformer-based视觉模型上的实验表明，经过微调的4-bit模型平均精度提高6.6%，优于四种最先进的量化技术；50% 4-bit模型仅损失0.6%精度，却获得全4-bit模型相对于8-bit模型40%的加速效果，且在NPU和GPU上延迟评估显示运行时开销极小。

Conclusion: FlexiQ通过自适应混合精度量化实现了精度与延迟的良好权衡，具备硬件效率高、实时调节能力强的优点，适用于处理动态变化的推理任务。

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [252] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文提出了一种基于控制理论中Hankel奇异值分析的训练时压缩方法，用于优化状态空间模型（SSM），在保持表达能力的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型在长序列建模中面临表达能力与计算开销之间的权衡，传统小维度模型易丢失关键结构，需寻找更优的压缩策略。

Method: 利用控制理论中的Hankel奇异值分析，在训练过程中识别并保留高影响的状态维度，实现动态的平衡截断压缩。

Result: 实验表明该方法在训练中显著加速优化过程，压缩后的模型仍保留任务关键结构，性能优于直接以小维度训练的模型。

Conclusion: 从大模型开始并在训练中收缩的SSM能在保持高性能的同时实现计算高效，验证了训练时压缩的有效性。

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [253] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: 本文重新审视了视觉自回归（VAR）模型，将其形式化为一种迭代细化框架，通过控制实验量化了影响其效率和保真度的三个设计选择，并探讨了该框架在图生成和天气预报中的扩展应用。


<details>
  <summary>Details</summary>
Motivation: 旨在理解VAR模型高效性和高质量生成的原因，并探索其更广泛的应用潜力。

Method: 将VAR模型视为构建拉普拉斯式潜在金字塔的确定性前向过程，并结合学习到的从粗到精的反向重建过程，引入扩散模型视角进行分析。

Result: 明确了三个关键设计选择对保真度和速度的贡献：在学习的潜在空间中进行细化、将预测视为离散分类、按空间频率划分任务。

Conclusion: 提出的框架不仅解释了VAR的优势，还为其利用扩散模型工具提供了接口，同时保持少步数和尺度并行生成的能力。

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [254] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 提出了一种名为SASL的可解释建模框架，用于从多模态生命日志数据中预测个性化健康结果，结合稀疏线性模型与基于置信度的轻量级LightGBM集成，在保持高透明度的同时实现与黑箱模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在预测个性化健康结果时缺乏可解释性，且难以处理个体间差异，限制了其在临床实践中的应用。

Method: 提出SASL框架，结合带个体交互项的最小二乘回归与嵌套F检验的向后特征消除，实现全局与个体效应分离；采用回归后阈值法优化有序分类指标，并通过置信度门控融合LightGBM输出以提升精度。

Result: 在CH-2025数据集上，SASL-LightGBM混合框架达到与复杂黑箱模型相当的预测性能（宏F1分数），但参数更少、可解释性更强。

Conclusion: SASL在保持高度透明和可解释性的同时，实现了对睡眠质量和压力等健康指标的有效个性化预测，为临床决策提供了实用且可操作的工具。

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [255] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: 提出了一种名为Karma的知识感知模型，用于电池健康预测，结合信号分解与双流深度学习架构，并引入经验知识指导，显著提升了电池容量估计和剩余寿命预测的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型在电池健康预测中缺乏知识引导，难以应对非线性、噪声和容量再生等复杂退化行为，导致长期预测不可靠。

Method: 采用信号分解提取不同频段的电池信号，设计双流深度学习架构分别捕捉低频长期退化趋势和高频短期动态，并将电池退化建模为双指数函数，结合粒子滤波优化参数以实现物理一致性预测。

Result: 在两个主流数据集上，相比当前先进算法，平均误差分别降低了50.6%和32.6%，表现出优异的鲁棒性和泛化能力。

Conclusion: Karma模型通过知识引导和频率自适应学习，实现了更准确、可靠且具有不确定性量化的电池健康预测，具有广泛应用于电池管理系统的潜力。

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [256] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: RoiRL是一种轻量级的离线强化学习方法，用于提升大语言模型的推理能力，无需维护参考模型，训练更快且性能优于TTRL。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时强化学习（TTRL）依赖在线强化学习，计算成本高，且需要参考模型，限制了其可扩展性。

Method: 提出RoiRL，采用离线迭代强化学习，利用加权对数似然目标进行优化，使用多数投票奖励，避免维护参考模型。

Result: RoiRL训练速度提高2.5倍，在推理基准上持续优于TTRL，显著降低内存和计算需求。

Conclusion: RoiRL为无标签条件下可扩展的自改进大语言模型提供了一条高效路径。

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [257] [DMark: Order-Agnostic Watermarking for Diffusion Large Language Models](https://arxiv.org/abs/2510.02902)
*Linyu Wu,Linhao Zhong,Wenjie Qu,Yuexin Li,Yue Liu,Shengfang Zhai,Chunhua Shen,Jiaheng Zhang*

Main category: cs.LG

TL;DR: 本文提出了DMark，首个专为扩散大语言模型（dLLMs）设计的水印框架，解决了非自回归生成中传统水印失效的问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法依赖于自回归模型的顺序生成特性，无法适用于非顺序解码的dLLMs，因此需要新的水印机制。

Method: DMark引入三种策略：预测性水印、双向水印及两者的结合，利用模型预测和前后向依赖关系恢复水印可检测性。

Result: 在多个dLLMs上的实验显示，DMark在1%误报率下达到92.0-99.5%的检测率，显著优于现有方法的49.6-71.2%，且保持文本质量并具备抗文本篡改能力。

Conclusion: DMark证明了在非自回归语言模型上实现高效水印是可行的，为dLLMs的内容溯源提供了有效解决方案。

Abstract: Diffusion large language models (dLLMs) offer faster generation than
autoregressive models while maintaining comparable quality, but existing
watermarking methods fail on them due to their non-sequential decoding. Unlike
autoregressive models that generate tokens left-to-right, dLLMs can finalize
tokens in arbitrary order, breaking the causal design underlying traditional
watermarks. We present DMark, the first watermarking framework designed
specifically for dLLMs. DMark introduces three complementary strategies to
restore watermark detectability: predictive watermarking uses model-predicted
tokens when actual context is unavailable; bidirectional watermarking exploits
both forward and backward dependencies unique to diffusion decoding; and
predictive-bidirectional watermarking combines both approaches to maximize
detection strength. Experiments across multiple dLLMs show that DMark achieves
92.0-99.5% detection rates at 1% false positive rate while maintaining text
quality, compared to only 49.6-71.2% for naive adaptations of existing methods.
DMark also demonstrates robustness against text manipulations, establishing
that effective watermarking is feasible for non-autoregressive language models.

</details>


### [258] [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
*Jan-Philipp von Bassewitz,Adeel Pervez,Marco Fumero,Matthew Robinson,Theofanis Karaletsos,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出Cell-Mechanistic Neural Networks (Cell-MNN)，一种端到端的编码器-解码器模型，通过局部线性化ODE显式学习可解释的基因相互作用，用于建模单细胞分化动态。


<details>
  <summary>Details</summary>
Motivation: 现有细胞分化动态建模方法依赖计算昂贵的最优传输预处理和多阶段训练，且无法发现显式的基因相互作用。

Method: 设计Cell-MNN模型，其潜在表示为控制细胞从干细胞向组织细胞演化动态的局部线性化常微分方程（ODE），采用端到端训练，仅需标准PCA预处理。

Result: 在单细胞基准任务上表现具有竞争力，扩展性和多数据集联合训练优于现有最先进方法，并通过与TRRUST基因互作数据库比对验证了所学基因互作的生物学合理性。

Conclusion: Cell-MNN是一种高效、可解释且可扩展的单细胞动态建模方法，能够显式学习生物一致的基因调控关系。

Abstract: Modeling the dynamics of cellular differentiation is fundamental to advancing
the understanding and treatment of diseases associated with this process, such
as cancer. With the rapid growth of single-cell datasets, this has also become
a particularly promising and active domain for machine learning. Current
state-of-the-art models, however, rely on computationally expensive optimal
transport preprocessing and multi-stage training, while also not discovering
explicit gene interactions. To address these challenges we propose
Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture
whose latent representation is a locally linearized ODE governing the dynamics
of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end
(besides a standard PCA pre-processing) and its ODE representation explicitly
learns biologically consistent and interpretable gene interactions.
Empirically, we show that Cell-MNN achieves competitive performance on
single-cell benchmarks, surpasses state-of-the-art baselines in scaling to
larger datasets and joint training across multiple datasets, while also
learning interpretable gene interactions that we validate against the TRRUST
database of gene interactions.

</details>


### [259] [FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting](https://arxiv.org/abs/2510.02914)
*Tharuka Kasthuri Arachchige,Veselka Boeva,Shahrooz Abghari*

Main category: cs.LG

TL;DR: 提出了一种名为FeDABoost的新型联邦学习框架，通过动态提升机制和自适应梯度聚合策略，在非独立同分布（non-IID）环境下提高了模型性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 在非IID数据分布下，传统联邦学习方法在模型聚合和表现较差客户端的训练方面存在性能与公平性问题，因此需要一种能动态调整客户端权重并增强困难样本学习的方法。

Method: 结合Multiclass AdaBoost（SAMME）的加权机制，设计了根据客户端本地错误率分配聚合权重的自适应梯度聚合策略；同时，通过动态调整焦点损失中的聚焦参数，对表现较差的客户端进行局部训练增强。

Result: 在MNIST、FEMNIST和CIFAR10三个基准数据集上评估表明，FeDABoost相比FedAvg和Ditto在模型性能和公平性方面均有提升。

Conclusion: FeDABoost通过动态提升和自适应聚合有效改善了非IID场景下联邦学习的性能与公平性，具有良好的实际应用潜力。

Abstract: This work focuses on improving the performance and fairness of Federated
Learning (FL) in non IID settings by enhancing model aggregation and boosting
the training of underperforming clients. We propose FeDABoost, a novel FL
framework that integrates a dynamic boosting mechanism and an adaptive gradient
aggregation strategy. Inspired by the weighting mechanism of the Multiclass
AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to
clients with lower local error rates, thereby promoting more reliable
contributions to the global model. In parallel, FeDABoost dynamically boosts
underperforming clients by adjusting the focal loss focusing parameter,
emphasizing hard to classify examples during local training. We have evaluated
FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared
its performance with those of FedAvg and Ditto. The results show that FeDABoost
achieves improved fairness and competitive performance.

</details>


### [260] [RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification](https://arxiv.org/abs/2510.02936)
*Aydin Javadov,Samir Garibov,Tobias Hoesli,Qiyang Sun,Florian von Wangenheim,Joseph Ollier,Björn W. Schuller*

Main category: cs.LG

TL;DR: 提出一种基于检索增强的随机稀疏采样框架，用于可解释的变长医疗时间序列分类。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列存在稀疏、噪声和长度可变等挑战，现有方法在可解释性和鲁棒性方面仍有不足。

Method: 通过通道内相似性加权窗口预测，并在概率空间中聚合，实现检索引导的分类，生成凸性的序列级评分和明确的证据路径。

Result: 在四个医疗中心的iEEG数据上验证，取得了有竞争力的分类性能，同时提升了模型透明度和可解释性。

Conclusion: 该方法为可靠且可解释的临床变长时间序列分类提供了有效解决方案。

Abstract: Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.

</details>


### [261] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 本文首次从风险感知决策的角度对持续强化学习（continual RL）进行了形式化理论研究，提出了一类适用于持续学习的新型遍历性风险度量（ergodic risk measures），并通过案例研究和实验验证了其理论合理性和直观优势。


<details>
  <summary>Details</summary>
Motivation: 现有的持续强化学习主要基于风险中性决策，即优化长期性能的期望值。然而，在现实场景中，仅依赖均值可能不足以应对不确定性，因此需要引入风险感知决策来优化超越均值的性能度量。但经典的风险度量理论在持续学习场景中存在不兼容问题，亟需新的理论框架。

Method: 通过分析经典风险度量理论在持续RL中的局限性，构建了一个新的遍历性风险度量框架，并将其融入持续学习环境。同时设计了一个风险感知持续学习的案例研究，结合实证结果验证所提方法的有效性。

Result: 证明了传统风险度量理论无法直接适用于持续RL；提出了与持续学习兼容的遍历性风险度量；案例研究和实验结果表明该方法具有良好的理论性质和直观效果。

Conclusion: 本文建立了首个面向持续强化学习的风险感知理论框架，提出的遍历性风险度量为未来研究提供了理论基础，并推动了更安全、鲁棒的终身学习智能体的发展。

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [262] [ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data](https://arxiv.org/abs/2510.02952)
*Santanu Subhash Rathod,Francesco Ceccarelli,Sean B. Holden,Pietro Liò,Xiao Zhang,Jovan Tanevski*

Main category: cs.LG

TL;DR: ContextFlow是一种结合先验知识的上下文感知流匹配框架，用于从纵向空间分辨组学数据中推断组织结构动态变化。


<details>
  <summary>Details</summary>
Motivation: 理解发育、再生、疾病进展等过程中组织结构和功能的变化需要准确推断轨迹，但现有方法缺乏生物学合理性。

Method: ContextFlow将局部组织结构和配体-受体通讯模式整合到转移可行性矩阵中，以正则化最优传输目标，从而在流匹配框架中引入生物学上下文约束。

Result: 在三个数据集上，ContextFlow在定量和定性指标上均优于最先进的流匹配方法，生成的轨迹更具生物一致性。

Conclusion: ContextFlow是一个可泛化的框架，能生成既统计合理又生物学可解释的时空动态轨迹。

Abstract: Inferring trajectories from longitudinal spatially-resolved omics data is
fundamental to understanding the dynamics of structural and functional tissue
changes in development, regeneration and repair, disease progression, and
response to treatment. We propose ContextFlow, a novel context-aware flow
matching framework that incorporates prior knowledge to guide the inference of
structural tissue dynamics from spatially resolved omics data. Specifically,
ContextFlow integrates local tissue organization and ligand-receptor
communication patterns into a transition plausibility matrix that regularizes
the optimal transport objective. By embedding these contextual constraints,
ContextFlow generates trajectories that are not only statistically consistent
but also biologically meaningful, making it a generalizable framework for
modeling spatiotemporal dynamics from longitudinal, spatially resolved omics
data. Evaluated on three datasets, ContextFlow consistently outperforms
state-of-the-art flow matching methods across multiple quantitative and
qualitative metrics of inference accuracy and biological coherence. Our code is
available at: \href{https://github.com/santanurathod/ContextFlow}{ContextFlow}

</details>


### [263] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: 提出了一种用于无监督模型评估和排序的统一框架，利用预测的置信度和分散性来估计模型在分布偏移下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在真实场景中，标签数据往往不可用，因此需要在无监督条件下评估模型在分布偏移下的泛化能力。

Method: 提出并系统评估基于置信度、分散性以及二者结合的混合指标，使用预测矩阵的核范数等方法进行模型评估和排序。

Result: 混合指标在多种模型、数据集和分布偏移类型下均优于单一指标，核范数在真实数据和类别不平衡情况下表现稳健。

Conclusion: 置信度与分散性是评估模型泛化能力的有效互补信号，所提出的框架为实际部署中的无监督模型评估提供了实用且通用的解决方案。

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


### [264] [From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime](https://arxiv.org/abs/2510.03003)
*Akriti Sharma,Dogan Altan,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: 本文提出了一种基于迁移学习的船舶轴功率预测方法，先在高频数据上训练模型，再用低频的午报数据进行微调，显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 准确预测船舶轴功率对节能降耗至关重要，但高质量传感器数据难以获取且成本高，因此需要利用午报等低频数据提升预测效果。

Method: 采用迁移学习方法，先在某一船舶的高频数据上预训练模型，然后使用其他船舶的每日午报数据进行微调，并在不同相似度的船舶上进行验证。

Result: 相较于仅使用午报数据训练的模型，该方法在姊妹船、相似船和不同船上的平均绝对百分比误差分别降低了10.6%、3.6%和5.3%。

Conclusion: 迁移学习能有效利用有限的低频数据提升轴功率预测精度，具有良好的跨船舶适用性和实际应用价值。

Abstract: With the growth of global maritime transportation, energy optimization has
become crucial for reducing costs and ensuring operational efficiency. Shaft
power is the mechanical power transmitted from the engine to the shaft and
directly impacts fuel consumption, making its accurate prediction a paramount
step in optimizing vessel performance. Power consumption is highly correlated
with ship parameters such as speed and shaft rotation per minute, as well as
weather and sea conditions. Frequent access to this operational data can
improve prediction accuracy. However, obtaining high-quality sensor data is
often infeasible and costly, making alternative sources such as noon reports a
viable option. In this paper, we propose a transfer learning-based approach for
predicting vessels shaft power, where a model is initially trained on
high-frequency data from a vessel and then fine-tuned with low-frequency daily
noon reports from other vessels. We tested our approach on sister vessels
(identical dimensions and configurations), a similar vessel (slightly larger
with a different engine), and a different vessel (distinct dimensions and
configurations). The experiments showed that the mean absolute percentage error
decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,
and 5.3 percent for a different vessel, compared to the model trained solely on
noon report data.

</details>


### [265] [BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia](https://arxiv.org/abs/2510.03004)
*Tianzheng Hu,Qiang Li,Shu Liu,Vince D. Calhoun,Guido van Wingen,Shujian Yu*

Main category: cs.LG

TL;DR: 提出了一种基于信息瓶颈原理的端到端图神经网络框架BrainIB++，用于从静息态fMRI数据中识别精神分裂症的功能脑网络生物标志物，在多个数据集上表现出优越的诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖人工特征工程引入偏差，而深度学习模型缺乏可解释性，限制了其在临床诊断中的应用。因此需要一种兼具高准确性与可解释性的自动化诊断模型。

Method: 提出BrainIB++，一种结合信息瓶颈原理的端到端图神经网络框架，自动识别最具信息量的脑区子图作为生物标志物，无需人工特征工程，并提升模型可解释性。

Result: 在三个多中心精神分裂症数据集上优于九种现有脑网络分类方法，具有更高的诊断准确率和对未见数据的良好泛化能力；所识别的子图与已知的精神分裂症临床生物标志物一致，主要集中于视觉、感觉运动和高级认知功能网络。

Conclusion: BrainIB++在保持高诊断性能的同时提升了模型的可解释性和临床适用性，为精神疾病基于fMRI的自动化诊断提供了可靠且可解释的新框架。

Abstract: The development of diagnostic models is gaining traction in the field of
psychiatric disorders. Recently, machine learning classifiers based on
resting-state functional magnetic resonance imaging (rs-fMRI) have been
developed to identify brain biomarkers that differentiate psychiatric disorders
from healthy controls. However, conventional machine learning-based diagnostic
models often depend on extensive feature engineering, which introduces bias
through manual intervention. While deep learning models are expected to operate
without manual involvement, their lack of interpretability poses significant
challenges in obtaining explainable and reliable brain biomarkers to support
diagnostic decisions, ultimately limiting their clinical applicability. In this
study, we introduce an end-to-end innovative graph neural network framework
named BrainIB++, which applies the information bottleneck (IB) principle to
identify the most informative data-driven brain regions as subgraphs during
model training for interpretation. We evaluate the performance of our model
against nine established brain network classification methods across three
multi-cohort schizophrenia datasets. It consistently demonstrates superior
diagnostic accuracy and exhibits generalizability to unseen data. Furthermore,
the subgraphs identified by our model also correspond with established clinical
biomarkers in schizophrenia, particularly emphasizing abnormalities in the
visual, sensorimotor, and higher cognition brain functional network. This
alignment enhances the model's interpretability and underscores its relevance
for real-world diagnostic applications.

</details>


### [266] [Distributional Inverse Reinforcement Learning](https://arxiv.org/abs/2510.03013)
*Feiyang Wu,Ye Zhao,Anqi Wu*

Main category: cs.LG

TL;DR: 提出了一种基于分布的离线逆强化学习框架，通过最小化一阶随机占优违规来联合建模奖励函数和回报分布的不确定性，引入扭曲风险度量，实现对奖励分布和风险感知策略的学习，在多种任务上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习方法通常仅估计确定性奖励或匹配期望回报，难以捕捉专家行为中的丰富结构和不确定性，尤其在风险敏感场景下表现不足。

Method: 提出一种分布式离线逆强化学习框架，联合建模奖励函数和回报分布的不确定性，通过最小化一阶随机占优（FSD）违规，将扭曲风险度量（DRM）融入策略学习，从而恢复奖励分布并学习分布感知策略。

Result: 在合成基准、真实神经行为数据和MuJoCo控制任务上的实验表明，该方法能恢复更具表达力的奖励表示，并在模仿学习性能上达到当前最优水平。

Conclusion: 该方法通过引入分布建模与风险度量，提升了逆强化学习对专家行为的建模能力，适用于行为分析与风险感知的模仿学习。

Abstract: We propose a distributional framework for offline Inverse Reinforcement
Learning (IRL) that jointly models uncertainty over reward functions and full
distributions of returns. Unlike conventional IRL approaches that recover a
deterministic reward estimate or match only expected returns, our method
captures richer structure in expert behavior, particularly in learning the
reward distribution, by minimizing first-order stochastic dominance (FSD)
violations and thus integrating distortion risk measures (DRMs) into policy
learning, enabling the recovery of both reward distributions and
distribution-aware policies. This formulation is well-suited for behavior
analysis and risk-aware imitation learning. Empirical results on synthetic
benchmarks, real-world neurobehavioral data, and MuJoCo control tasks
demonstrate that our method recovers expressive reward representations and
achieves state-of-the-art imitation performance.

</details>


### [267] [Learning Robust Diffusion Models from Imprecise Supervision](https://arxiv.org/abs/2510.03016)
*Dong-Dong Wu,Jiacheng Cui,Wei Wang,Zhiqiang She,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出DMIS框架，用于从不精确监督中训练鲁棒的扩散模型，通过分解目标为生成和分类组件，在多种任务中实现高质量、类可区分的样本生成。


<details>
  <summary>Details</summary>
Motivation: 由于条件输入中的噪声、模糊或不完整标签导致条件不匹配，影响生成质量，因此需要一种能够处理不精确监督的扩散模型训练方法。

Method: 基于似然最大化推导出DMIS框架，将目标分解为生成和分类两部分：生成部分建模不精确标签分布，分类部分利用扩散分类器推断类后验概率，并通过优化的时间步采样策略提升效率。

Result: 在图像生成、弱监督学习和噪声数据集压缩等多种不精确监督任务上进行了广泛实验，DMIS始终产生高质量且类可区分的样本。

Conclusion: DMIS是首个在扩散模型中系统研究不精确监督的统一框架，有效提升了模型在不精确标签下的生成性能和鲁棒性。

Abstract: Conditional diffusion models have achieved remarkable success in various
generative tasks recently, but their training typically relies on large-scale
datasets that inevitably contain imprecise information in conditional inputs.
Such supervision, often stemming from noisy, ambiguous, or incomplete labels,
will cause condition mismatch and degrade generation quality. To address this
challenge, we propose DMIS, a unified framework for training robust Diffusion
Models from Imprecise Supervision, which is the first systematic study within
diffusion models. Our framework is derived from likelihood maximization and
decomposes the objective into generative and classification components: the
generative component models imprecise-label distributions, while the
classification component leverages a diffusion classifier to infer
class-posterior probabilities, with its efficiency further improved by an
optimized timestep sampling strategy. Extensive experiments on diverse forms of
imprecise supervision, covering tasks of image generation, weakly supervised
learning, and noisy dataset condensation demonstrate that DMIS consistently
produces high-quality and class-discriminative samples.

</details>


### [268] [Differentially Private Wasserstein Barycenters](https://arxiv.org/abs/2510.03021)
*Anming Gu,Sasidhar Kunapuli,Mark Bun,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: 本文提出了首个在差分隐私下计算Wasserstein质心的算法，实验表明该方法在合成数据、MNIST和美国人口数据集上具有良好的隐私-精度权衡。


<details>
  <summary>Details</summary>
Motivation: 由于Wasserstein质心常基于敏感数据构建的经验分布进行计算，因此需要引入差分隐私保护机制。

Method: 设计了在差分隐私框架下计算Wasserstein质心的新算法。

Result: 在多种数据集上验证了所提方法能生成高质量的私有质心，并展现出优异的隐私与准确性权衡。

Conclusion: 所提出的差分隐私算法能够有效且准确地计算Wasserstein质心，适用于实际中的隐私保护场景。

Abstract: The Wasserstein barycenter is defined as the mean of a set of probability
measures under the optimal transport metric, and has numerous applications
spanning machine learning, statistics, and computer graphics. In practice these
input measures are empirical distributions built from sensitive datasets,
motivating a differentially private (DP) treatment. We present, to our
knowledge, the first algorithms for computing Wasserstein barycenters under
differential privacy. Empirically, on synthetic data, MNIST, and large-scale
U.S. population datasets, our methods produce high-quality private barycenters
with strong accuracy-privacy tradeoffs.

</details>


### [269] [Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling](https://arxiv.org/abs/2510.03027)
*Junyi Yao,Parham Eftekhar,Gene Cheung,Xujin Chris Liu,Yao Wang,Wei Hu*

Main category: cs.LG

TL;DR: 提出一种基于平衡符号图谱去噪的轻量级可解释神经网络方法，用于脑电图信号分类，性能媲美深度学习但参数更少。


<details>
  <summary>Details</summary>
Motivation: 利用EEG信号中固有的反相关性（由负边建模）提升癫痫患者与健康人分类的准确性，并增强模型可解释性。

Method: 通过展开平衡符号图上的谱去噪算法构建类Transformer神经网络；利用图拉普拉斯矩阵的相似变换将信号映射到正图，在正图上使用Lanczos近似实现高效理想低通滤波，并从数据中学习最优截止频率。使用两个去噪器的重构误差进行二分类。

Result: 所提方法在EEG信号分类任务中达到与典型深度学习模型相当的性能，但参数量显著减少。

Conclusion: 基于平衡符号图谱去噪的轻量化模型在保持高性能的同时具备良好的可解释性和效率，适用于EEG信号分析。

Abstract: Samples of brain signals collected by EEG sensors have inherent
anti-correlations that are well modeled by negative edges in a finite graph. To
differentiate epilepsy patients from healthy subjects using collected EEG
signals, we build lightweight and interpretable transformer-like neural nets by
unrolling a spectral denoising algorithm for signals on a balanced signed graph
-- graph with no cycles of odd number of negative edges. A balanced signed
graph has well-defined frequencies that map to a corresponding positive graph
via similarity transform of the graph Laplacian matrices. We implement an ideal
low-pass filter efficiently on the mapped positive graph via Lanczos
approximation, where the optimal cutoff frequency is learned from data. Given
that two balanced signed graph denoisers learn posterior probabilities of two
different signal classes during training, we evaluate their reconstruction
errors for binary classification of EEG signals. Experiments show that our
method achieves classification performance comparable to representative deep
learning schemes, while employing dramatically fewer parameters.

</details>


### [270] [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](https://arxiv.org/abs/2510.03046)
*Soohaeng Yoo Willow,Tae Hyeon Park,Gi Beom Sim,Sung Wook Moon,Seung Kyu Min,D. ChangMo Yang,Hyun Woo Kim,Juho Lee,Chang Woo Myung*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯E(3)等变机器学习势模型，通过联合能量-力负对数似然损失函数（NLL$_\text{JEF}$）显式建模能量和力的不确定性，在不确定性预测、OOD检测、校准和主动学习任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习势模型在不确定性量化方面存在不足，限制了其在主动学习、校准和分布外检测中的可靠性。

Method: 引入贝叶斯E(3)等变神经网络与多体消息传递的迭代重采样，并提出NLL$_\text{JEF}$损失函数；系统评估多种贝叶斯方法在不确定性预测等方面的表现，采用BALD实现基于不确定性的主动学习。

Result: 所提方法在准确性与不确定性量化方面优于传统方法，NLL$_\text{JEF}$有助于高效主动学习，贝叶斯MLP在OOD检测和校准任务中表现良好，且优于随机采样和仅基于能量不确定性的采样。

Conclusion: 贝叶斯等变神经网络是构建面向大规模原子模拟的不确定性感知机器学习势的强大框架。

Abstract: Machine learning potentials (MLPs) have become essential for large-scale
atomistic simulations, enabling ab initio-level accuracy with computational
efficiency. However, current MLPs struggle with uncertainty quantification,
limiting their reliability for active learning, calibration, and
out-of-distribution (OOD) detection. We address these challenges by developing
Bayesian E(3) equivariant MLPs with iterative restratification of many-body
message passing. Our approach introduces the joint energy-force negative
log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models
uncertainty in both energies and interatomic forces, yielding superior accuracy
compared to conventional NLL losses. We systematically benchmark multiple
Bayesian approaches, including deep ensembles with mean-variance estimation,
stochastic weight averaging Gaussian, improved variational online Newton, and
laplace approximation by evaluating their performance on uncertainty
prediction, OOD detection, calibration, and active learning tasks. We further
demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by
quantifying energy and force uncertainties. Using Bayesian active learning by
disagreement (BALD), our framework outperforms random sampling and
energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs
achieve competitive accuracy with state-of-the-art models while enabling
uncertainty-guided active learning, OOD detection, and energy/forces
calibration. This work establishes Bayesian equivariant neural networks as a
powerful framework for developing uncertainty-aware MLPs for atomistic
simulations at scale.

</details>


### [271] [ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization](https://arxiv.org/abs/2510.03051)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Johannes Dürholt,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: 提出了一种名为ZeroShotOpt的预训练模型，用于解决昂贵、无导数的黑箱函数全局优化问题，通过离线强化学习在大规模优化轨迹上训练，实现了跨不同问题景观的零样本泛化，表现出与现有领先优化器相当或更优的样本效率。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化（BO）虽然先进，但其性能依赖于难以通用的手调超参数，缺乏跨问题的泛化能力。

Method: 利用离线强化学习，在12种BO变体生成的大规模优化轨迹上预训练模型，并通过生成数百万个具有多样景观的合成高斯过程函数来扩展预训练规模。

Result: ZeroShotOpt在多种未见基准测试中实现了强大的零样本泛化能力，样本效率达到或超过当前领先的全局优化方法，包括贝叶斯优化。

Conclusion: ZeroShotOpt是一种高效、可重用的通用优化框架，无需微调即可广泛适用于2D到20D的黑箱优化任务，为未来优化算法的发展提供了基础。

Abstract: Global optimization of expensive, derivative-free black-box functions
requires extreme sample efficiency. While Bayesian optimization (BO) is the
current state-of-the-art, its performance hinges on surrogate and acquisition
function hyper-parameters that are often hand-tuned and fail to generalize
across problem landscapes. We present ZeroShotOpt, a general-purpose,
pretrained model for continuous black-box optimization tasks ranging from 2D to
20D. Our approach leverages offline reinforcement learning on large-scale
optimization trajectories collected from 12 BO variants. To scale pretraining,
we generate millions of synthetic Gaussian process-based functions with diverse
landscapes, enabling the model to learn transferable optimization policies. As
a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array
of unseen benchmarks, matching or surpassing the sample efficiency of leading
global optimizers, including BO, while also offering a reusable foundation for
future extensions and improvements. Our open-source code, dataset, and model
are available at: https://github.com/jamisonmeindl/zeroshotopt

</details>


### [272] [Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation](https://arxiv.org/abs/2510.03064)
*Ubayd Bapoo,Clement N Nyirenda*

Main category: cs.LG

TL;DR: 本研究评估了SAC、GAC和TQC在高维决策任务中的表现，提出PAGAC算法在训练速度和回报方面均优于其他方法，适用于复杂动作空间的快速收敛与稳定性能。


<details>
  <summary>Details</summary>
Motivation: 为了在完全可观测环境中高效处理参数化动作空间的高维决策问题，避免使用循环网络，并提升强化学习算法在离散-连续混合动作空间中的性能。

Method: 采用Platform-v0和Goal-v0作为基准环境，使用微软NNI进行超参数优化，并对GAC和TQC的代码库进行修改以确保可复现性，比较PASAC、PATQC与PAGAC的表现。

Result: PAGAC在两个基准任务中均实现了最快的训练速度和最高的回报，5000回合分别耗时41分24秒（Platform）和24分04秒（Goal），且表现出更高的稳定性与效率。

Conclusion: PAGAC在参数化动作空间中优于现有方法，具有显著的速度和稳定性优势，适合需要快速收敛和鲁棒性能的应用场景；未来可探索熵正则化与截断方法结合的混合策略。

Abstract: This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor
Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional
decision-making tasks using fully observable environments. The focus is on
parametrized action (PA) spaces, eliminating the need for recurrent networks,
with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to
continuous action-parameter spaces. Hyperparameter optimization was performed
with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC
and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC)
outperformed other algorithms, achieving the fastest training times and highest
returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform
game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide
clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC
demonstrated superior efficiency and reliability, making it ideal for tasks
requiring rapid convergence and robust performance. Future work could explore
hybrid strategies combining entropy-regularization with truncation-based
methods to enhance stability and expand investigations into generalizability.

</details>


### [273] [A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem](https://arxiv.org/abs/2510.03065)
*Mingfeng Fan,Jiaqi Cheng,Yaoxin Wu,Yifeng Zhang,Yibin Yang,Guohua Wu,Guillaume Sartoretti*

Main category: cs.LG

TL;DR: 提出一种基于深度强化学习的统一双解码器框架（UD3RL），用于求解近似旅行商问题（CETSP），通过分离节点选择与路径点确定，在不同问题规模和邻域半径下均表现出优异的性能和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 近似旅行商问题（CETSP）由于其基于邻域的访问机制建模困难，受到的关注较少，现有方法难以有效处理节点访问的灵活性和复杂空间推理需求。

Method: 构建基于离散化的马尔可夫决策过程（MDP）模型，设计包含适配编码器、节点解码器和位置解码器的UD3RL框架，并引入k近邻子图交互策略增强空间推理，采用定制化的REINFORCE算法进行端到端训练。

Result: UD3RL在解的质量和运行时间上均优于传统方法，具备跨问题规模、空间分布和邻域半径的良好泛化能力，并在动态环境中表现出鲁棒性。

Conclusion: UD3RL为CETSP提供了一种高效且通用的深度强化学习解决方案，验证了双解码结构和子图交互策略在复杂路径规划问题中的有效性。

Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for
solving the NP-hard traveling salesman problem (TSP). However, limited
attention has been given to the close-enough TSP (CETSP), primarily due to the
challenge introduced by its neighborhood-based visitation criterion, wherein a
node is considered visited if the agent enters a compact neighborhood around
it. In this work, we formulate a Markov decision process (MDP) for CETSP using
a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)
framework that separates decision-making into node selection and waypoint
determination. Specifically, an adapted encoder is employed for effective
feature extraction, followed by a node-decoder and a loc-decoder to handle the
two sub-tasks, respectively. A k-nearest neighbors subgraph interaction
strategy is further introduced to enhance spatial reasoning during location
decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a
unified model capable of generalizing across different problem sizes and
varying neighborhood radius types (i.e., constant and random radii).
Experimental results show that UD3RL outperforms conventional methods in both
solution quality and runtime, while exhibiting strong generalization across
problem scales, spatial distributions, and radius ranges, as well as robustness
to dynamic environments.

</details>


### [274] [Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs](https://arxiv.org/abs/2510.03086)
*Marc Lelarge*

Main category: cs.LG

TL;DR: 提出了一种基于链式图神经网络（GNN）的新方法，用于解决图对齐这一NP难问题，在合成基准上显著优于现有方法，尤其在正则图上表现突出，并可与传统优化结合进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在组合优化问题上难以超越传统优化方法，限制了其实际应用。本文旨在提升GNN在图对齐这一基础且困难的NP-hard问题上的性能，以增强其在纯结构信息下的匹配能力。

Method: 提出一种链式训练策略，通过一系列GNN逐步迭代优化节点相似性矩阵；每个GNN利用前一轮的离散排序信息来改进对齐结果。同时采用作用于节点对的新型架构，捕捉标准消息传递GNN无法建模的全局结构模式。

Result: 在合成数据集上，该方法比现有方法准确率提高3倍以上，是唯一能成功解决正则图对齐问题的方法；结合传统优化后处理后，在图对齐基准上显著超越最先进的求解器。

Conclusion: 链式GNN框架显著提升了GNN在图对齐任务上的性能，证明了GNN可在NP-hard组合问题上超越传统方法，尤其是在结合结构建模创新与迭代 refinement 策略时具有巨大潜力。

Abstract: Graph neural networks (GNNs) have struggled to outperform traditional
optimization methods on combinatorial problems, limiting their practical
impact. We address this gap by introducing a novel chaining procedure for the
graph alignment problem, a fundamental NP-hard task of finding optimal node
correspondences between unlabeled graphs using only structural information. Our
method trains a sequence of GNNs where each network learns to iteratively
refine similarity matrices produced by previous networks. During inference,
this creates a bootstrap effect: each GNN improves upon partial solutions by
incorporating discrete ranking information about node alignment quality from
prior iterations. We combine this with a powerful architecture that operates on
node pairs rather than individual nodes, capturing global structural patterns
essential for alignment that standard message-passing networks cannot
represent. Extensive experiments on synthetic benchmarks demonstrate
substantial improvements: our chained GNNs achieve over 3x better accuracy than
existing methods on challenging instances, and uniquely solve regular graphs
where all competing approaches fail. When combined with traditional
optimization as post-processing, our method substantially outperforms
state-of-the-art solvers on the graph alignment benchmark.

</details>


### [275] [Distilled Protein Backbone Generation](https://arxiv.org/abs/2510.03095)
*Liyang Xie,Haoran Zhang,Zhendong Wang,Wesley Tansey,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 本研究通过改进Score identity Distillation (SiD) 方法，成功训练出可在少量步骤内生成蛋白质骨架的快速生成模型，在保持生成质量的同时实现超过20倍的采样速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在蛋白质骨架生成中表现优异，但因反向扩散过程需要大量迭代步骤而导致生成速度慢，限制了其在大规模蛋白设计中的应用。因此，亟需提升生成效率。

Method: 采用并改进Score identity Distillation (SiD) 策略，结合多步生成和推理时噪声调制技术，对预训练模型进行蒸馏，以训练快速生成模型。

Result: 蒸馏后的少步生成器在设计性、多样性和新颖性方面与教师模型相当，同时采样速度提升超过20倍。

Conclusion: 该方法显著降低了扩散模型在蛋白质设计中的推理成本，推动其向实际蛋白质工程应用迈进。

Abstract: Diffusion- and flow-based generative models have recently demonstrated strong
performance in protein backbone generation tasks, offering unprecedented
capabilities for de novo protein design. However, while achieving notable
performance in generation quality, these models are limited by their generating
speed, often requiring hundreds of iterative steps in the reverse-diffusion
process. This computational bottleneck limits their practical utility in
large-scale protein discovery, where thousands to millions of candidate
structures are needed. To address this challenge, we explore the techniques of
score distillation, which has shown great success in reducing the number of
sampling steps in the vision domain while maintaining high generation quality.
However, a straightforward adaptation of these methods results in unacceptably
low designability. Through extensive study, we have identified how to
appropriately adapt Score identity Distillation (SiD), a state-of-the-art score
distillation strategy, to train few-step protein backbone generators which
significantly reduce sampling time, while maintaining comparable performance to
their pretrained teacher model. In particular, multistep generation combined
with inference time noise modulation is key to the success. We demonstrate that
our distilled few-step generators achieve more than a 20-fold improvement in
sampling speed, while achieving similar levels of designability, diversity, and
novelty as the Proteina teacher model. This reduction in inference cost enables
large-scale in silico protein design, thereby bringing diffusion-based models
closer to real-world protein engineering applications.

</details>


### [276] [Adaptive Node Feature Selection For Graph Neural Networks](https://arxiv.org/abs/2510.03096)
*Ali Azizpour,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一种自适应节点特征选择方法，用于图神经网络，在训练过程中识别并移除不必要的特征。


<details>
  <summary>Details</summary>
Motivation: 图结构数据的复杂依赖性使得传统特征重要性度量不适用，需要一种能动态评估特征贡献的方法。

Method: 基于验证性能变化（通过置换特征值）来确定相关特征，具有模型和任务无关性，并理论分析了GNN性能与节点数据及图结构关系的依赖。

Result: 不仅在训练后提供特征重要性评分，还追踪特征被逐步丢弃时的相关性变化，验证了该方法在不同图架构和复杂学习场景下的灵活性和适应性。

Conclusion: 该方法能有效消除无用特征，提升模型可解释性和性能，适用于多种GNN架构和挑战性图学习任务。

Abstract: We propose an adaptive node feature selection approach for graph neural
networks (GNNs) that identifies and removes unnecessary features during
training. The ability to measure how features contribute to model output is key
for interpreting decisions, reducing dimensionality, and even improving
performance by eliminating unhelpful variables. However, graph-structured data
introduces complex dependencies that may not be amenable to classical feature
importance metrics. Inspired by this challenge, we present a model- and
task-agnostic method that determines relevant features during training based on
changes in validation performance upon permuting feature values. We
theoretically motivate our intervention-based approach by characterizing how
GNN performance depends on the relationships between node data and graph
structure. Not only do we return feature importance scores once training
concludes, we also track how relevance evolves as features are successively
dropped. We can therefore monitor if features are eliminated effectively and
also evaluate other metrics with this technique. Our empirical results verify
the flexibility of our approach to different graph architectures as well as its
adaptability to more challenging graph learning settings.

</details>


### [277] [AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks](https://arxiv.org/abs/2510.03101)
*Irene Tenison,Soumyajit Chatterjee,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: 本文提出了AdaBet，一种无需梯度和标签的层选择方法，通过贝蒂数分析激活空间的拓扑特征，实现高效的设备端模型重训练，显著提升准确率并降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的边缘和移动设备上高效适应用户特定的数据分布，需要一种不依赖标签和大量计算的轻量级模型重训练方法。

Method: 提出AdaBet方法，利用前向传播计算各层激活空间的贝蒂数（Betti Numbers），通过分析其拓扑特征来评估层的重要性，从而选择最具学习能力的层进行重训练。

Result: 在十六个模型-数据集组合上的实验表明，AdaBet相比基于梯度的方法平均提升了5%的分类准确率，并将峰值内存消耗平均降低了40%。

Conclusion: AdaBet是一种高效、低开销的层选择方法，适用于资源受限设备上的模型自适应，无需标签或反向传播，具有良好的实用性和扩展性。

Abstract: To utilize pre-trained neural networks on edge and mobile devices, we often
require efficient adaptation to user-specific runtime data distributions while
operating under limited compute and memory resources. On-device retraining with
a target dataset can facilitate such adaptations; however, it remains
impractical due to the increasing depth of modern neural nets, as well as the
computational overhead associated with gradient-based optimization across all
layers. Current approaches reduce training cost by selecting a subset of layers
for retraining, however, they rely on labeled data, at least one full-model
backpropagation, or server-side meta-training; limiting their suitability for
constrained devices. We introduce AdaBet, a gradient-free layer selection
approach to rank important layers by analyzing topological features of their
activation spaces through Betti Numbers and using forward passes alone. AdaBet
allows selecting layers with high learning capacity, which are important for
retraining and adaptation, without requiring labels or gradients. Evaluating
AdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves
an average gain of 5% more classification accuracy over gradient-based
baselines while reducing average peak memory consumption by 40%.

</details>


### [278] [Real Time Headway Predictions in Urban Rail Systems and Implications for Service Control: A Deep Learning Approach](https://arxiv.org/abs/2510.03121)
*Muhammad Usama,Haris Koutsopoulos*

Main category: cs.LG

TL;DR: 提出一种基于ConvLSTM的深度学习框架，用于预测城市地铁线路的列车头车时距时空传播，支持实时调度决策。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注乘客负荷预测或异常干扰场景，缺乏对日常运营中主动调度控制的支持，难以实现高效、可靠的实时调度。

Method: 构建以ConvLSTM为核心的模型，结合历史头车时距数据和计划终点站头车时距输入，捕捉头车时距在时间和空间上的动态变化，并模拟多种调度策略以评估其影响。

Result: 在大规模实际地铁数据上验证了模型的有效性，能够准确预测头车时距的时空演化，为不同调度策略提供快速评估能力。

Conclusion: 该框架为地铁运营方提供了一种计算高效、实用性强的工具，有助于优化实时调度策略，提升服务一致性与乘客满意度。

Abstract: Efficient real-time dispatching in urban metro systems is essential for
ensuring service reliability, maximizing resource utilization, and improving
passenger satisfaction. This study presents a novel deep learning framework
centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to
predict the complex spatiotemporal propagation of train headways across an
entire metro line. By directly incorporating planned terminal headways as a
critical input alongside historical headway data, the proposed model accurately
forecasts future headway dynamics, effectively capturing both their temporal
evolution and spatial dependencies across all stations. This capability
empowers dispatchers to evaluate the impact of various terminal headway control
decisions without resorting to computationally intensive simulations. We
introduce a flexible methodology to simulate diverse dispatcher strategies,
ranging from maintaining even headways to implementing custom patterns derived
from observed terminal departures. In contrast to existing research primarily
focused on passenger load predictioning or atypical disruption scenarios, our
approach emphasizes proactive operational control. Evaluated on a large-scale
dataset from an urban metro line, the proposed ConvLSTM model demonstrates
promising headway predictions, offering actionable insights for real-time
decision-making. This framework provides rail operators with a powerful,
computationally efficient tool to optimize dispatching strategies, thereby
significantly improving service consistency and passenger satisfaction.

</details>


### [279] [Signature-Informed Transformer for Asset Allocation](https://arxiv.org/abs/2510.03129)
*Yoontae Hwang,Stefan Zohren*

Main category: cs.LG

TL;DR: 提出了一种名为Signature-Informed Transformer (SIT) 的新框架，通过端到端优化风险感知的金融目标来解决深度学习在资产配置中因目标不匹配和误差放大而失败的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在资产配置中常因预测与优化分离、误差累积以及缺乏对金融路径几何特性的建模而导致性能不佳。

Method: SIT利用路径签名捕捉资产动态的几何特征，并设计了签名增强的注意力机制，将金融先验知识（如领先-滞后效应）嵌入模型，实现端到端的风险感知目标优化。

Result: 在标普100日度股票数据上的实验表明，SIT显著优于传统方法和深度学习基线模型，尤其在预测-再优化范式上表现更优。

Conclusion: 结果表明，在机器学习系统中进行风险感知的资金配置时，投资组合感知的目标函数和几何感知的归纳偏置至关重要。

Abstract: Robust asset allocation is a key challenge in quantitative finance, where
deep-learning forecasters often fail due to objective mismatch and error
amplification. We introduce the Signature-Informed Transformer (SIT), a novel
framework that learns end-to-end allocation policies by directly optimizing a
risk-aware financial objective. SIT's core innovations include path signatures
for a rich geometric representation of asset dynamics and a signature-augmented
attention mechanism embedding financial inductive biases, like lead-lag
effects, into the model. Evaluated on daily S\&P 100 equity data, SIT
decisively outperforms traditional and deep-learning baselines, especially when
compared to predict-then-optimize models. These results indicate that
portfolio-aware objectives and geometry-aware inductive biases are essential
for risk-aware capital allocation in machine-learning systems. The code is
available at:
https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation

</details>


### [280] [Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation](https://arxiv.org/abs/2510.03134)
*Flavio Giorgi,Matteo Silvestri,Cesare Campagnano,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 提出一种利用大小语言模型生成反事实解释叙述的新型管道，通过知识蒸馏和优化机制提升小型模型的表现力和推理能力，并设计了评估自然语言叙述的有效方法。


<details>
  <summary>Details</summary>
Motivation: 反事实解释虽有潜力，但通常复杂且技术性强，难以被非专家理解。因此需要一种更易解释的方法来提高可解释性AI的实用性。

Method: 结合大、小语言模型，使用知识蒸馏和细化机制构建生成反事实解释叙述的管道，并提出一种简单有效的评估方法来验证生成叙述与事实和反事实真实情况的一致性。

Result: 该管道显著提升了小型语言模型在生成高质量叙述方面的能力，使其性能接近大型模型，同时保持良好的推理能力和实际应用性能。

Conclusion: 所提方法有效增强了小型模型在可解释人工智能中的适用性，推动了面向非专家用户的可解释性技术发展。

Abstract: Explainable Artificial Intelligence has become a crucial area of research,
aiming to demystify the decision-making processes of deep learning models.
Among various explainability techniques, counterfactual explanations have been
proven particularly promising, as they offer insights into model behavior by
highlighting minimal changes that would alter a prediction. Despite their
potential, these explanations are often complex and technical, making them
difficult for non-experts to interpret. To address this challenge, we propose a
novel pipeline that leverages Language Models, large and small, to compose
narratives for counterfactual explanations. We employ knowledge distillation
techniques along with a refining mechanism to enable Small Language Models to
perform comparably to their larger counterparts while maintaining robust
reasoning abilities. In addition, we introduce a simple but effective
evaluation method to assess natural language narratives, designed to verify
whether the models' responses are in line with the factual, counterfactual
ground truth. As a result, our proposed pipeline enhances both the reasoning
capabilities and practical performance of student models, making them more
suitable for real-world use cases.

</details>


### [281] [Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking](https://arxiv.org/abs/2510.03149)
*Dhruv Rohatgi,Abhishek Shetty,Donya Saless,Yuchen Li,Ankur Moitra,Andrej Risteski,Dylan J. Foster*

Main category: cs.LG

TL;DR: 本文提出了一种新的测试时采样算法VGB，通过理论指导的回溯机制提高对验证器错误的鲁棒性，在语言生成任务中表现出优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的验证器在推理过程中容易因小错误导致严重失败，且传统解码方法难以应对验证器误差的放大问题。

Method: 提出VGB算法，将自回归生成视为在部分生成树上的随机游走，利用过程验证器和基础模型引导转移概率，并引入概率性回溯机制，其理论基础源自计算理论中的Sinclair-Jerrum随机游走。

Result: 在合成和真实语言建模任务上，VGB在多个指标上优于基线方法，展现出更强的鲁棒性和生成质量。

Conclusion: VGB通过结合理论驱动的回溯策略，有效缓解了验证器错误带来的负面影响，为测试时推理算法的设计提供了新的思路和改进方向。

Abstract: Test-time algorithms that combine the generative power of language models
with process verifiers that assess the quality of partial generations offer a
promising lever for eliciting new reasoning capabilities, but the algorithmic
design space and computational scaling properties of such approaches are still
opaque, and their benefits are far from apparent when one accounts for the cost
of learning a high-quality verifier. Our starting point is the observation that
seemingly benign errors in a learned verifier can lead to catastrophic failures
for standard decoding techniques due to error amplification during the course
of generation. We then ask: can this be improved with more sophisticated
decoding strategies?
  We introduce a new process-guided test-time sampling algorithm, VGB, which
uses theoretically grounded backtracking to achieve provably better robustness
to verifier errors. VGB interprets autoregressive generation as a random walk
on a tree of partial generations, with transition probabilities guided by the
process verifier and base model; crucially, backtracking occurs
probabilistically. This process generalizes the seminal Sinclair-Jerrum random
walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and
sampling in theoretical computer science, and a conceptual contribution of our
work is to highlight parallels with this literature. Empirically, we
demonstrate on both synthetic and real language modeling tasks that VGB
outperforms baselines on a variety of metrics.

</details>


### [282] [Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective](https://arxiv.org/abs/2510.03151)
*Yehuda Dar*

Main category: cs.LG

TL;DR: 本文利用经典高比特率量化理论，研究了用于回归任务的混合专家（MoE）模型，提出了输入空间分割与单参数专家的近似误差分析，并探讨了专家数量对逼近误差与估计误差权衡的影响。


<details>
  <summary>Details</summary>
Motivation: 受高比特率量化理论启发，希望从理论上理解大规模MoE模型在回归任务中的逼近能力，尤其是在输入空间被细分为小区域时的表现。

Method: 通过将输入空间划分为多个小区域，每个区域由一个零计算开销的单参数专家（常数预测器）处理；基于高比特率量化理论，推导一维和多维输入下的测试误差及其上界，并研究最优分割与专家配置；同时分析给定分割下专家参数的学习性质。

Result: 在一维情况下精确表述了测试误差及最优解；在多维情况下给出了测试误差上界并研究其最小化；揭示了专家数量对逼近误差与估计误差之间权衡的影响。

Conclusion: MoE模型的性能受专家数量显著影响，存在理论上的最优规模，平衡逼近与估计误差，为设计高效低计算成本的MoE提供了理论依据。

Abstract: This paper uses classical high-rate quantization theory to provide new
insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is
defined by a segmentation of the input space to regions, each with a
single-parameter expert that acts as a constant predictor with zero-compute at
inference. Motivated by high-rate quantization theory assumptions, we assume
that the number of experts is sufficiently large to make their input-space
regions very small. This lets us to study the approximation error of our MoE
model class: (i) for one-dimensional inputs, we formulate the test error and
its minimizing segmentation and experts; (ii) for multidimensional inputs, we
formulate an upper bound for the test error and study its minimization.
Moreover, we consider the learning of the expert parameters from a training
dataset, given an input-space segmentation, and formulate their statistical
learning properties. This leads us to theoretically and empirically show how
the tradeoff between approximation and estimation errors in MoE learning
depends on the number of experts.

</details>


### [283] [Calibrated Uncertainty Sampling for Active Learning](https://arxiv.org/abs/2510.03162)
*Ha Manh Bui,Iliana Maifeld-Carucci,Anqi Liu*

Main category: cs.LG

TL;DR: 提出一种新的主动学习获取函数，通过估计校准误差并优先查询校准误差最高的样本，从而在池式主动学习中降低分类器的校准误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型不确定性的获取函数在不确定性未校准的情况下效果受限，尤其是深度神经网络的不确定性通常未校准，导致泛化性能差和校准误差高。

Method: 利用协变量偏移下的核校准误差估计器，提出新的获取函数，优先选择校准误差高的样本进行查询，并理论证明该方法能有效控制未标记池和测试数据上的校准误差。

Result: 实验表明，所提方法在多种池式主动学习设置下，相比其他基线方法具有更低的校准误差和泛化误差。

Conclusion: 该方法有效提升了模型的校准性和泛化能力，适用于深度神经网络主导的主动学习场景。

Abstract: We study the problem of actively learning a classifier with a low calibration
error. One of the most popular Acquisition Functions (AFs) in pool-based Active
Learning (AL) is querying by the model's uncertainty. However, we recognize
that an uncalibrated uncertainty model on the unlabeled pool may significantly
affect the AF effectiveness, leading to sub-optimal generalization and high
calibration error on unseen data. Deep Neural Networks (DNNs) make it even
worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we
propose a new AF by estimating calibration errors and query samples with the
highest calibration error before leveraging DNN uncertainty. Specifically, we
utilize a kernel calibration error estimator under the covariate shift and
formally show that AL with this AF eventually leads to a bounded calibration
error on the unlabeled pool and unseen test data. Empirically, our proposed
method surpasses other AF baselines by having a lower calibration and
generalization error across pool-based AL settings.

</details>


### [284] [Why Do We Need Warm-up? A Theoretical Perspective](https://arxiv.org/abs/2510.03164)
*Foivos Alimisis,Rustem Islamov,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 本文提出了一种基于$(L_0, L_1)$-平滑性条件的理论框架，解释了学习率预热为何能提升深度学习训练效果，并在常见神经网络架构上验证了该条件的有效性，理论与实验结果均表明预热可加速收敛。


<details>
  <summary>Details</summary>
Motivation: 学习率预热在实践中被广泛使用，但其理论基础尚不清楚，本文旨在从理论上解释预热为何有效。

Method: 引入并利用一种推广的$(L_0, L_1)$-平滑性条件，分析梯度下降在预热调度下的收敛性，并推导出上下复杂度界。

Result: 理论证明预热调度比固定步长能实现更快的收敛速度，并在语言和视觉模型上通过实验验证了理论发现。

Conclusion: 预热之所以有效，是因为损失函数在训练初期具有与损失次优性相关的局部曲率特性，$(L_0, L_1)$-平滑性条件为理解预热提供了坚实的理论基础。

Abstract: Learning rate warm-up - increasing the learning rate at the beginning of
training - has become a ubiquitous heuristic in modern deep learning, yet its
theoretical foundations remain poorly understood. In this work, we provide a
principled explanation for why warm-up improves training. We rely on a
generalization of the $(L_0, L_1)$-smoothness condition, which bounds local
curvature as a linear function of the loss sub-optimality and exhibits
desirable closure properties. We demonstrate both theoretically and empirically
that this condition holds for common neural architectures trained with
mean-squared error and cross-entropy losses. Under this assumption, we prove
that Gradient Descent with a warm-up schedule achieves faster convergence than
with a fixed step-size, establishing upper and lower complexity bounds.
Finally, we validate our theoretical insights through experiments on language
and vision models, confirming the practical benefits of warm-up schedules.

</details>


### [285] [FTTE: Federated Learning on Resource-Constrained Devices](https://arxiv.org/abs/2510.03165)
*Irene Tenison,Anna Murphy,Charles Beauville,Lalana Kagal*

Main category: cs.LG

TL;DR: 提出了一种名为FTTE的新型半异步联邦学习框架，通过稀疏参数更新和基于更新年龄与方差的老化加权聚合机制，在资源受限的边缘设备上实现了更快的收敛、更低的内存和通信开销，并在大规模异构网络中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统同步和异步联邦学习在异构大规模网络中存在延迟和收敛慢的问题，且在资源受限的边缘设备上部署困难，因此需要一种高效、可扩展的解决方案。

Method: 提出FTTE框架，采用稀疏参数更新和结合更新年龄与方差的老化加权聚合策略，实现半异步联邦学习。

Result: 实验表明，相比同步FL（如FedAvg），FTTE收敛速度提升81%，设备内存使用降低80%，通信负载减少69%；在高达500个客户端且90%为拖拉机的场景下，性能优于或相当于现有半异步方法（如FedBuff）。

Conclusion: FTTE是首个适用于异构且资源受限边缘设备的大规模联邦学习实用可扩展方案，显著提升了实际部署的可行性。

Abstract: Federated learning (FL) enables collaborative model training across
distributed devices while preserving data privacy, but deployment on
resource-constrained edge nodes remains challenging due to limited memory,
energy, and communication bandwidth. Traditional synchronous and asynchronous
FL approaches further suffer from straggler induced delays and slow convergence
in heterogeneous, large scale networks. We present FTTE (Federated Tiny
Training Engine),a novel semi-asynchronous FL framework that uniquely employs
sparse parameter updates and a staleness-weighted aggregation based on both age
and variance of client updates. Extensive experiments across diverse models and
data distributions - including up to 500 clients and 90% stragglers -
demonstrate that FTTE not only achieves 81% faster convergence, 80% lower
on-device memory usage, and 69% communication payload reduction than
synchronous FL (eg.FedAVG), but also consistently reaches comparable or higher
target accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.
These results establish FTTE as the first practical and scalable solution for
real-world FL deployments on heterogeneous and predominantly
resource-constrained edge devices.

</details>


### [286] [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.03181)
*Ha Manh Bui,Felix Parker,Kimia Ghobadi,Anqi Liu*

Main category: cs.LG

TL;DR: 提出了一种名为DQUCB的非平稳强化学习算法，通过引入转移密度函数检测分布变化并改进Q-learning UCB的不确定性估计，在理论和实验上均表现出优于QUCB的后悔界。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中，传统QUCB算法因分布偏移可能导致策略退化为次优，需设计能感知分布变化并保持探索与利用平衡的新算法。

Method: 提出Density-QUCB（DQUCB），利用转移密度函数检测分布变化，并结合其似然性增强Q-learning UCB的不确定性估计，提升对非平稳环境的适应能力。

Result: 理论证明DQUCB相比QUCB具有更优的后悔界；实验表明其在多种RL任务及真实世界COVID-19患者分配任务中表现更好且计算高效。

Conclusion: DQUCB是一种有效的非平稳强化学习方法，能够在分布变化下保持良好性能，兼顾模型自由性与高效率。

Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution
shifts in both finite-horizon episodic and infinite-horizon discounted Markov
Decision Processes (MDPs). In the finite-horizon case, the transition functions
may suddenly change at a particular episode. In the infinite-horizon setting,
such changes can occur at an arbitrary time step during the agent's interaction
with the environment. While the Q-learning Upper Confidence Bound algorithm
(QUCB) can discover a proper policy during learning, due to the distribution
shifts, this policy can exploit sub-optimal rewards after the shift happens. To
address this issue, we propose Density-QUCB (DQUCB), a shift-aware
Q-learning~UCB algorithm, which uses a transition density function to detect
distribution shifts, then leverages its likelihood to enhance the uncertainty
estimation quality of Q-learning~UCB, resulting in a balance between
exploration and exploitation. Theoretically, we prove that our oracle DQUCB
achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the
computational efficiency of model-free RL and outperforms QUCB baselines by
having a lower regret across RL tasks, as well as a real-world COVID-19 patient
hospital allocation task using a Deep-Q-learning architecture.

</details>


### [287] [PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning](https://arxiv.org/abs/2510.03185)
*Wanjia Zhao,Qinwei Ma,Jingzhe Shi,Shirley Wu,Jiaqi Han,Yijia Xiao,Si-Yuan Chen,Xiao Luo,Ludwig Schmidt,James Zou*

Main category: cs.LG

TL;DR: PRISM-Physics 是一个面向复杂物理推理问题的流程级评估框架，使用有向无环图（DAG）表示解题过程，实现细粒度、可解释且理论可靠的评分。


<details>
  <summary>Details</summary>
Motivation: 现有物理评测大多只评估最终答案，无法捕捉推理过程；现有逐步评分方法依赖启发式判断或线性假设，限制了可靠性与诊断有效性。

Method: 提出 PRISM-Physics 框架，将解题过程建模为公式间的有向无环图（DAG），显式编码中间步骤的因果依赖，并结合基于规则的符号公式等价匹配方法进行全自动、无启发式的评分。

Result: 实验证明该框架评分结果更贴近人类专家，能有效揭示大模型在物理推理中的深层缺陷，且具备理论最优性保证。

Conclusion: PRISM-Physics 通过结构严谨性、理论保障和符号验证，为科学推理能力的评估与模型发展提供了原则性基础。

Abstract: Benchmarks for competition-style reasoning have advanced evaluation in
mathematics and programming, yet physics remains comparatively explored. Most
existing physics benchmarks evaluate only final answers, which fail to capture
reasoning processes, while recent stepwise methods rely on heuristic
LLM-as-judge scoring or restrictive linear assumptions, limiting reliability
and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation
framework and benchmark for complex physics reasoning problems. Solutions are
represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding
causal dependencies among intermediate steps to enable fine-grained,
interpretable, and theoretically grounded scoring. We prove the optimality of
the DAG representation and the corresponding scoring policy. Combining with a
fully rule-based method for symbolic formula equivalence matching that we
developed, we ensure consistent validation across diverse formulations without
heuristic judgments. Results show that our evaluation framework is more aligned
with human experts' scoring. Experiments on state-of-the-art LLMs reveal
persistent reasoning failures in physics, while step-level scoring offers both
diagnostic insight and rich signals for later training. By combining structural
rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides
a principled foundation for advancing process-level evaluation and guiding the
development of models with deeper scientific reasoning capabilities.

</details>


### [288] [Superposition disentanglement of neural representations reveals hidden alignment](https://arxiv.org/abs/2510.03186)
*André Longon,David Klindt,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 本文探讨了超叠加（superposition）现象如何影响深度神经网络和大脑之间的表征对齐度量，发现不同的超叠加排列会降低对齐分数，而通过稀疏自编码器解耦后可提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 研究超叠加是否以不良方式干扰现有的表征对齐度量方法，从而影响对神经表征相似性的准确评估。

Method: 提出理论分析超叠加排列对排列不变度量的影响，并利用稀疏自编码器在玩具模型中解耦超叠加，验证其对DNN-DNN和DNN-脑对齐的影响。

Result: 实验表明，使用稀疏潜在代码替代原始神经元后，对齐分数普遍提高，说明超叠加结构会影响现有对齐指标的表现。

Conclusion: 超叠加的解耦是揭示神经表征真实对齐关系的关键步骤，当前对齐度量可能因超叠加结构差异而低估实际对齐程度。

Abstract: The superposition hypothesis states that a single neuron within a population
may participate in the representation of multiple features in order for the
population to represent more features than the number of neurons. In
neuroscience and AI, representational alignment metrics measure the extent to
which different deep neural networks (DNNs) or brains represent similar
information. In this work, we explore a critical question: \textit{does
superposition interact with alignment metrics in any undesirable way?} We
hypothesize that models which represent the same features in \textit{different
superposition arrangements}, i.e., their neurons have different linear
combinations of the features, will interfere with predictive mapping metrics
(semi-matching, soft-matching, linear regression), producing lower alignment
than expected. We first develop a theory for how the strict permutation metrics
are dependent on superposition arrangements. This is tested by training sparse
autoencoders (SAEs) to disentangle superposition in toy models, where alignment
scores are shown to typically increase when a model's base neurons are replaced
with its sparse overcomplete latent codes. We find similar increases for
DNN\(\rightarrow\)DNN and DNN\(\rightarrow\)brain linear regression alignment
in the visual domain. Our results suggest that superposition disentanglement is
necessary for mapping metrics to uncover the true representational alignment
between neural codes.

</details>


### [289] [Estimation of Resistance Training RPE using Inertial Sensors and Electromyography](https://arxiv.org/abs/2510.03197)
*James Thomas,Johan Wahlström*

Main category: cs.LG

TL;DR: 该研究利用可穿戴惯性传感器和肌电图（EMG）数据，通过机器学习模型（特别是随机森林）估计单臂哑铃弯举中的主观用力程度（RPE），结果显示该方法可行，但存在泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 准确估计RPE有助于个性化训练反馈和预防运动损伤，现有方法依赖主观报告，缺乏客观、实时的评估手段。

Method: 收集了69组超过1000次重复的自定义数据集，提取统计特征，比较多种机器学习模型，使用惯性传感器和EMG数据训练随机森林等分类器。

Result: 随机森林表现最佳，精确准确率为41.4%，±1 RPE准确率达85.9%；加入EMG数据仅小幅提升性能；离心阶段重复时间是最重要的预测特征。

Conclusion: 基于可穿戴传感器的RPE估算是可行的，但需解决数据质量、传感器放置和模型泛化能力等问题以进一步提升性能。

Abstract: Accurate estimation of rating of perceived exertion (RPE) can enhance
resistance training through personalized feedback and injury prevention. This
study investigates the application of machine learning models to estimate RPE
during single-arm dumbbell bicep curls, using data from wearable inertial and
electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000
repetitions was collected, with statistical features extracted for model
training. Among the models evaluated, a random forest classifier achieved the
highest performance, with 41.4% exact accuracy and 85.9% $\pm1$ RPE accuracy.
While the inclusion of EMG data slightly improved model accuracy over inertial
sensors alone, its utility may have been limited by factors such as data
quality and placement sensitivity. Feature analysis highlighted eccentric
repetition time as the strongest RPE predictor. The results demonstrate the
feasibility of wearable-sensor-based RPE estimation and identify key challenges
for improving model generalizability.

</details>


### [290] [Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling](https://arxiv.org/abs/2510.03199)
*Qiwei Di,Kaixuan Ji,Xuheng Li,Heyang Zhao,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出了一种新的LLM推理策略Best-of-Majority (BoM)，在Pass@$k$设置下具有更优的理论性能和实验表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多数投票和Best-of-N策略在Pass@$k$推理设置下无法很好地随$k$和采样预算$N$扩展，导致性能不佳。

Method: 结合多数投票和Best-of-N的优点，提出BoM策略：先从$N$个样本中筛选高频响应，再从中选择奖励最高的前$k$个。理论分析证明其在Regret界上具有最优性。

Result: 当采样预算$N=\tilde\Omega(C^*)$时，BoM的regret为$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$，并建立了匹配的下界，证明其最小最大最优；实验显示BoM在数学问题推理上优于多数投票和BoN。

Conclusion: BoM是一种在Pass@$k$设置下更优的推理策略，兼具理论最优性和实际性能优势，且性能不随$N$增加而下降。

Abstract: LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.

</details>


### [291] [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2510.03207)
*Yuda Song,Dhruv Rohatgi,Aarti Singh,J. Andrew Bagnell*

Main category: cs.LG

TL;DR: 本文研究了在部分可观测强化学习中，利用特权信息（如模拟器中的潜在状态）进行专家蒸馏与标准强化学习之间的算法权衡，发现潜在动态的随机性是影响性能的关键因素，且最优潜在策略未必是最适合蒸馏的策略。


<details>
  <summary>Details</summary>
Motivation: 部分可观测性给强化学习带来挑战，需要学习依赖历史的复杂策略。尽管特权专家蒸馏能提升训练效率，但存在已知的失效模式，因此需系统分析其与标准RL的权衡。

Method: 提出一个名为扰动Block MDP的理论模型，并在具有挑战性的模拟运动任务上进行受控实验，对比特权专家蒸馏与无特权信息的标准强化学习方法。

Result: （1）潜在动态的随机性决定了两种方法的性能权衡，理论上的可解码性与信念收缩特性可解释该现象；（2）最优潜在策略并不总是最适合用于蒸馏的策略。

Conclusion: 研究结果为在部分可观测场景中有效利用特权信息提供了新指导原则，有助于提升实际应用中策略学习的效率。

Abstract: Partial observability is a notorious challenge in reinforcement learning
(RL), due to the need to learn complex, history-dependent policies. Recent
empirical successes have used privileged expert distillation--which leverages
availability of latent state information during training (e.g., from a
simulator) to learn and imitate the optimal latent, Markovian policy--to
disentangle the task of "learning to see" from "learning to act". While expert
distillation is more computationally efficient than RL without latent state
information, it also has well-documented failure modes. In this paper--through
a simple but instructive theoretical model called the perturbed Block MDP, and
controlled experiments on challenging simulated locomotion tasks--we
investigate the algorithmic trade-off between privileged expert distillation
and standard RL without privileged information. Our main findings are: (1) The
trade-off empirically hinges on the stochasticity of the latent dynamics, as
theoretically predicted by contrasting approximate decodability with belief
contraction in the perturbed Block MDP; and (2) The optimal latent policy is
not always the best latent policy to distill. Our results suggest new
guidelines for effectively exploiting privileged information, potentially
advancing the efficiency of policy learning across many practical partially
observable domains.

</details>


### [292] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的低概率正则化方法（Lp-Reg），用于解决强化学习中奖励可验证性框架下推理探索不足的问题，通过保护有价值的低概率token（称为“推理火花”）来维持有效探索，显著提升了大语言模型在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 在基于可验证奖励的强化学习（RLVR）中，策略熵的崩溃导致探索能力丧失，现有方法虽试图保持高熵但可能引入噪声，缺乏对有意义探索机制的深入理解。

Method: 提出低概率正则化（Lp-Reg），构建一个过滤噪声token后的启发式代理分布，并以该分布为目标进行KL散度正则化，从而增强“推理火花”token的概率并防止其被过度惩罚消除。

Result: 实验表明，Lp-Reg可在约1000步内实现稳定的on-policy训练，而基线方法在此阶段已崩溃；在五个数学基准上平均准确率达到60.17%，较先前方法提升2.66%。

Conclusion: Lp-Reg通过有选择地保留关键低概率token，有效改善了RLVR中的探索质量，实现了更稳定和高效的训练，推动了大语言模型在复杂推理任务上的性能边界。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>
