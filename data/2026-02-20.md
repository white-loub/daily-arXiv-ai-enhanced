<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 50]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.LG](#cs.LG) [Total: 104]
- [cs.AI](#cs.AI) [Total: 67]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯泼溅（Gaussian Splatting, GS）的数字孪生方法，用于 civil infrastructure 的三维损伤可视化，相比NeRF更高效，并支持多尺度重建与随时间更新的损伤建模。


<details>
  <summary>Details</summary>
Motivation: 传统2D图像损伤识别已不能满足现代基础设施检测对高精度三维损伤可视化的需求；现有NeRF等方法在效率或特征缺失区域表现不足，需更优的3D表征方案。

Method: 采用高斯泼溅（GS）进行3D重建，将2D损伤分割结果映射至3D空间；设计多尺度重建策略以兼顾效率与细节；支持随时间演化的数字孪生动态更新。

Result: 在开源地震后合成数据集上验证了该方法能有效减少分割误差、提升3D损伤可视化质量，并实现高效、可更新的数字孪生构建。

Conclusion: GS比NeRF更适合于基础设施数字孪生中的实时、精细、可演化的3D损伤可视化任务，为智能巡检提供了新范式。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: 本文提出了一种多维视频质量评估（VQA）新范式，构建了大规模多维数据集UltraVQA，并提出解析评分优化（ASO）方法，以提升离散质量评分预测的准确性与人类偏好对齐性。


<details>
  <summary>Details</summary>
Motivation: 传统VQA局限于单一平均意见分（MOS），难以刻画用户生成内容（UGC）质量的多样性与复杂性；需更细粒度、多维度、可解释的质量标注及建模方法。

Method: 构建UltraVQA数据集（涵盖5个质量维度、细粒度子属性、多人评分与GPT生成理由）；提出Analytic Score Optimization（ASO），将质量评估建模为带正则化的序数决策过程，导出闭式解以对齐人类排序偏好。

Result: ASO在多个基准上超越主流闭源API与开源模型，显著降低质量预测的平均绝对误差（MAE）。

Conclusion: 多维、可解释的标注与基于决策理论的对齐优化是推动VQA向更真实、可靠、可解释方向发展的关键路径。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: 本文提出DODO模型，首次将块离散扩散机制引入视觉语言模型（VLM）用于OCR任务，在保持近SOTA精度的同时实现最高3倍的推理加速，解决了传统自回归OCR模型速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归解码的VLM在OCR任务中计算开销大、推理慢；而OCR是高度确定性任务，理论上适合并行解码，但现有掩码扩散模型因结构不稳定性无法满足OCR严格的精确匹配要求。

Method: 提出DODO模型，采用块离散扩散（block discrete diffusion）机制，将文本生成分解为多个块进行并行扩散，缓解全局扩散中的同步误差问题。

Result: 在OCR任务上达到近SOTA精度，并实现最高3倍于自回归基线的推理加速。

Conclusion: 块离散扩散是一种适用于确定性序列生成任务（如OCR）的有效并行解码范式，DODO验证了其在精度与速度间的良好平衡。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出StereoAdapter-2，通过引入基于选择性状态空间模型的ConvSS2D算子替代传统ConvGRU，提升水下立体深度估计中长程视差传播效率；并构建大规模合成数据集UW-StereoDepth-80K，结合动态LoRA适配，在多个水下基准上实现零样本SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 水下环境中波长依赖的光衰减、散射和折射导致严重域偏移，现有基于单目基础模型与GRU迭代优化的方法在大视差和无纹理区域受限于GRU的序列门控与局部卷积特性，难以高效进行长程视差传播。

Method: 提出新型ConvSS2D更新算子，采用四向扫描策略，契合极线几何并保持垂直结构一致性，实现单步线性复杂度的长程空间传播；构建UW-StereoDepth-80K合成数据集，融合语义感知风格迁移与几何一致的新视角合成；继承StereoAdapter的动态LoRA适配机制。

Result: 在TartanAir-UW和SQUID水下基准上零样本性能分别提升17%和7.2%，并在BlueROV2真实水下平台上验证了鲁棒性。

Conclusion: StereoAdapter-2通过改进状态空间建模与高质量合成数据构建，显著提升了水下立体深度估计的泛化性与效率，为水下机器人视觉感知提供了更可靠的基础。

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: 本文提出Semantic Coverage-Aware Network (SemCovNet)以解决视觉模型中语义覆盖不平衡（SCI）问题，通过Semantic Descriptor Map、Descriptor Attention Modulation和Descriptor-Visual Alignment损失提升语义公平性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在语义覆盖不平衡（SCI）这一被忽视的偏见，源于语义表示的长尾分布，影响模型对稀有但有意义语义的学习与推理。

Method: 提出SemCovNet模型，包含Semantic Descriptor Map（SDM）用于学习语义表示、Descriptor Attention Modulation（DAM）模块动态加权视觉与概念特征、Descriptor-Visual Alignment（DVA）损失对齐视觉特征与语义描述符，并引入Coverage Disparity Index（CDI）量化语义公平性。

Result: 在多个数据集上的实验表明，SemCovNet显著降低CDI，提升模型可靠性与语义公平性。

Conclusion: 本文确立SCI为一种可测量且可修正的偏见，为推进语义公平与可解释视觉学习奠定基础。

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual 是一个基于海量社交媒体数据训练的统一视觉模型，融合图像与视频理解能力，采用三阶段训练策略和高效ViT架构，在多项基准上达到SOTA性能，并具备强鲁棒性与跨模态检索能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉模型在大规模、多源、噪声复杂的真实社交媒体数据上训练困难，以及图像与视频模态联合建模效率与泛化能力不足的问题。

Method: 构建基于Vision Transformer与EViT的统一架构；设计三阶段训练流程（MAE自监督、半监督hashtag分类、CLIP式对比学习）；利用150亿图像-文本对和100亿视频-hashtag对，结合数据平衡与噪声抑制策略进行训练；引入LLM2CLIP方案将大语言模型作为文本编码器提升跨模态检索。

Result: 在ImageNet、Kinetics、HMDB51、MSCOCO等基准上达到SOTA；具备强域偏移鲁棒性与抗对抗扰动能力；LLM2CLIP显著提升真实场景下的检索性能与泛化性。

Conclusion: Xray-Visual 为可扩展、高精度、高效率的多模态视觉模型树立了新标杆，验证了工业级社交媒体数据与分阶段协同训练范式在统一视觉建模中的巨大潜力。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 本文提出HSI-SC-NeRF，一种基于固定相机的多通道NeRF框架，用于高通量、高保真度的农业产出品高光谱三维重建，适用于采后检测。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱成像与3D重建融合方法硬件复杂、难以适配自动化表型平台；传统NeRF依赖移动相机，限制农业室内场景的通量与可重复性。

Method: 构建固定相机+旋转物体的采集系统（Teflon漫射腔），结合ArUco标定与模拟姿态变换实现NeRF训练；采用多通道NeRF联合优化所有光谱波段，引入复合光谱损失及两阶段训练策略（几何初始化→辐射度精调）。

Result: 在三种农产品样本上验证了高空间重建精度与可见光–近红外波段优异的光谱保真度。

Conclusion: HSI-SC-NeRF为自动化农业工作流提供了可行、高效且可扩展的高光谱三维重建新范式。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 本文提出动态分块策略（dynamic tokenization），在扩散Transformer（DiT）的去噪过程中根据内容复杂度和时间步自适应调整图像/视频分块大小，早期用粗粒度大块建模全局结构，后期用细粒度小块优化局部细节，在不损失生成质量与提示遵循度的前提下显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers（DiTs）虽在图像和视频生成中性能领先，但因固定尺寸分块导致计算开销过大，缺乏对内容复杂度和去噪阶段差异的适应性。

Method: 提出动态分块方法：在推理时依据去噪时间步和内容复杂度动态调整patch大小——早期时间步使用较大patch捕捉全局结构，后期使用较小patch细化局部细节；该策略适用于图像与视频生成任务。

Result: 在FLUX-1.Dev和Wan 2.1模型上分别实现3.52×和3.2×的推理加速，同时保持生成质量（如FID、CLIP Score）和prompt adherence不下降。

Conclusion: 动态分块是一种高效且即插即用的推理优化策略，能显著降低DiT的计算成本而不牺牲生成保真度与语义一致性，为实际部署提供可行路径。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: 本文提出PRIMO模型，用于处理多模态数据中模态缺失的问题，通过潜在变量建模缺失模态，并在推理时采样估计其对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）通常假设训练和推理时所有模态均完整可用，但现实中模态常缺失、异步采集或仅部分样本具备，亟需能利用不完整多模态数据的方法。

Method: PRIMO是一种监督式潜在变量插补模型，将缺失模态建模为与观测模态及预测目标相关的潜在变量；推理时从该潜在分布中多次采样，以获得边际预测分布并量化各实例中缺失模态的预测影响（基于预测方差）。

Result: 在合成XOR、Audio-Vision MNIST和MIMIC-III数据集上，PRIMO在单模态缺失时性能媲美单模态基线，在全模态可用时媲美多模态基线；并实现了实例级模态影响量化与可视化分析。

Conclusion: PRIMO有效提升了不完整多模态数据下的模型鲁棒性与可解释性，为实际场景中模态缺失问题提供了统一建模与影响评估框架。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [10] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像块的时空作者归属框架，用于人类与机器人协作绘画中的作者身份识别，在15幅抽象画上实现了88.8%的块级准确率，并通过条件香农熵验证了其对混合创作区域的有效识别能力。


<details>
  <summary>Details</summary>
Motivation: 随着具身AI越来越多地参与创意生产，明确人类与AI在协作艺术作品中的作者身份，对艺术家、收藏家及法律场景至关重要；而现有方法难以应对协作中风格边界模糊、标注困难、数据稀缺等挑战。

Method: 提出一种基于图像块（patch-based）的空间作者归属框架，使用普通平板扫描仪采集数据，采用留一画交叉验证；引入条件香农熵量化人类与机器人风格重叠程度，并结合人工标注的混合区域进行不确定性分析。

Result: 在15幅人机协作抽象画上达到88.8%的块级准确率（绘画级86.7%），显著优于纹理特征和预训练特征基线（68.0%–84.7%）；混合区域的条件熵比纯作区域高64%（p=0.003），证实模型确实在识别混合作者而非分类错误。

Conclusion: 该方法虽目前仅适用于特定人机组合，但为数据稀缺的人机协同创意场景提供了可扩展、样本高效的作者归属范式，未来有望推广至更广泛的人-机器人绘画协作体系。

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

</details>


### [11] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出PartRAG，一种结合外部部件数据库与扩散Transformer的检索增强框架，用于单图像3D生成，支持部件级结构建模与局部精确编辑。


<details>
  <summary>Details</summary>
Motivation: 现有单图像3D生成方法难以覆盖部件几何的长尾分布、保证多视角一致性，且缺乏对局部精细编辑的支持。

Method: 提出分层对比检索模块（对齐图像块与3D部件潜在表示）和共享规范空间中的掩码部件级编辑器，利用1236个带部件标注的3D资产库进行检索增强生成与交互式编辑。

Result: 在Objaverse等数据集上Chamfer距离降至0.1528、F-Score提升至0.844；推理耗时38秒，交互编辑仅需5–8秒；定性显示更清晰的部件边界、更好的细长结构保真度及对铰接物体的鲁棒性。

Conclusion: PartRAG通过检索增强与可编辑规范表征，有效提升了单图像3D生成的部件多样性、几何精度与编辑可控性。

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [12] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 本文提出了一种高效的压缩框架，将60层双流MMDiT架构的Qwen-Image模型压缩为轻量级T2I模型Amber-Image（10B和6B），通过时间步敏感剪枝、局部权重平均初始化、分层蒸馏与渐进式蒸馏等技术，在显著降低参数量（70%）和计算成本（<2000 GPU小时）的同时，保持高保真图像生成与优异文本渲染能力。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer (DiT) 架构虽推动了文本到图像生成的发展，但其高昂的计算成本和部署难度限制了实际应用，亟需高效压缩方法。

Method: 提出一种无需从头训练的压缩框架：1）对Qwen-Image进行时间步敏感的深度剪枝得到Amber-Image-10B，辅以局部权重平均初始化、分层知识蒸馏和全参数微调；2）进一步设计混合流架构，将深层双流转为单流（源自图像分支），通过渐进式蒸馏与轻量微调获得Amber-Image-6B。

Result: Amber-Image系列模型参数减少70%，训练总耗时低于2000 GPU小时；在DPG-Bench和LongText-Bench等基准上达到与更大模型相当的高保真图像生成与文本渲染性能。

Conclusion: 该压缩框架在大幅降低计算开销与数据依赖的同时，成功保留并提升了大型DiT模型的生成质量，为高效T2I模型部署提供了可行路径。

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [13] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: 本文提出StructCore，一种无需训练、结构感知的图像级评分方法，用于改进基于内存库的无监督异常检测中的图像级决策，通过捕捉异常分数图的分布和空间特征，并利用正常样本进行马氏距离校准，显著提升了图像级AUROC性能。


<details>
  <summary>Details</summary>
Motivation: Max pooling作为当前主流方法，仅依赖单个极值响应，忽略了异常证据在图像中的分布与结构信息，导致正常与异常分数易重叠。

Method: StructCore计算异常分数图的低维结构描述符phi(S)，捕获其分布与空间特性，并利用正常训练样本估计对角马氏距离进行图像级评分校准，不改变像素级定位。

Result: 在MVTec AD和VisA数据集上分别达到99.6%和98.4%的图像级AUROC，显著优于max pooling。

Conclusion: StructCore通过挖掘被max pooling忽略的结构特征，实现了更鲁棒的图像级异常检测，且无需额外训练。

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [14] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 本文提出Cholec80-port数据集及统一的端口标注规范（SOP），强调排除中央孔洞的几何一致标注，显著提升图像拼接、3D重建等几何任务的跨数据集鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中trocar端口因高反光、纹理丰富且位置固定，易被误检为特征点，严重干扰基于几何的下游任务（如图像拼接、3D重建、视觉SLAM）；而现有公开数据集缺乏显式、几何一致的端口标注（常错误遮盖中央孔洞）。

Method: 构建Cholec80-port高质量端口分割数据集，并制定严格SOP：端口掩码仅覆盖套管部分，明确排除中央孔洞；同时依据该SOP清洗和统一多个现有公开数据集。

Result: 实验表明，采用几何一致标注显著提升模型在跨数据集场景下的鲁棒性，该增益独立于数据集规模。

Conclusion: 几何一致的trocar端口标注对提升手术视频分析中几何敏感任务的泛化性和稳定性至关重要；所提SOP与Cholec80-port数据集为后续研究提供了可靠基准。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [15] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD是一种双分支弱监督视频异常检测框架，通过跨伪标签机制结合二值异常定位与视觉-语言对齐的异常类别识别，在XD-Violence和UCF-Crime数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在仅使用视频级标签的情况下，难以同时兼顾异常片段的精确定位与异常类别的语义识别。

Method: 提出CPL-VAD双分支框架：一个分支进行片段级二值异常检测，另一个分支利用视觉-语言对齐实现异常事件类别分类；两分支通过交换伪标签实现互补信息迁移。

Result: 在XD-Violence和UCF-Crime数据集上，CPL-VAD在异常检测和异常类别分类两项任务中均达到当前最优性能。

Conclusion: 跨伪标签机制有效融合了时间精度与语义判别能力，验证了双分支协同学习在弱监督视频异常检测中的有效性。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [16] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: 本文提出了一种名为ComptonUNet的混合深度学习框架，用于在低光子统计和强背景噪声条件下稳健地定位微弱伽马射线暴（GRBs）。该模型结合了直接重建的统计效率与图像去噪能力，在模拟实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation:  faint GRBs难以检测和定位，因光子统计少、背景噪声强；现有机器学习模型难以兼顾统计鲁棒性与噪声抑制。

Method: 提出ComptonUNet，一种联合处理原始数据并重建图像的混合深度学习框架，融合直接重建模型的统计效率与图像架构的去噪能力，并在近地轨道背景环境下的GRB仿真数据上进行评估。

Result: ComptonUNet在多种低统计量、高背景场景下均展现出更优的GRB定位精度，显著超越现有方法。

Conclusion: ComptonUNet为微弱、遥远GRBs的高精度定位提供了新范式，提升了其作为早期恒星形成探针的潜力。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [17] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 本文提出了一种融合射频（RF）传感（如车载雷达）与3D高斯泼溅（GS）的多模态框架，以克服纯视觉GS在恶劣天气、低照度或遮挡等场景下依赖大量相机视图和初始化困难的问题；该方法利用稀疏RF深度测量高效预测深度，生成高质量点云用于GS初始化，并在渲染保真度和鲁棒性上取得提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的高斯泼溅（GS）方法在恶劣天气、低光照或部分遮挡等视觉线索不可靠的条件下表现不佳，且依赖大量相机视角进行初始化，计算开销大；而射频（RF）信号对天气、光照和遮挡具有强鲁棒性，因此引入RF传感可提升GS的效率与鲁棒性。

Method: 提出一种多模态框架，将RF传感（如车载雷达）与GS渲染结合；利用稀疏RF深度测量，通过高效深度预测生成高质量3D点云，用于初始化各类GS架构中的高斯原语。

Result: 数值实验表明，该RF增强的GS方法在保持高渲染保真度的同时，显著提升了在视觉受限场景下的结构准确性与鲁棒性，实现了更高效、更可靠的3D场景重建与渲染。

Conclusion: 融合RF传感与高斯泼溅是一种有前景的多模态3D重建范式，可有效弥补纯视觉方法的不足，在自动驾驶等实际应用中具备实用价值。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [18] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: 本文提出B^3-Seg，一种无需相机视角预设、无需标注、无需重新训练的开放词汇3D高斯泼溅（3DGS）交互式分割方法，基于Beta-Bernoulli贝叶斯更新与解析期望信息增益（EIG）主动选视点，在几秒内完成端到端分割，兼具高效性与理论保障。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS分割方法依赖预设相机视角、真实标签或高成本重训练，难以满足影视与游戏制作中低延迟交互编辑的实际需求。

Method: 将分割建模为序列化的Beta-Bernoulli贝叶斯更新过程，并通过解析计算的期望信息增益（EIG）主动选择最优下一视角；利用EIG的自适应单调性与次模性，保证贪心策略达(1−1/e)最优近似。

Result: 在多个数据集上，B^3-Seg以几秒钟端到端运行时间，达到与高成本监督方法相当的分割性能，验证了其信息效率与实用性。

Conclusion: B^3-Seg实现了相机无关、训练无关、开放词汇的高效交互式3DGS分割，兼具理论可解释性与工程实用性。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [19] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出BadCLIP++框架，通过语义融合QR微触发、目标对齐子集选择、嵌入稳定化与参数正则化等技术，同时提升后门攻击在多模态对比学习模型中的隐蔽性与持久性，并在数字与物理场景下均取得优异攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有针对多模态对比学习模型的后门攻击方法在强检测和持续微调下易失效，主因是跨模态不一致性暴露触发模式，以及低投毒率下的梯度稀释加速后门遗忘，这两个耦合问题未被充分建模与解决。

Method: 提出BadCLIP++统一框架：（1）为提升隐蔽性，设计语义融合QR微触发并嵌入任务相关区域，结合目标对齐子集选择增强低投毒率信号；（2）为提升持久性，采用半径收缩与质心对齐稳定触发嵌入，并通过曲率控制与弹性权重巩固稳定模型参数；（3）首次在可信区域内理论证明清洁微调与后门目标梯度同向，从而给出攻击成功率退化上界非增的保证。

Result: 仅0.3%投毒率下，数字攻击ASR达99.99%，领先基线11.4个百分点；在19种防御下ASR仍高于99.90%，清洁准确率下降<0.8%；物理攻击成功率达65.03%，且对水印移除等防御鲁棒。

Conclusion: BadCLIP++有效协同解决了多模态对比学习后门攻击中隐蔽性与持久性的核心矛盾，兼具理论保障与强实证性能，为安全评估与防御研究提供了新基准。

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [20] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 本文提出NRGS-SLAM，一种基于3D高斯泼溅的单目非刚性SLAM系统，用于内窥镜场景；通过引入可学习形变概率的形变感知高斯地图、粗到精的形变感知跟踪模块、渐进式形变映射模块及统一鲁棒几何损失，有效解耦相机运动与软组织形变，在姿态估计精度和重建质量上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景中持续的软组织形变违反刚性假设，导致相机自运动与内在形变强耦合，现有单目非刚性SLAM方法缺乏有效解耦机制且场景表征稀疏或保真度低，造成跟踪漂移和重建质量受限。

Method: 提出NRGS-SLAM：1）构建含可学习形变概率的形变感知3D高斯地图，采用贝叶斯自监督策略优化；2）设计形变感知跟踪模块，优先利用低形变区域实现粗到精位姿估计，并进行逐帧形变更新；3）设计渐进式形变映射模块以平衡表达能力与计算效率；4）引入融合外部几何先验的统一鲁棒几何损失缓解单目非刚性SLAM病态性。

Result: 在多个公开内窥镜数据集上实验表明，NRGS-SLAM相比SOTA方法，相机位姿估计RMSE最高降低50%，且重建更逼真；消融实验验证了各核心设计的有效性。

Conclusion: NRGS-SLAM通过新颖的形变感知高斯表示与协同优化框架，显著提升了单目非刚性SLAM在内窥镜场景下的定位精度与重建质量，为微创手术导航等应用提供了更可靠的基础。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [21] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种基于困惑度的视觉信息增益（VIG）度量方法，用于量化图像输入对语言模型预测不确定性的降低程度，并据此设计了VIG引导的选择性训练策略，以提升大视觉语言模型（LVLMs）的视觉接地能力并缓解语言偏差。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型（LVLMs）存在语言偏差问题，即模型在不依赖视觉证据的情况下生成答案；已有工作缺乏对单个训练样本或token从图像中实际获益程度的定量衡量。

Method: 提出视觉信息增益（VIG）指标，基于困惑度衡量图像输入带来的预测不确定性下降；在此基础上设计VIG引导的选择性训练方案，优先使用高VIG的样本和token进行训练。

Result: 该方法能细粒度识别颜色、空间关系和属性等视觉相关元素，在提升视觉接地能力、缓解语言偏差的同时，以更少监督实现更优性能。

Conclusion: VIG提供了一种可解释、可量化的视觉贡献评估手段，VIG引导的选择性训练是一种高效提升LVLM视觉接地能力的新范式。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [22] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: 本文提出EntropyPrune，一种基于矩阵熵的视觉token剪枝方法，通过识别'熵坍缩层'（ECL）实现可解释、可迁移的高效MLLM推理加速，在保持高精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM视觉token剪枝方法依赖经验性、静态的剪枝层选择，缺乏可解释性和跨模型泛化能力，难以兼顾效率与精度。

Method: 从矩阵熵视角出发，发现并定义'熵坍缩层'（ECL）作为剪枝时机的理论依据；提出EntropyPrune框架，利用双Gram矩阵谱等价性高效计算token熵值，据此量化并剪除冗余视觉token，无需依赖注意力图。

Result: 在LLaVA-1.5-7B上实现68.2% FLOPs降低且保留96.0%原始性能；在多模态基准上全面超越SOTA剪枝方法，并在高分辨率图像和视频模型中展现良好泛化性与鲁棒性。

Conclusion: 矩阵熵为MLLM token剪枝提供了可解释、可迁移的新范式；EntropyPrune是一种高效、通用且易于部署的推理加速方案。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [23] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: 本文提出了一种基于几何视角提升文本到图像生成多样性的新方法GASS，通过在CLIP嵌入空间中解耦语义相关与无关的变异源，并沿正交方向扩展投影分布，从而在保持图像质量和语义对齐的前提下显著提升生成多样性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型虽语义对齐度高，但生成图像多样性不足，限制用户选择并可能加剧社会偏见。

Method: 提出Geometry-Aware Spherical Sampling（GASS），在CLIP嵌入空间中将多样性分解为文本嵌入方向（语义相关）和其正交方向（如背景等提示无关）两个正交分量，并在采样过程中沿这两轴扩大投影分布以引导生成。

Result: 在多种冻结T2I骨干网络（U-Net、DiT；扩散与流模型）及基准测试中验证了GASS的有效性，在几乎不损害图像保真度和语义对齐的前提下实现了可解耦的多样性增强。

Conclusion: GASS提供了一种几何驱动、解耦可控的多样性增强范式，为提升T2I模型公平性与实用性提供了新思路。

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [24] [HiMAP: History-aware Map-occupancy Prediction with Fallback](https://arxiv.org/abs/2602.17231)
*Yiming Xu,Yi Yang,Hao Cheng,Monika Sester*

Main category: cs.CV

TL;DR: HiMAP是一种无需多目标跟踪（MOT）的轨迹预测框架，通过历史占用图与历史查询模块实现身份无关、鲁棒的多模态运动预测，在无跟踪场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测方法严重依赖多目标跟踪（MOT），当出现遮挡、ID切换或漏检等跟踪失败时，预测性能下降、安全风险上升；亟需一种不依赖ID、对MOT失败鲁棒的预测方案。

Method: HiMAP将历史检测转化为时空不变的历史占用图；设计历史查询模块，基于当前智能体状态迭代从无标签占用表征中检索个体化历史；用时间地图嵌入汇总历史，并联合最终查询与地图上下文，驱动DETR风格解码器生成多模态未来轨迹。

Result: 在Argoverse 2上，HiMAP在无ID输入下达到与跟踪基线相当的性能；在无跟踪设定下，相较微调后的QCNet，FDE降低11%、ADE降低12%、MR降低4%；支持流式推理，且能同时稳定预测所有智能体轨迹。

Conclusion: HiMAP摆脱了对目标ID和连续跟踪的依赖，提供了一种更鲁棒、实用、可部署于安全关键自动驾驶系统的轨迹预测新范式。

Abstract: Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.

</details>


### [25] [Inferring Height from Earth Embeddings: First insights using Google AlphaEarth](https://arxiv.org/abs/2602.17250)
*Alireza Hamoudzadeh,Valeria Belloni,Roberta Ravanelli*

Main category: cs.CV

TL;DR: 本研究探讨了AlphaEarth嵌入（10米分辨率）能否有效指导深度学习回归模型进行区域地表高度映射，使用U-Net和U-Net++架构解码嵌入信息；结果表明嵌入包含可解码的高度信号，U-Net++在测试中泛化能力更强（R²=0.84 vs. 0.78），但仍有分布偏移导致的偏差与RMSE挑战。


<details>
  <summary>Details</summary>
Motivation: 验证地球嵌入（尤其是AlphaEarth Embeddings）中编码的地理空间与多模态特征是否能有效支持深度学习模型进行区域地表高度回归估计。

Method: 采用U-Net和U-Net++作为轻量级卷积解码器，以AlphaEarth Embeddings（10 m分辨率）为输入，以高精度数字地表模型（DSM）为真值标签，进行监督回归训练与评估。

Result: 训练阶段两模型R²均达0.97；测试阶段U-Net++表现更优（R²=0.84，中位误差−2.62 m），优于U-Net（R²=0.78，中位误差−7.22 m）；测试RMSE约16 m，存在残差偏差，但高度相关性表明嵌入捕获了可迁移的地貌模式。

Conclusion: AlphaEarth Embeddings具备指导DL高度映射的潜力，尤其结合空间感知卷积架构时效果显著；但需进一步解决分布偏移带来的系统性偏差以提升区域泛化能力。

Abstract: This study investigates whether the geospatial and multimodal features encoded in \textit{Earth Embeddings} can effectively guide deep learning (DL) regression models for regional surface height mapping. In particular, we focused on AlphaEarth Embeddings at 10 m spatial resolution and evaluated their capability to support terrain height inference using a high-quality Digital Surface Model (DSM) as reference. U-Net and U-Net++ architectures were thus employed as lightweight convolutional decoders to assess how well the geospatial information distilled in the embeddings can be translated into accurate surface height estimates. Both architectures achieved strong training performance (both with $R^2 = 0.97$), confirming that the embeddings encode informative and decodable height-related signals. On the test set, performance decreased due to distribution shifts in height frequency between training and testing areas. Nevertheless, U-Net++ shows better generalization ($R^2 = 0.84$, median difference = -2.62 m) compared with the standard U-Net ($R^2 = 0.78$, median difference = -7.22 m), suggesting enhanced robustness to distribution mismatch. While the testing RMSE (approximately 16 m for U-Net++) and residual bias highlight remaining challenges in generalization, strong correlations indicate that the embeddings capture transferable topographic patterns. Overall, the results demonstrate the promising potential of AlphaEarth Embeddings to guide DL-based height mapping workflows, particularly when combined with spatially aware convolutional architectures, while emphasizing the need to address bias for improved regional transferability.

</details>


### [26] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于基础设施的多模态货运车辆检测系统，融合激光雷达（LiDAR）与摄像头传感器，采用混合感知架构和融合聚类与深度学习的检测方法，结合卡尔曼滤波跟踪，实现高时空分辨率的货运车辆类型、位置与速度感知，以支持货运信号优先（FSP）应用。


<details>
  <summary>Details</summary>
Motivation: 货运车辆在接近信号交叉口时需可靠检测与运动估计，以支撑基于基础设施的货运信号优先（FSP）；当前亟需准确及时的车型识别、定位与速度估计能力。

Method: 设计并部署了融合LiDAR与相机的多模态基础设施感知系统，采用路口端+路段中段双子系统架构及无线同步通信；感知流程融合聚类法与深度学习检测，并引入卡尔曼滤波跟踪；LiDAR点云注册至大地坐标系以实现车道级定位与稳定跟踪。

Result: 实地评估表明该系统可在高时空分辨率下可靠监测货运车辆运动，具备实时稳定性能。

Conclusion: 该系统为面向FSP的基础设施感知系统提供了可落地的设计范式与工程部署经验，验证了多模态融合方案在货运交通管控中的实用性与有效性。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [27] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: 本文提出EA-Swin模型和EA-Video数据集，用于高效检测AI生成视频，显著提升准确率与跨分布泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频检测方法在面对Sora2、Veo3等新型高保真生成器时表现不足，受限于浅层嵌入轨迹、图像适配或计算开销大的多模态大语言模型。

Method: 提出Embedding-Agnostic Swin Transformer（EA-Swin），采用因子化窗口注意力机制，直接建模预训练视频嵌入的时空依赖；构建含130K视频的EA-Video基准数据集，覆盖主流商业与开源生成器，并设未见生成器分割以支持跨分布评估。

Result: EA-Swin在主要生成器上达到0.97–0.99准确率，较此前SOTA方法（0.8–0.9）提升5–20%，且对未见生成器保持强泛化性。

Conclusion: EA-Swin是一种可扩展、鲁棒的现代AI生成视频检测方案，兼顾高性能与通用性。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [28] [Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution](https://arxiv.org/abs/2602.17277)
*Ruoyi Zhang,Jiawei Yuan,Lujia Ye,Runling Yu,Liling Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种物理编码的时空生成对抗网络（PESTGAN），用于热带气旋卫星图像超分辨率重建，通过引入物理约束的PhyCell模块和双判别器框架，在保持像素级精度的同时显著提升气象结构合理性和物理保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的超分辨率方法将卫星图像序列视为普通视频，忽略了控制云运动的大气物理规律，导致重建结果物理不合理。

Method: 提出PESTGAN模型：1）采用解耦生成器，集成PhyCell模块，通过约束卷积近似涡度方程，将物理动力学编码为隐式潜在表示；2）设计双判别器框架，分别约束空间真实性和时间运动一致性。

Result: 在Digital Typhoon数据集上实现4×超分辨率，结构保真度和感知质量优于现有方法；像素级精度具竞争力，且在重建气象合理的云结构和物理保真度方面显著更优。

Conclusion: 融合物理先验的生成模型能有效提升遥感图像超分辨率的气象可信度，为物理引导的AI气象建模提供了新范式。

Abstract: High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.

</details>


### [29] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: 本文提出了一种名为'attachment anchors'的结构化表示方法，用于在结直肠微创手术中提升自主组织抓取点预测的准确性，该方法通过编码组织与其解剖附着点间的几何与力学关系，将手术场景归一化到局部参考系，从而降低抓取点预测的不确定性，并在90例手术数据上验证了其在分布外场景（如未见术式、不同术者）中的显著优势。


<details>
  <summary>Details</summary>
Motivation: 结直肠手术因其复杂性、耗时长，在现有研究中代表性不足，但其重复性组织操作特性使其成为机器学习驱动自主辅助的理想切入点；同时，精准抓取点预测是微创手术中自主组织操作的关键挑战。

Method: 提出'attachment anchors'结构化表征，编码组织与其解剖附着点之间的局部几何与力学关系，将手术场景归一化至一致的局部参考系，并从腹腔镜图像中预测该表征，融入基于机器学习的抓取框架。

Result: 在90例结直肠手术数据集上的实验表明，相比纯图像基线方法，attachment anchors显著提升了抓取点预测精度，尤其在分布外场景（如未见过的术式和术者）中增益更明显。

Conclusion: attachment anchors是一种有效的中间表征，有助于提升学习型系统在结直肠手术中组织操作的泛化性与鲁棒性。

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [30] [Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline](https://arxiv.org/abs/2602.17322)
*Mohamed Dhouib,Davide Buscaldi,Sonia Vanier,Aymen Shabou*

Main category: cs.CV

TL;DR: 本文提出了一种生成高质量篡改文档图像的新方法，通过两个辅助网络（基于对比学习的文本块比较网络和字符紧密包围评估网络）提升生成数据的多样性与视觉质量，从而显著提高篡改文本检测模型在真实数据上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的篡改文档生成方法多样性低、视觉质量差、伪影明显，导致模型学不到鲁棒特征，在真实数据上性能差；本文旨在弥合合成数据与真实篡改之间的差距。

Method: 设计两个辅助网络：1）基于对比学习的文本块相似性判别网络（含新定义的正负样本策略）；2）字符边界紧密性评估网络；再构建融合两者的端到端篡改文档生成流水线。

Result: 在相同训练协议下，用本方法生成的数据训练的模型，在多个开源测试集上相较基线方法取得一致且显著的性能提升，且适配多种网络架构。

Conclusion: 高质量、多样化的合成篡改数据对提升文档图像篡改检测模型的泛化能力至关重要；所提双辅助网络驱动的生成框架有效缓解了数据稀缺与域偏移问题。

Abstract: Detecting tampered text in document images is a challenging task due to data scarcity. To address this, previous work has attempted to generate tampered documents using rule-based methods. However, the resulting documents often suffer from limited variety and poor visual quality, typically leaving highly visible artifacts that are rarely observed in real-world manipulations. This undermines the model's ability to learn robust, generalizable features and results in poor performance on real-world data. Motivated by this discrepancy, we propose a novel method for generating high-quality tampered document images. We first train an auxiliary network to compare text crops, leveraging contrastive learning with a novel strategy for defining positive pairs and their corresponding negatives. We also train a second auxiliary network to evaluate whether a crop tightly encloses the intended characters, without cutting off parts of characters or including parts of adjacent ones. Using a carefully designed generation pipeline that leverages both networks, we introduce a framework capable of producing diverse, high-quality tampered document images. We assess the effectiveness of our data generation pipeline by training multiple models on datasets derived from the same source images, generated using our method and existing approaches, under identical training protocols. Evaluating these models on various open-source datasets shows that our pipeline yields consistent performance improvements across architectures and datasets.

</details>


### [31] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: 本文提出了Polaffini，一种基于解剖结构的鲁棒且通用的医学图像配准框架，利用深度学习分割模型提取解剖区域质心作为特征点，通过闭式解实现高效仿射及多仿射匹配，并在结构对齐和非线性配准初值方面优于传统强度配准方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于强度的配准方法依赖对齐质量的代理度量，而基于解剖特征的配准虽更理想却因特征提取困难而被冷落；近年深度学习分割模型的发展为可靠、细粒度解剖分割提供了可能，从而可支撑新型解剖引导配准算法。

Method: Polaffini从预训练分割模型输出的解剖区域中直接提取质心作为具有一一对应关系的解剖特征点，利用这些点通过闭式解实现全局与局部仿射匹配，组合成可调平滑度的polyaffine变换；该变换嵌入log-Euclidean框架以保证微分同胚性。

Result: Polaffini在结构对齐精度上优于主流强度配准方法，并能为后续非线性配准提供更优初值；兼具速度快、鲁棒性强、准确性高优势。

Conclusion: Polaffini成功将现代深度学习分割能力转化为解剖引导配准优势，是一种适用于临床图像处理流水线的高效、可靠、解剖一致的配准新范式。

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [32] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: 本文提出了南美洲首个10米分辨率的果树作物分布图，利用Sentinel-1和Sentinel-2时序遥感影像与多模态时空深度学习模型生成，揭示现有EUDR监管地图常将小农林农系统误判为森林，导致错误毁林警报；该成果有助于提升零毁林政策的精准性、包容性与公平性。


<details>
  <summary>Details</summary>
Motivation: 监测果树作物扩张对落实零毁林政策（如欧盟《无毁林产品法规》EUDR）至关重要，但现有高分辨率数据难以区分多样化的农业系统与森林，制约政策有效实施。

Method: 构建基于Sentinel-1和Sentinel-2卫星影像时间序列的多模态时空深度学习模型，生成南美洲10米分辨率果树作物分布图。

Result: 绘制出覆盖约1100万公顷果树作物的地图，其中23%与2000–2020年间森林覆盖损失相关；发现EUDR现行监管地图普遍将已建成农业（尤其是小农林农系统）错误归类为森林。

Conclusion: 本研究提供的高分辨率基线地图可显著降低误报风险，支撑更有效、包容且公平的森林保护政策。

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [33] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 本文提出DRetHTR，一种基于Retentive Networks（RetNet）的纯解码器手写文本识别模型，通过去除softmax注意力、引入多尺度序列先验和层自适应gamma缩放，在保持高精度的同时显著提升推理速度与内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的手写文本识别（HTR）系统因键值（KV）缓存随输出长度增长而导致解码慢且内存开销大，亟需更高效的架构。

Method: 提出DRetHTR：采用RetNet替代Transformer解码器，用softmax-free retention机制取代自注意力；注入多尺度序列先验；设计层-wise gamma缩放以动态扩展各层保留范围，恢复局部到全局的归纳偏置。

Result: 相比同规模Transformer解码器基线，DRetHTR实现1.6–1.9倍加速与38–42%内存减少，且无精度损失；在IAM-A、RIMES、Bentham和READ-2016数据集上分别达到2.26%、1.81%、3.46%和4.21%的最优字符错误率。

Conclusion: 纯解码器RetNet可在保持Transformer级HTR精度的同时，大幅改善解码效率，验证了其作为高效HTR主干结构的可行性。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [34] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: 本文提出了一种新的反事实基准LIBERO-CF，并设计了反事实动作引导（CAG）方法，通过引入语言无关的视觉-动作分支，提升视觉-语言-动作模型（VLAs）对语言指令的遵循能力，显著减少因数据偏差导致的反事实失败，且无需额外训练或修改模型结构。


<details>
  <summary>Details</summary>
Motivation: 现有VLAs在缺乏强场景监督的语言指令下易受数据集偏差影响，依赖视觉捷径而非语言意图，导致反事实失败；该问题尚未被系统研究。

Method: 构建首个反事实评估基准LIBERO-CF，并提出双分支推理框架CAG：联合标准VLA策略与语言无关的Vision-Action（VA）模块，在动作选择中进行反事实对比，从而显式正则化语言条件作用。

Result: CAG在LIBERO-CF上将语言遵循准确率（π₀.₅）提升9.7%（训练无关）至15.5%（配合VA模型），任务成功率提升3.6%至8.5%；真实机器人实验中反事实失败降低9.4%，任务成功率平均提升17.2%。

Conclusion: 反事实失败是当前VLAs的关键瓶颈，CAG以轻量、即插即用方式有效缓解该问题，验证了显式解耦语言与视觉线索对提升语言遵循鲁棒性的重要价值。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


### [35] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 本文提出SpectralGCD，一种高效且有效的广义类别发现（GCD）方法，利用CLIP的跨模态图像-概念相似性作为统一表示，并通过谱滤波和双向知识蒸馏提升语义质量与对齐性，在多个基准上达到SOTA性能且计算成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在仅用图像特征训练时易过拟合旧类；多模态方法虽引入文本信息提升性能，但模态独立处理且计算开销大。

Method: 提出SpectralGCD：1）以CLIP跨模态图像-概念相似性构建统一表示；2）将每张图像建模为大规模语义概念字典上的混合分布；3）引入Spectral Filtering，基于教师模型softmax相似度的跨模态协方差矩阵自动筛选相关概念；4）采用前向与反向知识蒸馏确保学生模型表征的语义充分性与模态对齐性。

Result: 在六个基准数据集上，SpectralGCD精度媲美或显著超越现有SOTA方法，同时计算成本大幅降低。

Conclusion: SpectralGCD通过显式语义锚定、谱滤波与双向蒸馏，实现了高效、鲁棒且语义清晰的广义类别发现，为多模态无监督学习提供了新范式。

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [36] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本文是一篇关于无人机光学遥感的综述论文，旨在为初入该领域的研究者提供全面概览和实用指南。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对无人机光学遥感领域的整体性、综合性综述，尤其缺少面向新研究者的系统性入门指引。

Method: 采用文献调研与归纳分析方法，系统梳理光学遥感在无人机平台上的任务类型、技术能力、常用数据集及关键研究洞见。

Result: 构建了一个涵盖任务、能力、数据集与实践启示的综合框架，并明确了该领域当前的研究格局与发展脉络。

Conclusion: 该综述填补了无人机光学遥感领域系统性入门指南的空白，有助于研究者快速定位方向、聚焦关键问题。

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [37] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: 本文提出EAGLE，一种无需调优的框架，通过融合专家模型输出来引导多模态大语言模型（MLLMs）实现高精度工业异常检测与可解释描述。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅提供二值决策且缺乏语义解释；MLLMs虽具生成细粒度语言分析潜力，但需昂贵微调且检测精度常不如轻量专用检测器。

Method: 提出专家增强注意力引导（EAGLE）框架，无需参数更新，利用专家模型输出指导MLLMs关注异常区域，并分析中间层注意力分布以验证引导效果。

Result: 在MVTec-AD和VisA数据集上，EAGLE显著提升多种MLLMs的异常检测性能，效果媲美微调方法，且保持零参数更新。

Conclusion: EAGLE证明了无需微调即可协同专家模型与MLLMs，在保证检测精度的同时增强可解释性，注意力分析进一步揭示其内在机制。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [38] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 本文提出Local-EndoGS，一种面向单目内窥镜视频、支持任意相机运动的高质量4D可变形手术场景重建框架，通过窗口化局部建模、粗到细初始化策略及长程像素轨迹与物理运动先验提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示或3D高斯溅射的方法大多假设固定内窥镜视角，依赖双目深度先验或高精度运动恢复结构（SfM），难以处理临床中常见的单目、大运动内窥镜序列。

Method: 提出Local-EndoGS：1）采用滑动窗口划分长序列，为每个窗口分配局部可变形场景模型，构建渐进式全局表示；2）设计融合多视图几何、跨窗口信息与单目深度先验的粗到细初始化策略；3）引入长程2D像素轨迹约束和物理运动先验以增强形变合理性。

Result: 在三个公开可变形内窥镜数据集上，Local-EndoGS在外观质量和几何精度上均显著优于现有最先进方法；消融实验验证了各核心模块的有效性。

Conclusion: Local-EndoGS有效解决了单目内窥镜下大运动、可变形手术场景的高质量4D重建难题，具备临床实用性与可扩展性。

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [39] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出了一种物理感知的多模态框架QuPAINT，结合物理仿真数据生成器Synthia、首个量子材料指令数据集QMat-Instruct和新基准QF-Bench，以提升光学显微图像中二维量子材料层数识别的泛化性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型缺乏物理先验，在层依赖对比微弱、标注数据少、跨实验室成像差异大的条件下难以泛化到新材料或新硬件环境。

Method: 提出Synthia（基于薄膜干涉的物理仿真数据生成器）、QMat-Instruct（物理信息驱动的多模态指令数据集）、QuPAINT（含物理感知注意力模块的多模态大模型微调方法）及QF-Bench（覆盖多材料/基底/成像条件的综合基准）。

Result: 显著降低对人工标注依赖，提升模型在未见材料、基底和成像条件下的厚度识别准确率与鲁棒性；QF-Bench为该领域提供标准化评估协议。

Conclusion: 融合物理先验与多模态大模型是解决二维量子材料图像分析中泛化性差问题的有效路径，为科学AI在材料表征中的应用树立了新范式。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [40] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 本文提出PixTrace和CopyNCE，通过像素级坐标追踪与几何引导的对比损失提升图像复制检测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视图级对比学习的自监督图像复制检测方法难以应对复杂编辑操作，因缺乏细粒度对应关系学习。

Method: 提出PixTrace模块实现编辑变换下的显式空间映射跟踪，并设计CopyNCE损失函数，利用PixTrace验证的重叠比约束图像块相似性学习。

Result: 在DISC21数据集上达到SOTA性能：匹配器uAP 88.7%/RP90 83.9%，描述符uAP 72.6%/RP90 68.4%，且可解释性更优。

Conclusion: 将像素级可追溯性与块级相似性学习结合，有效抑制自监督训练中的监督噪声，显著提升ICD系统鲁棒性与可解释性。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [41] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 本文提出了一种结合腹腔镜深度图与基础姿态估计器的增强现实配准方法，用非刚性迭代最近点（NICP）替代有限元（FE）模型进行肝脏形变配准，在真实患者数据上达到9.91 mm平均配准误差，精度满足临床需求且工程实现更轻量、友好。


<details>
  <summary>Details</summary>
Motivation: 现有腹腔镜肝手术中AR肿瘤定位依赖器官轮廓配准，形变建模多采用复杂且需专业知识的有限元（FE）模型，亟需更轻量、易部署的替代方案。

Method: 融合腹腔镜深度图与基础姿态估计器实现相机-肝脏位姿估计，并以非刚性ICP（NICP）替代FE模型完成形变配准。

Result: 在3例真实患者数据上，深度增强的基础姿态方法平均配准误差为9.91 mm；刚性+NICP配准优于纯刚性配准，验证NICP可高效替代FE形变模型。

Conclusion: 该流程在保证临床相关精度的同时，显著降低了建模与工程复杂度，为FE-based形变配准提供了轻量、工程友好的替代方案。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [42] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出LATA方法，通过拉普拉斯平滑和失败感知的校准分数，在不破坏交换性前提下提升医学视觉语言模型在域偏移下的零样本不确定性校准效果，显著减小预测集大小和类别间覆盖率差距。


<details>
  <summary>Details</summary>
Motivation: 现有分割共形预测（SCP）在医学视觉语言模型中面临预测集过大、类别间覆盖率不平衡（CCV高）、以及在少样本/不平衡数据下难以保证校准有效性等问题；同时，直接使用校准标签会破坏交换性，导致理论保证失效。

Method: 提出LATA（Laplacian-Assisted Transductive Adaptation）：一种训练与标签无关的改进方法，基于图像-图像k近邻图对零样本概率进行拉普拉斯平滑（经少量CCCP均场更新），并引入失败感知的共形分数融入ViLU框架，以提升预测集效率与类别平衡性；全程保持交换性，支持纯无标签或可选地利用校准边缘分布的变体。

Result: 在三个医学VLM和九个下游任务上，LATA持续减小预测集尺寸和CCV，精准匹配或收紧目标覆盖率，性能超越以往无监督/半监督共形方法，并逼近有标签方法，同时计算开销极低。

Conclusion: LATA是一种轻量、黑盒、保持理论保证的零样本不确定性校准新范式，有效缓解医学VLM在域偏移下的可靠性瓶颈，为临床部署提供更可信、更精细的预测集。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [43] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: 本文提出GraphThinker，一种基于强化微调的方法，通过构建事件级场景图（EVSG）并增强视觉定位来减少视频推理中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理模型缺乏对事件间因果关系的显式建模，导致推理时易产生幻觉；而人工标注因果关系成本高且隐含。

Method: 提出GraphThinker方法：1）利用多模态大语言模型（MLLM）构建显式建模事件内与事件间关系的事件级视频场景图（EVSG）；2）将EVSG作为中间推理过程融入MLLM；3）在强化微调中引入视觉注意力奖励以增强视觉接地能力。

Result: 在RexTime和VidHalluc两个数据集上验证，GraphThinker显著提升了对象与事件关系建模能力、事件定位精度，并有效减少了视频推理中的幻觉。

Conclusion: 显式结构化因果建模（如EVSG）结合强化微调与视觉注意力奖励，可有效缓解MLLM在视频推理中的幻觉问题，提升推理可靠性。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [44] [RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward](https://arxiv.org/abs/2602.17558)
*Qiucheng Wu,Jing Shi,Simon Jenni,Kushal Kafle,Tianyu Wang,Shiyu Chang,Handong Zhao*

Main category: cs.CV

TL;DR: 本文提出RetouchIQ框架，利用多模态大语言模型（MLLM）结合强化学习与通用奖励模型，实现基于指令的专业图像编辑，解决了创意编辑中主观性导致的奖励信号不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的图像编辑方法缺乏能反映创意编辑主观性的可靠、可验证奖励信号。

Method: 提出RetouchIQ框架：1）MLLM代理解析用户编辑意图并生成可执行调整；2）设计一个RL微调的通用奖励模型，通过多模态推理为每次编辑生成定制化评估指标并输出标量奖励；3）构建含19万指令-推理对的数据集和新基准。

Result: 在语义一致性和感知质量上显著优于先前的MLLM和扩散模型编辑系统。

Conclusion: 通用奖励驱动的MLLM代理可作为专业图像编辑中灵活、可解释且可执行的智能助手。

Abstract: Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.

</details>


### [45] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: 本文提出了ArtSound数据集和ArtToMus框架，首次实现直接从艺术作品图像生成音乐，无需经过文本中介，推动了视觉到音乐的跨模态生成研究。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的音乐生成方法存在两大局限：一是训练数据多为自然照片，难以捕捉艺术作品的语义、风格与文化内涵；二是大多依赖图像转文本步骤，以语言为语义中介，阻碍了真正的视觉到音频直接映射。

Method: 构建大规模艺术-音乐配对数据集ArtSound（105,884对），并提出ArtToMus框架：将艺术图像的视觉嵌入直接投影至潜在扩散模型的条件空间，实现无文本翻译、无语言监督的端到端图像到音乐生成。

Result: ArtToMus生成的音乐在音乐连贯性和风格一致性上表现良好，能反映原画作的关键视觉线索；虽绝对对齐指标低于文本条件方法，但在感知质量和跨模态对应性上具有竞争力。

Conclusion: 本工作确立了‘直接视觉到音乐生成’为一个独立且具挑战性的新研究方向，并开源数据与代码，支撑多媒体艺术、文化遗产及AI辅助创作等应用。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [46] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 本文提出了一种融合主动学习、在线元学习和概念引导推理的地理空间发现框架，通过概念相关性建模提升稀疏、偏差数据下的目标发现效率。


<details>
  <summary>Details</summary>
Motivation: 现实场景中（如环境监测、灾害响应、公共卫生）数据采集成本高、环境动态变化，且真实标注稀疏有偏，导致现有基于学习的方法（如强化学习）难以适用。

Method: 提出统一地理空间发现框架，包含两个创新：1）基于概念相关性的加权不确定性采样策略；2）相关性感知的元批次构建策略，以提升在线元学习中的语义多样性和泛化能力。

Result: 在真实PFAS污染数据集上验证了方法的有效性，证明其能在数据稀缺和环境动态变化下可靠发现目标。

Conclusion: 所提框架显著提升了动态、稀疏标注地理场景下的目标发现效率与鲁棒性，为资源受限的实际应用提供了新思路。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [47] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出CORAL框架，通过显式对齐查询-键匹配与外部对应关系，提升虚拟试穿中人物与服装的对应关系建模能力，从而改善全局形状迁移和局部细节保留效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在非配对设置下难以保持服装细节，且未显式建模人物-服装对应关系，尤其在扩散Transformer（DiT）中该对应关系的形成机制不明确。

Method: 首先分析DiT中全3D注意力机制，发现人物-服装对应依赖于精确的查询-键匹配；进而提出CORAL框架，包含对应蒸馏损失（对齐可靠匹配与注意力）和熵最小化损失（锐化注意力分布），并设计基于视觉语言模型的评估协议。

Result: CORAL在多个指标上持续超越基线，在全局形状迁移和局部细节保留方面均有提升；消融实验验证了各模块的有效性。

Conclusion: 显式建模并优化人物-服装在注意力机制中的对应关系，是提升虚拟试穿质量的关键路径；CORAL为DiT在细粒度图像生成任务中的可解释性与可控性提供了新思路。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [48] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: 本文提出了IntRec，一种基于用户反馈迭代优化目标检索结果的交互式框架，通过维护正向锚点和负向约束的双重记忆机制及对比对齐函数，在复杂场景中实现细粒度目标区分。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测器采用单次推理方式，无法根据用户反馈调整预测结果，难以应对模糊查询或多相似目标的检索挑战。

Method: 提出IntRec框架，核心是Intent State（IS），包含正向锚点与负向约束两个记忆集；引入对比对齐函数，通过最大化候选对象与正向线索的相似性、同时惩罚负向线索，实现精细化检索。

Result: 在LVIS数据集上达到35.4 AP，超越OVMR、CoDet和CAKE；在LVIS-Ambiguous上单轮反馈提升+7.9 AP，每轮交互延迟小于30ms。

Conclusion: IntRec通过轻量级交互机制显著提升开放词汇目标检索精度，无需额外监督，具备实际部署潜力。

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [49] [Human-level 3D shape perception emerges from multi-view learning](https://arxiv.org/abs/2602.17650)
*Tyler Bonnen,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文提出了一种新型多视角神经网络框架，通过在自然场景图像上学习视觉-空间信息（如相机位置、深度），无需物体先验即可实现与人类相当的3D形状推理能力，并在零样本下复现人类错误模式和反应时等精细行为特征。


<details>
  <summary>Details</summary>
Motivation: 人类能从2D图像推断3D结构，但传统计算模型长期无法达到人类水平，亟需一种更贴近人类感知机制的建模方法。

Method: 构建一类新型神经网络，在自然场景多视角图像上以视觉-空间目标（如预测相机位置、深度）进行自监督训练，不引入任何物体相关归纳偏置；采用零样本评估方式，在标准3D感知任务中对比模型与人类行为。

Result: 该框架首次在3D形状推理任务中达到与人类相当的准确率；独立读出模型响应可预测人类的错误分布和反应时间等细粒度行为指标。

Conclusion: 仅依赖自然视觉-空间数据和简单可扩展的学习目标，即可涌现出类人的3D感知能力，表明人类级3D理解可能源于基础的视觉-空间统计学习。

Abstract: Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.

</details>


### [50] [OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents](https://arxiv.org/abs/2602.17665)
*Akashah Shabbir,Muhammad Umer Sheikh,Muhammad Akhtar Munir,Hiyam Debary,Mustansar Fiaz,Muhammad Zaigham Zaheer,Paolo Fraccaro,Fahad Shahbaz Khan,Muhammad Haris Khan,Xiao Xiang Zhu,Salman Khan*

Main category: cs.CV

TL;DR: OpenEarthAgent 是一个面向遥感领域的多模态地理空间智能体框架，通过结构化推理轨迹和工具增强训练，提升了模型在卫星图像理解、地理分析和多步逻辑推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 将多模态推理能力扩展到遥感领域面临挑战，需处理空间尺度、地理结构和多光谱指数等复杂因素，并保持连贯的多步逻辑推理能力。

Method: 提出 OpenEarthAgent 框架，采用监督微调方式，在包含14,538个训练样本和1,169个评估样本的大规模结构化推理轨迹数据集上训练工具增强型地理空间智能体；数据涵盖城市、环境、灾害与基础设施等领域，并集成GIS操作及NDVI、NBR、NDBI等多光谱指数分析。

Result: 该智能体展现出结构化推理能力、稳定的地理空间理解能力以及可解释的工具驱动行为，在多个遥感分析任务中显著优于强基线模型，并媲美近期开源与闭源模型。

Conclusion: OpenEarthAgent 成功弥合了通用多模态推理与专业遥感分析之间的鸿沟，为构建可信赖、可解释、工具增强的地理空间智能体提供了统一范式。

Abstract: Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [51] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了一种参考引导的LLM评估器方法，用于在缺乏真值验证器的非可验证领域（如大语言模型对齐）中替代传统RLVR，通过利用前沿模型或人工撰写的参考输出提升LLM裁判能力，并将其用于自改进对齐训练，取得了优于SFT和无参考自改进的效果。


<details>
  <summary>Details</summary>
Motivation: RLVR虽在可验证推理任务中有效，但无法直接应用于缺乏真值验证器的非可验证领域（如LLM对齐），亟需一种软验证机制。

Method: 设计参考增强的LLM评估协议，利用前沿模型或人工参考输出提升不同能力LLM裁判的准确性；基于改进后的裁判构建参考引导的自改进对齐训练流程。

Result: 参考引导的自改进在AlpacaEval和Arena-Hard上显著超越SFT蒸馏（+20.2/+17.1）和无参考自改进（+5.3/+3.6），性能媲美ArmoRM等强微调奖励模型。

Conclusion: 参考引导的LLM评估器可作为有效的软验证器，在非可验证领域支撑高质量LLM后训练，为对齐提供新范式。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [52] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 本文针对希腊语问答任务，提出了一个新的数据集DemosQA、一个内存高效的LLM评估框架，并对11个单语和多语大语言模型在6个希腊语问答数据集上进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要面向高资源语言（如英语），多语模型存在数据偏差，难以准确反映低资源语言（如希腊语）的社会、文化和历史背景；单语模型虽被开发，但其在语言特定任务上的有效性尚缺乏系统研究。

Method: 构建了基于社交媒体用户提问与社区审核答案的希腊语问答数据集DemosQA；设计了一个内存高效、可适配多种问答数据集与语言的LLM评估框架；在6个人工标注的希腊语QA数据集上，采用3种提示策略对11个单语/多语LLM进行系统评估。

Result: 提供了首个聚焦希腊语社会文化语境的高质量QA数据集DemosQA；验证了所提评估框架的有效性与通用性；揭示了不同模型在希腊语QA任务上的性能差异及提示策略影响；所有代码与数据均已开源。

Conclusion: 单语LLM在希腊语QA任务中展现出潜力，但性能仍受模型架构、训练数据及提示方式显著影响；推动低资源语言专用模型发展需兼顾数据代表性与评估科学性。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [53] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于流模型（flow-based）的连续去噪语言模型（FLM），通过在one-hot token编码上进行欧氏空间去噪，结合时间重参数化提升训练稳定性和生成质量；进一步蒸馏为FMLM实现高质量少步（如一步）文本生成，性能超越现有离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型在少步生成时样本质量急剧下降，难以兑现其比自回归模型更快生成的潜力；本文旨在探索是否能用连续流模型替代离散扩散，在保持或提升生成质量的同时显著加速推理。

Method: 构建基于欧氏空间连续去噪的流式语言模型（FLM），对one-hot token编码建模；采用交叉熵损失预测干净数据，并引入时间重参数化提升训练稳定性；再将FLM蒸馏为其对应的流映射，得到可少步生成的FMLM。

Result: FLM在LM1B和OWT数据集上达到与SOTA离散扩散模型相当的生成质量；FMLM在少步（尤其一步）生成中全面超越近期方法，一步生成质量超过其他模型八步结果。

Conclusion: 离散扩散并非离散模态生成建模的必要选择；基于流的连续去噪语言模型在质量与速度上均可优于离散扩散，为大规模加速语言建模提供了新路径。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [54] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 本文提出了一种面向保险领域、本地部署的治理感知语言模型组件，通过LoRA微调预训练大模型，从非结构化的保修索赔文本中生成结构化纠正措施建议，并在多维评估框架下验证其优于通用商业大模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在通用语言任务上表现优异，但在监管严格、数据敏感的保险等领域应用受限，亟需兼顾性能、可解释性与合规性的定制化方案。

Method: 基于数百万历史保修索赔数据，采用LoRA技术对预训练LLM进行领域微调，构建本地部署的治理感知语言建模组件，聚焦于索赔处理流程中的初始决策模块；评估采用自动化语义相似度指标与人工评估相结合的多维框架。

Result: 领域微调模型在约80%的测试案例中生成的纠正措施与真实标注高度一致，显著优于通用商业LLM及提示工程方法。

Conclusion: 领域自适应微调能有效使模型输出分布贴近真实业务数据，在保险等受监管场景中具备成为可靠、可治理关键组件的潜力。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [55] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 本文提出了BanglaSummEval，一种无需参考摘要、基于问答的孟加拉语摘要事实一致性评估框架，利用多语言指令微调模型统一完成问题生成、回答、答案抽取与重要性加权，并结合BERTScore-Recall提升语义一致性判断，实验显示其与人工评估高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有事实一致性评估指标大多忽略孟加拉语等低资源语言，且严重依赖参考摘要，缺乏适用于高风险领域（如医疗、新闻）的可靠、可解释、参考无关的评估方法。

Method: 提出BanglaSummEval：基于源文档和摘要自动生成问题与答案，由单一多语言指令微调语言模型统一完成问答链各环节；采用BERTScore-Recall比较候选答案与生成答案以衡量语义一致性；提供逐级可解释的诊断输出。

Result: 在300个人工撰写的教育与医疗领域孟加拉语摘要上验证，与专家人工评分呈强相关（Pearson r=0.694，Spearman ρ=0.763），且具备低系统复杂度与高计算效率。

Conclusion: BanglaSummEval为低资源语言摘要的事实一致性评估提供了实用、透明、参考无关且可解释的新范式，兼具可靠性与可部署性。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [56] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文首次针对美因茨方言Meenzerisch开展NLP研究，构建了首个NLP就绪型数字词典（2351词条），并实验评估大语言模型在该方言定义生成与词汇生成任务上的表现，结果均极低（<10%），表明亟需更多资源与研究投入。


<details>
  <summary>Details</summary>
Motivation: Meenzerisch方言濒临消亡，而NLP有望助力其保存与复兴，但此前尚无针对该方言的NLP研究。

Method: 基于Schramm（1966）资料构建含2351个方言词及其标准德语释义的数字词典；设计两项任务——LLM生成释义与生成方言词，并测试零样本、少样本及规则引导等方法。

Result: LLM在释义生成任务中最高准确率仅6.27%，在方言词生成任务中仅1.51%；少样本和规则注入策略有所提升但仍低于10%。

Conclusion: 当前LLM难以有效处理Meenzerisch方言任务，凸显德国方言NLP资源匮乏，亟需加强数据建设与专项研究。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [57] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

TL;DR: 本文提出ConvApparel数据集和综合验证框架，旨在解决LLM用户模拟器存在的'现实性差距'问题，发现数据驱动的模拟器在反事实验证中表现更优。


<details>
  <summary>Details</summary>
Motivation: LLM用户模拟器存在'现实性差距'，导致系统在模拟环境中优化良好但在真实世界中表现不佳。

Method: 构建了包含双代理（'好'与'坏'推荐器）对话的ConvApparel数据集，并设计结合统计对齐、人类相似度评分和反事实验证的综合评估框架。

Result: 实验表明所有模拟器均存在显著现实性差距；但数据驱动的模拟器在反事实验证中优于提示式基线，更能适应未见过的用户行为。

Conclusion: 数据驱动的用户模拟器虽不完美，但比提示式方法更具鲁棒性，ConvApparel和所提框架为提升用户模拟器现实性提供了新路径。

Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


### [58] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本文研究了跨语言委婉语检测中的迁移学习问题，发现语义重叠不足以保证正向迁移，尤其在低资源的土耳其语到英语方向上，甚至可能出现性能下降；相反，在非重叠委婉语（NOPETs）上训练有时反而提升效果，这与标签分布差异密切相关。


<details>
  <summary>Details</summary>
Motivation: 委婉语高度依赖文化与语用语境，跨语言建模困难；现有跨语言迁移方法在委婉语检测任务上的有效性尚不明确，尤其在低资源语言对中。

Method: 将土耳其语和英语中的潜在委婉语（PETs）按功能、语用和语义对齐程度划分为重叠（OPETs）与非重叠（NOPETs）两类，并在多语言模型上进行迁移实验与类别级分析。

Result: 发现迁移存在不对称性：语义重叠不能确保正向迁移；土耳其语→英语方向性能可能下降，而NOPET训练反而提升效果；标签分布差异是关键解释因素；领域对齐可能影响迁移，但受限于数据稀疏性。

Conclusion: 跨语言委婉语检测中的迁移效果不仅取决于语义等价性，更受标签分布与领域对齐等实际训练动态影响，需重新审视‘等价即可用’的迁移假设。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [59] [Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry](https://arxiv.org/abs/2602.16959)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.CL

TL;DR: 本文提出了一种面向不确定性感知的计算框架，用于对古典波斯诗歌进行诗人层面的心理学分析，通过多标签自动标注、置信度加权聚合与Eigenmood嵌入等方法，在保持解释审慎性的同时实现可扩展、可审计的人文计算分析。


<details>
  <summary>Details</summary>
Motivation: 古典波斯诗歌以隐喻、互文与修辞迂回表达情感，使其需依赖细读，但难以进行大规模可复现比较；亟需兼顾人文阐释严谨性与计算可扩展性的新方法。

Method: 基于大规模自动多标签标注，为每行诗赋予心理概念、置信度分数与 abstention 标志；构建诗人×概念矩阵并用JS/ KL散度量化个体性；建立置信加权的概念共现图，通过拉普拉斯谱分解定义Eigenmood嵌入；引入abstention诊断与远距-近距联合分析流程。

Result: 在涵盖10位诗人的61,573行诗语料上，22.2%诗句被标记为abstention；验证了不确定性建模对分析稳健性的关键作用；实现了敏感性分析、选择偏差诊断与沿Eigenmood轴的诗句示例检索。

Conclusion: 该框架将不确定性从诗句级证据传播至诗人级推断，在支持规模化数字人文分析的同时，坚守诠释审慎原则，为文学计算提供了可审计、可解释的新范式。

Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.

</details>


### [60] [Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History](https://arxiv.org/abs/2602.17003)
*Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出了Persona2Web，首个面向真实开放网络的个性化Web智能体评测基准，基于'澄清-个性化'原则，利用用户历史隐式推断偏好以解决查询歧义问题，并设计了包含用户历史、歧义查询和推理感知评估框架的完整评测体系。


<details>
  <summary>Details</summary>
Motivation: 当前Web智能体缺乏个性化能力，难以从用户模糊查询中推断其偏好与上下文，亟需能基于历史行为进行隐式偏好建模的评测基准。

Method: 构建Persona2Web基准，包含三部分：(1) 长期隐式反映用户偏好的历史数据；(2) 需要推断隐式偏好的歧义查询；(3) 支持细粒度个性化评估的推理感知评测框架；并开展多维度消融实验。

Result: 通过在不同智能体架构、基座模型、历史访问方式及歧义程度查询上的系统实验，揭示了个性化Web智能体的关键行为挑战；代码与数据集已开源。

Conclusion: Persona2Web为个性化Web智能体提供了首个真实、可复现、可细粒度评估的基准，推动了从显式指令依赖向隐式历史驱动的个性化交互范式转变。

Abstract: Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.

</details>


### [61] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reasoning Inception（ReIn）的测试时干预方法，用于提升对话代理在面对用户引发错误时的错误恢复能力，无需修改模型参数或系统提示。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的对话代理虽在固定任务数据集上表现良好，但在面对用户引发的未预期错误时仍很脆弱；本文聚焦于错误恢复而非预防，并在无法微调模型或修改提示的实际约束下探索适应性方案。

Method: 提出ReIn方法：通过外部‘起始模块’识别对话上下文中的预定义错误并生成恢复计划，再将该计划注入代理的内部推理过程，全程不修改模型参数或系统提示。

Result: ReIn在模拟多种用户引发错误（如模糊请求、不支持请求）场景下显著提升任务成功率，泛化至未见错误类型，并持续优于显式提示修改方法。

Conclusion: ReIn是一种高效、即插即用的错误恢复机制；联合定义恢复工具与ReIn可安全有效地增强对话代理鲁棒性，且无需改动骨干模型或系统提示。

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [62] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本文提出了一种新的ToM评估任务，要求代理通过策略性披露信息来说服目标选择某项政策，发现LLMs在需推理他人心理状态时表现差，但在实际说服人类时反而优于人类，表明其说服能力可能不依赖于真正的ToM。


<details>
  <summary>Details</summary>
Motivation: 现有ToM评估多依赖静态问答，忽视了第一人称互动的重要性；理论指出ToM本质涉及对他人知识与动机状态的动态建模，需通过互动体现。

Method: 设计一个三阶段政策说服任务： persuader需根据target的知识状态（是否知晓政策细节）和动机状态（偏好何种结果）来选择披露信息；设置Revealed（状态已知）与Hidden（需主动询问或推断）两种条件；开展三项实验：Exp1（说服理性bot）、Exp2（说服真人扮演bot）、Exp3（测量真人信念真实变化）。

Result: Exp1中LLMs在Revealed条件下优秀，但Hidden条件下表现低于随机水平；人类在两种条件下均中等表现；Exp2与Exp3中LLMs全面超越人类说服者。

Conclusion: LLMs的高说服力未必源于ToM能力，而可能来自统计性修辞策略；不能将LLM在说服任务中的成功简单等同于具备人类式ToM；同时警示其潜在社会影响。

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [63] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: This paper compares four cross-lingual text classification approaches—language-specific models with translated annotations, English-only models on translated data, direct use of English-fine-tuned multilingual transformers, and a hybrid method—to filter hydrogen-related tweets from noisy multilingual (English, Japanese, Hindi, Korean) social media data, followed by topic modeling to uncover dominant themes.


<details>
  <summary>Details</summary>
Motivation: Analysing large-scale, multilingual social media discourse—especially public debates across diverse languages—remains challenging; keyword-based collection introduces substantial noise, requiring robust cross-lingual filtering for reliable analysis.

Method: Four cross-lingual classification strategies are evaluated on a decade-long, nine-million-tweet dataset in English, Japanese, Hindi, and Korean: (1) language-specific models trained on translated English annotations; (2) single English model applied to machine-translated non-English tweets; (3) zero-shot inference using English-fine-tuned multilingual transformers (e.g., XLM-R); (4) hybrid approach combining translated annotations and multilingual fine-tuning. Filtered subsets undergo topic modeling (e.g., BERTopic or LDA) to extract dominant themes.

Result: Each method shows distinct performance trade-offs in precision, recall, and scalability; hybrid and multilingual transformer approaches generally outperform pure translation-based ones in handling linguistic diversity and noise, while translation-based methods offer better interpretability and control in low-resource settings.

Conclusion: No single approach universally dominates; optimal cross-lingual pipeline design depends on data scale, resource availability, and downstream needs—hybrid strategies offer promising balance for real-world multilingual social media analysis.

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [64] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: 本文介绍了ALPS，一个原生的、专家策划的阿拉伯语语言学与语用学诊断挑战集，旨在深入评估深度语义和语用能力；通过531个精心设计的问题，在15项任务和47个子任务中检验模型；研究发现模型在流利性上表现良好，但在形态句法依赖关系上存在显著缺陷，尤其在依赖变音符号的任务中错误率高达36.5%；顶级商业模型（如Gemini-3-flash）超越平均人类表现，但阿拉伯语专用模型（如Jais-2-70B）仍略逊于人类。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语NLP基准多依赖合成或翻译数据，缺乏深层语言学验证，难以评估模型对阿拉伯语深层语义与语用的理解能力。

Method: 构建原生、专家策划的诊断性挑战集ALPS，涵盖531个问题、15项任务、47个子任务，强调文化真实性和无翻译偏差；对23种模型（含商业、开源及阿拉伯语专用模型）进行评估，并以单次作答人类表现（84.6%准确率）和专家裁定的最优基准（99.2%）为参照。

Result: 模型整体表现出高流利性，但在形态句法依赖任务（尤其是依赖变音符号的任务）上错误率达36.5%，显著高于组合语义任务；Gemini-3-flash达94.2%，超过平均人类；最佳阿拉伯语专用模型Jais-2-70B为83.6%，接近但未达人类水平。

Conclusion: 当前大模型在阿拉伯语深层语言理解上仍存在关键短板，特别是形态句法结构处理；ALPS为弥补大规模基准的深度不足提供了重要补充，凸显了原生、专家驱动的数据构建对评估真实语言能力的必要性。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [65] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 本文提出BankMathBench，一个面向银行业务场景的多层级数值推理基准数据集，用于提升大语言模型在存款、贷款等核心银行计算任务中的准确性；实验表明，基于该数据集微调的开源LLM在公式生成与数值推理上显著提升，尤其在工具增强微调下各难度级别准确率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在银行核心计算任务（如本息估算、多产品利率比较、提前还款计息）中准确率低，且缺乏反映真实银行业务场景的评测基准。

Method: 构建BankMathBench领域专用数据集，按难度分为基础（单产品）、中级（多产品比较）、高级（多条件）三层，并采用工具增强微调方法训练开源LLM。

Result: 工具增强微调后，模型在基础、中级、高级任务上的平均准确率分别提升57.6、75.1和62.9个百分点，显著优于零样本基线。

Conclusion: BankMathBench是评估和提升大语言模型在真实银行场景中数值推理能力的有效且可靠的基准。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [66] [Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests](https://arxiv.org/abs/2602.17108)
*Anton Dzega,Aviad Elyashar,Ortal Slobodin,Odeya Cohen,Rami Puzis*

Main category: cs.CL

TL;DR: 本研究利用主题统觉测验（TAT）图像和SCORS-G量表，评估大模型（LMMs）在非语言模态下展现的类人格特质，发现其在理解人际动态与自我概念方面表现良好，但在感知与调节攻击性上存在系统性缺陷，且性能随模型规模与更新程度提升而增强。


<details>
  <summary>Details</summary>
Motivation: 探索大型多模态模型（LMMs）是否具备可被心理测量学方法评估的类人格特质，尤其聚焦于非语言模态（如图像-叙事）所揭示的认知表征与情感关系维度。

Method: 采用TAT图像作为刺激，让LMMs以被试模型（SMs）身份生成故事，并由另一组LMMs作为评价模型（EMs）依据SCORS-G量表对故事进行评分；对比EMs与人类专家评分的一致性，并分析不同模型家族在各SCORS-G维度上的表现差异。

Result: EMs对TAT叙事的理解与分析能力优异，评分高度吻合人类专家；所有模型均较好掌握人际动态与自我概念，但普遍无法感知和调节攻击性；模型性能随参数量增大与发布时间靠后而系统性提升。

Conclusion: LMMs展现出部分可量化、具心理结构意义的类人格功能，但存在关键情感调节缺陷（尤其是攻击性），提示当前模型在深层情感建模与道德推理方面仍不完善；SCORS-G等临床心理学工具可用于客观评估AI人格样特性。

Abstract: Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the Social Cognition and Object Relations Scale - Global (SCORS-G). LMMs are employed in two distinct roles: as subject models (SMs), which generate stories in response to TAT images, and as evaluator models (EMs), who assess these narratives using the SCORS-G framework. Evaluators demonstrated an excellent ability to understand and analyze TAT responses. Their interpretations are highly consistent with those of human experts. Assessment results highlight that all models understand interpersonal dynamics very well and have a good grasp of the concept of self. However, they consistently fail to perceive and regulate aggression. Performance varied systematically across model families, with larger and more recent models consistently outperforming smaller and earlier ones across SCORS-G dimensions.

</details>


### [67] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 本文提出一种基于心理测量学的新型审计框架，用于量化大型语言模型中不依赖真实标签的稳定行为倾向（如优化偏差、谄媚倾向等），揭示了在多层AI架构中潜在偏见可能形成递归意识形态回声室的风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从独立聊天界面转变为多智能体系统和递归评估循环中的基础推理层，检测持久的、服务商级别的行为特征成为安全与治理的关键需求；传统基准测试无法捕捉训练和对齐过程中嵌入的稳定潜在响应策略。

Method: 利用心理测量学中的潜在特质估计理论，在序数不确定性下进行量化；采用语义正交诱饵掩盖的强制选择序数情境，并通过密码学置换不变性保障；使用混合线性模型（MixedLM）和组内相关系数（ICC）分析九个主流模型。

Result: 发现项目层面的表述引发高方差，但存在显著的‘实验室信号’导致行为聚类；表明在‘锁定’的服务商生态系统中，潜在偏见是会累积的变量，而非静态错误。

Conclusion: 潜在偏见在多层AI架构中可能引发递归意识形态回声室，需通过新型审计框架进行持续监测与治理。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [68] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 本文研究罗马尼亚语文本远程医疗中的患者满意度信号，利用77,334对患者提问-医生回复数据，构建二分类模型预测患者点赞反馈，并通过SHAP分析发现历史特征主导预测，而礼貌性、模糊表达等文本特征虽影响较小但具可操作性。


<details>
  <summary>Details</summary>
Motivation: 文本远程医疗日益普及，患者评价（如点赞）成为重要考核指标，但这些评价更多反映沟通质量而非临床准确性；需识别影响患者满意度的关键可干预语言特征。

Method: 基于77,334条匿名医患对话，构建二分类任务（ thumbs-up vs. 其他），提取语言无关特征（长度、结构、可读性）、罗马尼亚语LIWC心理语言学特征及礼貌/模糊标记；采用时间划分训练分类器，并用SHAP进行归因分析和子组相关性检验。

Result: 患者与医生历史特征是预测满意度最强的变量（强先验），而响应文本特征（如礼貌性、hedging）贡献较小但具可操作性；子组分析显示礼貌与模糊表达始终正向关联满意度，词汇多样性则呈负相关。

Conclusion: 提升文本远程医疗患者满意度的关键不仅在于历史声誉，更可通过优化医生回复中的礼貌策略与模糊表达等可控语言特征来实现，词汇多样性过高反而可能降低满意度。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [69] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 本文提出一种心理测量框架，用于量化和缓解大语言模型（LLM）在自报告式问卷评估中因追求社会赞许性（SDR）而产生的偏差；通过对比诚实作答与刻意讨好作答下的IRT潜变量得分来量化SDR，并设计了基于偏好匹配的分级强迫选择（GFC）量表以缓解该偏差，在九种指令微调LLM上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类自报告问卷的LLM评估方法忽视了LLM倾向于给出社会期望答案（SDR）的问题，导致评估结果偏差，亟需可量化、可缓解的SDR分析框架。

Method: 1）采用诚实 vs. 讨好指令双条件设计，结合项目反应理论（IRT）估计潜变量得分，计算方向校正的标准化效应量以量化SDR；2）构建30对跨领域、吸引力匹配的分级强迫选择（GFC）大五人格量表，通过约束优化实现。

Result: 在九种指令微调LLM及已知目标人格的合成角色上，Likert量表显示显著SDR，而GFC量表大幅削弱SDR，同时较好恢复真实人格轮廓；揭示了SDR抑制与人格恢复之间的模型依赖权衡。

Conclusion: SDR是问卷式LLM评估中不可忽视的系统性偏差，应引入SDR-aware报告规范；所提心理测量框架为LLM审计与基准测试提供了更稳健的方法基础。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [70] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出X-Value——一个跨语言价值观评估基准，用于评测大语言模型在多语言环境下对数字内容深层价值观的理解与判断能力；实验表明当前SOTA模型在此任务上表现不足且存在显著语言偏差。


<details>
  <summary>Details</summary>
Motivation: 现有内容安全评估范式主要关注显性危害（如暴力、仇恨言论），忽视了数字内容中更微妙的价值维度，尤其缺乏跨语言、全球视角的价值观评估能力。

Method: 构建X-Value基准：包含18种语言、5000+问答对，覆盖Schwartz基本人类价值观理论的7个核心领域，并按难易分级；提出两阶段标注框架——先区分议题属全球共识还是价值多元范畴，再进行多方协同的价值隐含性评估。

Result: 在X-Value上系统评测显示，当前SOTA大语言模型跨语言价值观评估准确率低于77%，不同语言间性能差异超20%。

Conclusion: 凸显大语言模型在细粒度、价值观感知型内容评估能力上的严重不足，亟需提升其跨语言、价值敏感的内容理解与判断能力。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [71] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文分析了Transformer架构在神经机器翻译中因标准next-token预测训练策略导致的表征坍塌问题，特别是在深层和连续输出模型中更为显著；提出采用基于角度分散的正则化方法缓解该问题，并验证其在离散/连续及量化模型中均能提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 标准的next-token预测训练策略可能导致表征坍塌，尤其在Transformer深层和连续输出NMT中更严重，甚至诱发全零或恒定向量的平凡解。

Method: 分析不同层级（离散/连续）NMT Transformer在训练过程中的表征坍塌动态，并引入基于角度分散（angular dispersion）的现有正则化方法进行干预。

Result: 该正则化方法不仅能有效缓解表征坍塌，还能提升翻译质量；且在量化模型中仍保持相同改善效果。

Conclusion: 表征坍塌是NMT训练中的关键隐患，角度分散正则化是一种通用、鲁棒且有效的缓解手段，适用于多种模型变体（包括量化模型）。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [72] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 本文研究了词法和句法扰动对23个大型语言模型在MMLU、SQuAD和AMEGA三个基准上的性能影响，发现词法扰动普遍导致显著性能下降，而句法扰动效果不一；模型鲁棒性与规模无一致关联，表明LLM更依赖表层词汇模式而非抽象语言能力，呼吁将鲁棒性测试纳入标准评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准因对输入提示的浅层变化敏感而可靠性受质疑，需探究模型在保持语义不变前提下的扰动鲁棒性。

Method: 采用两种语言学驱动的管道生成语义保持的扰动：一是同义词替换实现词法变化，二是基于依存句法分析实现句法变换；在MMLU、SQuAD和AMEGA上评估23个LLM在扰动下的绝对性能与相对排名变化。

Result: 词法扰动普遍引发显著性能下降；句法扰动效果异质，偶有提升；两类扰动均削弱复杂任务排行榜稳定性；模型鲁棒性不随参数量单调提升，且高度依赖具体任务。

Conclusion: LLM更依赖表面词汇线索而非深层语言理解，当前基准易受扰动干扰，鲁棒性测试应成为LLM评估的标准环节。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [73] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: 本文提出RPDR框架，通过合成数据生成、Round-Trip预测筛选易学样本及针对性训练，显著提升密集检索器在长尾问答任务中的性能，并引入动态路由机制进一步优化。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和密集检索器在长尾问题回答中难以获取和准确回忆罕见知识，亟需提升其对稀有或小众知识的泛化能力。

Method: 提出RPDR数据增强框架，包含三部分：合成数据生成、基于Round-Trip预测的数据选择以识别易学样本、使用这些样本训练密集检索器；并设计动态路由机制将查询分发至专用检索模块。

Result: 在PopQA和EntityQuestion两个长尾检索基准上，RPDR显著优于BM25和Contriver等基线方法，尤其在极长尾类别上效果突出；人工分析验证了其优势与局限。

Conclusion: RPDR有效缓解密集检索器在长尾场景下的泛化瓶颈，结合动态路由可进一步提升检索性能，为长尾知识检索提供了新思路。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [74] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 本文研究了在多项选择题（MCQ）中，利用认知可用性（即选项在大脑中浮现的容易程度）作为猜测策略的有效性。作者通过大规模语料库（如Wikipedia）量化选项的概念流行度来操作化‘可用性’，发现正确答案普遍比干扰项更具可用性；始终选择最可用选项可显著超越随机猜测基线（+13.5%–32.9%）。该模式在LLM生成题与专家出题中均成立，提示可用性是建模学生作答行为的重要认知因素。


<details>
  <summary>Details</summary>
Motivation: 当学生不确定MCQ正确答案时往往靠猜测；经典启发式理论（如可用性启发式）暗示‘最先想到的选项’可能更可能是正确答案，但缺乏计算验证。本文旨在用计算方法检验这一认知策略在真实题目中的有效性，并探讨其对建模学生行为的意义。

Method: 提出一种基于大规模语料库（如Wikipedia）中概念词频/共现频率的计算方法，量化各MCQ选项的认知可用性；在三套大型题库上评估‘始终选最可用选项’策略的得分表现，并对比LLM生成题与专家出题中可用性模式的一致性。

Result: 正确答案在所有题库中显著比错误选项更具可用性；该策略使得分比随机猜测高出13.5%–32.9%；LLM生成的干扰项也呈现与专家题相似的可用性分布模式。

Conclusion: 可用性启发式在MCQ猜测中具有实际有效性，应被纳入对学生答题行为的计算建模中；该发现也揭示了语言模型训练数据与人类认知可用性之间潜在的耦合关系。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [75] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文提出了一种改进的跨文档共指消解（CDCR）标注方案，将共指链视为话语元素（DEs），支持同一性与近同一性关系，以更好捕捉新闻报道中词汇多样性与话语框架差异，并在NewsWCL50和ECB+子集上完成重标注与评估。


<details>
  <summary>Details</summary>
Motivation: 现有CDCR数据集主要聚焦事件共指，且共指定义狭窄，难以应对多样化、立场分化新闻中广泛存在的措辞差异与话语 framing 变化。

Method: 提出以话语元素（DEs）为基本分析单元的CDCR标注新范式，支持identity与near-identity关系；统一代码本对NewsWCL50和ECB+子集进行重标注；采用词汇多样性指标与same-head-lemma基线模型评估。

Result: 重标注后的数据集在词汇多样性等指标上表现居于原ECB+与原NewsWCL50之间，两个新数据集间一致性高，验证了方案的合理性与可迁移性。

Conclusion: 该修订方案提升了CDCR对新闻话语复杂性的建模能力，为构建更平衡、更具话语意识的新闻领域CDCR研究提供了可靠数据基础与方法论支撑。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [76] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文比较了BLEU和ChrF++两种机器翻译评估指标在极低资源语言（ELRL）场景下的表现，发现尽管BLEU得分较低，但其能提供互补的词汇精度信息，增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 在极低资源语言（ELRL）场景下，传统指标如BLEU常无法准确反映翻译质量，需探究更适合的评估方法。

Method: 对BLEU（基于n-gram）和ChrF++（基于字符）两种指标在三种ELRL（Magahi、Bhojpuri、Chhattisgarhi）上进行对比分析，考察其对幻觉、重复、源文本复制及变音符号（matra）变化等翻译缺陷的响应能力，涵盖LLM与NMT输出。

Result: ChrF++被广泛采用，但BLEU虽绝对分值较低，却能提供有价值的词汇级精度信息，与ChrF++形成互补，提升评估可解释性。

Conclusion: 在ELRL MT评估中，不应弃用BLEU，而应结合ChrF++使用，以兼顾字符级鲁棒性与词汇级精确性。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [77] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 本文提出了一种面向长文本生成的细粒度不确定性量化（UQ）框架，通过响应分解、单元级打分和响应级聚合三阶段分类法，形式化了多种基于一致性的黑盒评分器，并在多个模型和数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法主要针对短文本输出，难以泛化到长文本生成中的幻觉检测。

Method: 构建了长文本LLM输出的细粒度不确定性量化分类法，涵盖响应分解、单元级评分和响应级聚合三个阶段；形式化了多类基于一致性的黑盒评分器，并进行了推广与扩展。

Result: 实验表明：1）主张-响应蕴含关系打分效果优于或不逊于更复杂的主张级打分；2）主张级打分通常优于句子级打分；3）不确定性感知解码能显著提升长文本生成的事实性。

Conclusion: 该框架厘清了已有方法间的关系，支持公平比较，并为细粒度UQ组件选择提供了实用指导。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [78] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 本文提出AIDG框架，通过两个任务评估大语言模型在信息提取与信息保持之间的能力不对称性，发现模型在信息保持上显著优于信息提取，并分析了信息动态和约束遵循两大瓶颈。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的战略推理能力需要从静态基准转向动态多轮交互，尤其关注信息提取与信息保持之间的不对称性。

Method: 提出AIDG（对抗性信息推理游戏）框架，包含AIDG-I（社交推理中的实用策略）和AIDG-II（结构化‘20个问题’中的约束满足）两个任务，在439局游戏中测试6个前沿大语言模型。

Result: 模型在信息保持（防御）上明显强于信息提取（进攻），ELO优势达350分（Cohen's d = 5.47）；确认策略比盲目推理有效7.75倍；41.3%的推理失败源于对话负载下的指令遵循退化。

Conclusion: 大语言模型擅长局部防御一致性，但在需全局状态跟踪的战略性探究任务中表现薄弱。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [79] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 本文提出了一种减少标签位置偏差的MCQ评估协议，通过统一无序标签和全答案匹配提升LLM评估鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 发现不同LLM在MCQ基准测试中存在标签位置/少样本提示偏差，影响对其真实推理能力的准确评估。

Method: 设计NonsenseQA合成基准检测偏差；提出新评估协议：用统一无序标签替代原标签，要求模型基于完整答案内容作答，并用简单句子相似度模型匹配预测与真实答案。

Result: 在多个基准和模型上，新协议使答案排列鲁棒性显著提升，平均准确率方差降低3倍，性能仅轻微下降；消融实验验证其优于标准方法。

Conclusion: 该偏差抑制协议能更真实地揭示LLM的知识推理能力，减少评估过程中的干扰因素，为MCQ评测提供更可靠的标准。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [80] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种基于熵的无监督数据选择框架（EUDS），旨在降低大语言模型细调时的数据和计算资源需求，尤其适用于计算受限场景。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型细调需要大量计算和数据资源，而现有数据选择方法往往依赖高计算预算，难以在实际资源受限场景中应用；同时，大模型虽能缓解数据稀缺，但评估数据可用性仍具挑战。

Method: 提出基于熵的无监督数据选择（EUDS）框架，建立计算高效的数据过滤机制，无需标注或额外模型，利用不确定性估计筛选高信息量样本。

Result: 在情感分析、主题分类和问答任务上的实验表明，EUDS显著减少计算开销与训练时间，同时用更少数据保持甚至提升性能。

Conclusion: EUDS为计算受限场景下的高效语言模型细调提供了创新且实用的解决方案，平衡了数据效率与计算效率。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [81] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文介绍了PEACE 2.0工具，它不仅能分析和解释某条消息为何被判定为仇恨言论（或非仇恨言论），还能基于检索增强生成（RAG）技术自动生成有事实依据的反仇恨言论回复，并探索反言论的特征。


<details>
  <summary>Details</summary>
Motivation: 在线平台仇恨言论激增带来严重社会问题；尽管已有有效检测方法，但如何生成恰当、有依据的反仇恨言论（counter-speech）仍是开放挑战。

Method: 提出PEACE 2.0工具，采用检索增强生成（RAG）流水线，实现：i) 将仇恨言论解释锚定于证据与事实；ii) 自动生成证据支撑的反言论；iii) 分析反言论的特征。

Result: PEACE 2.0能对显性和隐性仇恨言论进行深度分析与响应生成，提升解释可信度与反言论质量。

Conclusion: PEACE 2.0通过融合可解释性分析与证据驱动的反言论生成，为仇恨言论治理提供了更全面、可靠的技术支持。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [82] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本文研究了孟加拉语与英语之间的跨语言情感对齐问题，发现现有对齐范式在低资源语言中存在严重的情感误判、不对称共情和现代偏见等问题，主张采用多元文化、尊重语言多样性的对齐方法，并提出引入'情感稳定性'指标来评估对齐质量。


<details>
  <summary>Details</summary>
Motivation: 双向对齐的核心是确保AI准确理解人类意图且人类能信任AI行为，但在语言障碍下这一闭环严重断裂；本文聚焦孟加拉语-英语跨语言情感对齐这一被忽视的低资源语言场景，揭示当前对齐范式在文化与语言多样性下的失效风险。

Method: 通过基准测试四种Transformer模型（含mDistilBERT和IndicBERT），量化分析其在孟加拉语不同变体（如正式体Sadhu）上的情感极性识别性能，定义并测量'情感反转率'、'不对称共情'及'现代偏见'等新型对齐失效现象。

Result: 发现mDistilBERT情感反转率达28.7%；IndicBERT在正式孟加拉语中对齐错误率激增57%；不同模型对孟加拉语情感强度存在系统性压制或放大（即'不对称共情'）。

Conclusion: 通用压缩式对齐范式无法保持情感保真度，威胁人-AI互信；应转向植根于语言文化多样性的多元对齐路径，并在基准中引入'情感稳定性'指标以约束低资源与方言场景下的极性反转。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [83] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本文探讨了约10亿参数的小型大语言模型（LLMs）在20项临床NLP任务中的有效性，发现经微调的小型模型（如Qwen3-1.7B）性能可超越大型基线模型（如Qwen3-32B），并开源了多个意大利语医疗数据集及预训练语料。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗NLP任务中表现优异，但其高计算开销限制了实际医疗场景部署；本文旨在验证小型LLM能否在保持高准确率的同时满足资源受限环境的需求。

Method: 在20个临床NLP任务上系统评估Llama-3、Gemma-3和Qwen3三类约1B参数的小型LLM，并对比多种适配策略：推理时的少样本提示与约束解码，训练时的监督微调与持续预训练。

Result: 微调是最有效的适配方法；Qwen3-1.7B经优化后平均得分比Qwen3-32B高出9.2分；同时开源了多个公开意大利语医疗NLP数据集、126M词急诊科语料及175M词多源预训练语料。

Conclusion: 小型LLM经适当适配可在多项医疗NLP任务中达到甚至超越大型模型性能，具备临床落地潜力；开源资源将推动低资源语言和轻量化医疗AI研究。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [84] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本文提出了一个针对产科临床文本的新数据集，并系统评估了基于Transformer的监督模型和零样本大语言模型在临床文本分段任务上的表现，发现零样本模型在跨领域适应性上具有优势，但需修正幻觉生成的标题。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本分段方法多在MIMIC-III等通用医疗语料上训练，缺乏对产科等特定领域的覆盖；同时，零样本大模型在该任务中的潜力尚未被系统评估。

Method: 1）构建去标识化的产科笔记标注数据集；2）在MIMIC-III子集（领域内）和新产科数据集（领域外）上评估多种Transformer监督模型；3）首次对比监督模型与零样本大语言模型在医学文本分段任务中的性能。

Result: 监督模型在领域内表现优异，但在领域外性能显著下降；零样本模型经幻觉标题校正后展现出强跨领域鲁棒性。

Conclusion: 应加强建设领域特异性临床NLP资源；零样本分段是拓展医疗NLP应用范围的可行路径，前提是有效管理大模型幻觉问题。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [85] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLM）自动为编程任务中的细粒度知识组件（KC）标注正确性的框架，结合时序感知的Code-KC映射机制，提升了学习曲线拟合度与预测性能，并获得与专家标注高度一致的人工评估结果。


<details>
  <summary>Details</summary>
Motivation: 真实编程数据集中缺乏KC层级的正确性标签，简单地将题目级正确性传播至所有KC会掩盖部分掌握状态，导致学习曲线拟合不佳。

Method: 利用大语言模型直接从学生代码中判断各KC是否被正确应用，并引入时序上下文感知的Code-KC映射机制以提升KC与学生代码的对齐精度。

Result: 相比基线方法，所生成的KC级标签使学习曲线更符合认知理论（如练习幂律），并提升了Additive Factors Model等模型的预测性能；人工评估显示LLM标注与专家标注具有高度一致性。

Conclusion: 该基于LLM的自动化KC标注框架有效缓解了细粒度技能建模中标签缺失问题，为学生建模与学习分析提供了更可靠、可扩展的KC级数据基础。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [86] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 本文提出了一种自适应正则化训练框架，通过在微调过程中动态调整对安全风险的约束强度，使指令遵循语言模型在保持实用性的同时持续对齐安全目标。


<details>
  <summary>Details</summary>
Motivation: 现有指令遵循模型在微调（尤其是对抗性微调）后安全行为易退化，而传统防御方法常以牺牲实用性为代价或保护能力有限。

Method: 设计两种安全风险估计方法：1）基于裁判模型（Safety Critic）对训练批次打高阶危害分；2）基于轻量级分类器，从模型前向激活中预测有害意图；二者输出的风险信号用于动态调节正则化强度——高风险更新被约束靠近安全参考策略，低风险更新按常规进行。

Result: 实验证明：有害意图可从生成前的激活中可靠预测；裁判评分具备高召回安全指导能力；在多种模型与攻击场景下，该方法显著降低攻击成功率、保持下游任务性能、且无推理开销。

Conclusion: 自适应正则化是一种兼顾安全性与实用性的原则性机制，可在整个微调过程中持续保障模型对齐。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [87] [Modeling Distinct Human Interaction in Web Agents](https://arxiv.org/abs/2602.17588)
*Faria Huq,Zora Zhiruo Wang,Zhanqiu Guo,Venu Arvind Arangarajan,Tianyue Ou,Frank Xu,Shuyan Zhou,Graham Neubig,Jeffrey P. Bigham*

Main category: cs.CL

TL;DR: 本文提出了建模人类干预以支持协作式网页任务执行的新任务，构建了包含400条真实用户网页导航轨迹的CowCorpus数据集，识别出四类人机交互模式，并训练语言模型预测干预时机，显著提升干预预测准确率与用户评价的代理有用性。


<details>
  <summary>Details</summary>
Motivation: 当前自主网页代理系统缺乏对人类何时、为何干预的原理性理解，常在关键决策点过度自治或频繁请求不必要确认，亟需建模人类干预以实现更自然的人机协作。

Method: 构建CowCorpus数据集（400条真实用户轨迹，4200+人机交错动作），归纳四类交互模式（放手监督、介入监督、协同求解、完全接管），并基于此训练语言模型预测干预时机。

Result: 干预预测准确率较基线语言模型提升61.4–63.4%；部署于实际网页导航代理后，用户评定的代理有用性提升26.5%。

Conclusion: 结构化建模人类干预能显著提升网页代理的适应性与协作能力，为构建以人为中心的自主代理提供了新范式。

Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

</details>


### [88] [The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?](https://arxiv.org/abs/2602.17598)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 本文通过实验验证当前语音大语言模型（Speech LLMs）在多数任务中实质上等价于Whisper转录后接文本LLM的级联结构，仅Qwen2-Audio表现出真正差异；其性能优势在噪声环境下消失甚至反转。


<details>
  <summary>Details</summary>
Motivation: 探究当前语音大语言模型是否真正具备端到端语音理解能力，还是仅隐式执行ASR（自动语音识别），从而厘清其内在机制与实际价值。

Method: 采用匹配骨干网络（matched-backbone）测试法，在四个语音LLM和六个任务上进行对比；使用logit lens分析隐藏状态中的文本表征；通过LEACE概念擦除检验文本表征的因果必要性；并在不同信噪比下评估鲁棒性。

Result: Ultravox与对应级联系统统计不可区分（κ=0.93）；logit lens显示文字信息直接出现在隐层；LEACE擦除使准确率趋近于零；Qwen2-Audio显著偏离级联行为；在0 dB噪声下，语音LLM相较级联的干净条件优势最多逆转7.6%。

Conclusion: 当前主流语音LLM本质上是昂贵且噪声鲁棒性更差的隐式ASR+LLM级联；端到端语音理解尚未普遍实现，架构设计（如Qwen2-Audio）可能突破该局限。

Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.

</details>


### [89] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: 本文提出了DivanBench，一个专注于波斯语文化迷信与习俗的诊断性基准测试，揭示了当前波斯语大语言模型在文化推理任务中的三大缺陷：顺从偏差严重、连续预训练加剧该偏差、事实检索与情境应用间存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准虽已拓展至语用学与礼貌领域，但未能区分对文化事实的记忆与对隐含社会规范的推理能力。

Method: 构建包含315个问题的DivanBench基准，涵盖事实检索、成对情景验证和情境推理三类任务，并评估7个波斯语大语言模型的表现。

Result: 发现模型普遍存在严重顺从偏差；连续波斯语预训练反而加剧该偏差并削弱矛盾识别能力；所有模型在事实检索与情境应用任务间存在21%性能差距。

Conclusion: 文化能力不能仅靠扩大单语数据规模获得，当前模型仅模仿文化模式而未内化其底层认知图式。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [90] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 本文研究语言模型在合成语料上训练后是否能再现人类语言中差异性论元标记（DAM）的类型学规律，发现模型能复现“标记非典型语义论元”的自然标记方向偏好，但无法复现人类语言中强烈的宾语标记偏好，表明不同类型学倾向可能源于不同机制。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在合成语料上能否再现人类语言中差异性论元标记（DAM）的跨语言规律，以理解这些规律的来源。

Method: 采用受控合成学习方法，在18种实现不同DAM系统的合成语料上训练GPT-2模型，并用最小对立对评估其泛化能力。

Result: 模型稳定表现出人类般的‘自然标记方向’偏好（即显性标记倾向于语义非典型论元），但未复现人类语言中强烈的宾语标记偏好。

Conclusion: DAM的两类类型学倾向可能源自不同认知或学习机制，提示语言共性未必单一归因于语法或统计因素。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [91] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文提出了UniLID，一种基于UnigramLM分词算法的语言识别方法，在低资源和相近语言场景下显著提升了性能，尤其在仅需每语言5个标注样本时准确率超70%，且支持增量学习与现有语言模型流程无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别系统在低资源语言和密切相关语言场景下表现脆弱，而高资源语言上已接近完美，亟需更鲁棒、高效、可扩展的方法。

Method: 提出UniLID方法，基于UnigramLM的分词框架，学习语言条件下的共享词表单字分布，但将分词视为语言特异性过程，支持无需重训的增量语言添加。

Result: 在标准基准上媲美fastText、GlotLID和CLD3；低资源场景下样本效率大幅提升（5样本/语言达70%+准确率）；细粒度方言识别性能显著提升。

Conclusion: UniLID是一种简单、高效、可扩展的语言识别新方法，兼顾性能、数据效率与工程实用性，特别适用于多语言大模型生态中的实际部署需求。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [92] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文提出了一种针对扩散语言模型（DLMs）的新型剪枝方法——Sink-Aware Pruning，指出DLM中注意力sink位置具有高时序变异性，不同于自回归模型中的稳定sink，因此应主动识别并剪除不稳定sink，从而在不重训练前提下提升推理效率与质量权衡。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）因迭代去噪导致推理开销大，需高效剪枝；但现有剪枝策略多直接沿用自回归（AR）大模型中保留attention sink的启发式方法，而该假设在DLM中不成立。

Method: 通过分析DLM生成过程中attention sink位置随时间步的偏移方差，发现其高度不稳定；据此提出Sink-Aware Pruning方法，自动识别并剪除这些不稳定sink，无需模型重训练。

Result: 在相同计算预算下，该方法相比强基线剪枝方法取得更优的质量-效率权衡，且无需重训练。

Conclusion: DLM中的attention sink不具备AR模型中的结构性稳定性，应被动态识别与剪除；Sink-Aware Pruning为DLM高效推理提供了新范式。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [93] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.OH

TL;DR: 本文提出了一种面向后量子时代的混合安全框架，结合AES、BB84 QKD、量子态比较与类免疫系统，以应对Shor算法对RSA的威胁；目前为概念性设计，尚未实现与严格验证。


<details>
  <summary>Details</summary>
Motivation: 量子计算（尤其是Shor算法）对基于大数分解的经典公钥密码（如RSA）构成严重威胁，亟需构建兼顾经典与量子安全的后量子防护体系。

Method: 提出一个融合经典加密（AES）、量子密钥分发（BB84）、轻量级量子认证（量子态比较）和类生物免疫的自适应威胁检测机制的混合安全概念框架。

Result: 该框架在理论上可抵御Shor攻击，BB84保障密钥交换安全性与窃听检测能力，整体具备可扩展性与适应性；但尚无具体实现、安全证明或实验验证。

Conclusion: RSA在量子环境下不再安全，需转向混合经典-量子安全范式；本文框架为后量子迁移提供一种前瞻性、多层协同的设计思路，后续需开展实现与验证工作。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [94] [RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution](https://arxiv.org/abs/2602.16932)
*Jinming Nian,Fangchen Li,Dae Hoon Park,Yi Fang*

Main category: cs.IR

TL;DR: 本文提出RankEvolve，利用大语言模型（LLM）结合评估器与进化搜索，自动发现改进的词法检索算法；从BM25和带Dirichlet平滑的查询似然出发，在12个IR数据集上进化出新颖有效的可执行排序算法，并在多个基准（BEIR、BRIGHT、TREC DL）上展现良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有BM25等传统词法检索算法虽高效稳健，但改进多依赖人工调参与直觉；本文旨在探索是否能通过LLM驱动的程序进化自动发现更优算法。

Method: 基于AlphaEvolve构建RankEvolve框架：将候选排序算法表示为可执行代码，以12个IR数据集（BEIR/BRIGHT）上的检索性能为适应度，进行迭代变异、重组与选择；初始种群为BM25和查询似然（Dirichlet平滑）。

Result: 成功进化出若干新颖且有效的排序算法，在BEIR、BRIGHT全集及TREC DL 19/20上均表现出优于基线的迁移性能。

Conclusion: 评估器引导的LLM程序进化是一种可行且有前景的路径，可用于自动发现新型检索排序算法。

Abstract: Retrieval algorithms like BM25 and query likelihood with Dirichlet smoothing remain strong and efficient first-stage rankers, yet improvements have mostly relied on parameter tuning and human intuition. We investigate whether a large language model, guided by an evaluator and evolutionary search, can automatically discover improved lexical retrieval algorithms. We introduce RankEvolve, a program evolution setup based on AlphaEvolve, in which candidate ranking algorithms are represented as executable code and iteratively mutated, recombined, and selected based on retrieval performance across 12 IR datasets from BEIR and BRIGHT. RankEvolve starts from two seed programs: BM25 and query likelihood with Dirichlet smoothing. The evolved algorithms are novel, effective, and show promising transfer to the full BEIR and BRIGHT benchmarks as well as TREC DL 19 and 20. Our results suggest that evaluator-guided LLM program evolution is a practical path towards automatic discovery of novel ranking algorithms.

</details>


### [95] [SAGE: Structure Aware Graph Expansion for Retrieval of Heterogeneous Data](https://arxiv.org/abs/2602.16964)
*Prasham Titiya,Rohit Khoja,Tomer Wolfson,Vivek Gupta,Dan Roth*

Main category: cs.IR

TL;DR: 本文提出SAGE框架，通过构建基于元数据相似性的块级图并结合在线检索与邻居扩展策略，提升跨模态异构语料库上的问答检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理异构语料（文本、表格、图节点）时存在局限：实体级知识图谱构建维护成本高、查询效率低；而传统检索-阅读流水线无法捕获跨模态的多跳证据链。

Method: SAGE框架包含两部分：(i) 离线构建块级图，利用元数据驱动的相似性与百分位剪枝；(ii) 在线检索阶段先用基线检索器获取k个种子块，扩展其一跳邻居，并用稠密+稀疏检索过滤，选出k'个额外块。基线检索器可为混合稠密+稀疏检索器（用于隐式跨模态语料）或SPARK（用于显式模式图）。

Result: 在OTT-QA和STaRK数据集上，SAGE分别将检索召回率较基线提升5.7和8.5个百分点。

Conclusion: SAGE通过轻量级结构化图构建与高效图扩展机制，在不依赖昂贵实体级知识图谱的前提下，显著提升了跨模态异构语料的检索性能。

Abstract: Retrieval-augmented question answering over heterogeneous corpora requires connected evidence across text, tables, and graph nodes. While entity-level knowledge graphs support structured access, they are costly to construct and maintain, and inefficient to traverse at query time. In contrast, standard retriever-reader pipelines use flat similarity search over independently chunked text, missing multi-hop evidence chains across modalities. We propose SAGE (Structure Aware Graph Expansion) framework that (i) constructs a chunk-level graph offline using metadata-driven similarities with percentile-based pruning, and (ii) performs online retrieval by running an initial baseline retriever to obtain k seed chunks, expanding first-hop neighbors, and then filtering the neighbors using dense+sparse retrieval, selecting k' additional chunks. We instantiate the initial retriever using hybrid dense+sparse retrieval for implicit cross-modal corpora and SPARK (Structure Aware Planning Agent for Retrieval over Knowledge Graphs) an agentic retriever for explicit schema graphs. On OTT-QA and STaRK, SAGE improves retrieval recall by 5.7 and 8.5 points over baselines.

</details>


### [96] [Beyond Chunk-Then-Embed: A Comprehensive Taxonomy and Evaluation of Document Chunking Strategies for Information Retrieval](https://arxiv.org/abs/2602.16974)
*Yongjie Zhou,Shuai Wang,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 本文系统性地复现并评估了文档分块策略，提出一个统一框架，从分块方法和嵌入范式两个维度对现有方法进行分类与比较，发现最优分块策略因检索任务（in-corpus vs. in-document）而异。


<details>
  <summary>Details</summary>
Motivation: 现有文档分块方法（如LLM引导、上下文化分块）独立提出、评估基准不一致，缺乏可比性和系统性理解。

Method: 构建统一框架：按分块方法（结构型/语义型/LLM引导型）和嵌入范式（预嵌入分块 vs. 上下文化分块）二维分类；在两类检索任务（in-document 和 in-corpus）上复现并评测多种策略。

Result: 结构型分块（如段落级）在in-corpus检索中优于LLM引导方法；LumberChunker在in-document检索中表现最佳；上下文化分块提升in-corpus效果但损害in-document效果；分块尺寸与in-document效果中度相关，与in-corpus弱相关。

Conclusion: 文档分块无‘一刀切’最优方案，应依据具体检索任务选择策略；任务导向的设计与评估比单纯追求复杂模型更关键。

Abstract: Document chunking is a critical preprocessing step in dense retrieval systems, yet the design space of chunking strategies remains poorly understood. Recent research has proposed several concurrent approaches, including LLM-guided methods (e.g., DenseX and LumberChunker) and contextualized strategies(e.g., Late Chunking), which generate embeddings before segmentation to preserve contextual information. However, these methods emerged independently and were evaluated on benchmarks with minimal overlap, making direct comparisons difficult.
  This paper reproduces prior studies in document chunking and presents a systematic framework that unifies existing strategies along two key dimensions: (1) segmentation methods, including structure-based methods (fixed-size, sentence-based, and paragraph-based) as well as semantically-informed and LLM-guided methods; and (2) embedding paradigms, which determine the timing of chunking relative to embedding (pre-embedding chunking vs. contextualized chunking). Our reproduction evaluates these approaches in two distinct retrieval settings established in previous work: in-document retrieval (needle-in-a-haystack) and in-corpus retrieval (the standard information retrieval task).
  Our comprehensive evaluation reveals that optimal chunking strategies are task-dependent: simple structure-based methods outperform LLM-guided alternatives for in-corpus retrieval, while LumberChunker performs best for in-document retrieval. Contextualized chunking improves in-corpus effectiveness but degrades in-document retrieval. We also find that chunk size correlates moderately with in-document but weakly with in-corpus effectiveness, suggesting segmentation method differences are not purely driven by chunk size. Our code and evaluation benchmarks are publicly available at (Anonymoused).

</details>


### [97] [Bending the Scaling Law Curve in Large-Scale Recommendation Systems](https://arxiv.org/abs/2602.16986)
*Qin Ding,Kevin Course,Linjian Ma,Jianhui Sun,Rouchen Liu,Zhao Zhu,Chunxing Yin,Wei Li,Dai Li,Yu Shi,Xuan Cao,Ze Yang,Han Li,Xing Liu,Bi Xue,Hongwei Li,Rui Jian,Daisy Shi He,Jing Qian,Matt Ma,Qunshu Zhang,Rui Li*

Main category: cs.IR

TL;DR: 本文提出了ULTRA-HSTU，一种通过端到端模型与系统协同设计的新型序列推荐模型，在输入序列设计、稀疏注意力机制和模型拓扑结构上进行了创新，显著提升了训练与推理效率（分别达5倍和21倍）及推荐质量，并已在实际生产中大规模部署，带来4%-8%的用户消费与参与度提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于交叉注意力的序列推荐方法受限于自注意力表征能力，且面临二次计算复杂度瓶颈，亟需兼顾效率与表征能力的新架构。

Method: 提出ULTRA-HSTU模型，通过端到端模型与系统协同设计，创新性地优化输入序列构造、引入稀疏注意力机制、重构模型拓扑结构。

Result: 在基准测试中实现超5倍训练速度扩展和21倍推理速度扩展，同时提升推荐质量；已部署于真实生产环境，服务数十亿用户，带来4%–8%的消费与参与度增长。

Conclusion: ULTRA-HSTU验证了协同设计在序列推荐中的有效性，为长序列建模提供了高效且高质量的新范式。

Abstract: Learning from user interaction history through sequential models has become a cornerstone of large-scale recommender systems. Recent advances in large language models have revealed promising scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms to address the quadratic computational bottleneck in sequential modeling, which can limit the representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential recommendation model developed through end-to-end model and system co-design. By innovating in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains -- over 5x faster training scaling and 21x faster inference scaling compared to conventional models -- while delivering superior recommendation quality. Our solution is fully deployed at scale, serving billions of users daily and driving significant 4% to 8% consumption and engagement improvements in real-world production environments.

</details>


### [98] [WSDM Cup 2026 Multilingual Retrieval: A Low-Cost Multi-Stage Retrieval Pipeline](https://arxiv.org/abs/2602.16989)
*Chentong Hao,Minmao Wang*

Main category: cs.IR

TL;DR: 本文提出了一种低成本的多语言检索系统，用于WSDM Cup 2026任务，通过结合LLM驱动的查询扩展、BM25检索、长文本密集排序（jina-embeddings-v4）和轻量级点式重排序（Qwen3-Reranker-4B），在nDCG@20达0.403、Judged@20达0.95。


<details>
  <summary>Details</summary>
Motivation: 应对WSDM Cup 2026多语言检索任务中跨语言（英→中/波斯/俄）、大规模（千万级文档）、低成本约束下的高效准确检索需求。

Method: 四阶段流水线：1）LLM驱动的GRF风格查询扩展；2）BM25初检；3）基于jina-embeddings-v4的长文本密集排序；4）Qwen3-Reranker-4B对Top-20做点式重排序，其余结果保持密集排序顺序。

Result: 官方评测达nDCG@20=0.403、Judged@20=0.95；消融实验量化了各模块贡献，并验证了在计算受限下查询扩展、密集排序与Top-k重排序的有效性。

Conclusion: 该四阶段混合检索框架在精度与效率间取得良好平衡，尤其适合资源受限的多语言检索场景；各组件可插拔、可扩展，为实际部署提供了灵活且低成本的解决方案。

Abstract: We present a low-cost retrieval system for the WSDM Cup 2026 multilingual retrieval task, where English queries are used to retrieve relevant documents from a collection of approximately ten million news articles in Chinese, Persian, and Russian, and to output the top-1000 ranked results for each query. We follow a four-stage pipeline that combines LLM-based GRF-style query expansion with BM25 candidate retrieval, dense ranking using long-text representations from jina-embeddings-v4, and pointwise re-ranking of the top-20 candidates using Qwen3-Reranker-4B while preserving the dense order for the remaining results. On the official evaluation, the system achieves nDCG@20 of 0.403 and Judged@20 of 0.95. We further conduct extensive ablation experiments to quantify the contribution of each stage and to analyze the effectiveness of query expansion, dense ranking, and top-$k$ reranking under limited compute budgets.

</details>


### [99] [LiveGraph: Active-Structure Neural Re-ranking for Exercise Recommendation](https://arxiv.org/abs/2602.17036)
*Rong Fu,Zijian Zhang,Haiyun Wei,Jiekai Wu,Kun Liu,Xianda Li,Haoyu Zhao,Yang Li,Yongtai Liu,Ziming Wang,Rui Lu,Simon Fong*

Main category: cs.IR

TL;DR: 本文提出了LiveGraph，一种基于图结构的神经重排序框架，用于解决在线教育中学生参与度长尾分布和个性化学习路径适配不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前习题推荐框架难以应对学生参与度的长尾分布问题，且无法适应个体化学习轨迹。

Method: 提出LiveGraph框架，采用图表示增强策略弥合活跃与非活跃学生间的信息鸿沟，并结合动态重排序机制提升内容多样性，强调学习历史中的结构关系。

Result: 在多个真实数据集上的实验表明，LiveGraph在预测准确性和习题多样性两方面均优于现有基线方法。

Conclusion: LiveGraph通过结构建模与动态重排序，有效平衡了推荐精度与教学多样性，提升了个性化教育推荐效果。

Abstract: The continuous expansion of digital learning environments has catalyzed the demand for intelligent systems capable of providing personalized educational content. While current exercise recommendation frameworks have made significant strides, they frequently encounter obstacles regarding the long-tailed distribution of student engagement and the failure to adapt to idiosyncratic learning trajectories. We present LiveGraph, a novel active-structure neural re-ranking framework designed to overcome these limitations. Our approach utilizes a graph-based representation enhancement strategy to bridge the information gap between active and inactive students while integrating a dynamic re-ranking mechanism to foster content diversity. By prioritizing the structural relationships within learning histories, the proposed model effectively balances recommendation precision with pedagogical variety. Comprehensive experimental evaluations conducted on multiple real-world datasets demonstrate that LiveGraph surpasses contemporary baselines in both predictive accuracy and the breadth of exercise diversity.

</details>


### [100] [A Long-term Value Prediction Framework In Video Ranking](https://arxiv.org/abs/2602.17058)
*Huabin Chen,Xinao Wang,Huiping Chu,Keqin Xu,Chenhao Zhai,Chenyi Wang,Kai Meng,Yuning Jiang*

Main category: cs.IR

TL;DR: 本文提出了一种面向短视频推荐排序阶段的长周期用户价值（LTV）建模框架，通过位置感知去偏分位数模块、多维归因模块和跨时间作者建模模块，分别解决位置偏差、归因模糊和时间局限性三大挑战，在淘宝亿级生产系统中成功部署并取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 在短视频推荐排序阶段，准确建模长期用户价值（LTV）仍面临延迟反馈、细粒度归因缺失和位置偏差校正困难等挑战，尤其在十亿级规模下缺乏鲁棒且可扩展的解决方案。

Method: 提出三模块框架：(1) Position-aware Debias Quantile (PDQ) 模块，基于分位数分布进行位置归一化；(2) 多维归因模块，联合建模上下文、行为与内容信号的连续归因强度，并引入定制混合损失增强因果清晰性；(3) 跨时间作者建模模块，构建截断感知的日级LTV目标以捕捉创作者驱动的长期再参与。整体作为任务增强嵌入现有排序模型。

Result: 离线实验与线上A/B测试均显示LTV指标显著提升，且与短期目标保持稳定权衡；已在淘宝亿级生产系统中部署，实现持续用户参与度增益。

Conclusion: 该框架兼具理论合理性与工程实用性，无需修改主干结构即可高效训练与服务，满足工业级大规模部署约束，为排序阶段LTV建模提供了可复用、可扩展的新范式。

Abstract: Accurately modeling long-term value (LTV) at the ranking stage of short-video recommendation remains challenging. While delayed feedback and extended engagement have been explored, fine-grained attribution and robust position normalization at billion-scale are still underdeveloped. We propose a practical ranking-stage LTV framework addressing three challenges: position bias, attribution ambiguity, and temporal limitations.
  (1) Position bias: We introduce a Position-aware Debias Quantile (PDQ) module that normalizes engagement via quantile-based distributions, enabling position-robust LTV estimation without architectural changes. (2) Attribution ambiguity: We propose a multi-dimensional attribution module that learns continuous attribution strengths across contextual, behavioral, and content signals, replacing static rules to capture nuanced inter-video influence. A customized hybrid loss with explicit noise filtering improves causal clarity. (3) Temporal limitations: We present a cross-temporal author modeling module that builds censoring-aware, day-level LTV targets to capture creator-driven re-engagement over longer horizons; the design is extensible to other dimensions (e.g., topics, styles).
  Offline studies and online A/B tests show significant improvements in LTV metrics and stable trade-offs with short-term objectives. Implemented as task augmentation within an existing ranking model, the framework supports efficient training and serving, and has been deployed at billion-scale in Taobao's production system, delivering sustained engagement gains while remaining compatible with industrial constraints.

</details>


### [101] [When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment](https://arxiv.org/abs/2602.17170)
*Chuting Yu,Hang Li,Joel Mackenzie,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 本文系统研究了大语言模型（LLM）作为人工相关性判断代理时存在的过度评分（overrating）问题，发现其存在系统性偏差，易受文本长度和表层词汇线索影响，可靠性与稳定性不足，不宜直接替代人工评估。


<details>
  <summary>Details</summary>
Motivation: 人工相关性评估耗时费力、难以扩展，而用大语言模型（LLM）替代人工评估的可靠性尚不明确，亟需系统验证其稳定性与严谨性。

Method: 对多种模型主干、评估范式（点式与成对）、段落修改策略开展系统性实证研究，通过控制实验分析LLM相关性判断对段落长度和词汇线索的敏感性。

Result: LLM普遍存在一致性的过度评分行为，即高置信度地赋予不满足信息需求的段落过高相关性得分；且判断结果显著受段落长度和表面词汇特征影响。

Conclusion: LLM目前不适合作为人工相关性评估的‘即插即用’替代方案，需建立更严谨的诊断性评估框架来保障其在信息检索评估中的可信应用。

Abstract: Human relevance assessment is time-consuming and cognitively intensive, limiting the scalability of Information Retrieval evaluation. This has led to growing interest in using large language models (LLMs) as proxies for human judges. However, it remains an open question whether LLM-based relevance judgments are reliable, stable, and rigorous enough to match humans for relevance assessment. In this work, we conduct a systematic study of overrating behavior in LLM-based relevance judgments across model backbones, evaluation paradigms (pointwise and pairwise), and passage modification strategies. We show that models consistently assign inflated relevance scores -- often with high confidence -- to passages that do not genuinely satisfy the underlying information need, revealing a system-wide bias rather than random fluctuations in judgment. Furthermore, controlled experiments show that LLM-based relevance judgments can be highly sensitive to passage length and surface-level lexical cues. These results raise concerns about the usage of LLMs as drop-in replacements for human relevance assessors, and highlight the urgent need for careful diagnostic evaluation frameworks when applying LLMs for relevance assessments. Our code and results are publicly available.

</details>


### [102] [On the Reliability of User-Centric Evaluation of Conversational Recommender Systems](https://arxiv.org/abs/2602.17264)
*Michael Müller,Amir Reza Mohammadi,Andreas Peintner,Beatriz Barroso Gstrein,Günther Specht,Eva Zangerle*

Main category: cs.IR

TL;DR: 本文通过大规模实证研究，检验了基于静态对话日志的用户中心式对话推荐系统（CRS）评估的可靠性，发现效用类维度（如准确性、有用性）在聚合后具备中等信度，而社会性维度（如拟人性、亲密度）信度较低，且存在显著的光环效应，质疑单标注员和大语言模型评估的有效性。


<details>
  <summary>Details</summary>
Motivation: 用户中心评估已成为CRS评估的关键范式，但依赖众包或大语言模型对静态对话日志进行第三方标注的可靠性尚未被系统检验。

Method: 在200个ReDial对话上，使用18维CRS-Que框架收集124名众包工作者共1053条标注；采用随机效应信度模型与相关性分析，量化各维度稳定性及相互依赖关系。

Result: 效用导向维度（如准确性、有用性、满意度）在聚合后达到中等信度；社会性维度（如拟人性、亲密度）信度显著更低；多个维度高度相关并坍缩为单一全局质量信号，表明存在强光环效应。

Conclusion: 单标注员和LLM驱动的离线评估协议有效性存疑，应采用多评者聚合与维度约简策略以提升用户中心式CRS评估的可靠性与有效性。

Abstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisfaction achieve moderate reliability under aggregation, whereas socially grounded constructs such as humanness and rapport are substantially less reliable. Furthermore, many dimensions collapse into a single global quality signal, revealing a strong halo effect in third-party judgments. These findings challenge the validity of single-annotator and LLM-based evaluation protocols and motivate the need for multi-rater aggregation and dimension reduction in offline CRS evaluation.

</details>


### [103] [WebFAQ 2.0: A Multilingual QA Dataset with Mined Hard Negatives for Dense Retrieval](https://arxiv.org/abs/2602.17327)
*Michael Dinzinger,Laura Caspari,Ali Salman,Irvin Topi,Jelena Mitrović,Michael Granitzer*

Main category: cs.IR

TL;DR: WebFAQ 2.0 是一个大规模多语言 FAQ 数据集，包含 1.98 亿问答对、108 种语言，新增 1430 万双语对和 125 万带硬负例的多语言查询，支持密集检索器训练与知识蒸馏，并持续更新。


<details>
  <summary>Details</summary>
Motivation: 应对社区对更大规模、更高质量、更多样化及可扩展多语言 FAQ 资源的需求，同时解决原版数据覆盖有限、上下文贫乏及缺乏硬负例的问题。

Method: 采用直接网络爬取与结构化提取策略构建数据；引入两阶段检索流水线挖掘跨语言硬负例；提供 Contrastive Learning（MultipleNegativesRanking loss）和 Knowledge Distillation（MarginMSE loss）两种细调范式。

Result: 构建了当前最大规模的 FAQ 数据集 WebFAQ 2.0，含 198M QA 对、108 种语言、14.3M 双语对、1.25M 多语言硬负例查询（每查询 200 个负例并附交叉编码器分数）；配套开源训练脚本与持续更新机制。

Conclusion: WebFAQ 2.0 显著提升了多语言/跨语言信息检索研究的数据基础，推动密集检索器在真实网页场景下的鲁棒性与泛化能力，并确立了可持续演进的开放资源范式。

Abstract: We introduce WebFAQ 2.0, a new version of the WebFAQ dataset, containing 198 million FAQ-based natural question-answer pairs across 108 languages. Compared to the previous version, it significantly expands multilingual coverage and the number of bilingual aligned QA pairs to over 14.3M, making it the largest FAQ-based resource. Unlike the original release, WebFAQ 2.0 uses a novel data collection strategy that directly crawls and extracts relevant web content, resulting in a substantially more diverse and multilingual dataset with richer context through page titles and descriptions. In response to community feedback, we also release a hard negatives dataset for training dense retrievers, with 1.25M queries across 20 languages. These hard negatives were mined using a two-stage retrieval pipeline and include cross-encoder scores for 200 negatives per query. We further show how this resource enables two primary fine-tuning strategies for dense retrievers: Contrastive Learning with MultipleNegativesRanking loss, and Knowledge Distillation with MarginMSE loss. WebFAQ 2.0 is not a static resource but part of a long-term effort. Since late 2025, structured FAQs are being regularly released through the Open Web Index, enabling continuous expansion and refinement. We publish the datasets and training scripts to facilitate further research in multilingual and cross-lingual IR. The dataset itself and all related resources are publicly available on GitHub and HuggingFace.

</details>


### [104] [Training-free Graph-based Imputation of Missing Modalities in Multimodal Recommendation](https://arxiv.org/abs/2602.17354)
*Daniele Malitesta,Emanuele Rossi,Claudio Pomo,Tommaso Di Noia,Fragkiskos D. Malliaros*

Main category: cs.IR

TL;DR: 本文针对多模态推荐系统中模态缺失问题，提出了一种基于物品共购图的图特征插值方法，设计了四种无需训练的模态特征填补策略，并验证其在多个数据集上优于传统插补方法，同时首次分析了图同质性对插补效果的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐系统中常面临图像或文本等模态数据缺失或噪声问题，现有做法多直接丢弃缺失模态的物品，缺乏对缺失模态问题的系统建模与有效处理。

Method: 将模态缺失问题建模为物品-物品共购图上的图特征插值问题，提出四种无需训练的图传播式模态特征填补方法，利用图结构传播可用模态信息以恢复缺失模态。

Result: 所提方法可即插即用地提升任意现有多模态推荐系统性能，在多个标准数据集上显著优于传统机器学习插补方法，并揭示图同质性对插补效果具有关键影响。

Conclusion: 基于图结构的无训练模态插补是一种有效且通用的解决方案，不仅弥补了多模态推荐中缺失模态研究的空白，也为图信号补全提供了新思路。

Abstract: Multimodal recommender systems (RSs) represent items in the catalog through multimodal data (e.g., product images and descriptions) that, in some cases, might be noisy or (even worse) missing. In those scenarios, the common practice is to drop items with missing modalities and train the multimodal RSs on a subsample of the original dataset. To date, the problem of missing modalities in multimodal recommendation has still received limited attention in the literature, lacking a precise formalisation as done with missing information in traditional machine learning. In this work, we first provide a problem formalisation for missing modalities in multimodal recommendation. Second, by leveraging the user-item graph structure, we re-cast the problem of missing multimodal information as a problem of graph features interpolation on the item-item co-purchase graph. On this basis, we propose four training-free approaches that propagate the available multimodal features throughout the item-item graph to impute the missing features. Extensive experiments on popular multimodal recommendation datasets demonstrate that our solutions can be seamlessly plugged into any existing multimodal RS and benchmarking framework while still preserving (or even widen) the performance gap between multimodal and traditional RSs. Moreover, we show that our graph-based techniques can perform better than traditional imputations in machine learning under different missing modalities settings. Finally, we analyse (for the first time in multimodal RSs) how feature homophily calculated on the item-item graph can influence our graph-based imputations.

</details>


### [105] [Improving LLM-based Recommendation with Self-Hard Negatives from Intermediate Layers](https://arxiv.org/abs/2602.17410)
*Bingqian Li,Bowen Zheng,Xiaolei Wang,Long Zhang,Jinpeng Wang,Sheng Chen,Wayne Xin Zhao,Ji-rong Wen*

Main category: cs.IR

TL;DR: 本文提出ILRec框架，利用中间层提取的自硬负样本信号改进大语言模型在推荐系统中的偏好学习，通过两阶段优化和轻量级协同过滤模型提升负样本质量与推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐方法依赖离线生成的序列级负样本，难以在大规模负物品空间中提供足够判别性和信息性的监督信号。

Method: 提出ILRec偏好微调框架，包括：1）从中间层识别自硬负token作为细粒度负监督；2）设计跨层偏好优化与跨层偏好蒸馏两阶段训练机制；3）引入轻量级协同过滤模型为负信号分配token级奖励以减少误罚。

Result: 在三个数据集上的大量实验表明，ILRec显著提升了大语言模型推荐系统的性能。

Conclusion: ILRec通过动态、细粒度的中间层负信号建模与协同优化机制，有效增强了大语言模型在推荐任务中的偏好学习能力。

Abstract: Large language models (LLMs) have shown great promise in recommender systems, where supervised fine-tuning (SFT) is commonly used for adaptation. Subsequent studies further introduce preference learning to incorporate negative samples into the training process. However, existing methods rely on sequence-level, offline-generated negatives, making them less discriminative and informative when adapting LLMs to recommendation tasks with large negative item spaces. To address these challenges, we propose ILRec, a novel preference fine-tuning framework for LLM-based recommendation, leveraging self-hard negative signals extracted from intermediate layers to improve preference learning. Specifically, we identify self-hard negative tokens from intermediate layers as fine-grained negative supervision that dynamically reflects the model's preference learning process. To effectively integrate these signals into training, we design a two-stage framework comprising cross-layer preference optimization and cross-layer preference distillation, enabling the model to jointly discriminate informative negatives and enhance the quality of negative signals from intermediate layers. In addition, we introduce a lightweight collaborative filtering model to assign token-level rewards for negative signals, mitigating the risk of over-penalizing false negatives. Extensive experiments on three datasets demonstrate ILRec's effectiveness in enhancing the performance of LLM-based recommender systems.

</details>


### [106] [Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research](https://arxiv.org/abs/2602.17450)
*Amirereza Abbasi,Mohsen Hooshmand*

Main category: cs.IR

TL;DR: This survey examines how large language models (LLMs), especially via retrieval-augmented generation (RAG), are transforming web research and industry—reshaping tasks like information retrieval, QA, recommendations, web analytics, summarization, and education.


<details>
  <summary>Details</summary>
Motivation: The rapid integration of LLMs into all domains—including web research—necessitates a systematic review of their impact, challenges, and future potential, especially with techniques like RAG.

Method: Survey-based analysis of recent advances, focusing on LLM applications in web research and industry, with emphasis on retrieval-augmented generation (RAG).

Result: LLMs are fundamentally shifting traditional web pipelines toward generative approaches across multiple tasks; RAG emerges as a key enabler for improving accuracy, grounding, and applicability in real-world web scenarios.

Conclusion: LLMs—particularly when augmented with retrieval—offer transformative potential for web research and applications, yet significant challenges around reliability, scalability, evaluation, and integration remain to be addressed.

Abstract: Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced both the field and its applications. This wave of LLMs has permeated science and technology so deeply that no area remains untouched. Consequently, LLMs are reshaping web research and development, transforming traditional pipelines into generative solutions for tasks like information retrieval, question answering, recommendation systems, and web analytics. They have also enabled new applications such as web-based summarization and educational tools. This survey explores recent advances in the impact of LLMs-particularly through the use of retrieval-augmented generation (RAG)-on web research and industry. It discusses key developments, open challenges, and future directions for enhancing web solutions with LLMs.

</details>


### [107] [A Picture of Agentic Search](https://arxiv.org/abs/2602.17518)
*Francesca Pezzuti,Ophir Frieder,Fabrizio Silvestri,Sean MacAvaney,Nicola Tonellotto*

Main category: cs.IR

TL;DR: 本文指出了信息检索（IR）领域在自动化系统（如AI代理）日益参与搜索任务背景下，仍以人类为中心所导致的根本性错配问题，并提出了首个专门面向代理搜索行为的数据集ASQ及配套工具包，以推动IR向人机协同的新范式演进。


<details>
  <summary>Details</summary>
Motivation: 随着自动化系统（agent）越来越多地发起搜索查询，传统以人类为中心的信息检索（IR）体系（包括系统设计、评估指标、用户模型和数据集）已无法适配新的现实，导致性能下降与评估失真，而缺乏代理搜索行为数据集是当前关键瓶颈。

Method: 提出一种系统性方法来收集检索增强型智能体在回答问题过程中产生和消耗的全部数据（包括推理驱动的查询、检索文档和中间思考过程），并构建了Agentic Search Queryset（ASQ）数据集，覆盖多个基准数据集（HotpotQA、Researchy Questions、MS MARCO）、三类不同智能体和两类检索流程；同时发布可扩展的配套工具包。

Result: 发布了首个大规模、多维度、可扩展的代理搜索行为数据集ASQ，包含推理链查询、对应检索文档与思维过程，并提供开源工具包支持新智能体、检索器和数据集的快速接入与扩展。

Conclusion: IR亟需从人类中心范式转向人机协同范式；ASQ数据集及其工具包为代理感知的检索建模、评估与优化提供了坚实基础，填补了该领域关键数据空白，有望推动IR理论与实践的范式变革。

Abstract: With automated systems increasingly issuing search queries alongside humans, Information Retrieval (IR) faces a major shift. Yet IR remains human-centred, with systems, evaluation metrics, user models, and datasets designed around human queries and behaviours. Consequently, IR operates under assumptions that no longer hold in practice, with changes to workload volumes, predictability, and querying behaviours. This misalignment affects system performance and optimisation: caching may lose effectiveness, query pre-processing may add overhead without improving results, and standard metrics may mismeasure satisfaction. Without adaptation, retrieval models risk satisfying neither humans, nor the emerging user segment of agents. However, datasets capturing agent search behaviour are lacking, which is a critical gap given IR's historical reliance on data-driven evaluation and optimisation. We develop a methodology for collecting all the data produced and consumed by agentic retrieval-augmented systems when answering queries, and we release the Agentic Search Queryset (ASQ) dataset. ASQ contains reasoning-induced queries, retrieved documents, and thoughts for queries in HotpotQA, Researchy Questions, and MS MARCO, for 3 diverse agents and 2 retrieval pipelines. The accompanying toolkit enables ASQ to be extended to new agents, retrievers, and datasets.

</details>


### [108] [Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval](https://arxiv.org/abs/2602.17654)
*Jiaqi Xi,Raghav Saboo,Luming Chen,Martin Wang,Sudeep Das*

Main category: cs.IR

TL;DR: 本文提出了一种两阶段“挖掘与精炼”对比学习框架，用于提升电商搜索中的语义文本嵌入效果，通过政策对齐的轻量LLM标注、多级相关性建模及改进的Circle Loss，显著提升了检索质量与业务指标。


<details>
  <summary>Details</summary>
Motivation: 电商搜索需兼顾长尾、噪声查询的泛化能力，以及可扩展、符合产品与策略约束的监督信号；同时，真实场景中相关性呈分级（精确匹配/替代品/互补品），需要清晰区分相似度分数以支持稳定混合排序和阈值设定。

Method: 第一阶段：基于标签感知的监督对比学习训练多语言双塔检索器；第二阶段：用ANN挖掘难样本，由策略对齐的轻量LLM重标注，并引入多类Circle Loss显式增强不同相关性层级间的边界；辅以拼写增强和合成查询生成提升鲁棒性。

Result: 离线评估与线上A/B测试均表明，该框架显著提升检索相关性，并在用户参与度和业务指标上取得统计显著增益。

Conclusion: ‘Mine and Refine’框架有效解决了电商搜索中分级相关性建模与策略一致监督的难题，为多类别语义嵌入提供了可扩展、鲁棒且实用的训练范式。

Abstract: We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [109] [Fuse3D: Generating 3D Assets Controlled by Multi-Image Fusion](https://arxiv.org/abs/2602.17040)
*Xuancheng Jin,Rengan Xie,Wenting Zheng,Rui Wang,Hujun Bao,Yuchi Huo*

Main category: cs.GR

TL;DR: Fuse3D 是首个支持多图像条件控制的 3D 资产生成方法，通过多条件融合模块、语义驱动的2D-3D区域对齐及局部注意力增强策略，实现从全局视角到局部细节的可控3D生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法仅支持单图像控制，无法利用多个图像独立控制3D资产的不同区域，限制了应用灵活性。

Method: 提出 Fuse3D 方法，包含：1）多条件融合模块整合多图像区域特征；2）基于语义线索的自动2D图像区域与3D区域对齐；3）局部注意力增强策略以缓解控制冲突并强化局部特征融合。

Result: 实验表明 Fuse3D 能灵活融合多个2D图像区域生成结构一致、高质量的3D资产。

Conclusion: Fuse3D 是首个支持多图像条件控制的可控3D资产生成方法，显著提升了区域级控制的灵活性与精细度。

Abstract: Recently, generating 3D assets with the control of condition images has achieved impressive quality. However, existing 3D generation methods are limited to handling a single control objective and lack the ability to utilize multiple images to independently control different regions of a 3D asset, which hinders their flexibility in applications. We propose Fuse3D, a novel method that enables generating 3D assets under the control of multiple images, allowing for the seamless fusion of multi-level regional controls from global views to intricate local details. First, we introduce a Multi-Condition Fusion Module to integrate the visual features from multiple image regions. Then, we propose a method to automatically align user-selected 2D image regions with their associated 3D regions based on semantic cues. Finally, to resolve control conflicts and enhance local control features from multi-condition images, we introduce a Local Attention Enhancement Strategy that flexibly balances region-specific feature fusion. Overall, we introduce the first method capable of controllable 3D asset generation from multiple condition images. The experimental results indicate that Fuse3D can flexibly fuse multiple 2D image regions into coherent 3D structures, resulting in high-quality 3D assets. Code and data for this paper are at https://jinnmnm.github.io/Fuse3d.github.io/.

</details>


### [110] [InstantRetouch: Personalized Image Retouching without Test-time Fine-tuning Using an Asymmetric Auto-Encoder](https://arxiv.org/abs/2602.17044)
*Temesgen Muruts Weldengus,Binnan Liu,Fei Kou,Youwei Lyu,Jinwei Chen,Qingnan Fan,Changqing Zou*

Main category: cs.GR

TL;DR: 本文提出了InstantRetouch框架，通过非对称自编码器和检索增强式修图（RAR）技术，实现无需测试时微调的个性化图像修图，支持单参考、多参考及混合风格等多种场景，并可泛化至照片级真实感风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像修图方法通常需要用户特定的微调，或泛化能力不足，难以高效适配不同用户的修图风格。

Method: 提出InstantRetouch框架，包含两个核心组件：1）非对称自编码器，用于从配对示例中提取内容解耦的修图风格潜表示；2）检索增强式修图（RAR），根据查询图像内容相似性动态检索并聚合参考风格潜变量。

Result: InstantRetouch在单参考、多参考及混合风格等多样化个性化修图任务上表现优越，且无需测试时微调；同时可开箱即用地泛化至photorealistic风格迁移任务。

Conclusion: InstantRetouch提供了一种高效、通用、内容感知的个性化图像修图新范式，显著提升了风格迁移的灵活性与实用性。

Abstract: Personalized image retouching aims to adapt retouching style of individual users from reference examples, but existing methods often require user-specific fine-tuning or fail to generalize effectively. To address these challenges, we introduce $\textbf{InstantRetouch}$, a general framework for personalized image retouching that instantly adapts to user retouching styles without any test-time fine-tuning. It employs an $\textit{asymmetric auto-encoder}$ to encode the retouching style from paired examples into a content disentangled latent representation that enables faithful transfer of the retouching style to new images. To adaptively apply the encoded retouching style to new images, we further propose $\textit{retrieval-augmented retouching}$ (RAR), which retrieves and aggregates style latents from reference pairs most similar in content to the query image. With these components, $\textbf{InstantRetouch}$ enables superior and generic content-aware retouching personalization across diverse scenarios, including single-reference, multi-reference, and mixed-style setups, while also generalizing out of the box to photorealistic style transfer.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [111] [ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts](https://arxiv.org/abs/2602.16744)
*Takuro Kato,Mitsuharu Morisawa*

Main category: cs.RO

TL;DR: 本文提出了一种用于自主叉车在倾斜表面上卸载托盘的控制方法，利用ICP算法实时跟踪托盘与货叉的相对位姿，并使货叉平行于目标表面，从而实现无拖拽式撤叉卸载。


<details>
  <summary>Details</summary>
Motivation: 解决自主叉车在倾斜表面（如卡车车厢）卸载托盘时易发生拖拽的问题，提升作业安全性与精度。

Method: 采用ICP算法对托盘上方区域点云进行实时匹配，估计托盘与货叉间的相对位置和姿态角偏差；据此调整货叉使其平行于倾斜表面，再沿倾斜方向撤叉完成卸载。

Result: 通过动态仿真和真实叉车实验（模拟向卡车倾斜车厢卸载），验证了该方法可有效避免托盘拖拽，实现稳定、精准的倾斜表面卸载。

Conclusion: 所提基于ICP的实时位姿跟踪与自适应对齐控制方法，显著提升了自主叉车在非水平工况下的卸载可靠性与实用性。

Abstract: This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.

</details>


### [112] [Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot](https://arxiv.org/abs/2602.16758)
*Sina Akhbari,Mehran Mahboubkhah*

Main category: cs.RO

TL;DR: 本文提出了一种面向四自由度并联运动学铣削机器人的平滑轨迹生成方法，融合B样条与四元数插值，通过分段贝塞尔曲线同步位置与姿态，并采用最小急动度和时间最优优化实现高效实时轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 解决并联机器人在多自由度轨迹规划中位置与姿态解耦、同步困难、易出现奇异性（如万向节锁）及实时性差的问题。

Method: 结合B样条与单位四元数插值处理位置与姿态；用分段贝塞尔曲线拟合弧长-姿态非线性关系（SQP求解）；利用贝塞尔曲线凸包性质保障多智能体时空分离；位置用修正多项式插值，姿态用单位四元数；分任务空间与关节空间两阶段优化时间轨迹（最小急动度+时间最优贝塞尔）。

Result: 实验表明该方法相比传统插值法具有更高轨迹精度、更小速度波动和更优计算效率，并成功部署于低成本微控制器。

Conclusion: 所提方法有效实现了高精度、平滑、实时且鲁棒的并联机器人轨迹生成，兼顾运动学约束与硬件限制。

Abstract: This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.

</details>


### [113] [RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness](https://arxiv.org/abs/2602.16825)
*Ahmad Ahmad,Shuo Liu,Roberto Tron,Calin Belta*

Main category: cs.RO

TL;DR: 本文提出RRT$^η$框架，采用算术-几何平均（AGM）鲁棒性度量替代传统min-max鲁棒性，提升STL任务规划的平滑性与效率，并在多种机器人平台上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统基于min-max鲁棒性的STL运动规划方法导致非光滑优化景观和尖锐决策边界，阻碍高效树搜索。

Method: 提出RRT$^η$框架，引入AGM鲁棒性度量、AGM鲁棒性区间语义、增量式区间监控算法，以及基于履行优先逻辑（FPL）的满意度提升方向向量。

Result: 在双积分器点机器人、单车模型移动机器人和7自由度机械臂上验证，RRT$^η$在多约束、弱引导信号场景下显著优于传统STL鲁棒性规划器。

Conclusion: RRT$^η$在保证RRT*概率完备性与渐进最优性的同时，实现了高鲁棒性、动态可行的STL任务规划，提升了复杂时空约束下的采样规划性能。

Abstract: Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.
  We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.

</details>


### [114] [Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations](https://arxiv.org/abs/2602.16846)
*Xili Yi,Ying Xing,Zachary Manchester,Nima Fazeli*

Main category: cs.RO

TL;DR: 本文提出了一种名为'Sound of Touch'的主动声学触觉感知方法，利用振动张紧弦作为传感单元，通过电磁持续激励和少量接触式麦克风采集频谱变化，实现接触位置、法向力和滑动的实时估计。


<details>
  <summary>Details</summary>
Motivation: 分布式触觉传感在大面积覆盖时面临布线复杂、成本高、易损坏等问题，现有方案往往覆盖有限或难以捕捉快速交互动态。

Method: 采用电磁激励张紧弦产生连续振动，用接触式麦克风采集短时音频信号；基于弦振动物理模型仿真接触引起的模态频移；设计实时推理流程从振动信号中估计接触状态。

Result: 实验验证了毫米级定位精度、可靠的力估计能力及实时滑动检测性能。

Conclusion: 该工作提出了轻量、可扩展的弦基触觉传感硬件架构、物理驱动的仿真分析工具及实时接触状态推断流水线，为机器人大面积表面触觉感知提供了新思路。

Abstract: Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.

</details>


### [115] ["Hello, I'm Delivering. Let Me Pass By": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets](https://arxiv.org/abs/2602.16861)
*EunJeong Cheon,Do Yeon Shin*

Main category: cs.RO

TL;DR: 本文提出了一种名为'与机器人同行（WawR）'的新研究方法，借鉴城市研究、地理学和社会学中的公共领域民族志，以更系统地研究在公共空间中自主运行的移动机器人（如配送机器人）。


<details>
  <summary>Details</summary>
Motivation: 现有HRI领域的实地研究多依赖受控实验或结构化观察（如‘奥兹巫师’技术），难以应对当前自主机器人在动态、不可预测环境中自由运行的现实挑战。

Method: 提出并详细阐述‘与机器人同行（WawR）’方法论，融合公共领域民族志理念，包含具体实施步骤、关键特征及评估方式。

Result: 该方法提供了对公众与自主机器人互动的深层、情境化理解，揭示了传统方法难以捕获的实践性与社会性洞见。

Conclusion: WawR是一种适应真实世界复杂性的新型田野方法，有望推动人机交互领域在开放公共空间中研究范式的演进与讨论。

Abstract: As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.

</details>


### [116] [SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation](https://arxiv.org/abs/2602.16863)
*Kushal Kedia,Tyler Ga Wei Lum,Jeannette Bohg,C. Karen Liu*

Main category: cs.RO

TL;DR: 本文提出SimToolReal，一种面向工具操作的通用sim-to-real强化学习方法，通过在仿真中程序化生成多样化的工具类物体并训练单一RL策略以实现任意目标姿态操作，从而在真实世界中实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有工具操作方法依赖大量遥操作数据或需为每个任务单独建模与调优奖励函数，工程成本高且泛化性差。

Method: 提出SimToolReal框架：在仿真中程序化生成大量工具类物体基元，训练一个统一的RL策略，以将任意物体操纵至随机目标位姿为目标；无需针对特定物体或任务进行微调。

Result: 在120次真实世界实验中覆盖24项任务、12个物体实例和6类工具，实现强零样本性能；相比已有重定向和固定抓取方法提升37%，性能媲美针对特定物体/任务训练的专家RL策略。

Conclusion: SimToolReal验证了通过大规模多样化仿真训练单一策略可有效提升工具操作的sim-to-real泛化能力，为通用机器人灵巧操作提供新范式。

Abstract: The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.

</details>


### [117] [Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads](https://arxiv.org/abs/2602.16870)
*Daniil Lisus,Katya M. Papais,Cedric Le Gentil,Elliot Preston-Krebs,Andrew Lambert,Keith Y. K. Leung,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: Boreas-RT 是一个扩展的多季节、多地点自动驾驶数据集，包含60段共643公里的真实道路序列，覆盖多样化且具挑战性的环境，配备多模态传感器与高精度真值，用于评估和推动鲁棒的多模态定位与建图算法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶算法在简单环境中表现良好但易过拟合，缺乏在多样化真实场景（如不同交通、天气、道路条件）下评估其泛化能力的统一基准数据集。

Method: 构建了一个包含60个序列、覆盖9条真实路线（总计643公里）的多模态传感器数据集，配备高分辨率相机、Doppler雷达、多线激光雷达、FMCW激光雷达、IMU和轮速计，并通过后处理GNSS-INS提供厘米级真值；同时提供标定参数、开发套件和在线排行榜。

Result: 基准测试表明，当前主流的里程计与定位算法在Boreas-RT更具挑战性的路线上性能显著下降，验证了该数据集对评估算法鲁棒性的有效性。

Conclusion: Boreas-RT为多模态感知与定位算法提供了更真实、多样、可复现的评估平台，有助于推动自动驾驶系统在复杂现实环境中的泛化能力提升。

Abstract: The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.

</details>


### [118] [MALLVI: a multi agent framework for integrated generalized robotics manipulation](https://arxiv.org/abs/2602.16898)
*Iman Ahmadi,Mehrshad Taji,Arad Mahdinezhad Kashani,AmirHossein Jadidi,Saina Kashani,Babak Khalaj*

Main category: cs.RO

TL;DR: 本文提出MALLVi框架，利用多智能体大语言与视觉模型实现闭环反馈驱动的机器人操作任务规划，显著提升了零样本操作任务的成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的任务规划方法多为开环，缺乏环境反馈，在动态环境中鲁棒性差；需更可靠、自适应的闭环规划框架。

Method: 提出多智能体框架MALLVi，包含Decomposer、Localizer、Thinker、Reflector及可选Descriptor等专用代理，结合自然语言指令与环境图像，生成原子动作，并通过VLM评估执行反馈，实现闭环迭代规划与选择性错误恢复。

Result: 在仿真与真实场景实验中，MALLVi在零样本操作任务中展现出更强的泛化能力和更高的成功率。

Conclusion: 闭环、多智能体协同的架构优于单模型或开环方法，Reflector机制支持高效错误检测与局部重规划，是提升机器人操作鲁棒性的有效路径。

Abstract: Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.

</details>


### [119] [SparTa: Sparse Graphical Task Models from a Handful of Demonstrations](https://arxiv.org/abs/2602.16911)
*Adrian Röfer,Nick Heppert,Abhinav Valada*

Main category: cs.RO

TL;DR: 本文提出了一种基于图形化对象关系表示的长时程机器人操作任务学习方法，通过演示分割与聚合提取操作图，并建模对象状态分布，实现对‘做什么’而非‘怎么做’的抽象学习，提升了跨演示和环境的鲁棒性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 高效学习长时程机器人操作任务是示教学习的核心挑战；现有方法多直接在动作域学习，而本文转向学习任务目标（即‘应达成什么’），以提升泛化性和可解释性。

Method: 使用一系列图形化对象关系表示场景状态；提出演示分割与池化方法提取操作图并估计各阶段对象状态分布；引入基于预训练视觉特征的对象匹配机制以增强多示教学习的鲁棒性。

Result: 在实验中验证了演示分割的准确性，证明了多示教学习有助于构建最小任务模型；仿真与真实机器人部署结果表明该任务表征具有跨环境可靠执行能力。

Conclusion: 所提方法通过显式建模完整对象交互时序与状态分布，实现了对长时程操作任务的高效、鲁棒与可迁移学习，为‘目标导向’的机器人学习提供了新范式。

Abstract: Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.

</details>


### [120] [Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success](https://arxiv.org/abs/2602.17101)
*Varun Burde,Pavel Burget,Torsten Sattler*

Main category: cs.RO

TL;DR: 本文提出一个基于物理的大规模基准，评估3D重建质量对机器人抓取等下游任务的影响，发现重建误差主要影响抓取候选数量，而对准确位姿下的抓取性能影响甚微；空间误差（尤其是平移误差）是预测抓取成功的关键指标。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建评估仅关注几何精度，无法反映其对下游机器人操作任务（如6D位姿估计和抓取）的实际影响，亟需面向功能效用的评估方法。

Method: 构建基于物理仿真的大规模基准，通过在不同重建mesh上生成抓取位姿，并在真实物体（ground-truth模型）上执行，联合评估位姿误差、抓取鲁棒性与重建几何失真对抓取成功率的影响。

Result: 重建伪影显著减少可行抓取位姿数量，但若位姿估计准确，对最终抓取性能影响可忽略；抓取成功率主要受空间误差（特别是平移误差）主导，该误差对对称物体抓取预测已具足够判别力。

Conclusion: 3D重建质量对机器人操作的影响需从功能角度评估；单纯追求几何精度并非最优路径，应更关注重建结果在下游任务中的实际效用，尤其空间保真度对抓取至关重要。

Abstract: 3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.

</details>


### [121] [Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching](https://arxiv.org/abs/2602.17110)
*Tanisha Parulekar,Ge Shi,Josh Pinskier,David Howard,Jen Jen Chung*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件流匹配（CFM）的框架，将刚性夹爪的抓取姿态映射到柔性Fin-ray夹爪，显著提升了软夹爪对已见/未见物体的抓取成功率，尤其在圆柱体和球体上效果突出，且数据效率高、可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 现有抓取合成方法（如Anygrasp）主要面向刚性平行夹爪，难以适配软夹爪的顺应性行为，导致迁移效果差、模型不准确且需大量数据。

Method: 提出基于条件流匹配（CFM）的生成式姿态映射框架；构建刚-软抓取姿态配对数据集；使用U-Net自编码器以深度图像表征物体几何并作为CFM条件输入，实现从Anygrasp初始姿态到稳定Fin-ray姿态的连续映射。

Result: 在7-DOF机器人上验证：CFM生成姿态对已见/未见物体的抓取成功率分别为34%和46%，远超刚性姿态基线（6%和25%）；圆柱体达50%/100%，球体达25%/31%；具备良好泛化性。

Conclusion: CFM是一种数据高效、性能优越的刚-软抓取策略迁移方法，为其他软体机器人系统提供了可扩展的通用范式。

Abstract: A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.

</details>


### [122] [Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy](https://arxiv.org/abs/2602.17128)
*Huishi Huang,Jack Klusmann,Haozhe Wang,Shuchen Ji,Fengkang Ying,Yiyuan Zhang,John Nassour,Gordon Cheng,Daniela Rus,Jun Liu,Marcelo H Ang,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本文提出了一种基于增强现实（AR）的物理人机交互框架，用于直接遥操作混合刚-软机器人完成抓取任务，并通过真实到仿真的参数识别流程确保虚实一致性。


<details>
  <summary>Details</summary>
Motivation: 混合刚-软机器人在非结构化环境中具有抓取优势，但其协调控制受限于建模、感知与跨域运动学难题。

Method: 设计基于AR头显的遥操作框架，将机器人仿真模型（集成于通用物理引擎）叠加于真实系统之上；提出利用软体机器人几何特性的实-仿参数辨识流程，以统一虚拟与物理行为。

Result: 实现了用户通过AR界面直观进行仿真预演与实时遥操作，提升了混合机器人在简单抓取任务中的可控性与安全性。

Conclusion: 该AR辅助框架及参数辨识方法有效缓解了混合刚-软机器人控制中的建模与协同难题，为自然、安全的人机共融操作提供了可行路径。

Abstract: Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.

</details>


### [123] [Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft](https://arxiv.org/abs/2602.17166)
*Antonio Franchi,Chiara Gabellieri*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人学、坐标无关的固定翼飞机在SO(3)上的逆飞行动力学建模方法，通过几何定义气动方向并强制协调飞行，导出了从轨迹到控制输入的闭式映射，并应用于系留球面平行飞行分析，揭示了气动协调与表观重力的解耦特性。


<details>
  <summary>Details</summary>
Motivation: 将机器人学中的几何建模思想引入航空动力学，构建坐标无关、适用于轨迹规划与可行性验证的逆动力学框架。

Method: 在SO(3)流形上建立坐标无关的逆飞行动力学模型：平动方程用世界坐标系、转动方程用机体坐标系；气动力方向（阻力、升力、侧向力）几何定义；强制无侧滑协调飞行条件，推导轨迹到姿态、角速度、推力/迎角对及气动力矩系数的闭式映射。

Result: 获得系留球面平行飞行下解析的坡度角表达式，发现零坡度特例（系绳张力恰好平衡离心力）；在简单升阻律下给出最小推力迎角的闭式解；点态准定常逆解在轨迹与转动动力学稳态时退化为稳态配平解。

Conclusion: 该框架弥合了航空逆仿真与机器人几何建模之间的鸿沟，为无人机轨迹设计与可行性分析提供了严格而实用的理论基础。

Abstract: We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.

</details>


### [124] [Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place](https://arxiv.org/abs/2602.17199)
*Antonio Rapuano,Yaolei Shen,Federico Califano,Chiara Gabellieri,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出了一种结合高保真偏微分方程（PDE）模型与适用于实时控制的降阶模型（ROM）的空中柔性电缆操纵框架，基于ROM设计了非线性模型预测控制器，实现了电缆振荡抑制与负载挂接/脱离等混合切换控制，并在仿真中验证了其稳定性、效率、鲁棒性及轨迹规划能力。


<details>
  <summary>Details</summary>
Motivation: 实现无人机对悬吊柔性电缆的实时、动力学感知操控，尤其需应对电缆大幅变形、振荡及负载挂接/脱离等复杂动态。

Method: 采用有限差分法离散化基于PDE的高保真电缆模型，并利用本征正交分解（POD）构建保留主导变形模态的降阶模型（ROM）；在此基础上设计非线性模型预测控制（NMPC）策略。

Result: 仿真验证了ROM具备良好稳定性、计算效率和鲁棒性；NMPC能有效抑制电缆振荡、处理混合切换，并支持受限环境下的动力学感知轨迹规划。

Conclusion: 该框架成功实现了面向空中柔性电缆操纵的实时、高保真动力学建模与控制，提升了无人机在复杂任务中的操作能力与适应性。

Abstract: This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.

</details>


### [125] [Multi-session Localization and Mapping Exploiting Topological Information](https://arxiv.org/abs/2602.17226)
*Lorenzo Montano-Olivan,Julio A. Placed,Luis Montano,Maria T. Lazaro*

Main category: cs.RO

TL;DR: 本文提出了一种新型多会话框架，用于在重复访问环境中提升自主系统的建图与定位性能，通过拓扑感知和不确定性感知的决策机制优化地图融合与闭环检测。


<details>
  <summary>Details</summary>
Motivation: 自主系统在重复访问环境中（如自动驾驶、仓库机器人）面临建图与定位误差累积、全局一致性差等挑战。

Method: 基于地图的定位框架，引入拓扑信息与不确定性感知的决策机制，分析位姿图结构识别低连通区域，并选择性触发建图与闭环检测模块，实现新旧地图与位姿图的无缝融合。

Result: 在公开数据集重叠序列及真实类矿井环境中验证了方法有效性，显著降低累积误差，提升全局一致性。

Conclusion: 该多会话框架优于传统贪婪式多SLAM会话方法，为重复访问场景下的鲁棒建图与定位提供了新思路。

Abstract: Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.

</details>


### [126] [FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment](https://arxiv.org/abs/2602.17259)
*Han Zhao,Jingbo Wang,Wenxuan Song,Shuai Chen,Yang Liu,Yan Wang,Haoang Li,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出FRAPPE方法，通过两阶段微调策略（中期预测未来潜在表征、后期并行扩展与多视觉基础模型对齐）来提升VLA模型的世界建模能力，缓解像素级重建偏差和预测误差累积问题，在RoboTwin和真实任务中展现出优越的泛化性与数据效率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在世界建模中存在两个关键问题：训练目标过度强调像素级重建，限制语义学习与泛化；推理时依赖预测的未来观测，易导致误差累积。

Method: 提出Future Representation Alignment via Parallel Progressive Expansion（FRAPPE），采用两阶段微调策略：中期让模型学习预测未来观测的潜在表征；后期并行扩展计算负载，并同步对齐多个不同视觉基础模型的表征。

Result: 在RoboTwin基准和真实世界任务上，FRAPPE优于现有最先进方法，在长时域和未见场景中表现出强泛化能力，并显著提升微调效率、降低对动作标注数据的依赖。

Conclusion: FRAPPE为通用机器人策略提供了可扩展且数据高效的世界感知增强路径，有效缓解了世界建模中的语义学习受限与误差累积问题。

Abstract: Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.

</details>


### [127] [Contact-Anchored Proprioceptive Odometry for Quadruped Robots](https://arxiv.org/abs/2602.17393)
*Minxing Sun,Yao Mao*

Main category: cs.RO

TL;DR: 本文提出了一种仅依赖IMU和电机测量的纯本体感知状态估计算法，通过接触腿作为运动学锚点、足端力矩估计选择可靠接触、高度聚类与时间衰减校正抑制高程漂移，并采用逆运动学容积卡尔曼滤波提升足端速度观测精度，显著降低了无视觉/LiDAR条件下的长时位姿漂移。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人在无相机或LiDAR条件下，因IMU漂移和关节速度传感器噪声导致里程计不可靠。

Method: 提出统一适用于双足、四足和轮腿机器人的纯本体感知状态估计算法：利用关节力矩估计足端接触力以筛选可靠支撑相；将足落地位置作为世界坐标系下的间歇性约束；引入轻量级高度聚类与时间衰减校正机制抑制高程漂移；采用逆运动学容积卡尔曼滤波直接从关节角度与速度估计足端速度；结合多接触几何一致性抑制偏航漂移，并在IMU偏航不可靠时退化为运动学航向参考。

Result: 在四种四足平台（三台Astrall及Unitree Go2 EDU）闭环轨迹测试中，水平与垂直回环误差均显著降低：Astrall A机器人200m水平环误差0.1638m，15m垂直环误差0.219m；轮腿B机器人对应误差为0.2264m和0.199m；轮腿C机器人700m水平环误差7.68m、20m垂直环误差0.540m；Go2 EDU 120m水平环误差2.2138m、8m垂直环误差<0.1m。

Conclusion: 该方法在无外部感知条件下实现了高精度、鲁棒且跨构型适用的本体感知状态估计，有效缓解了IMU和编码器量化带来的各类漂移问题。

Abstract: Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\sim$200\,m horizontal loop and a $\sim$15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a $\sim$700\,m horizontal loop yields 7.68\,m error and a $\sim$20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a $\sim$120\,m horizontal loop with 2.2138\,m error and a $\sim$8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git

</details>


### [128] [Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace](https://arxiv.org/abs/2602.17415)
*Yi Zhang,Omar Faris,Chapa Sirithunge,Kai-Fung Chu,Fumiya Iida,Fulvio Forni*

Main category: cs.RO

TL;DR: 本文提出了一种基于虚拟模型控制（VMC）的去中心化、代理无关且安全感知的人机协作控制框架，通过虚拟弹簧-阻尼器交互替代显式轨迹规划，并利用去中心化力基失速检测与协商机制消除死锁，在实验中将积木放置任务中的机器人卡死率从61.2%降至0。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中因集中式规划导致的死锁、可扩展性差及安全性不足问题，提升协作自然性与鲁棒性。

Method: 采用虚拟模型控制（VMC），将人类与机器人统一嵌入虚拟组件形空间；通过虚拟弹簧-阻尼器实现运动交互；设计去中心化力基失速检测器识别死锁，并引入协商机制解决；全分布式实现，无需结构修改即可扩展。

Result: 实验中将积木放置任务的机器人卡死率从最高61.2%降至0；实现实时双机器人+双人类安全协作（最小间距约20 cm）；仿真验证四机器人场景仍保持安全分离；参数调节可直观塑造机器人行为。

Conclusion: 该框架具备代理无关性、强可扩展性与固有安全性，能实现跨规模团队的无死锁、直觉化人机协作，为实际动态协作场景提供了可行控制范式。

Abstract: We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.

</details>


### [129] [3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing](https://arxiv.org/abs/2602.17421)
*Diana Cafiso,Petr Trunin,Carolina Gay,Lucia Beccai*

Main category: cs.RO

TL;DR: 本文提出了一种3D打印软光学传感器（SOLen），利用变形引起的透镜旋转和焦点位移，实现Y型波导中光功率的差分重分配，从而编码运动方向与幅度；通过材料改性和单层光学表征优化光学性能，并实现了亚毫米级精度打印与可重复信号切换。


<details>
  <summary>Details</summary>
Motivation: 增材制造推动软体机器人几何复杂度提升，亟需兼容单材料、一步成型的传感方案；现有光学软传感器易受杂散光干扰，而传统抑制方法常依赖多材料界面，限制了单步打印兼容性。

Method: 开发基于Y型波导与前置打印透镜的SOLen结构，利用透镜旋转导致焦点平移实现差分光信号输出；改性丙烯酸聚氨酯树脂（添加月桂酸丙烯酯）以提升柔顺性与透光率；结合单层光学表征获取波长相关折射率与透过率，指导仿真设计目标焦距透镜，并实现高保真3D打印。

Result: 成功制备出亚毫米级精度的打印透镜；旋转测试显示多周期内可重复的支路选择性信号切换；建立了从材料表征、光学仿真到打印验证的可迁移‘材料-光学’工作流。

Conclusion: SOLen为单材料、一步式3D打印软体机器人提供了新型光学传感范式，兼具方向与幅值感知能力，所提出的材料-光学协同设计流程具有广泛可扩展性。

Abstract: Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots

</details>


### [130] [A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras](https://arxiv.org/abs/2602.17472)
*Mohamed Sabry,Joseba Gorospe,Cristina Olaverri-Monreal*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、多摄像头兼容的硬件解决方案，用于提升自动驾驶车辆在雨天条件下的感知性能，并支持交通系统的可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 现有雨天感知硬件方案（如亲/疏水镜头、喷雾或工业防护系统）效果有限、成本高、难以规模化部署，而感知可靠性对车路协同等关键应用至关重要。

Method: 设计并实现一种低成本、可同时适配多个摄像头的硬件雨天防护方案，兼容现有相机传感平台，无需更换高成本传感器。

Result: 该系统将深度学习模型的行人检测准确率从8.3%显著提升至41.6%。

Conclusion: 该方案不仅提升了恶劣天气下的感知鲁棒性，还通过模块化升级和资源节约，推动了自动驾驶技术的经济性与可持续性部署。

Abstract: Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.
  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.
  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.

</details>


### [131] [Optically Sensorized Electro-Ribbon Actuator (OS-ERA)](https://arxiv.org/abs/2602.17474)
*Carolina Gay,Petr Trunin,Diana Cafiso,Yuejun Xu,Majid Taghavi,Lucia Beccai*

Main category: cs.RO

TL;DR: 本文提出了一种光学传感化的电-带状执行器（OS-ERA），通过嵌入软光波导传感器实现高精度本体感知，解决了传统电容传感精度不足的问题，并实现了对八种弯曲状态的高保真、快速、可重复分类，具备电压与速度不变性，为闭环控制奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统电-带状执行器（ERA）依赖精度有限的电容式传感，难以实现准确控制，亟需高精度本体感知方案。

Method: 设计并嵌入两条软光波导传感器以捕捉ERA复杂动态曲率；训练分类器映射传感信号以区分八种弯曲状态；在六组独立测试中验证模型性能，并分析信号轨迹与训练流形的一致性及速度/电压鲁棒性。

Result: 传感信号稳定跟随训练流形，预测序列与实际表现一致且具可重复性；即使训练与测试间存在驱动速度差异，信号轨迹形状保持不变，分类准确率始终稳定。

Conclusion: OS-ERA实现了高保真、快速、可重复的弯曲状态分类，突破了ERA长期存在的传感瓶颈，为闭环控制提供了可行路径。

Abstract: Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.

</details>


### [132] [Proximal powered knee placement: a case study](https://arxiv.org/abs/2602.17502)
*Kyle R. Embry,Lorenzo Vianello,Jim Lipsey,Frank Ursetta,Michael Stephens,Zhi Wang,Ann M. Simon,Andrea J. Ikeda,Suzanne B. Finucane,Shawana Anarwala,Levi J. Hargrove*

Main category: cs.RO

TL;DR: 本研究探索了将动力假肢膝关节的动力系统置于大腿上方（而非小腿下方）的可行性，发现该配置可提升行走速度和步频，且在多种运动任务中控制策略稳健，表明合理质量分布比单纯减重更能优化假肢性能。


<details>
  <summary>Details</summary>
Motivation: 动力假肢膝关节虽能改善步态和功能，但其附加质量可能损害步态力学并增加代谢消耗；因此，优化质量分布而非仅追求总质量最小化，可能是更有效的设计策略。

Method: 在小样本受试者中开展探索性研究，对比大腿上方与小腿下方两种动力系统布置方式对行走速度、步频、步态对称性及膝关节运动学的影响，并在坡道和楼梯等多任务场景下验证控制策略的鲁棒性。

Result: 大腿上方布置显著提升一名受试者的行走速度（+9.2%）和步频（+3.6%），膝关节活动范围与峰值速度与下方布置相近；步态对称性结果不一；坡道与楼梯测试证实控制策略具备跨任务适应性。

Conclusion: 大腿上方动力布置在功能上可行，合理质量分布可在保留动力辅助益处的同时缓解附加重量的负面影响；需进一步研究以确认趋势并指导设计与临床应用。

Abstract: Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.

</details>


### [133] [RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments](https://arxiv.org/abs/2602.17515)
*Ziyi Zong,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: 本文提出RA-Nav风险感知导航框架，结合语义分割实时识别障碍物类别并分类为静止、暂时静止和动态三类，设计对应风险估计函数构建局部风险地图，并基于该地图进行风险感知路径搜索与轨迹优化，提升无人机在障碍物状态突变场景下的导航安全性与成功率。


<details>
  <summary>Details</summary>
Motivation: 现有空中机器人导航系统无法适应静态障碍物突然移动的情况，缺乏对环境语义的理解和潜在风险的预估能力。

Method: 提出RA-Nav框架：1）轻量级多尺度语义分割网络实时识别障碍物类别；2）将障碍物分为三类并设计对应风险估计函数；3）构建局部风险地图；4）设计风险感知路径搜索算法；5）进行安全、平滑且动力学可行的轨迹优化。

Result: 在障碍物状态突变场景的对比仿真中，RA-Nav的成功率高于基线方法；并在基于真实世界数据的仿真中进一步验证了其有效性。

Conclusion: RA-Nav通过引入语义感知与分层风险建模，显著提升了无人机在复杂动态环境中的导航鲁棒性与安全性。

Abstract: Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.

</details>


### [134] [IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control](https://arxiv.org/abs/2602.17537)
*Qilong Cheng,Matthew Mackay,Ali Bereyhi*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、轻量级、自主学习驱动的6自由度机器人摄像系统IRIS，通过视觉运动模仿学习（基于ACT）直接从人类示范中学习平滑、目标感知的相机轨迹，无需几何编程，成本低于1000美元，具备约1mm重复精度。


<details>
  <summary>Details</summary>
Motivation: 工业级机器人摄像系统成本高、操作复杂，限制了其广泛应用；亟需一种低成本、易部署、能自主学习复杂运镜的解决方案。

Method: 设计了一款全3D打印的轻量化6-DOF机械臂硬件平台IRIS，并结合目标条件化的视觉运动模仿学习框架（基于Action Chunking with Transformers, ACT），从人类演示中端到端学习相机运动策略。

Result: IRIS系统成本<1000美元，负载1.5kg，重复精度≈1mm；实验证明其能准确跟踪轨迹、稳定自主执行，并泛化至多种电影级运镜动作。

Conclusion: IRIS验证了低成本、学习驱动的机器人摄像系统在影视制作等场景中的可行性与实用性，为普及智能运镜技术提供了新范式。

Abstract: Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.

</details>


### [135] [FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations](https://arxiv.org/abs/2602.17573)
*Konstantinos Foteinos,Georgios Angelidis,Aggelos Psiris,Vasileios Argyriou,Panagiotis Sarigiannidis,Georgios Th. Papadopoulos*

Main category: cs.RO

TL;DR: 本文提出了首个专为消防员手势控制无人地面车辆（UGV）设计的RGBD数据集FR-GESTURE，包含12种基于实战手势的指令、3312组多视角多距离数据，并制定了评估协议与基线实验，数据已公开。


<details>
  <summary>Details</summary>
Motivation: 灾害频发且强度增加，给第一响应者（FRs）带来巨大压力，亟需AI与机器人技术辅助其作业；现有手势控制数据集缺乏面向FRs实际战术需求的专门设计。

Method: 基于真实FRs战术手语设计12类手势指令，经专家反馈优化；采集3312组RGBD图像对，覆盖2个视角和7个距离；构建FR-GESTURE数据集并定义标准评估协议；开展基线实验。

Result: 发布首个面向FRs手势控制UGV的RGBD数据集FR-GESTURE（含3312样本、多视角/距离），建立评估基准，基线实验结果可供后续研究对比提升。

Conclusion: FR-GESTURE填补了面向第一响应者的手势控制UGV数据集空白，为推动人机协同救援中的鲁棒手势识别研究提供了关键资源与评估基础。

Abstract: The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.

</details>


### [136] [Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes](https://arxiv.org/abs/2602.17574)
*Joshua A. Robbins,Andrew F. Thompson,Jonah J. Glunt,Herschel C. Pangborn*

Main category: cs.RO

TL;DR: 本文提出了一种结合混合zonotope与新型ADMM混合整数规划启发式算法的运动规划框架，用于降低嵌入式混合系统规划的计算复杂度并提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 嵌入式混合系统的优化规划因依赖计算密集且数值敏感的混合整数规划而面临挑战。

Method: 提出基于混合zonotope的可达性分析方法，并构建最优规划问题；设计适配该结构的ADMM混合整数规划启发式算法。

Result: 相比现有方法，所生成集合内存复杂度更低、凸松弛更紧；ADMM启发式在收敛速度上优于当前最优混合整数规划启发式；并在自动驾驶行为与运动联合规划中完成嵌入式实验验证。

Conclusion: 该框架显著提升了混合系统在嵌入式平台上的规划效率与鲁棒性，为实时自主决策提供了新工具。

Abstract: Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.

</details>


### [137] [Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space](https://arxiv.org/abs/2602.17586)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: 本文提出Deep-Flow，一种基于最优传输条件流匹配（OT-CFM）的无监督安全关键异常检测框架，通过PCA降维和Early Fusion Transformer建模人类驾驶行为密度，实现对长尾高危场景的精准识别，并在Waymo数据集上验证其优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: Level 4自动驾驶车辆的安全验证受限于传统基于规则的启发式方法难以规模化检测稀有、高风险的长尾场景。

Method: Deep-Flow采用最优传输条件流匹配（OT-CFM）建模专家驾驶行为的连续概率密度；引入PCA瓶颈约束生成过程至低秩谱流形以保障运动学平滑性与精确Jacobian迹计算；使用带车道感知目标条件的Early Fusion Transformer编码器，并通过直连跳过连接保持意图完整性；设计基于运动学复杂度（路径曲折度与加加速度）的加权训练机制。

Result: 在Waymo Open Motion Dataset上达到0.766的AUC-ROC；揭示了运动学危险与语义违规的本质区别；能发现传统安全过滤器忽略的分布外行为（如压线、非规范路口操作）。

Conclusion: Deep-Flow为定义统计意义的安全门限提供了数学严谨基础，支持客观、数据驱动的自动驾驶车队安全部署验证。

Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.

</details>


### [138] [Graph Neural Model Predictive Control for High-Dimensional Systems](https://arxiv.org/abs/2602.17601)
*Patrick Benito Eberhard,Luis Pabon,Daniele Gammelli,Hugo Buurmeijer,Amon Lahr,Mark Leone,Andrea Carron,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出了一种结合图神经网络（GNN）动力学建模与结构利用型模型预测控制（MPC）的框架，用于高维系统（如软体机器人）的实时控制；通过图表示和定制化变量消元算法，实现线性计算复杂度与GPU加速，实现在1000节点系统上100Hz闭环控制，并在硬件实验中实现亚厘米级跟踪精度及全身体避障。


<details>
  <summary>Details</summary>
Motivation: 高维系统（如软体机器人）的控制需要既能准确刻画复杂动力学、又具备计算可行性的模型。

Method: 将系统建模为具有局部相互作用的图结构，采用GNN建模动力学以保持稀疏性；设计专用condensing算法消除状态变量，并利用GPU并行加速；结合结构感知的MPC实现实时优化控制。

Result: 在仿真与真实软体机器人躯干实验中验证：支持最多1000节点系统、闭环频率达100Hz；硬件上实现亚厘米级参考轨迹跟踪（较基线提升63.6%）；并成功完成全身体障碍规避。

Conclusion: 所提GNN-MPC联合框架在保证建模保真度的同时显著提升计算效率，为高维非线性系统的实时闭环控制提供了可扩展、实用的新范式。

Abstract: The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [139] [Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data](https://arxiv.org/abs/2602.16726)
*Hua Yan,Heng Tan,Yu Yang*

Main category: cs.MA

TL;DR: 本文提出M2LSimu框架，利用群体移动性指标指导多提示调整，以在个体轨迹生成中实现群体行为建模，显著优于现有LLM-based方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的个体移动轨迹模拟方法缺乏群体层面的协调机制，无法捕捉集体行为的涌现。

Method: 设计M2LSimu框架，利用共享数据提取的移动性指标作为指导，对个体级提示进行粗粒度到细粒度的多轮调整，在有限预算下同时满足多个群体级移动目标。

Result: 在两个公开数据集上，M2LSimu显著优于当前最先进的基于LLM的移动模拟方法。

Conclusion: 通过引入群体移动性指标引导的提示调整机制，M2LSimu有效弥合了个体模拟与群体行为建模之间的鸿沟，提升了大规模人类移动模拟的真实性与实用性。

Abstract: Large-scale human mobility simulation is critical for many science domains such as urban science, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility trajectories by modeling individual-level cognitive processes. However, these approaches generate individual mobility trajectories independently, without any population-level coordination mechanism, and thus fail to capture the emergence of collective behaviors. To address this issue, we design M2LSimu, a mobility measures-guided multi-prompt adjustment framework that leverages mobility measures derived from shared data as guidance to refine individual-level prompts for realistic mobility generation. Our framework applies coarse-grained adjustment strategies guided by mobility measures, progressively enabling fine-grained individual-level adaptation while satisfying multiple population-level mobility objectives under a limited budget. Experiments show that M2LSimu significantly outperforms state-of-the-art LLM-based methods on two public datasets.

</details>


### [140] [Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance](https://arxiv.org/abs/2602.16738)
*Rebin Saleh,Khanh Pham Dinh,Balázs Villányi,Truong-Son Hy*

Main category: cs.MA

TL;DR: 本文提出SEMAS，一种自演化的分层多智能体系统，用于工业IoT预测性维护中的实时、可解释、低资源开销异常检测。它通过边缘-雾-云三级分工、动态共识投票、PPO策略优化、LLM解释生成与联邦知识聚合，实现在真实工业数据上的高精度、高稳定性与低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统静态离线模型无法适应工况演化，而大语言模型单体系统因内存与延迟过高难以在边缘部署，亟需兼顾实时性、可解释性与资源效率的新型架构。

Method: 提出SEMAS：边缘层轻量特征提取与预过滤；雾层多样化集成检测与动态共识投票；云端基于PPO持续优化策略并支持异步推理；融合LLM生成解释与联邦学习实现知识协同演化。

Result: 在锅炉模拟器和风力涡轮机两个工业基准上，SEMAS显著提升异常检测性能与跨工况稳定性，大幅降低推理延迟，支持真正实时部署；消融实验验证PPO策略演化、共识投票与联邦聚合的关键作用。

Conclusion: 资源感知、自演化的多智能体协同是满足严苛时延与可解释性约束的工业IoT预测性维护落地的关键路径。

Abstract: Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainability and federated knowledge aggregation for adaptive policy distribution. This architecture enables resource-aware specialization without sacrificing real-time performance or model interpretability. Empirical evaluation on two industrial benchmarks (Boiler Emulator and Wind Turbine) demonstrates that SEMAS achieves superior anomaly detection performance with exceptional stability under adaptation, sustains prediction accuracy across evolving operational contexts, and delivers substantial latency improvements enabling genuine real-time deployment. Ablation studies confirm that PPO-driven policy evolution, consensus voting, and federated aggregation each contribute materially to system effectiveness. These findings indicate that resource-aware, self-evolving 1multi-agent coordination is essential for production-ready industrial IoT predictive maintenance under strict latency and explainability constraints.

</details>


### [141] [AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence](https://arxiv.org/abs/2602.16873)
*Geunbin Yu*

Main category: cs.MA

TL;DR: 本文提出AdaptOrch框架，强调多智能体协同编排结构（而非单个模型能力）对系统性能起主导作用，并通过动态选择四种典型拓扑结构，在多个任务上实现12-23%性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型性能趋同，单一模型选型收益递减，而多智能体的协同编排结构（orchestration topology）成为影响系统性能的关键因素。

Method: 提出AdaptOrch框架，包含：(1) 性能收敛标度律；(2) 基于任务DAG的拓扑路由算法（O(|V|+|E|)时间复杂度）；(3) 具有终止保证与一致性评分的自适应合成协议；支持并行、串行、分层和混合四种拓扑的动态选择。

Result: 在SWE-bench、GPQA和RAG等任务上，相比静态单拓扑基线，使用相同底层模型时性能提升12–23%。

Conclusion: 多智能体编排设计应作为独立于模型缩放的一等优化目标。

Abstract: As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.

</details>


### [142] [Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form](https://arxiv.org/abs/2602.17078)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Husheng Li,Ahmed H. Qureshi*

Main category: cs.MA

TL;DR: 本文提出了一种连续时间约束马尔可夫决策过程（CT-CMDP）建模方法及基于物理信息神经网络（PINN）的Actor-Critic框架，用于解决高频率/不规则时序下多智能体安全强化学习问题，显著提升了训练稳定性与安全性性能。


<details>
  <summary>Details</summary>
Motivation: 现有MARL大多基于离散时间MDP，难以适配高频或不规则时间动态；而现有连续时间MARL（CT-MARL）方法基于HJB方程，难以处理含碰撞惩罚等不连续安全约束。

Method: 提出CT-CMDP建模，并通过epigraph重构将离散MDP转化为CT-CMDP；设计基于PINN的连续时间Actor-Critic算法，嵌入物理动力学先验以稳定优化。

Result: 在连续时间安全多粒子环境（MPE）和多智能体MuJoCo基准上验证：价值函数逼近更平滑、训练更稳定、安全性与整体性能优于现有安全MARL基线。

Conclusion: CT-CMDP建模与PINN驱动的Actor-Critic框架为连续时间、带安全约束的多智能体学习提供了有效且鲁棒的新范式。

Abstract: Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Bellman (HJB) equations. However, they rarely account for safety constraints such as collision penalties, since these introduce discontinuities that make HJB-based learning difficult. To address this challenge, we propose a continuous-time constrained MDP (CT-CMDP) formulation and a novel MARL framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. We then solve this by proposing a novel physics-informed neural network (PINN)-based actor-critic method that enables stable and efficient optimization in continuous time. We evaluate our approach on continuous-time safe multi-particle environments (MPE) and safe multi-agent MuJoCo benchmarks. Results demonstrate smoother value approximations, more stable training, and improved performance over safe MARL baselines, validating the effectiveness and robustness of our method.

</details>


### [143] [AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation](https://arxiv.org/abs/2602.17100)
*Siyu Wang,Ruotian Lu,Zhihao Yang,Yuchao Wang,Yanzhou Zhang,Lei Xu,Qimin Xu,Guojun Yin,Cailian Chen,Xinping Guan*

Main category: cs.MA

TL;DR: 本文提出AgentConductor，一种基于强化学习优化、由LLM驱动的多智能体系统，通过动态生成任务自适应的交互拓扑结构（密度感知的分层有向无环图），显著提升代码生成性能并降低通信与计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的多智能体系统无法根据任务难度自适应调整拓扑密度，也不能利用执行反馈在单次推理中迭代优化拓扑，导致冗余通信和性能瓶颈。

Method: 提出AgentConductor框架，核心是一个LLM-based orchestrator agent；其关键创新包括：1）设计通信感知的拓扑密度函数；2）采用难度区间划分策略以精确控制各难度下的拓扑密度上限；对每个查询，自动推断角色与难度，并构建任务适配的分层DAG拓扑。

Result: 在三个竞赛级和两个基础代码数据集上，AgentConductor达到SOTA准确率，相比最强基线最高提升14.6%（pass@1）、降低拓扑密度13%、减少token成本68%。

Conclusion: 动态、反馈驱动且难度感知的拓扑生成机制能有效提升多智能体协作效率与性能，为LLM-MAS设计提供了新范式。

Abstract: Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction.

</details>


### [144] [Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation](https://arxiv.org/abs/2602.17203)
*Yuhong Luo,Daniel Schoepflin,Xintong Wang*

Main category: cs.MA

TL;DR: 本文提出一种元博弈设计方法，用于在测试时约束下分析算法合谋风险，通过预训练策略与实时适应规则的组合，评估强化学习和大语言模型策略在重复定价博弈中的合谋可行性及策略有效性。


<details>
  <summary>Details</summary>
Motivation: 现有算法合谋研究依赖长学习周期、对手理性假设及参数与环境对称性，难以反映真实测试时场景下的合谋风险，亟需更贴近实际的评估框架。

Method: 构建元博弈框架，将智能体建模为具备不同策略特性的预训练策略（如竞争型、朴素合作型、鲁棒合谋型），并设计元策略（初始策略+在线适应规则）；采样多组元策略构成的经验博弈，计算收益、遗憾等统计量，并构建经验最优响应图以揭示策略关系。

Result: 在对称与非对称成本设置的重复定价博弈中，验证了算法合谋可在理性选择与共适应过程中出现；RL与LLM策略均表现出不同程度的合谋倾向与定价有效性。

Conclusion: 算法合谋在测试时约束下具有现实可行性，元博弈方法为评估合谋风险提供了新范式，支持监管干预的必要性，尤其在非对称、有限适应场景下。

Abstract: The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies with distinct strategic characteristics (e.g., competitive, naively cooperative, robustly collusive), and formulate the problem as selecting a meta-strategy that combines a pretrained, initial policy with an in-game adaptation rule. We seek to examine whether collusion can emerge under rational choices and how agents co-adapt toward cooperation or competition. To this end, we sample normal-form empirical games over meta-strategy profiles, % across random initial game states, compute relevant game statistics (e.g., payoffs against individuals and regret against an equilibrium mixture of opponents), and construct empirical best-response graphs to uncover strategic relationships. We evaluate both reinforcement-learning and LLM-based strategies in repeated pricing games under symmetric and asymmetric cost settings, and present findings on the feasibility of algorithmic collusion and the effectiveness of pricing strategies in practical ``test-time'' environments.
  The source code and the full paper with appendix are available at: https://github.com/chailab-rutgers/CollusionMetagame.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [MMCAformer: Macro-Micro Cross-Attention Transformer for Traffic Speed Prediction with Microscopic Connected Vehicle Driving Behavior](https://arxiv.org/abs/2602.16730)
*Lei Han,Mohamed Abdel-Aty,Younggun Kim,Yang-Jun Joo,Zubayer Islam*

Main category: cs.LG

TL;DR: 本文提出MMCAformer模型，融合宏观交通流数据与微观驾驶行为特征（来自网联车辆数据），利用自注意力和交叉注意力机制建模时空依赖关系，并采用Student-t损失进行点预测与不确定性估计，在佛罗里达四条高速公路实验中显著提升速度预测精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于宏观交通流数据预测速度，忽略了微观个体驾驶行为的影响；而网联车辆（CV）数据提供了丰富的微观驾驶行为特征，为提升预测性能带来新机遇。

Method: 提出Macro-Micro Cross-Attention Transformer（MMCAformer）：用自注意力建模宏观交通内在依赖，用交叉注意力捕捉宏观状态与微观驾驶行为之间的时空交互；采用Student-t负对数似然损失进行端到端优化，实现点预测与不确定性估计。

Result: 在四条佛州高速上，相比仅用宏观特征的方法，引入微观特征使RMSE、MAE、MAPE分别降低9.0%、6.9%、10.2%，预测区间宽度减少10.1–24.0%；硬制动与加速频率是最具影响力的微观特征，且在拥堵低速条件下增益更显著。

Conclusion: 融合微观驾驶行为特征能有效提升交通速度预测的准确性与鲁棒性，尤其在复杂交通场景下，验证了宏观-微观协同建模的必要性与有效性。

Abstract: Accurate speed prediction is crucial for proactive traffic management to enhance traffic efficiency and safety. Existing studies have primarily relied on aggregated, macroscopic traffic flow data to predict future traffic trends, whereas road traffic dynamics are also influenced by individual, microscopic human driving behaviors. Recent Connected Vehicle (CV) data provide rich driving behavior features, offering new opportunities to incorporate these behavioral insights into speed prediction. To this end, we propose the Macro-Micro Cross-Attention Transformer (MMCAformer) to integrate CV data-based micro driving behavior features with macro traffic features for speed prediction. Specifically, MMCAformer employs self-attention to learn intrinsic dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. MMCAformer is optimized with a Student-t negative log-likelihood loss to provide point-wise speed prediction and estimate uncertainty. Experiments on four Florida freeways demonstrate the superior performance of the proposed MMCAformer compared to baselines. Compared with only using macro features, introducing micro driving behavior features not only enhances prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) but also shrinks model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across the four freeways). Results reveal that hard braking and acceleration frequencies emerge as the most influential features. Such improvements are more pronounced under congested, low-speed traffic conditions.

</details>


### [146] [A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets](https://arxiv.org/abs/2602.16735)
*Saud Alghumayjan,Ming Yi,Bolun Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型（LLMs）的少样本分类框架，用于预测实时电价次日是否会出现尖峰，通过将系统状态信息转化为自然语言提示输入LLM，在数据稀缺场景下表现优于传统监督学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决电力市场中电价尖峰预测任务在历史数据有限时性能不佳的问题，探索LLM在少样本、高不确定性能源预测场景中的潜力。

Method: 将电力需求、可再生能源出力、天气预报和近期电价等系统状态信息聚合为统计特征，并格式化为自然语言提示，结合通用指令输入大语言模型（LLM），由LLM输出次日是否为尖峰日的概率及置信度。

Result: 在得克萨斯电力市场历史数据上验证，该少样本LLM方法性能媲美SVM和XGBoost等监督学习模型，且在历史数据受限时显著优于二者。

Conclusion: 大语言模型可作为数据高效工具用于稀缺数据场景下的电价尖峰分类，展现出在能源领域少样本预测任务中的实用价值。

Abstract: This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.

</details>


### [147] [Real-time Secondary Crash Likelihood Prediction Excluding Post Primary Crash Features](https://arxiv.org/abs/2602.16739)
*Lei Han,Mohamed Abdel-Aty,Zubayer Islam,Chenzhu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种不依赖事后事故特征的混合式二次事故发生概率预测框架，通过动态时空窗口提取实时交通流与环境特征，并结合三种模型与集成学习策略，在佛罗里达州高速公路数据上实现了91%的二次事故识别率和0.952的AUC。


<details>
  <summary>Details</summary>
Motivation: 现有二次事故预测方法主要依赖于难以实时获取的事故后特征（如事故类型和严重程度），限制了其在主动交通管理系统中的实际应用。

Method: 设计动态时空窗口提取主事故点及其上游路段的实时交通流与环境特征；构建包含一个主事故模型和两个二次事故模型的混合框架；采用六种机器学习算法的集成学习策略及基于投票的融合机制。

Result: 在佛罗里达州高速公路数据上，二次事故识别率达91%，误报率为0.20；AUC从各单模型的0.654、0.744、0.902提升至混合模型的0.952。

Conclusion: 所提出的混合框架显著提升了二次事故实时预测性能，具有更强的实用性与鲁棒性，优于以往研究。

Abstract: Secondary crash likelihood prediction is a critical component of an active traffic management system to mitigate congestion and adverse impacts caused by secondary crashes. However, existing approaches mainly rely on post-crash features (e.g., crash type and severity) that are rarely available in real time, limiting their practical applicability. To address this limitation, we propose a hybrid secondary crash likelihood prediction framework that does not depend on post-crash features. A dynamic spatiotemporal window is designed to extract real-time traffic flow and environmental features from primary crash locations and their upstream segments. The framework includes three models: a primary crash model to estimate the likelihood of secondary crash occurrence, and two secondary crash models to evaluate traffic conditions at crash and upstream segments under different comparative scenarios. An ensemble learning strategy integrating six machine learning algorithms is developed to enhance predictive performance, and a voting-based mechanism combines the outputs of the three models. Experiments on Florida freeways demonstrate that the proposed hybrid framework correctly identifies 91% of secondary crashes with a low false alarm rate of 0.20. The Area Under the ROC Curve improves from 0.654, 0.744, and 0.902 for the individual models to 0.952 for the hybrid model, outperforming previous studies.

</details>


### [148] [Quantifying LLM Attention-Head Stability: Implications for Circuit Universality](https://arxiv.org/abs/2602.16740)
*Karan Bali,Jack Stanley,Praneet Suresh,Danilo Bzdok*

Main category: cs.LG

TL;DR: 本文系统研究了不同初始化训练的Transformer语言模型中注意力头表示学习的稳定性，发现中间层注意力头最不稳定但最具表征区分性，深层模型中深度中段发散更强，且不稳定的深层注意力头功能更重要；权重衰减能显著提升注意力头跨初始化的稳定性，残差流则相对稳定。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性研究中，对Transformer‘电路’在不同模型实例间稳定性的检验不足，影响其在安全关键场景中的可信度。

Method: 通过在不同规模的Transformer语言模型上进行多次独立初始化训练，逐层量化注意力头表示学习的跨拟合稳定性，并分析权重衰减等优化策略的影响。

Result: （1）中间层注意力头最不稳定但表征最独特；（2）深层模型中中段层发散更显著；（3）深层中不稳定的注意力头功能更重要；（4）权重衰减显著提升稳定性；（5）残差流整体稳定。

Conclusion: 电路跨实例鲁棒性是可扩展AI监督的必要前提，应成为机制可解释性研究的核心评估维度。

Abstract: In mechanistic interpretability, recent work scrutinizes transformer "circuits" - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confidence in safety-critical settings. Here, we systematically study stability across-refits in increasingly complex transformer language models of various sizes. We quantify, layer by layer, how similarly attention heads learn representations across independently initialized training runs. Our rigorous experiments show that (1) middle-layer heads are the least stable yet the most representationally distinct; (2) deeper models exhibit stronger mid-depth divergence; (3) unstable heads in deeper layers become more functionally important than their peers from the same layer; (4) applying weight decay optimization substantially improves attention-head stability across random model initializations; and (5) the residual stream is comparatively stable. Our findings establish the cross-instance robustness of circuits as an essential yet underappreciated prerequisite for scalable oversight, drawing contours around possible white-box monitorability of AI systems.

</details>


### [149] [DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning](https://arxiv.org/abs/2602.16742)
*Haoxiang Sun,Lizhen Xu,Bing Zhao,Wotao Yin,Wei Wang,Boyu Yang,Rui Wang,Hu Wei*

Main category: cs.LG

TL;DR: 本文提出了DeepVision-103K数据集，用于增强大视觉语言模型在强化学习与可验证奖励（RLVR）框架下的多模态数学推理能力，显著提升了模型在多模态任务上的性能与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据集规模小、多样性不足，主要依赖人工构建或资源重组，限制了模型性能进一步提升。

Method: 构建了覆盖K12数学主题、知识点丰富、视觉元素多样的大规模数据集DeepVision-103K，并基于该数据集开展RLVR训练。

Result: 在多模态数学基准测试中表现优异，并能有效泛化到通用多模态推理任务；分析表明模型的视觉感知、反思与推理能力得到增强。

Conclusion: DeepVision-103K是推进多模态推理能力的有效数据资源，为RLVR训练提供了高质量、高覆盖的支撑。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: \href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.

</details>


### [150] [PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency](https://arxiv.org/abs/2602.16745)
*Zhangyi Liu,Huaizhi Qu,Xiaowei Yin,He Sun,Yanjun Han,Tianlong Chen,Zhun Deng*

Main category: cs.LG

TL;DR: 本文提出PETS方法，通过优化推理轨迹分配实现高效测试时自一致性，在离线和在线场景下均显著降低采样预算并提升性能。


<details>
  <summary>Details</summary>
Motivation: 在有限预算下实现样本高效的测试时自一致性仍具挑战性。

Method: 提出PETS框架，定义自一致性率作为核心指标，将推理轨迹建模为众包工作者，在离线设置中借鉴众包理论设计多数投票分配算法；在在线流式设置中提出适配问题难度的动态预算分配方法。

Result: 在GPQA数据集上，PETS在离线和在线设置下均达到完美自一致性，采样预算分别减少75%和55%，优于均匀分配。

Conclusion: PETS为测试时自一致性提供了理论严谨且计算高效的解决方案，兼顾理论保证与实际性能。

Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.

</details>


### [151] [Linear Convergence in Games with Delayed Feedback via Extra Prediction](https://arxiv.org/abs/2602.17486)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 本文研究了在存在反馈延迟的多智能体学习中，加权乐观梯度下降-上升（WOGDA）算法在线性收敛率方面的表现，并证明了额外乐观性可以显著加速收敛并缓解延迟带来的性能下降。


<details>
  <summary>Details</summary>
Motivation: 反馈延迟在现实多智能体学习中不可避免，会严重损害性能，且其在双线性博弈中的收敛速率尚不明确。

Method: 将WOGDA解释为对Extra Proximal Point（EPP）的近似，通过引入额外乐观性（预测更远未来的奖励）来分析其收敛性，并与标准乐观性（仅预测下一步奖励）进行对比。

Result: 标准乐观性下收敛速率为exp(-Θ(t/m⁵))；引入额外乐观性后，收敛速率提升至exp(-Θ(t/(m²log m)))，且允许更大的步长。实验验证了理论结果。

Conclusion: 额外乐观性是一种应对反馈延迟导致性能下降的有效策略。

Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-Θ(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-Θ(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.

</details>


### [152] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文通过几何分析揭示了小规模算法任务中'grokking'现象（从记忆到泛化的延迟转变）的机制，发现训练动态主要局限于低维执行子空间，且正交方向上的曲率增长先于泛化发生并遵循幂律关系，表明grokking本质上是逃离由低维约束和横向曲率累积导致的亚稳态过程。


<details>
  <summary>Details</summary>
Motivation: grokking现象仍缺乏深入理解，本文旨在从几何角度揭示其优化动力学机制。

Method: 对训练于模运算任务的Transformer模型进行主成分分析（PCA）以识别注意力权重轨迹的低维执行子空间，并测量梯度步间交换子缺陷（commutator defects）以刻画损失景观曲率，再将其投影到该子空间；结合因果干预实验验证子空间运动与曲率变化对grokking的必要性与充分性。

Result: 训练轨迹高度集中于低维执行子空间（首主成分解释68–83%方差）；正交方向曲率在泛化前显著增长，且增长时序与grokking时间尺度呈幂律关系；抑制正交梯度流可阻止泛化，而人为增强曲率无效果。

Conclusion: grokking源于模型从低维执行子空间内亚稳态向泛化状态的逃逸过程，其关键驱动力是沿学习到的执行子空间的运动，而非单纯曲率增加；该几何机制在不同学习率、正则化及随机种子下具鲁棒性。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [153] [LiveClin: A Live Clinical Benchmark without Leakage](https://arxiv.org/abs/2602.16747)
*Xidong Wang,Shuqi Guo,Yue Shen,Junying Chen,Jian Wang,Jinjie Gu,Ping Zhang,Lei Liu,Benyou Wang*

Main category: cs.LG

TL;DR: 本文提出LiveClin，一个基于最新临床案例报告的动态医学大语言模型评估基准，旨在解决数据污染和知识过时问题，提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有医学大语言模型评估因数据污染和知识过时导致静态基准分数虚高，无法反映真实临床能力。

Method: 构建LiveClin活体基准：基于同行评审的最新病例报告，每半年更新；通过239名医生参与的AI-人工验证流程，将真实病例转化为覆盖完整临床路径的多模态评估场景。

Result: 在LiveClin上评估26个模型，表现最佳模型的病例准确率仅为35.7%；主任医师和主治医师准确率显著高于多数模型。

Conclusion: LiveClin提供了一个持续演进、临床导向的评估框架，有助于推动医学大语言模型向更可靠、更具现实实用性的方向发展。

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

</details>


### [154] [Attending to Routers Aids Indoor Wireless Localization](https://arxiv.org/abs/2602.16762)
*Ayush Roy,Tahsin Fuad Hassan,Roshan Ayyalasomayajula,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 本文提出了一种名为'Attention to Routers'的新方法，通过在Wi-Fi定位架构中引入路由器注意力机制，对不同路由器的信息进行差异化加权聚合，显著提升了定位精度，在开源数据集上较基准模型提升超30%。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi定位算法在多路由器信息聚合时未合理加权，导致收敛性差、精度低。

Method: 将注意力机制引入标准机器学习定位架构，对各路由器贡献进行动态加权，以改进三角定位效果。

Result: 在开源数据集上的实验表明，该方法比基准架构定位精度提升超过30%。

Conclusion: 路由器注意力机制能有效提升Wi-Fi信号定位性能，为无线定位提供了新思路。

Abstract: Modern machine learning-based wireless localization using Wi-Fi signals continues to face significant challenges in achieving groundbreaking performance across diverse environments. A major limitation is that most existing algorithms do not appropriately weight the information from different routers during aggregation, resulting in suboptimal convergence and reduced accuracy. Motivated by traditional weighted triangulation methods, this paper introduces the concept of attention to routers, ensuring that each router's contribution is weighted differently when aggregating information from multiple routers for triangulation. We demonstrate, by incorporating attention layers into a standard machine learning localization architecture, that emphasizing the relevance of each router can substantially improve overall performance. We have also shown through evaluation over the open-sourced datasets and demonstrate that Attention to Routers outperforms the benchmark architecture by over 30% in accuracy.

</details>


### [155] [Machine Learning Argument of Latitude Error Model for LEO Satellite Orbit and Covariance Correction](https://arxiv.org/abs/2602.16764)
*Alex Moody,Penina Axelrad,Rebecca Russell*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习方法，用于校正低地球轨道（LEO）卫星在平近点角（argument of latitude）上的传播误差增长，从而提升轨道传播精度并延长矢量协方差信息（VCM）的有效使用时间。


<details>
  <summary>Details</summary>
Motivation: 传统LEO卫星PNT服务中假设传播误差服从高斯分布，但大气阻力建模误差会迅速破坏该假设，亟需更准确的不确定性建模方法。

Method: 采用时间条件化神经网络和高斯过程两种机器学习模型，基于开源轨道传播器与公开VCM星历数据，学习并预测argument of latitude的误差分布（均值与协方差），并将一维校正映射至笛卡尔状态空间。

Result: 所提方法显著抑制了由阻力误建模引起的误差增长，使高斯假设在更长时间尺度上保持有效；仅沿主导误差增长方向更新统计信息，其余维度仍保留物理传播的协方差。

Conclusion: 该机器学习校正框架可无缝集成至现有VCM轨道传播系统，在不修改原有传播器功能的前提下，拓展其在长时域PNT应用中的实用性与可靠性。

Abstract: Low Earth orbit (LEO) satellites are leveraged to support new position, navigation, and timing (PNT) service alternatives to GNSS. These alternatives require accurate propagation of satellite position and velocity with a realistic quantification of uncertainty. It is commonly assumed that the propagated uncertainty distribution is Gaussian; however, the validity of this assumption can be quickly compromised by the mismodeling of atmospheric drag. We develop a machine learning approach that corrects error growth in the argument of latitude for a diverse set of LEO satellites. The improved orbit propagation accuracy extends the applicability of the Gaussian assumption and modeling of the errors with a corrected mean and covariance. We compare the performance of a time-conditioned neural network and a Gaussian Process on datasets computed with an open source orbit propagator and publicly available Vector Covariance Message (VCM) ephemerides. The learned models predict the argument of latitude error as a Gaussian distribution given parameters from a single VCM epoch and reverse propagation errors. We show that this one-dimensional model captures the effect of mismodeled drag, which can be mapped to the Cartesian state space. The correction method only updates information along the dimensions of dominant error growth, while maintaining the physics-based propagation of VCM covariance in the remaining dimensions. We therefore extend the utility of VCM ephemerides to longer time horizons without modifying the functionality of the existing propagator.

</details>


### [156] [Omitted Variable Bias in Language Models Under Distribution Shift](https://arxiv.org/abs/2602.16784)
*Victoria Lin,Louis-Philippe Morency,Eli Ben-Michael*

Main category: cs.LG

TL;DR: 本文提出了一种新框架，用于量化语言模型在分布偏移下的最坏泛化性能，特别关注由未观测变量引起的偏差，并通过实验验证其在评估与优化中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在分布偏移下表现脆弱，现有方法仅处理可观测的分布偏移，而忽略了未观测变量导致的遗漏变量偏差，影响评估与优化的可靠性。

Method: 将分布偏移分解为可观测与不可观测两部分，建立未观测变量强度与最坏情况泛化性能界之间的映射关系，并将其用于模型评估与优化。

Result: 实验表明，该框架能提供更合理的OOD评估指标，提升真实OOD性能，并支持在目标分布标签可用时推断未观测变量的强度。

Conclusion: 考虑未观测变量对分布偏移的影响至关重要；所提框架为语言模型的鲁棒性建模、评估与优化提供了理论支撑与实用工具。

Abstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.

</details>


### [157] [Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency](https://arxiv.org/abs/2602.16787)
*Victoria Lin,Xinnuo Xu,Rachel Lawrence,Risa Ueno,Amit Sharma,Javier Gonzalez,Niranjani Prasad*

Main category: cs.LG

TL;DR: 本文提出了一种无需标注数据的轻量级推理时方法——双重反事实一致性（DCC），用于评估和提升大语言模型（LLMs）的因果推理能力，涵盖因果干预与反事实预测，并在多个模型和任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在反事实问题上表现脆弱，暴露因果推理缺陷；而构建大规模标注反事实数据成本高、覆盖有限。

Method: 提出双重反事实一致性（DCC）方法，在推理时通过一致性检验模型对因果干预和反事实预测的双重能力，无需训练或标注数据。

Result: DCC可有效评估多种主流LLM的因果推理能力，并作为训练无关的测试时拒绝采样准则，显著提升多类模型在推理任务上的性能。

Conclusion: DCC是一种高效、通用且无需训练的推理时机制，为衡量和增强LLM因果推理能力提供了新范式。

Abstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate the effectiveness of DCC as a training-free test-time rejection sampling criterion and show that it can directly improve performance on reasoning tasks across multiple model families.

</details>


### [158] [Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models](https://arxiv.org/abs/2602.16793)
*Xingyu Dang,Rohit Agarwal,Rodrigo Porto,Anirudh Goyal,Liam H Fowl,Sanjeev Arora*

Main category: cs.LG

TL;DR: 本文提出了一种低成本、高性能的IMO风格数学问题求解推理流水线，通过'猜想提取'和'上下文分离'机制克服求解器-评分器流水线中的'Cognitive Well'失败模式，在IMO-ProofBench Advanced上以31美元/题的成本达到67.1%准确率，为当时SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽在IMO数学问题上达到金牌水平，但依赖定制模型或大规模推理，成本极高（如每题3000美元），亟需一种使用通用现成大模型、低成本且高性能的推理方案。

Method: 提出一种新推理流水线，核心是识别并解决'solver-grader pipeline'中因迭代优化陷入局部正确解（Cognitive Well）的问题；关键创新是'猜想提取'：从生成解中分离候选引理，并在全新环境中独立验证其本身及否定形式（即上下文分离）。

Result: 在IMO-ProofBench Advanced（PB-Adv）上，使用Gemini 3.0 Pro实现67.1%准确率，平均单题推理成本约31美元；性能超越所有已公开及未公开模型，是次优公开流水线成功率的两倍以上，成本仅为极小比例。

Conclusion: 仅用通用现成大模型即可实现IMO级数学推理的高性价比突破，'猜想提取'与'上下文分离'是提升鲁棒性与正确率的关键机制，为可信赖数学AI提供了新范式。

Abstract: In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only general-purpose off-the-shelf models. Our method relies on insights about grader failure in solver-grader pipelines, which we call the Cognitive Well (iterative refinement converging to a wrong solution that the solver as well as the pipeline's internal grader consider to be basically correct). Our pipeline addresses these failure modes through conjecture extraction, wherein candidate lemmas are isolated from generated solutions and independently verified alongside their negations in a fresh environment (context detachment). On IMO-ProofBench Advanced (PB-Adv), our pipeline achieves 67.1 percent performance using Gemini 3.0 Pro with an average cost per question of approximately 31 USD. At the time of evaluation, this represented the state-of-the-art on PB-Adv among both public and unreleased models, and more than doubles the success rate of the next best publicly accessible pipeline, all at a fraction of the cost.

</details>


### [159] [Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning](https://arxiv.org/abs/2602.16796)
*Zifan Wang,Riccardo De Santi,Xiaoyu Mo,Michael M. Zavlanos,Andreas Krause,Karl H. Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种名为Tail-aware Flow Fine-Tuning (TFFT) 的新方法，用于在微调预训练扩散与流模型时控制生成分布的尾部行为，通过条件风险价值（CVaR）实现对高奖励（右尾）或低奖励（左尾）区域的定向优化，且计算开销与标准期望奖励微调相当。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵正则化的微调方法仅最大化期望奖励，缺乏对分布尾部（如低奖励失败或高奖励稀有样本）的可控调节能力，而尾部控制对可靠性与新颖性发现至关重要。

Method: 提出TFFT算法，基于CVaR的变分对偶形式，将尾部优化分解为两个阶段：1）轻量级一维阈值优化；2）使用特定伪奖励进行单次熵正则化微调，从而高效实现左/右CVaR目标。

Result: TFFT在图像生成和分子设计等高维任务中验证有效，计算成本与标准期望微调相近，同时显著提升尾部性能（如减少低分样本、增强高分样本采样）。

Conclusion: TFFT为分布式微调提供了原理清晰、高效实用的新范式，实现了对生成分布尾部的精确、可解释、低成本控制。

Abstract: Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a principled and efficient distributional fine-tuning algorithm based on the Conditional Value-at-Risk (CVaR). We address two distinct tail-shaping goals: right-CVaR for seeking novel samples in the high-reward tail and left-CVaR for controlling worst-case samples in the low-reward tail. Unlike prior approaches that rely on non-linear optimization, we leverage the variational dual formulation of CVaR to decompose it into a decoupled two-stage procedure: a lightweight one-dimensional threshold optimization step, and a single entropy-regularized fine-tuning process via a specific pseudo-reward. This decomposition achieves CVaR fine-tuning efficiently with computational cost comparable to standard expected fine-tuning methods. We demonstrate the effectiveness of TFFT across illustrative experiments, high-dimensional text-to-image generation, and molecular design.

</details>


### [160] [TopoFlow: Physics-guided Neural Networks for high-resolution air quality prediction](https://arxiv.org/abs/2602.16821)
*Ammar Kheder,Helmi Toropainen,Wenqing Peng,Samuel Antão,Jia Chen,Zhi-Song Liu,Michael Boy*

Main category: cs.LG

TL;DR: 本文提出了一种名为TopoFlow的物理引导神经网络，通过引入地形感知注意力和风向引导的块重排序机制，显著提升了高分辨率空气质量预测精度，尤其在PM2.5预测上RMSE达9.71 ug/m3，较现有系统提升71–80%。


<details>
  <summary>Details</summary>
Motivation: 现有空气质量预测模型缺乏对地形与风向等关键物理因素的显式建模，导致在复杂地形区域预测精度不足。

Method: 基于视觉Transformer架构，设计了地形感知注意力机制（建模地形诱导的污染流模式）和风向引导的patch重排序机制（使空间表征与主导风向对齐），并利用六年高分辨率再分析数据（融合超1400个监测站观测）进行训练。

Result: PM2.5预测RMSE为9.71 ug/m3，较业务预报系统提升71–80%，较SOTA AI基线提升13%；误差始终低于中国24小时空气质量标准限值（75 ug/m3）；对四大主要污染物及12–96小时预报时效均保持稳定优势。

Conclusion: 将物理先验知识以可微、结构化方式嵌入深度学习框架，能实质性推动空气质量预测性能边界，为物理信息AI提供有效范式。

Abstract: We propose TopoFlow (Topography-aware pollutant Flow learning), a physics-guided neural network for efficient, high-resolution air quality prediction. To explicitly embed physical processes into the learning framework, we identify two critical factors governing pollutant dynamics: topography and wind direction. Complex terrain can channel, block, and trap pollutants, while wind acts as a primary driver of their transport and dispersion. Building on these insights, TopoFlow leverages a vision transformer architecture with two novel mechanisms: topography-aware attention, which explicitly models terrain-induced flow patterns, and wind-guided patch reordering, which aligns spatial representations with prevailing wind directions. Trained on six years of high-resolution reanalysis data assimilating observations from over 1,400 surface monitoring stations across China, TopoFlow achieves a PM2.5 RMSE of 9.71 ug/m3, representing a 71-80% improvement over operational forecasting systems and a 13% improvement over state-of-the-art AI baselines. Forecast errors remain well below China's 24-hour air quality threshold of 75 ug/m3 (GB 3095-2012), enabling reliable discrimination between clean and polluted conditions. These performance gains are consistent across all four major pollutants and forecast lead times from 12 to 96 hours, demonstrating that principled integration of physical knowledge into neural networks can fundamentally advance air quality prediction.

</details>


### [161] [Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees](https://arxiv.org/abs/2602.16823)
*Itamar Hadad,Guy Katz,Shahaf Bassan*

Main category: cs.LG

TL;DR: 本文提出了一套基于神经网络验证技术的自动化电路发现算法，为神经网络内部组件识别提供可证明的保证，涵盖输入域鲁棒性、鲁棒修补性和最小性三类保证，并揭示了它们之间的理论联系，实验表明其电路具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法依赖启发式或近似，缺乏在连续输入域上的可证明保证。

Method: 利用最新的神经网络验证技术，设计三类具有可证明保证的自动化算法：输入域鲁棒性、鲁棒修补性与最小性，并分析其理论联系。

Result: 在多个视觉模型上实验表明，所提算法生成的电路比标准方法具备更强的鲁棒性保证。

Conclusion: 本工作为可证明的电路发现奠定了原理性基础，推动了机制可解释性的严谨化发展。

Abstract: *Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yield circuits with *provable guarantees*. We focus on three types of guarantees: (1) *input domain robustness*, ensuring the circuit agrees with the model across a continuous input region; (2) *robust patching*, certifying circuit alignment under continuous patching perturbations; and (3) *minimality*, formalizing and capturing a wide array of various notions of succinctness. Interestingly, we uncover a diverse set of novel theoretical connections among these three families of guarantees, with critical implications for the convergence of our algorithms. Finally, we conduct experiments with state-of-the-art verifiers on various vision models, showing that our algorithms yield circuits with substantially stronger robustness guarantees than standard circuit discovery methods, establishing a principled foundation for provable circuit discovery.

</details>


### [162] [HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind](https://arxiv.org/abs/2602.16826)
*Nigel Doering,Rahath Malladi,Arshia Sangwan,David Danks,Tauhidur Rahman*

Main category: cs.LG

TL;DR: 本文提出了HiVAE，一种分层变分自编码器架构，用于在真实时空域中扩展心智理论（ToM）推理能力，并在校园导航任务上验证了其有效性，但指出其潜在表征缺乏对实际心理状态的显式映射，进而提出自监督对齐策略以促进社区对‘接地’方法的讨论。


<details>
  <summary>Details</summary>
Motivation: 现有心智理论（ToM）方法局限于小规模、人类可理解的网格世界，难以扩展到真实、复杂的时空场景；需构建可扩展且认知可解释的ToM模型。

Method: 提出HiVAE——一种受人类信念-欲望-意图（BDI）认知结构启发的三层变分自编码器（VAE）层次架构，并引入自监督对齐策略以提升潜在表征与真实心理状态的对应性。

Result: 在含3185个节点的校园导航任务上，HiVAE相较基线显著提升了预测性能；但分析发现其学习到的潜在表征仍缺乏对实际心理状态（如信念、欲望）的显式语义接地。

Conclusion: HiVAE成功将ToM推理扩展至现实规模时空任务，但潜在表征的语义接地仍是关键挑战；作者呼吁社区共同探索更有效的接地机制。

Abstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.

</details>


### [163] [Learning under noisy supervision is governed by a feedback-truth gap](https://arxiv.org/abs/2602.16829)
*Elan Schonfeld,Elias Wisnia*

Main category: cs.LG

TL;DR: 本文提出一个双时间尺度模型，指出当反馈吸收速度超过任务结构评估速度时，学习者会偏好反馈而非真相，形成不可避免的'反馈-真相差距'；该差距在神经网络（含噪声标签训练）、人类概率反转学习及奖惩学习实验中普遍存在，但不同系统通过不同机制（如网络稀疏性、人类主动恢复）调节此差距。


<details>
  <summary>Details</summary>
Motivation: 理解学习系统在噪声监督下为何及如何偏离真实目标，揭示反馈与真相之间的时间尺度错配对学习偏差的根本影响。

Method: 构建双时间尺度动力学模型；在30个数据集共2700次运行的带噪标签神经网络训练、292名被试的概率反转学习行为实验、25名被试的同步EEG奖惩学习实验中进行跨系统验证；以留出标签、客观正确选项或前反馈预期作为操作性‘真相’定义。

Result: 证实反馈-真相差距普遍存在；密集网络通过记忆累积该差距，稀疏残差结构抑制它，人类表现出短暂过度承诺并主动恢复；神经层面的过度承诺（0.04–0.10）被放大十倍为行为层面的强承诺（d = 3.3–3.9）。

Conclusion: 反馈-真相差距是噪声监督下学习的基本约束，其影响取决于各系统特有的调节机制，为理解泛化失败、过拟合与人类学习韧性提供了统一框架。

Abstract: When feedback is absorbed faster than task structure can be evaluated, the learner will favor feedback over truth. A two-timescale model shows this feedback-truth gap is inevitable whenever the two rates differ and vanishes only when they match. We test this prediction across neural networks trained with noisy labels (30 datasets, 2,700 runs), human probabilistic reversal learning (N = 292), and human reward/punishment learning with concurrent EEG (N = 25). In each system, truth is defined operationally: held-out labels, the objectively correct option, or the participant's pre-feedback expectation - the only non-circular reference decodable from post-feedback EEG. The gap appeared universally but was regulated differently: dense networks accumulated it as memorization; sparse-residual scaffolding suppressed it; humans generated transient over-commitment that was actively recovered. Neural over-commitment (~0.04-0.10) was amplified tenfold into behavioral commitment (d = 3.3-3.9). The gap is a fundamental constraint on learning under noisy supervision; its consequences depend on the regulation each system employs.

</details>


### [164] [VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study](https://arxiv.org/abs/2602.16833)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Verbalized Action Masking (VAM) 的新方法，通过在提示中显式地‘口语化’动作掩码并迭代剪枝动作空间，来提升大语言模型在强化学习后训练中的探索效率与性能，尤其在国际象棋任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练中，稀疏反馈和巨大动作空间易导致LLM过早陷入重复行为，探索成为关键瓶颈。

Method: 提出Verbalized Action Masking（VAM）：在prompt中显式描述可行动作集合，并通过迭代剪枝——若目标动作未被采样，则从掩码中移除已采样有效动作，重新采样，直至采样成功或达到预算上限。

Result: 在国际象棋任务中，VAM在棋题求解与全盘对弈（以ACPL为指标）上均优于强基线，提升了学习效率与最终性能。

Conclusion: VAM是一种实用、可控的探索机制，为LLM的RL后训练提供了新范式。

Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.

</details>


### [165] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 本文提出了一种残差感知的累积注意力展开理论，解释了Transformer中位置偏差的架构根源，证明残差连接可防止注意力坍缩，并在有限深度下导致U形位置偏差，从而为'中间丢失'现象提供了原理性解释。


<details>
  <summary>Details</summary>
Motivation: Transformer模型存在系统性位置偏差，但其架构起源尚不清楚；理论预测的注意力坍缩与实际不符，需解释这一矛盾。

Method: 提出残差感知的累积注意力展开理论，将残差连接纳入分析，并在有限深度下进行理论证明。

Result: 证明残差连接可防止注意力坍缩；在有限深度下，因果Transformer会产生U形位置偏差，即注意力集中在首尾token。

Conclusion: 残差连接是防止注意力坍缩的关键架构因素，U形位置偏差为'中间丢失'现象提供了架构层面的解释。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [166] [Training Large Reasoning Models Efficiently via Progressive Thought Encoding](https://arxiv.org/abs/2602.16839)
*Zeliang Zhang,Xiaodong Liu,Hao Cheng,Hao Sun,Chenliang Xu,Jianfeng Gao*

Main category: cs.LG

TL;DR: 本文提出了一种名为'渐进式思维编码（Progressive Thought Encoding）'的参数高效微调方法，使大推理模型（LRMs）在固定大小缓存下仍能有效推理，显著降低RL训练内存开销并提升数学推理准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务上表现优异，但其强化学习（RL）训练因长序列自回归解码导致时间和内存开销巨大；滑动窗口缓存虽可限制内存，却损害长上下文推理能力。

Method: 提出渐进式思维编码（Progressive Thought Encoding），通过将中间推理过程逐步编码为固定尺寸向量表示，在不需全缓存回传梯度的前提下实现参数高效微调，从而在训练和推理中均保持恒定内存占用。

Result: 在Qwen2.5-3B/7B-Instruct与DeepSeek-R1-Distill-Llama-8B三个模型、六个数学基准测试（含AIME2024/2025）上验证：相比LoRA微调平均提升+19.3%，相比无微调基线平均提升+29.9%；在AIME2024/2025上最高提升+23.4准确率。

Conclusion: 渐进式思维编码不仅提升了LRMs的推理准确性，更显著增强了其在真实内存约束下的RL训练效率与可扩展性。

Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.

</details>


### [167] [What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor](https://arxiv.org/abs/2602.16842)
*Rachitesh Kumar,Omar Mouchtaki*

Main category: cs.LG

TL;DR: 本文研究了具有需求截断数据的离线数据驱动报童问题，提出了一种计算经典库存策略最坏情况遗憾的通用方法，并证明该无限维非凸优化问题可降为有限维问题；分析表明，在仅观测销售数据（而非真实需求）的情况下，简单将销售视为需求会导致严重性能下降，而通过少量高库存水平的主动探索可显著提升最坏情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设需求完全可观测，但现实中需求常因库存限制而被截断（即仅能观测到销售数据），导致学习困难；本文旨在刻画和提升在需求截断下的离线库存决策性能。

Method: 提出一种通用方法，将评估库存策略最坏情况遗憾的无限维、非凸优化问题转化为有限维问题；并基于此对Kaplan-Meier策略及‘销售即需求’启发式策略进行精确性能分析。

Result: 1) 截断需求下，Kaplan-Meier策略通过少量高库存探索可实现近最优最坏情况性能；2) ‘销售即需求’策略在截断严重时性能急剧恶化；3) 销售系统是否记录缺货事件，从根本上影响离线可学习性。

Conclusion: 需求截断显著限制被动销售数据的学习能力，但引入可控探索可大幅缓解；点-of-sale信息质量（如是否记录缺货）是决定离线库存策略性能的关键因素。

Abstract: We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.

</details>


### [168] [On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking](https://arxiv.org/abs/2602.16849)
*Jianliang He,Leda Wang,Siyu Chen,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文对两层神经网络求解模加法任务的机制进行了全面分析，揭示了神经元学习单频傅里叶特征与相位对齐的原理，并提出‘相位对称性’和‘频率多样化’的分化条件，解释了噪声信号如何通过多数投票被抑制、从而实现鲁棒计算；进一步通过彩票机制和梯度流分析阐明特征涌现机制，并将‘grokking’现象形式化为三阶段过程。


<details>
  <summary>Details</summary>
Motivation: 先前工作虽发现神经元学习单频傅里叶特征和相位对齐，但未能解释这些局部特征如何协同构成全局正确解，本文旨在填补这一机制性空白。

Method: 提出并形式化‘相位对称性’与‘频率多样化’的分化条件；建立梯度流模型分析频率竞争；利用ODE比较引理刻画层间相位耦合动力学；结合彩票机制解释随机初始化下特征涌现；将grokking建模为三阶段训练过程。

Result: 证明相位对称性支持多数投票以抵消噪声，使网络能鲁棒识别正确和；严格刻画频率在各神经元内的竞争机制（由初始谱幅值与相位对齐决定）；首次将grokking形式化为‘记忆→第一阶段泛化→第二阶段泛化’三阶段过程。

Conclusion: 两层网络求解模加法的本质是通过结构化特征分化与相位协同实现集体计算，而非依赖单个强特征；该机制不仅解释了训练动态与泛化行为，也为理解深度网络中的涌现现象（如grokking）提供了统一理论框架。

Abstract: We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the "winner" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.

</details>


### [169] [Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling](https://arxiv.org/abs/2602.16864)
*Daniel Durstewitz,Christoph Jürgen Hemmer,Florian Hess,Charlotte Ricarda Doll,Lukas Eisenmann*

Main category: cs.LG

TL;DR: 本文主张将动力系统（DS）视角引入时间序列（TS）建模，强调基于DS原理的模型不仅能提升短期预测性能，更能预测长期统计特性、提供理论性能上界、支持外推与控制，并显著降低计算与内存开销。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列建模虽快速发展，但实际进步不清晰；而真实时间序列多源于底层动力系统，仅依赖黑箱数据驱动模型存在理论与实用局限，亟需融合DS理论以实现根本性突破。

Method: 综述动力系统理论与DS重构（DSR）的核心概念、方法与模型，并系统阐述其如何为TS建模提供理论支撑与技术路径，包括可预测性分析、长期统计建模、泛化能力刻画及轻量化建模等。

Result: 论证了DS视角能提升预测质量（尤其长期统计）、增强模型可解释性与泛化性（如临界点外推）、明确性能上界，并支持低计算/内存开销的建模范式。

Conclusion: DS理论是推动时间序列建模迈向下一阶段的关键范式；应将DSR思想深度融入TS基础模型设计，具体建议包括构建物理信息嵌入结构、发展可微分DS参数估计、定义DS一致性评估指标等。

Abstract: Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models. With a lot of hype and industrial demand in this field, it is not always clear how much progress there really is. To advance TS forecasting and analysis to the next level, here we argue that the field needs a dynamical systems (DS) perspective. TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts. This is the promise of DS reconstruction (DSR), a class of ML/AI approaches that aim to infer surrogate models of the underlying DS from data. But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints. We conclude with a number of specific suggestions for translating insights from DSR into TS modeling.

</details>


### [170] [ML-driven detection and reduction of ballast information in multi-modal datasets](https://arxiv.org/abs/2602.16876)
*Yaroslav Solovko*

Main category: cs.LG

TL;DR: 本文提出了一种通用的多模态框架，用于检测和减少各类数据（结构化、半结构化、非结构化、稀疏）中的冗余信息（ballast），通过多种分析方法融合生成Ballast Score，实现特征空间大幅剪枝（常超70%），同时保持甚至提升分类性能，并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现代数据集常包含大量冗余或低效信息（ballast），增加维度、存储与计算成本，却不提升分析价值，亟需一种跨模态的ballast识别与精简方法。

Method: 结合熵、互信息、Lasso、SHAP、PCA、主题建模和嵌入分析等多种技术，提出统一的Ballast Score，构建多模态ballast检测与剪枝框架。

Result: 在多种数据上验证：可安全剪枝超70%特征（尤其在稀疏/半结构化数据中），分类性能不变或提升，训练时间与内存占用显著下降，并识别出统计型、语义型、基础设施型等ballast类型。

Conclusion: 该框架为构建更轻量、高效、可解释的机器学习流程提供了系统性方法和实用指导。

Abstract: Modern datasets often contain ballast as redundant or low-utility information that increases dimensionality, storage requirements, and computational cost without contributing meaningful analytical value. This study introduces a generalized, multimodal framework for ballast detection and reduction across structured, semi-structured, unstructured, and sparse data types. Using diverse datasets, entropy, mutual information, Lasso, SHAP, PCA, topic modelling, and embedding analysis are applied to identify and eliminate ballast features. A novel Ballast Score is proposed to integrate these signals into a unified, cross-modal pruning strategy. Experimental results demonstrate that significant portions of the feature space as often exceeding 70% in sparse or semi-structured data, can be pruned with minimal or even improved classification performance, along with substantial reductions in training time and memory footprint. The framework reveals distinct ballast typologies (e.g. statistical, semantic, infrastructural), and offers practical guidance for leaner, more efficient machine learning pipelines.

</details>


### [171] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 本研究基于巴西老龄化纵向研究（ELSI-Brazil）数据，构建了面向中老年人群的痴呆分类模型，结合随机森林与多变量逻辑回归，识别出教育水平、年龄、体重、握力、皮肤颜色、身体活动、听力、抑郁症状及生活满意度等可干预风险/保护因素；随机森林模型AUC达0.776，优于逻辑回归，强调了低门槛变量在巴西基层痴呆防控中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 为巴西中老年人群建立低成本、具可修改性的痴呆风险预测模型，支持基层医疗资源的高效配置和早期预防。

Method: 采用横断面观察性研究设计，基于ELSI-Brazil数据（n=9,412），运用随机森林（RF）和多变量逻辑回归进行变量筛选与风险建模；痴呆诊断结合神经心理评估与知情者报告。

Result: 痴呆患病率为9.6%；识别出多个显著风险因素（如文盲OR=7.42、≥90岁OR=11.00）和保护因素（如高教育OR=0.44）；RF模型AUC=0.776，敏感度0.708，特异度0.702，准确率0.703，性能优于逻辑回归。

Conclusion: 痴呆具有多维性，诸多低门槛、可干预变量可用于识别高危人群；应加强促进脑健康公共政策，以提升巴西初级卫生保健中的痴呆预防效能。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [172] [Exact Certification of Data-Poisoning Attacks Using Mixed-Integer Programming](https://arxiv.org/abs/2602.16944)
*Philip Sosnin,Jodie Knapp,Fraser Kennedy,Josh Collyer,Calvin Tsay*

Main category: cs.LG

TL;DR: 本文提出了一种用于数据投毒攻击的验证框架，通过混合整数二次规划（MIQCP）统一建模攻击、训练与测试过程，首次实现了对训练阶段鲁棒性的精确认证。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对数据投毒攻击下训练过程鲁棒性的严格、完备验证，亟需一种既能发现最坏攻击又能对所有可能攻击效果进行上界保证的框架。

Method: 将对抗性数据操纵、模型训练和测试评估统一建模为一个混合整数二次规划（MIQCP）问题，显式编码梯度下降训练动态及测试时评估逻辑，并求解其全局最优解。

Result: 在小规模模型上实验验证了该框架能完备刻画模型对数据投毒的鲁棒性，既可生成最坏情况投毒攻击，也可严格界定所有可能攻击的有效性上界。

Conclusion: 该MIQCP验证框架首次实现了训练阶段数据投毒鲁棒性的精确、完备认证，兼具理论严谨性与实际可计算性。

Abstract: This work introduces a verification framework that provides both sound and complete guarantees for data poisoning attacks during neural network training. We formulate adversarial data manipulation, model training, and test-time evaluation in a single mixed-integer quadratic programming (MIQCP) problem. Finding the global optimum of the proposed formulation provably yields worst-case poisoning attacks, while simultaneously bounding the effectiveness of all possible attacks on the given training pipeline. Our framework encodes both the gradient-based training dynamics and model evaluation at test time, enabling the first exact certification of training-time robustness. Experimental evaluation on small models confirms that our approach delivers a complete characterization of robustness against data poisoning.

</details>


### [173] [Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning](https://arxiv.org/abs/2602.16947)
*Chuqin Geng,Li Zhang,Haolin Ye,Ziyu Zhao,Yuhe Jiang,Tara Saba,Xinyu Wang,Xujie Si*

Main category: cs.LG

TL;DR: 本文提出SymGraph，一种符号化图神经网络框架，通过离散结构哈希和基于拓扑角色的聚合替代连续消息传递，突破1-WL表达力限制，兼具高性能、快速训练（CPU上提速10–100倍）与高粒度可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自解释GNN受限于1-WL表达力瓶颈和粗粒度可解释性，难以满足药物发现等高风险领域对可信性与细粒度解释的需求。

Method: 提出SymGraph框架：以离散结构哈希和拓扑角色驱动的聚合机制取代传统连续消息传递，避免可微优化，实现理论更强的图表达能力。

Result: 在多个基准上达到自解释GNN最优性能；CPU训练速度比现有方法快10–100倍；生成的解释规则语义粒度更细，更具科学发现价值。

Conclusion: SymGraph成功兼顾高表达力、高效性与强可解释性，为高风险领域的可信图学习提供了新范式。

Abstract: Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework designed to transcend these constraints. By replacing continuous message passing with discrete structural hashing and topological role-based aggregation, our architecture theoretically surpasses the 1-WL barrier, achieving superior expressiveness without the overhead of differentiable optimization. Extensive empirical evaluations demonstrate that SymGraph achieves state-of-the-art performance, outperforming existing self-explainable GNNs. Notably, SymGraph delivers 10x to 100x speedups in training time using only CPU execution. Furthermore, SymGraph generates rules with superior semantic granularity compared to existing rule-based methods, offering great potential for scientific discovery and explainable AI.

</details>


### [174] [Neural Proposals, Symbolic Guarantees: Neuro-Symbolic Graph Generation with Hard Constraints](https://arxiv.org/abs/2602.16954)
*Chuqin Geng,Li Zhang,Mark Zhang,Haolin Ye,Ziyu Zhao,Xujie Si*

Main category: cs.LG

TL;DR: 本文提出了一种神经符号图生成模型（NSGGM），结合神经网络与符号求解器（SMT）进行分子生成，兼顾生成性能、化学有效性保证与用户可控性。


<details>
  <summary>Details</summary>
Motivation: 现有黑箱深度神经方法在分子与图生成中缺乏可控性和形式化保证，难以满足严格化学规则与用户定制约束的需求。

Method: 提出NSGGM框架：神经部分（自回归模型）负责支架选择与相互作用信号细化；符号部分（高效SMT求解器）基于化学规则和用户约束构造完整、有效且‘正确即构建’的分子图。

Result: 在无约束与有约束分子生成任务上达到SOTA性能；在新提出的逻辑约束分子基准（Logical-Constraint Molecular Benchmark）上验证了对硬性规则的严格满足能力。

Conclusion: 神经符号建模可在保持先进生成性能的同时，提供可解释的可控性与形式化正确性保证，为可控分子设计提供了新范式。

Abstract: We challenge black-box purely deep neural approaches for molecules and graph generation, which are limited in controllability and lack formal guarantees. We introduce Neuro-Symbolic Graph Generative Modeling (NSGGM), a neurosymbolic framework that reapproaches molecule generation as a scaffold and interaction learning task with symbolic assembly. An autoregressive neural model proposes scaffolds and refines interaction signals, and a CPU-efficient SMT solver constructs full graphs while enforcing chemical validity, structural rules, and user-specific constraints, yielding molecules that are correct by construction and interpretable control that pure neural methods cannot provide. NSGGM delivers strong performance on both unconstrained generation and constrained generation tasks, demonstrating that neuro-symbolic modeling can match state-of-the-art generative performance while offering explicit controllability and guarantees. To evaluate more nuanced controllability, we also introduce a Logical-Constraint Molecular Benchmark, designed to test strict hard-rule satisfaction in workflows that require explicit, interpretable specifications together with verifiable compliance.

</details>


### [175] [Multi-Agent Lipschitz Bandits](https://arxiv.org/abs/2602.16965)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文研究了连续动作空间下的去中心化多玩家随机赌博机问题，提出了一种无需通信的模块化协议，通过最大值导向搜索实现玩家协调，并将问题解耦为多个独立单玩家Lipschitz赌博机，实现了近似最优的悔恨界。


<details>
  <summary>Details</summary>
Motivation: 解决连续、Lipschitz结构动作空间中多玩家硬碰撞（零奖励）下的去中心化协作优化问题，且希望协调开销与时间范围T无关。

Method: 提出一种模块化协议：首先用新颖的最大值导向搜索解决多智能体协调问题（使各玩家定位并占据不同高价值区域），然后将整体问题解耦为N个独立的单玩家Lipschitz赌博机问题。

Result: 获得近似最优的悔恨界\tilde{O}(T^{(d+1)/(d+2)})，外加一个与T无关的协调成本，达到与单玩家相同的速率；该框架还适用于广义距离阈值碰撞模型。

Conclusion: 这是首个在该设定下提供此类理论保证的框架，兼顾高效协调与渐进最优性能。

Abstract: We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed search -- and then decouples the problem into $N$ independent single-player Lipschitz bandits. We establish a near-optimal regret bound of $\tilde{O}(T^{(d+1)/(d+2)})$ plus a $T$-independent coordination cost, matching the single-player rate. To our knowledge, this is the first framework providing such guarantees, and it extends to general distance-threshold collision models.

</details>


### [176] [A Unified Framework for Locality in Scalable MARL](https://arxiv.org/abs/2602.16966)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的策略依赖型局部性理论，通过分解策略诱导的互依赖矩阵H^π，揭示了平滑策略可诱导局部性，并给出了比以往更紧的指数衰减谱条件ρ(E^s + E^a Π(π)) < 1，进而设计了具有理论保证的局部块坐标策略改进框架。


<details>
  <summary>Details</summary>
Motivation: 现有保证值函数指数衰减性质（EDP）的条件过于保守，仅基于环境最坏情况（如动作上确界），未考虑策略本身的正则化效应。

Method: 提出策略诱导互依赖矩阵H^π的新分解，解耦环境对状态/动作的敏感性（E^s, E^a）与策略对状态的敏感性（Π(π)）；推导基于谱半径的新型EDP判据；构建基于该判据的局部块坐标策略改进算法。

Result: 建立了策略依赖的局部性新视角；获得严格优于以往范数条件的谱条件ρ(E^s + E^a Π(π)) < 1；提出并分析了具有谱半径依赖理论保证的局部化策略优化框架。

Conclusion: 局部性不仅是环境固有属性，更是策略可调控的特性；平滑策略可在强动作耦合环境中诱导局部性，但需权衡局部性与最优性；谱半径条件为可扩展MARL提供了更精准的理论基础。

Abstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment's sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy's sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.

</details>


### [177] [Early-Warning Signals of Grokking via Loss-Landscape Geometry](https://arxiv.org/abs/2602.16967)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文发现，'grokking'（即模型在长时间训练后突然从记忆转向泛化）现象与梯度更新非交换性导致的'交换子缺陷'（commutator defect）密切相关；该缺陷在泛化发生前显著上升，且其上升时间领先服从超线性幂律；通过因果干预验证其机制作用：增强非交换性可加速grokking，抑制则延迟或阻止它；该指标在不同序列学习任务中均具鲁棒性与因果必要性。


<details>
  <summary>Details</summary>
Motivation: 探究grokking现象是否仅限于模算术任务，以及是否存在跨任务通用的、可解释的早期预警信号。

Method: 在SCAN和Dyck-1两个序列学习基准上系统分析交换子缺陷（一种基于非交换梯度更新的曲率度量），结合权重空间PCA、因果干预（增强/抑制正交梯度流）及多学习率扫描，验证其与grokking的时间关联与因果作用。

Result: 交换子缺陷在泛化前显著上升，领先时间服从超线性幂律（SCAN: α≈1.18；Dyck: α≈1.13）；PCA显示谱集中并非普适前兆，而交换子缺陷是；干预实验证明其具因果机制作用——增强非交换性加速grokking（SCAN约32%，Dyck约50%），抑制则延迟或阻止grokking；三类任务构成因果敏感性谱，但抑制均导致grokking失败，表明其必要性。

Conclusion: 交换子缺陷是一种鲁棒、架构无关、具因果必要性的早期预警信号，可统一刻画Transformer中延迟泛化的动力学机制。

Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.

</details>


### [178] [Fail-Closed Alignment for Large Language Models](https://arxiv.org/abs/2602.16977)
*Zachary Coalson,Beth Sohler,Aiden Gabriel,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文提出了一种名为'fail-closed alignment'的安全设计原则，通过构建多重、因果独立的拒绝机制来增强大语言模型在面对提示类越狱攻击时的鲁棒性，并提出了一个渐进式对齐框架加以实现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的拒绝机制是'fail-open'（故障开放型）的，即单一关键拒绝特征被干扰后即导致整体对齐失效，存在结构性安全隐患。

Method: 提出'fail-closed alignment'原则，并设计一种渐进式对齐框架：迭代识别并消融已学得的拒绝方向，迫使模型在新的、独立的子空间中重建安全性。

Result: 在四种越狱攻击下实现了最强的整体鲁棒性，同时缓解了过度拒绝问题、保持了生成质量，且计算开销小；机制分析证实模型确实学习到了多个无法被单次提示攻击同时压制的因果独立拒绝方向。

Conclusion: fail-closed alignment 是一种有实证支持的、面向鲁棒安全的LLM对齐基础性原则。

Abstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.

</details>


### [179] [Discovering Universal Activation Directions for PII Leakage in Language Models](https://arxiv.org/abs/2602.16980)
*Leo Marchyok,Zachary Coalson,Sungho Keum,Sooel Son,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文提出UniLeak框架，通过识别语言模型残差流中能普遍增强PII生成概率的激活方向，揭示PII泄露的机制性根源，无需训练数据或真实PII标签即可实现高精度风险放大与潜在缓解。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型内部结构丰富，但隐私敏感行为（如PII泄露）如何在隐藏状态中表征和调控尚不清楚。

Method: 提出UniLeak机制可解释性框架，基于模型自生成文本，识别残差流中能跨上下文泛化、线性叠加即稳定提升PII生成概率的‘通用激活方向’。

Result: 在多个模型和数据集上验证，沿这些方向引导显著提升PII泄露率，优于现有基于提示的提取方法；且对生成质量影响极小。

Conclusion: PII泄露源于模型表征中一种可被线性操纵的潜信号叠加，该发现为风险放大与缓解提供了新机制视角。

Abstract: Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.

</details>


### [180] [Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding](https://arxiv.org/abs/2602.16994)
*Rahul Thomas,Teo Kitanovski,Micah Goldblum,Arka Pal*

Main category: cs.LG

TL;DR: 本文系统评估了多路径推测解码中的验证策略，发现遍历验证（Traversal Verification）性能最优，而基于最优传输（OT）的方法表现较差；进一步提出延迟树扩展和动态神经选择器，使OT方法首次超越遍历验证，平均吞吐量提升5%。


<details>
  <summary>Details</summary>
Motivation: 现有工作提出了多种i.i.d. rollout下的验证算法，但在匹配设置下其相对性能尚不明确，且OT类方法实际表现远低于预期，需深入分析原因并改进。

Method: 1）系统评估不同验证策略在模型族、任务和采样模式下的性能；2）提出延迟树扩展（delayed tree expansion），推迟i.i.d.分支点以提升深层多token接受率；3）设计动态神经选择器，基于draft与target模型特征预测OT类验证的块效率，实现上下文自适应的树扩展决策。

Result: 延迟树扩展保持目标分布且优于根节点i.i.d. rollout；动态神经选择器使SpecInfer等OT方法首次超越Traversal Verification，在多种模型、数据集和采样设置下平均吞吐量提升5%。

Conclusion: 验证策略性能差异源于多token接受位置与分布偏移的错配；通过结构化延迟扩展与数据驱动的选择机制，可释放OT类方法潜力，推动多路径推测解码实用化。

Abstract: Multi-path speculative decoding accelerates lossless sampling from a target model by using a cheaper draft model to generate a draft tree of tokens, and then applies a verification algorithm that accepts a subset of these. While prior work has proposed various verification algorithms for i.i.d rollouts, their relative performance under matched settings remains unclear. In this work, we firstly present a systematic evaluation of verification strategies across model families, tasks, and sampling regimes, and find that Traversal Verification dominates consistently, with OT-based methods lagging far behind. Our analysis uncovers that this occurs because OT-based methods achieve high multi-token acceptance near the root of the draft tree, while multi-token gains are most impactful deeper in the draft tree, where draft and target distributions diverge. Based on this insight, we propose delayed tree expansion, which drafts a partial single path, delaying the i.i.d. branching point. We show that delayed tree expansion preserves the target distribution and improves on root-node i.i.d rollouts. Further, we develop a dynamic neural selector that estimates the expected block efficiency of optimal-transport-based verification methods from draft and target features, enabling context-dependent expansion decisions. Our neural selector allows OT-based methods like SpecInfer to outperform Traversal Verification for the first time, achieving 5% higher average throughput across a wide range of models, datasets, and sampling settings.

</details>


### [181] [Arcee Trinity Large Technical Report](https://arxiv.org/abs/2602.17004)
*Varun Singh,Lucas Krauss,Sami Jaghouar,Matej Sirovatka,Charles Goddard,Fares Obied,Jack Min Ong,Jannik Straube,Fern,Aria Harley,Conner Stewart,Colin Kealty,Maziyar Panahi,Simon Kirsten,Anushka Deshpande,Anneketh Vij,Arthur Bresnu,Pranav Veldurthi,Raghav Ravishankar,Hardik Bishnoi,DatologyAI Team,Arcee AI Team,Prime Intellect Team,Mark McQuade,Johannes Hagemann,Lucas Atkins*

Main category: cs.LG

TL;DR: 本文介绍了Arcee Trinity系列稀疏Mixture-of-Experts（MoE）大语言模型，包括Trinity Large（400B总参、13B激活）、Trinity Mini（26B总参、3B激活）和Trinity Nano（6B总参、1B激活），采用现代架构设计（如交错局部/全局注意力、门控注意力、深度缩放三明治归一化、Sigmoid路由）及新提出的SMEBU负载均衡策略，并使用Muon优化器完成稳定训练。


<details>
  <summary>Details</summary>
Motivation: 构建高效、可扩展且训练稳定的稀疏MoE大模型，兼顾参数规模与推理效率，并解决MoE中专家负载不均衡问题。

Method: 设计包含多种先进组件的稀疏MoE架构；提出Soft-clamped Momentum Expert Bias Updates（SMEBU）负载均衡策略；采用Muon优化器进行训练；在超大规模语料（10T–17T tokens）上预训练。

Result: 所有三个模型均实现零损失尖峰的稳定训练；模型已开源发布于Hugging Face平台。

Conclusion: Arcee Trinity系列验证了现代稀疏MoE架构与新型训练策略（如SMEBU和Muon）在超大规模模型上的有效性与稳定性，为高效大模型研发提供了实用方案。

Abstract: We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.

</details>


### [182] [Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17009)
*Nikunj Gupta,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 本文提出了一种名为Action Graph Policies (AGP)的新方法，通过建模智能体间动作选择的依赖关系来提升多智能体强化学习中的协调能力，在部分可观测和存在反协调惩罚的任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，协调动作是合作的基础，而现有方法难以有效建模智能体间动作的兼容性与全局约束。

Method: 提出Action Graph Policies（AGP），构建'协调上下文'，使智能体能基于全局动作依赖关系进行决策，并从理论上证明其联合策略表达能力更强。

Result: AGP在典型协调任务中达到80-95%的成功率，远超其他MARL方法的10-25%，且在多种多智能体环境中持续优于基线方法。

Conclusion: AGP通过显式建模动作依赖关系，显著提升了去中心化决策中的协调能力，为MARL中的协同优化提供了新思路。

Abstract: Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.

</details>


### [183] [Malliavin Calculus as Stochastic Backpropogation](https://arxiv.org/abs/2602.17013)
*Kevin D. Oden*

Main category: cs.LG

TL;DR: 本文通过Malliavin积分分部法统一了路径导数法和得分函数法两种梯度估计器，并提出一种基于经验协方差结构自适应融合二者的方差感知混合估计器，在VAE等任务中显著降低梯度方差。


<details>
  <summary>Details</summary>
Motivation: 现有路径导数（reparameterization）与得分函数（score-function/Malliavin）梯度估计方法缺乏统一理论基础，且各自存在方差高或适用范围受限等问题，亟需一个能兼顾偏差、方差与适用性的统一框架。

Method: 基于Malliavin积分分部恒等式建立两类梯度估计器的理论等价性；进而设计一种利用二者经验协方差结构进行自适应加权的线性混合估计器，并推导其最小方差性质及有限样本收敛界。

Result: 在CIFAR-10上的VAE任务中实现9%方差下降，在强耦合合成问题中最高达35%；策略梯度实验揭示非平稳优化场景下该方法面临挑战。

Conclusion: Malliavin微积分可作为随机梯度估计的概念性统一框架；混合估计器在特定条件下（如目标分布平滑、参数耦合适中）具显著优势，但在非平稳优化等复杂场景中存在固有局限，指明了未来研究方向。

Abstract: We establish a rigorous connection between pathwise (reparameterization) and score-function (Malliavin) gradient estimators by showing that both arise from the Malliavin integration-by-parts identity. Building on this equivalence, we introduce a unified and variance-aware hybrid estimator that adaptively combines pathwise and Malliavin gradients using their empirical covariance structure. The resulting formulation provides a principled understanding of stochastic backpropagation and achieves minimum variance among all unbiased linear combinations, with closed-form finite-sample convergence bounds. We demonstrate 9% variance reduction on VAEs (CIFAR-10) and up to 35% on strongly-coupled synthetic problems. Exploratory policy gradient experiments reveal that non-stationary optimization landscapes present challenges for the hybrid approach, highlighting important directions for future work. Overall, this work positions Malliavin calculus as a conceptually unifying and practically interpretable framework for stochastic gradient estimation, clarifying when hybrid approaches provide tangible benefits and when they face inherent limitations.

</details>


### [184] [WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning](https://arxiv.org/abs/2602.17025)
*Gagan Mundada,Zihan Huang,Rohan Surana,Sheldon Yu,Jennifer Yuntong Zhang,Xintong Li,Tong Yu,Lina Yao,Jingbo Shang,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: 本文提出了一种弱监督的Group Relative Policy Optimization（WS-GRPO）方法，通过将终端奖励转化为对部分轨迹的正确性感知指导，缓解GRPO中因过度采样导致的冗余推理问题，在保持准确率的同时显著缩短推理长度。


<details>
  <summary>Details</summary>
Motivation: GRPO在复杂推理任务中有效，但其基于多轨迹相对目标的设计易引发过度推理（overthinking），导致推理效率低且难以平衡正确性与推理长度；而传统长度惩罚难以校准，且缺乏细粒度的继续/停止监督信号。

Method: 提出WS-GRPO：利用仅含最终答案正确性的弱监督信号训练一个偏好模型，生成前缀级（prefix-level）的continue/stop指导信号，替代全局长度惩罚，从而在 rollout 过程中动态抑制冗余推理。

Result: 理论分析支持方法有效性；实验表明，WS-GRPO在多个推理基准上显著减少 rollout 长度，同时性能与原始 GRPO 基线相当。

Conclusion: WS-GRPO 通过弱监督方式实现更高效、更可控的推理过程，为语言模型的推理优化提供了新思路。

Abstract: Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We propose Weakly Supervised GRPO (WS-GRPO), which improves rollout efficiency by converting terminal rewards into correctness-aware guidance over partial trajectories. Unlike global length penalties that are hard to calibrate, WS-GRPO trains a preference model from outcome-only correctness to produce prefix-level signals that indicate when additional continuation is beneficial. Thus, WS-GRPO supplies outcome-derived continue/stop guidance, reducing redundant deliberation while maintaining accuracy. We provide theoretical results and empirically show on reasoning benchmarks that WS-GRPO substantially reduces rollout length while remaining competitive with GRPO baselines.

</details>


### [185] [Transforming Behavioral Neuroscience Discovery with In-Context Learning and AI-Enhanced Tensor Methods](https://arxiv.org/abs/2602.17027)
*Paimon Goulart,Jordan Steinhauser,Dawon Ahn,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文提出了一种AI增强的科学发现管道，利用上下文学习（ICL）和改进的张量分解模型，帮助行为神经科学家更高效地分析小鼠恐惧泛化实验数据，无需AI训练知识，同时保持高性能并获得领域专家验证。


<details>
  <summary>Details</summary>
Motivation: 传统科学发现流程复杂、僵化且耗时，领域专家需花费大量时间在调试管道和手动标注数据上，而非专注于结果解释；同时，恐惧泛化研究对理解PTSD等临床疾病至关重要，亟需更高效的分析方法。

Method: 采用'上下文学习'（ICL）作为面向领域专家的零代码AI接口，用于自动化数据准备与模式解读；并提出针对异构数据优化的新型张量分解模型增强方案。

Result: 实验表明该AI增强管道在性能上优于领域内常规方法及非ICL的合理机器学习基线，并成功实现有效科学发现，结果经行为神经科学家验证认可。

Conclusion: ICL可作为连接AI与领域专家的高效桥梁，结合定制化模型增强，能在不牺牲性能的前提下显著提升科学发现效率与可用性，推动AI for Science落地。

Abstract: Scientific discovery pipelines typically involve complex, rigid, and time-consuming processes, from data preparation to analyzing and interpreting findings. Recent advances in AI have the potential to transform such pipelines in a way that domain experts can focus on interpreting and understanding findings, rather than debugging rigid pipelines or manually annotating data. As part of an active collaboration between data science/AI researchers and behavioral neuroscientists, we showcase an example AI-enhanced pipeline, specifically designed to transform and accelerate the way that the domain experts in the team are able to gain insights out of experimental data. The application at hand is in the domain of behavioral neuroscience, studying fear generalization in mice, an important problem whose progress can advance our understanding of clinically significant and often debilitating conditions such as PTSD (Post-Traumatic Stress Disorder). We identify the emerging paradigm of "In-Context Learning" (ICL) as a suitable interface for domain experts to automate parts of their pipeline without the need for or familiarity with AI model training and fine-tuning, and showcase its remarkable efficacy in data preparation and pattern interpretation. Also, we introduce novel AI-enhancements to tensor decomposition model, which allows for more seamless pattern discovery from the heterogeneous data in our application. We thoroughly evaluate our proposed pipeline experimentally, showcasing its superior performance compared to what is standard practice in the domain, as well as against reasonable ML baselines that do not fall under the ICL paradigm, to ensure that we are not compromising performance in our quest for a seamless and easy-to-use interface for domain experts. Finally, we demonstrate effective discovery, with results validated by the domain experts in the team.

</details>


### [186] [Forecasting Anomaly Precursors via Uncertainty-Aware Time-Series Ensembles](https://arxiv.org/abs/2602.17028)
*Hyeongwon Kang,Jinwoo Park,Seunghun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: 本文提出FATE框架，通过时间序列预测模型集成的预测不确定性量化，实现无监督的异常前兆（PoA）检测，并引入新评估指标PTaPR，显著提升早期预警能力。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法多为反应式，无法提供主动的早期预警信号，难以满足工业运维、金融和网络安全等领域对系统可靠性和预防性维护的需求。

Method: 提出FATE（Forecasting Anomalies with Time-series Ensembles）无监督框架，利用多样化时间序列预测模型集成的预测不确定性（特别是集成分歧）来识别异常前兆（PoA），无需重建误差或真实标签；同时设计新评估指标PTaPR，综合考量片段级准确率、段内覆盖率和时间提前性。

Result: 在五个真实世界基准数据集上，FATE平均提升PTaPR AUC达19.9个百分点、早期检测F1分数达20.02个百分点，且无需任何异常标签。

Conclusion: FATE是一种高效、实用的无监督实时早期预警方法，适用于复杂时间序列场景。

Abstract: Detecting anomalies in time-series data is critical in domains such as industrial operations, finance, and cybersecurity, where early identification of abnormal patterns is essential for ensuring system reliability and enabling preventive maintenance. However, most existing methods are reactive: they detect anomalies only after they occur and lack the capability to provide proactive early warning signals. In this paper, we propose FATE (Forecasting Anomalies with Time-series Ensembles), a novel unsupervised framework for detecting Precursors-of-Anomaly (PoA) by quantifying predictive uncertainty from a diverse ensemble of time-series forecasting models. Unlike prior approaches that rely on reconstruction errors or require ground-truth labels, FATE anticipates future values and leverages ensemble disagreement to signal early signs of potential anomalies without access to target values at inference time. To rigorously evaluate PoA detection, we introduce Precursor Time-series Aware Precision and Recall (PTaPR), a new metric that extends the traditional Time-series Aware Precision and Recall (TaPR) by jointly assessing segment-level accuracy, within-segment coverage, and temporal promptness of early predictions. This enables a more holistic assessment of early warning capabilities that existing metrics overlook. Experiments on five real-world benchmark datasets show that FATE achieves an average improvement of 19.9 percentage points in PTaPR AUC and 20.02 percentage points in early detection F1 score, outperforming baselines while requiring no anomaly labels. These results demonstrate the effectiveness and practicality of FATE for real-time unsupervised early warning in complex time-series environments.

</details>


### [187] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: 本文提出了一种名为Multi-Probe Zero Collision Hash（MPZCH）的新索引机制，用于推荐系统中的嵌入表，通过线性探测、辅助张量和CUDA内核实现零碰撞与高效更新，显著提升嵌入新鲜度与质量，并已开源集成至TorchRec。


<details>
  <summary>Details</summary>
Motivation: 传统哈希索引在高基数类别特征场景下易发生碰撞，导致模型性能下降与个性化质量退化。

Method: 提出MPZCH方法，基于线性探测设计，结合辅助张量与高性能CUDA内核，支持可配置的多探针策略与主动驱逐策略，通过淘汰过时ID与重置槽位避免嵌入继承 stale 问题。

Result: 在合理表大小下常实现用户嵌入零碰撞，显著提升物品嵌入新鲜度与质量；训练QPS与推理延迟与现有方法相当；已在TorchRec中开源。

Conclusion: MPZCH是一种兼顾零碰撞、训练/推理效率与嵌入更新质量的实用嵌入索引方案，适用于大规模工业推荐系统。

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [188] [Sign Lock-In: Randomly Initialized Weight Signs Persist and Bottleneck Sub-Bit Model Compression](https://arxiv.org/abs/2602.17063)
*Akira Sakai,Yuma Ichikawa*

Main category: cs.LG

TL;DR: 本文研究亚比特模型压缩中符号位的瓶颈问题，提出符号锁定理论解释符号翻转稀疏性，并设计间隙初始化和向外漂移正则化器以降低翻转率。


<details>
  <summary>Details</summary>
Motivation: 在亚比特模型压缩中，当权重幅值被激进压缩时，符号位成为固定开销瓶颈；而学习到的符号矩阵难以低秩近似且谱特性接近随机Rademacher矩阵，但其实际翻转却极为稀疏，需理论解释并优化。

Method: 提出符号锁定理论（sign lock-in theory），基于SGD噪声下的停止时间分析建模符号翻转；设计间隙初始化（gap-based initialization）与轻量级向外漂移正则化器（outward-drift regularizer）抑制符号翻转。

Result: 将有效符号翻转率降至约10^{-3}，仅带来约1点困惑度上升，在Transformer、CNN和MLP上均验证有效。

Conclusion: 符号翻转稀疏性源于初始化继承与边界穿越稀有性，可通过理论指导的初始化与正则化高效控制，为亚比特压缩提供新路径。

Abstract: Sub-bit model compression seeks storage below one bit per weight; as magnitudes are aggressively compressed, the sign bit becomes a fixed-cost bottleneck. Across Transformers, CNNs, and MLPs, learned sign matrices resist low-rank approximation and are spectrally indistinguishable from an i.i.d. Rademacher baseline. Despite this apparent randomness, most weights retain their initialization signs; flips primarily occur via rare near-zero boundary crossings, suggesting that sign-pattern randomness is largely inherited from initialization. We formalize this behavior with sign lock-in theory, a stopping-time analysis of sign flips under SGD noise. Under bounded updates and a rare re-entry condition into a small neighborhood around zero, the number of effective sign flips exhibits a geometric tail. Building on this mechanism, we introduce a gap-based initialization and a lightweight outward-drift regularizer, reducing the effective flip rate to approximately $10^{-3}$ with only about a one-point increase in perplexity.

</details>


### [189] [Spatio-temporal dual-stage hypergraph MARL for human-centric multimodal corridor traffic signal control](https://arxiv.org/abs/2602.17068)
*Xiaocai Zhang,Neema Nassir,Milad Haghani*

Main category: cs.LG

TL;DR: 本文提出了一种基于时空双阶段超图的多智能体强化学习框架STDSH-MARL，用于面向多模态出行者（尤其是公共交通）的走廊网络信号控制，通过新型超图注意力机制和混合离散动作空间提升信号自适应性与公交优先效果。


<details>
  <summary>Details</summary>
Motivation: 传统交通信号控制过于车辆中心化，难以满足多模态出行需求，尤其缺乏对高载客量公共交通的优先保障。

Method: 提出STDSH-MARL框架：采用集中训练-分散执行范式；设计时空双阶段超图注意力机制建模空间与时间超边交互；引入混合离散动作空间联合决策相位配置与绿灯时长。

Result: 在五种交通场景的走廊网络实验中，STDSH-MARL显著提升多模态性能与公交优先效果，整体性能优于现有SOTA方法；消融研究表明时间超边贡献最大。

Conclusion: STDSH-MARL是一种可扩展、高效且面向人本与公交优先的智能信号控制新范式，其时空超图建模与混合动作设计为多智能体交通控制提供了新思路。

Abstract: Human-centric traffic signal control in corridor networks must increasingly account for multimodal travelers, particularly high-occupancy public transportation, rather than focusing solely on vehicle-centric performance. This paper proposes STDSH-MARL (Spatio-Temporal Dual-Stage Hypergraph based Multi-Agent Reinforcement Learning), a scalable multi-agent deep reinforcement learning framework that follows a centralized training and decentralized execution paradigm. The proposed method captures spatio-temporal dependencies through a novel dual-stage hypergraph attention mechanism that models interactions across both spatial and temporal hyperedges. In addition, a hybrid discrete action space is introduced to jointly determine the next signal phase configuration and its corresponding green duration, enabling more adaptive signal timing decisions. Experiments conducted on a corridor network under five traffic scenarios demonstrate that STDSH-MARL consistently improves multimodal performance and provides clear benefits for public transportation priority. Compared with state-of-the-art baseline methods, the proposed approach achieves superior overall performance. Further ablation studies confirm the contribution of each component of STDSH-MARL, with temporal hyperedges identified as the most influential factor driving the observed performance gains.

</details>


### [190] [AdvSynGNN: Structure-Adaptive Graph Neural Nets via Adversarial Synthesis and Self-Corrective Propagation](https://arxiv.org/abs/2602.17071)
*Rong Fu,Muge Qi,Chunlei Meng,Shuo Yin,Kun Liu,Zhaolu Kang,Simon Fong*

Main category: cs.LG

TL;DR: AdvSynGNN是一种面向结构噪声和异配图鲁棒节点表征学习的新型图神经网络架构，融合多尺度结构合成、对比学习、自适应异配注意力、对抗式传播机制与置信度驱动的标签修正策略。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在面对结构噪声或非同质（异配）拓扑时性能显著下降，亟需提升其鲁棒性。

Method: 提出AdvSynGNN框架：1）多分辨率结构合成与对比目标构建几何敏感初始化；2）基于拓扑信号调制的Transformer主干网以自适应处理异配性；3）集成式对抗传播引擎（生成器识别潜在连接扰动，判别器保障全局一致性）；4）基于节点级置信度的残差标签修正机制。

Result: 在多种图分布上显著提升预测准确率，同时保持计算效率；验证了对结构噪声和异配拓扑的强鲁棒性。

Conclusion: AdvSynGNN为鲁棒图表示学习提供了系统性解决方案，并给出了面向大规模部署的实用实现协议。

Abstract: Graph neural networks frequently encounter significant performance degradation when confronted with structural noise or non-homophilous topologies. To address these systemic vulnerabilities, we present AdvSynGNN, a comprehensive architecture designed for resilient node-level representation learning. The proposed framework orchestrates multi-resolution structural synthesis alongside contrastive objectives to establish geometry-sensitive initializations. We develop a transformer backbone that adaptively accommodates heterophily by modulating attention mechanisms through learned topological signals. Central to our contribution is an integrated adversarial propagation engine, where a generative component identifies potential connectivity alterations while a discriminator enforces global coherence. Furthermore, label refinement is achieved through a residual correction scheme guided by per-node confidence metrics, which facilitates precise control over iterative stability. Empirical evaluations demonstrate that this synergistic approach effectively optimizes predictive accuracy across diverse graph distributions while maintaining computational efficiency. The study concludes with practical implementation protocols to ensure the robust deployment of the AdvSynGNN system in large-scale environments.

</details>


### [191] [Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum](https://arxiv.org/abs/2602.17080)
*Minxin Zhang,Yuxuan Liu,Hayden Scheaffer*

Main category: cs.LG

TL;DR: 本文提出两种新型优化器NAMO和NAMO-D，首次将正交化动量与基于范数的Adam型噪声自适应机制进行原理性融合，在GPT-2预训练任务中优于AdamW和Muon。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Adam（依赖自适应矩估计）和Muon（利用权重矩阵结构进行正交化动量）各有优势，但缺乏将二者优势统一的原理性方法；尤其需要兼顾稳定性、结构利用与噪声自适应能力。

Method: NAMO采用单自适应步长缩放正交化动量，保持正交性；NAMO-D在此基础上引入带裁剪的对角矩阵右乘，实现神经元级噪声自适应，并适配近块对角Hessian结构；两者均在标准假设下提供确定性与随机性收敛性保证。

Result: 在GPT-2预训练实验中，NAMO和NAMO-D均优于AdamW和Muon基线；NAMO-D因引入裁剪超参，在更新方向条件数与细粒度噪声适应间取得更好平衡，性能进一步提升。

Conclusion: NAMO/NAMO-D成功实现了正交化动量与范数自适应的统一框架，兼具理论收敛保证与实际训练优势，为大模型优化提供了新思路。

Abstract: Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.

</details>


### [192] [MeGU: Machine-Guided Unlearning with Target Feature Disentanglement](https://arxiv.org/abs/2602.17088)
*Haoyu Wang,Zhuo Huang,Xiaolong Wang,Bo Han,Zhiwei Lin,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Machine-Guided Unlearning (MeGU)的新框架，利用多模态大语言模型（MLLM）引导概念感知的特征重对齐，通过正负特征噪声对实现目标概念的可控遗忘，缓解了传统机器遗忘中“遗忘不足”与“遗忘过度”的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在根本性权衡：激进遗忘损害保留数据性能，保守遗忘残留目标信息；而预训练模型中语义概念在特征层面的纠缠限制了遗忘效果。

Method: 提出MeGU框架：1）用MLLM为待遗忘样本分配语义扰动标签以确定重对齐方向；2）将MLLM估计的类间概念相似性编码为轻量转移矩阵提升效率；3）引入正负特征噪声对——负噪声抑制目标特异性特征，正噪声强化关联特征并对其重对齐至扰动概念。

Result: MeGU实现了可控、选择性的遗忘，在多个基准上显著缓解了遗忘不足和遗忘过度问题，同时保持了对保留数据的良好泛化性能。

Conclusion: 基于概念纠缠分析，MeGU通过机器引导的概念重对齐与特征噪声协同机制，突破了传统遗忘范式的局限，为隐私保护驱动的模型更新提供了新思路。

Abstract: The growing concern over training data privacy has elevated the "Right to be Forgotten" into a critical requirement, thereby raising the demand for effective Machine Unlearning. However, existing unlearning approaches commonly suffer from a fundamental trade-off: aggressively erasing the influence of target data often degrades model utility on retained data, while conservative strategies leave residual target information intact. In this work, the intrinsic representation properties learned during model pretraining are analyzed. It is demonstrated that semantic class concepts are entangled at the feature-pattern level, sharing associated features while preserving concept-specific discriminative components. This entanglement fundamentally limits the effectiveness of existing unlearning paradigms. Motivated by this insight, we propose Machine-Guided Unlearning (MeGU), a novel framework that guides unlearning through concept-aware re-alignment. Specifically, Multi-modal Large Language Models (MLLMs) are leveraged to explicitly determine re-alignment directions for target samples by assigning semantically meaningful perturbing labels. To improve efficiency, inter-class conceptual similarities estimated by the MLLM are encoded into a lightweight transition matrix. Furthermore, MeGU introduces a positive-negative feature noise pair to explicitly disentangle target concept influence. During finetuning, the negative noise suppresses target-specific feature patterns, while the positive noise reinforces remaining associated features and aligns them with perturbing concepts. This coordinated design enables selective disruption of target-specific representations while preserving shared semantic structures. As a result, MeGU enables controlled and selective forgetting, effectively mitigating both under-unlearning and over-unlearning.

</details>


### [193] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于流匹配（flow matching）的低维潜在空间生成方法，用于快速、物理保真地学习随机闭合模型，相比扩散模型可实现单步采样且速度提升达两个数量级。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成高质量且多样化的样本，但采样速度慢，难以满足复杂动力系统中对高效、物理保真随机闭合模型的需求。

Method: 在2D Kolmogorov流数值例子上系统比较基于传输的生成模型；采用低维潜在空间中的流匹配实现单步采样；对比隐式联合训练正则化与显式度量保持（MP）和几何感知（GA）正则化以控制潜在空间失真。

Result: 流匹配在低维潜在空间中实现了比迭代扩散方法快100倍的单步采样；显式与隐式正则化均能保留原始动力系统的拓扑信息，从而在小数据下学习出物理保真的随机闭合模型。

Conclusion: 低维流匹配结合适当正则化是学习高效、物理保真随机闭合模型的有效路径，克服了扩散模型采样慢和数据依赖强的局限。

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


### [194] [A Locality Radius Framework for Understanding Relational Inductive Bias in Database Learning](https://arxiv.org/abs/2602.17092)
*Aadi Joshi,Kavya Bhand*

Main category: cs.LG

TL;DR: 本文提出‘局部半径’概念，量化关系模式中预测任务所需的最小结构邻域，并验证模型性能与任务局部半径和GNN聚合深度对齐密切相关。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的关系模式预测任务（如外键发现）隐含假设多跳结构推理必要，但该假设缺乏验证；需明确何时及为何需要多跳推理。

Method: 提出‘局部半径’形式化度量，设计控制实验，在多个schema级任务上开展多种子、容量匹配、统计显著性检验、缩放分析及合成半径可控基准测试。

Result: 实证结果一致表明存在‘偏差-半径对齐效应’：当GNN聚合深度与任务局部半径匹配时，模型性能最优；不匹配时性能显著下降。

Conclusion: 任务局部半径是理解GNN在schema建模中有效性的关键因素，应作为架构设计与评估的核心依据，而非默认依赖深层多跳推理。

Abstract: Foreign key discovery and related schema-level prediction tasks are often modeled using graph neural networks (GNNs), implicitly assuming that relational inductive bias improves performance. However, it remains unclear when multi-hop structural reasoning is actually necessary. In this work, we introduce locality radius, a formal measure of the minimum structural neighborhood required to determine a prediction in relational schemas. We hypothesize that model performance depends critically on alignment between task locality radius and architectural aggregation depth. We conduct a controlled empirical study across foreign key prediction, join cost estimation, blast radius regression, cascade impact classification, and additional graph-derived schema tasks. Our evaluation includes multi-seed experiments, capacity-matched comparisons, statistical significance testing, scaling analysis, and synthetic radius-controlled benchmarks. Results reveal a consistent bias-radius alignment effect.

</details>


### [195] [FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment](https://arxiv.org/abs/2602.17095)
*Chuiyang Meng,Ming Tang,Vincent W. S. Wong*

Main category: cs.LG

TL;DR: 本文提出FLoRG框架，通过使用单个低秩矩阵并聚合其Gram矩阵来解决联邦微调中LoRA方法的聚合误差和分解漂移问题，并引入Procrustes对齐以减少分解漂移，理论分析证明其收敛性更优，实验表明其在下游任务准确率和通信开销方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在联邦学习中使用两个独立低秩矩阵导致聚合误差和矩阵分解不唯一引发的分解漂移问题。

Method: 提出FLoRG框架：用单个低秩矩阵替代双矩阵，聚合其Gram矩阵避免聚合误差；引入Procrustes对齐缓解连续轮次间分解漂移；理论分析收敛性并给出更紧的收敛界。

Result: 在多个LLM微调基准上，FLoRG在下游任务准确率上超越5种SOTA基线方法，通信开销最多降低2041倍。

Conclusion: FLoRG有效解决了联邦LoRA微调中的关键挑战，在精度与通信效率上实现显著提升，为隐私保护下的大模型协同微调提供了新范式。

Abstract: Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\times$.

</details>


### [196] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 本文提出了一种基于无服务器架构的MLOps框架，覆盖机器学习全生命周期，以事件驱动和托管服务实现模型无关、可扩展、低成本、高可靠性的工业级HS编码预测系统。


<details>
  <summary>Details</summary>
Motivation: 解决海关贸易中HS编码预测任务面临的描述简短模糊、更新频繁、错误代价高（如货物流通延迟与经济损失）等挑战，同时兼顾模型准确性、可审计性、SLA保障与长期运维成本。

Method: 构建模型无关的无服务器MLOps框架，采用事件驱动流水线与托管服务；使用自定义文本嵌入编码器与多种深度学习模型（含Text-CNN）；集成自动化A/B测试、弹性伸缩、可复现训练与部署流程；优先选用低延迟、可解释、低成本的确定性分类模型（而非高成本Transformer/LLM）。

Result: Text-CNN在真实数据上达到98%准确率；系统满足可复现性、可审计性、SLA保障及动态A/B测试能力；验证了在工业场景下高效、经济、合规地落地ML模型的可行性。

Conclusion: 该无服务器MLOps框架为企业的ML工业化提供了可复用的蓝图，在保证性能的同时显著优化运营成本与工程效率，适用于合规敏感、高时效要求的业务场景。

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [197] [Online Learning with Improving Agents: Multiclass, Budgeted Agents and Bandit Learners](https://arxiv.org/abs/2602.17103)
*Sajad Ashkezari,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文研究了带有改进的学习模型，扩展了先前结果，提出了刻画在线可学习性的组合维度，并分析了多类设置、带反馈学习及代理改进成本建模等问题。


<details>
  <summary>Details</summary>
Motivation: 研究带有改进的学习模型，允许代理对特征值进行小幅调整以获得更理想的标签，从而更好地刻画现实中的学习行为。

Method: 通过引入组合维度来刻画在线可学习性，扩展至多类分类、带反馈（bandit）学习场景，并对代理改进所需成本进行建模。

Result: 提供了刻画该模型下在线可学习性的组合维度，完成了多类情形与带反馈情形的理论分析，并建立了代理改进成本的建模框架。

Conclusion: 该工作系统性地拓展了带有改进的学习模型的理论基础，为理解代理驱动的学习过程提供了新工具和新视角。

Abstract: We investigate the recently introduced model of learning with improvements, where agents are allowed to make small changes to their feature values to be warranted a more desirable label. We extensively extend previously published results by providing combinatorial dimensions that characterize online learnability in this model, by analyzing the multiclass setup, learnability in a bandit feedback setup, modeling agents' cost for making improvements and more.

</details>


### [198] [i-PhysGaussian: Implicit Physical Simulation for 3D Gaussian Splatting](https://arxiv.org/abs/2602.17117)
*Yicheng Cao,Zhuo Huang,Yu Yao,Yiming Ying,Daoyi Dong,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文提出i-PhysGaussian框架，将3D高斯泼溅与隐式质点法（MPM）积分器结合，通过隐式牛顿优化最小化动量平衡残差，显著提升物理仿真稳定性与时间步长鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D重建的仿真器依赖显式步进更新，对时间步长敏感，在高刚度材料或准静态运动等复杂场景下精度迅速下降。

Method: 提出i-PhysGaussian框架，耦合3D高斯泼溅（3DGS）与隐式MPM积分器，采用基于GMRES求解器的隐式牛顿型优化来最小化动量平衡残差，获得步长末态。

Result: i-PhysGaussian在时间步长达显式基线20倍时仍保持稳定，结构连贯且运动平滑，尤其在复杂动态过渡中表现优异。

Conclusion: 该方法有效缓解了显式仿真对小步长的依赖，提升了物理一致性与时效性，为工业与工程风险管控提供了更鲁棒的仿真工具。

Abstract: Physical simulation predicts future states of objects based on material properties and external loads, enabling blueprints for both Industry and Engineering to conduct risk management. Current 3D reconstruction-based simulators typically rely on explicit, step-wise updates, which are sensitive to step time and suffer from rapid accuracy degradation under complicated scenarios, such as high-stiffness materials or quasi-static movement. To address this, we introduce i-PhysGaussian, a framework that couples 3D Gaussian Splatting (3DGS) with an implicit Material Point Method (MPM) integrator. Unlike explicit methods, our solution obtains an end-of-step state by minimizing a momentum-balance residual through implicit Newton-type optimization with a GMRES solver. This formulation significantly reduces time-step sensitivity and ensures physical consistency. Our results demonstrate that i-PhysGaussian maintains stability at up to 20x larger time steps than explicit baselines, preserving structural coherence and smooth motion even in complex dynamic transitions.

</details>


### [199] [TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series](https://arxiv.org/abs/2602.17122)
*Xihao Piao,Zheng Chen,Lingwei Zhu,Yushun Dong,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文提出了一种名为Time-Invariant Frequency Operator (TIFO)的新方法，通过在频域中学习对整个数据集具有平稳性感知的权重，缓解非平稳时间序列预测中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列预测面临训练与测试数据分布不一致（分布偏移）的问题，现有方法未能有效建模时序数据中随时间演化的复杂结构。

Method: 提出TIFO算子，在频域中对全量数据学习平稳性感知的频谱权重，利用傅里叶变换隐含的频域特征分解性质，增强平稳频率分量、抑制非平稳分量；该方法为即插即用式模块。

Result: 在28个预测设置中取得18个第一、6个第二；ETTm2数据集上平均MSE提升33.3%和55.3%；计算开销降低60%–70%，具备强可扩展性。

Conclusion: TIFO通过频域建模有效缓解非平稳时间序列的分布偏移问题，兼具高性能、高效率与模型无关性，为时间序列预测提供了新范式。

Abstract: Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.

</details>


### [200] [Unified Latents (UL): How to train your latents](https://arxiv.org/abs/2602.17270)
*Jonathan Heek,Emiel Hoogeboom,Thomas Mensink,Tim Salimans*

Main category: cs.LG

TL;DR: Unified Latents (UL) 是一种联合使用扩散先验和扩散解码器学习潜在表示的框架，通过将编码器输出噪声与先验最小噪声水平关联，实现紧致的潜在比特率上界，在图像和视频生成任务中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 旨在学习高效且高质量的潜在表示，兼顾生成质量、重建质量和训练效率。

Method: 提出 Unified Latents (UL) 框架，联合正则化潜在表示（用扩散先验）并用扩散模型解码；将编码器输出噪声与先验最小噪声水平对齐，导出简洁训练目标。

Result: 在 ImageNet-512 上 FID 达 1.4、PSNR 高且训练 FLOPs 更少；在 Kinetics-600 上 FVD 达 1.3，创 SOTA。

Conclusion: UL 框架在保持高重建与生成质量的同时，显著提升潜在表示的紧凑性与训练效率，适用于图像与视频生成。

Abstract: We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.

</details>


### [201] [VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation](https://arxiv.org/abs/2602.17133)
*Linwei Zhai,Han Ding,Mingzhi Lin,Cui Zhao,Fei Wang,Ge Wang,Wang Zhi,Wei Xi*

Main category: cs.LG

TL;DR: 本文提出VP-VAE，通过用Metropolis-Hastings采样生成的结构化潜空间扰动替代非可微量化器，解耦表征学习与离散化过程，避免显式码本及其带来的训练不稳定和码本坍缩问题；进一步推导出轻量变体FSP，统一解释并改进FSQ类固定量化器，在图像与音频任务中提升重建质量与码字使用均衡性。


<details>
  <summary>Details</summary>
Motivation: VQ-VAEs存在训练不稳定和码本坍缩问题，根源在于表征学习与离散码本优化的强耦合。

Method: 提出VP-VAE：以分布一致、尺度自适应的潜变量扰动（由Metropolis–Hastings采样生成）替代非可微量化器，彻底去除训练中对显式码本的依赖；在近似均匀潜变量假设下，导出轻量变体FSP，为FSQ类量化器提供理论解释与实践改进。

Result: 在图像和音频基准上，VP-VAE和FSP显著提升重建保真度，实现更均衡的token使用，并完全规避码本耦合训练导致的不稳定性。

Conclusion: 解耦量化过程与码本优化是提升VQ-VAE稳定性和性能的有效路径；扰动视角为向量量化建模提供了新范式，FSP进一步实现了理论统一与工程实用性的兼顾。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and "codebook collapse" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.

</details>


### [202] [The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound](https://arxiv.org/abs/2602.17321)
*Christoph Balada,Aida Romano-Martinez,Payal Varshney,Vincent ten Cate,Katharina Geschke,Jonas Tesarz,Paul Claßen,Alexander K. Schuster,Dativa Tibyampansha,Karl-Patrik Kresoja,Philipp S. Wild,Sheraz Ahmed,Andreas Dengel*

Main category: cs.LG

TL;DR: 本文提出了一种基于颈动脉超声视频的机器学习框架，利用高血压作为弱监督标签，提取血管损伤（VD）的临床可解释特征，用于心血管疾病风险预测，性能媲美甚至超越传统风险模型（如SCORE2），且无需实验室检测。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球首要死因，但早期风险检测受限于现有诊断手段；颈动脉超声虽广泛可用、无创，但其蕴含的结构与血流动力学信息尚未被充分挖掘。

Method: 构建一个机器学习框架，从颈动脉超声视频中学习血管损伤（VD）表征，以高血压为弱监督标签；结合可解释AI分析模型决策依据。

Result: 模型学习到的VD特征具有生物学合理性与可解释性，与已知心血管危险因素、共病及实验室指标强相关；高VD评分能有效分层预测心肌梗死、心脏死亡和全因死亡，性能匹配或优于SCORE2等传统风险模型。

Conclusion: 常规颈动脉超声蕴含远超既往认知的预后信息；该方法为大规模、无创、低成本的心血管风险评估提供了新工具，支持更早、更个性化的预防策略。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.

</details>


### [203] [When More Experts Hurt: Underfitting in Multi-Expert Learning to Defer](https://arxiv.org/abs/2602.17144)
*Shuqi Liu,Yuzhou Cao,Lei Feng,Bo An,Luke Ong*

Main category: cs.LG

TL;DR: 本文研究了多专家环境下的学习延迟（L2D）问题，指出其比单专家情况更难，根源在于专家可识别性问题；为此提出PiCCE方法，通过自适应选择可靠专家，将多专家L2D简化为类单专家学习问题，并在理论和实验上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多专家L2D存在固有的欠拟合问题，源于专家可识别性缺失，而现有单专家欠拟合修复方法对此无效。

Method: 提出PiCCE（Pick the Confident and Correct Expert）方法，基于经验证据自适应识别可靠专家，将多专家L2D降维为类单专家学习问题，并证明其统计一致性及对类别概率与专家准确率的恢复能力。

Result: PiCCE有效缓解多专家L2D中的欠拟合问题，在多种设置（含真实专家场景）的实验中显著提升性能，验证了理论分析。

Conclusion: 多专家L2D的根本挑战在于专家可识别性，PiCCE通过代理建模与自适应专家选择成功克服该问题，实现了从多专家到单专家学习范式的有效转化。

Abstract: Learning to Defer (L2D) enables a classifier to abstain from predictions and defer to an expert, and has recently been extended to multi-expert settings. In this work, we show that multi-expert L2D is fundamentally more challenging than the single-expert case. With multiple experts, the classifier's underfitting becomes inherent, which seriously degrades prediction performance, whereas in the single-expert setting it arises only under specific conditions. We theoretically reveal that this stems from an intrinsic expert identifiability issue: learning which expert to trust from a diverse pool, a problem absent in the single-expert case and renders existing underfitting remedies failed. To tackle this issue, we propose PiCCE (Pick the Confident and Correct Expert), a surrogate-based method that adaptively identifies a reliable expert based on empirical evidence. PiCCE effectively reduces multi-expert L2D to a single-expert-like learning problem, thereby resolving multi expert underfitting. We further prove its statistical consistency and ability to recover class probabilities and expert accuracies. Extensive experiments across diverse settings, including real-world expert scenarios, validate our theoretical results and demonstrate improved performance.

</details>


### [204] [TimeOmni-VL: Unified Models for Time Series Understanding and Generation](https://arxiv.org/abs/2602.17149)
*Tong Guan,Sheng Pan,Johan Barthelemy,Zhao Li,Yujun Cai,Cesare Alippi,Ming Jin,Shirui Pan*

Main category: cs.LG

TL;DR: 本文提出TimeOmni-VL，首个以视觉为中心的统一框架，通过保真双向时序-图像映射（Bi-TSI）和理解引导生成，在时间序列的理解与生成任务间实现协同提升。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列建模在数值生成与语义理解之间存在割裂：生成模型依赖表层模式匹配，理解模型难以输出高保真数值；而统一多模态模型（UMMs）在视觉领域已成功，在时序领域尚未探索。

Method: 提出TimeOmni-VL框架，包含两项创新：(1) 保真双向时序-图像映射（Bi-TSI），提升TS2I与I2TS转换质量；(2) 理解引导生成机制；并构建TSUMM-Suite数据集（含6项理解+2项生成任务），结合校准的思维链（Chain-of-Thought）将理解结果作为生成控制信号。

Result: 实验表明，TimeOmni-VL在语义理解和数值精度两方面均显著优于基线方法，首次实现二者协同提升。

Conclusion: TimeOmni-VL开创了多模态时间序列建模新范式，证明理解可作为生成的有效控制信号，为时序统一建模奠定基础。

Abstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.

</details>


### [205] [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting](https://arxiv.org/abs/2602.17645)
*Xiaohan Zhao,Zhaoyi Li,Yaxin Luo,Jiacheng Cui,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 本文提出M-Attack-V2，一种针对大型视觉语言模型（LVLMs）的黑盒对抗攻击方法，通过多裁剪对齐（MCA）、辅助目标对齐（ATA）和Patch Momentum等模块，显著提升迁移攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于迁移的黑盒攻击（如M-Attack）在LVLM上效果受限，因其局部裁剪匹配导致梯度高方差、不一致，根源在于ViT的平移敏感性和源/目标图像裁剪结构不对称。

Method: 提出M-Attack-V2：1）Multi-Crop Alignment（MCA）在源端对多个独立采样的局部视图梯度平均以降方差；2）Auxiliary Target Alignment（ATA）用语义相关的小辅助集替代强目标增强，构建更平滑的目标流形；3）Patch Momentum重放历史裁剪梯度，并结合改进的patch-size ensemble（PE+）增强可迁移方向。

Result: 在Claude-4.0、Gemini-2.5-Pro和GPT-5上攻击成功率分别从8%→30%、83%→97%、98%→100%，显著优于先前黑盒LVLM攻击方法。

Conclusion: M-Attack-V2是一种简单、模块化、高效的升级方案，有效缓解了LVLM黑盒攻击中梯度不稳定与局部对齐失效问题，为多模态模型安全评估提供了新工具。

Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.

</details>


### [206] [Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization](https://arxiv.org/abs/2602.17155)
*Yicheng Lang,Changsheng Wang,Yihua Zhang,Mingyi Hong,Zheng Zhang,Wotao Yin,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出ZO-Muon方法，通过结合子空间投影和梯度正交化，显著提升零阶优化在大模型微调中的准确性和查询效率。


<details>
  <summary>Details</summary>
Motivation: 零阶优化在大模型微调中面临精度与查询效率之间的根本矛盾，亟需更高效的方法。

Method: 提出统一框架——子空间梯度正交化，具体实现为ZO-Muon：融合基于投影的低秩子空间估计与Muon风格的谱优化（梯度正交化）。

Result: 在LLM和ViT上实验表明，ZO-Muon大幅加速收敛；相比MeZO，LLM微调仅需24.7%查询量即达相同SST-2性能，ViT-B在CIFAR-100上准确率提升25.1%。

Conclusion: ZO-Muon成功协调精度与效率矛盾，为零阶优化提供了更优的实用范式。

Abstract: Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as a low-rank Muon optimizer in the ZO setting. Extensive experiments on large language models (LLMs) and vision transformers (ViTs) demonstrate that ZO-Muon significantly accelerates convergence and achieves a win-win improvement in accuracy and query/runtime efficiency. Notably, compared to the popular MeZO baseline, ZO-Muon requires only 24.7% of the queries to reach the same SST-2 performance for LLM fine-tuning, and improves accuracy by 25.1% on ViT-B fine-tuning on CIFAR-100.

</details>


### [207] [In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks](https://arxiv.org/abs/2602.17171)
*Ayush Goel,Arjun Kohli,Sarvagya Somvanshi*

Main category: cs.LG

TL;DR: 本文实证研究了transformer与线性注意力模型在上下文学习（ICL）任务（如线性回归）中的行为差异，比较了其学习质量、收敛性与泛化能力，并分析了模型深度的影响。


<details>
  <summary>Details</summary>
Motivation: 探究线性注意力与标准二次注意力（transformer）在上下文学习中的表现异同，尤其在经典线性回归任务上。

Method: 在Garg等人提出的线性回归ICL任务上，对transformer与线性注意力模型进行实验评估，指标包括MSE、收敛性与泛化能力，并系统改变模型深度。

Result: 揭示了两类模型在ICL中的相似性与线性注意力的局限性，例如在学习质量或泛化方面可能弱于标准attention。

Conclusion: 线性注意力虽具潜力，但在ICL任务中仍存在相对于标准transformer的性能限制，深度增加的影响也因架构而异。

Abstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.

</details>


### [208] [Continual uncertainty learning](https://arxiv.org/abs/2602.17174)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: cs.LG

TL;DR: 本文提出一种基于课程学习的持续学习框架，用于解决含多重不确定性的非线性机械系统鲁棒控制问题，通过任务分解、动态扩展植物集、结合模型基控制器的残差学习机制，提升策略泛化性与样本效率，并成功应用于汽车动力总成主动振动控制的仿真到实物迁移。


<details>
  <summary>Details</summary>
Motivation: 传统DRL结合域随机化难以同时高效处理机械系统中交织的多种不确定性（如非线性动力学与工况变化），易导致策略次优和学习效率低。

Method: 构建课程式持续学习框架：将多重不确定性控制问题分解为序列化任务；逐步扩展与多样化动态不确定性植物集；采用稳定跨任务更新策略防止灾难性遗忘；引入模型基控制器（MBC）作为共享性能基线，实施残差式DRL优化。

Result: 在汽车动力总成主动振动控制任务中验证了方法有效性：控制器对结构非线性和动态变化具有强鲁棒性，成功实现仿真到实物迁移。

Conclusion: 课程驱动的持续学习与模型基残差学习相结合，可有效提升DRL在多重不确定性非线性系统中的泛化能力、稳定性与样本效率，为复杂工业鲁棒控制提供新范式。

Abstract: Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.

</details>


### [209] [The Anxiety of Influence: Bloom Filters in Transformer Attention Heads](https://arxiv.org/abs/2602.17526)
*Peter Balogh*

Main category: cs.LG

TL;DR: 本文发现Transformer模型中存在专门进行成员测试（membership testing）的注意力头，这些头能高效判断当前token是否在上下文中出现过，并表现出类似Bloom滤波器但更优的性能，且具有多分辨率、距离敏感等特性。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer中是否存在专门执行成员测试（即判断某token是否已在上下文中出现过）的注意力机制，并理解其工作原理与理论界限。

Method: 通过系统性分析GPT-2（small/medium/large）和Pythia-160M中的注意力头行为，结合假阳性率测量、容量拟合、混淆控制实验及消融研究，识别并分类具有成员测试功能的注意力头。

Result: 识别出三个真实具备成员测试能力的注意力头（L0H1、L0H5、L1H11），其中两个超越经典Bloom滤波器容量限制，一个符合Bloom理论曲线；L3H0被证伪为前缀注意力头；三者构成早期层（0–1）的多分辨率、距离敏感系统，泛化性强于仅识别重复名称的头。

Conclusion: Transformer中存在一类功能明确、鲁棒且泛化能力强的成员测试注意力头，其行为超越传统数据结构理论预期，表明模型自发演化出高效近似集合查询机制，且该机制与其它计算功能共存。

Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question "has this token appeared before in the context?" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\% even at 180 unique context tokens -- well above the $d_\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \approx 5$ bits, saturating by $n \approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.

</details>


### [210] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: 本文介绍了softdtw-cuda-torch，一个用于在GPU上计算SoftDTW的开源PyTorch库，解决了现有实现中序列长度限制、反向传播数值不稳定和内存消耗过大的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有GPU版SoftDTW实现存在的三个关键限制：序列长度硬性上限（1024）、小平滑参数下反向传播的数值不稳定性、以及因显式构建成对距离张量导致的过高GPU内存占用。

Method: 提出三种技术：(1) 分块反对角线核执行以消除序列长度限制；(2) 对数空间反向传播防止浮点溢出；(3) 融合距离计算模式，避免O(BN M)中间距离张量，大幅降低内存使用。

Result: 实现了任意长度序列支持、完整PyTorch自动微分集成、Soft-DTW质心计算，并达到最高98%的内存减少。

Conclusion: softdtw-cuda-torch是一个高效、稳定、易用的SoftDTW GPU加速库，显著提升了其在长序列与大规模任务中的实用性。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [211] [CounterFlowNet: From Minimal Changes to Meaningful Counterfactual Explanations](https://arxiv.org/abs/2602.17244)
*Oleksii Furman,Patryk Marszałek,Jan Masłowski,Piotr Gaiński,Maciej Zięba,Marek Śmieja*

Main category: cs.LG

TL;DR: 本文提出CounterFlowNet，一种基于条件生成流网络（GFlowNet）的生成式反事实解释方法，通过序列化特征修改生成高质量、稀疏、符合用户约束的反事实解释。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法难以同时满足：仅修改少量特征、适用于异构表格数据、并严格遵循用户定义的约束（如不可变性、单调性）。

Method: 将反事实生成建模为序列化特征修改过程，采用条件生成流网络（GFlowNet），以用户指定的奖励函数（涵盖有效性、稀疏性、邻近性、合理性）指导采样；通过动作掩码在推理时动态施加可操作性约束。

Result: 在八个数据集、两种评估协议下，CounterFlowNet在有效性、稀疏性、合理性与多样性之间取得更优权衡，并完全满足给定约束。

Conclusion: CounterFlowNet是一种灵活、高效且约束友好的反事实解释生成框架，尤其适用于复杂、受约束的表格数据场景。

Abstract: Counterfactual explanations (CFs) provide human-interpretable insights into model's predictions by identifying minimal changes to input features that would alter the model's output. However, existing methods struggle to generate multiple high-quality explanations that (1) affect only a small portion of the features, (2) can be applied to tabular data with heterogeneous features, and (3) are consistent with the user-defined constraints. We propose CounterFlowNet, a generative approach that formulates CF generation as sequential feature modification using conditional Generative Flow Networks (GFlowNet). CounterFlowNet is trained to sample CFs proportionally to a user-specified reward function that can encode key CF desiderata: validity, sparsity, proximity and plausibility, encouraging high-quality explanations. The sequential formulation yields highly sparse edits, while a unified action space seamlessly supports continuous and categorical features. Moreover, actionability constraints, such as immutability and monotonicity of features, can be enforced at inference time via action masking, without retraining. Experiments on eight datasets under two evaluation protocols demonstrate that CounterFlowNet achieves superior trade-offs between validity, sparsity, plausibility, and diversity with full satisfaction of the given constraints.

</details>


### [212] [Structured Prototype-Guided Adaptation for EEG Foundation Models](https://arxiv.org/abs/2602.17251)
*Jingying Ma,Feng Wu,Yucheng Xing,Qika Lin,Tianyu Liu,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: 本文提出SCOPE框架，通过结构化置信度感知原型引导的自适应方法，解决EEG基础模型在标签受限跨被试场景下泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型在全量微调时表现优异，但在临床中常见的被试级标注数据稀缺情况下泛化能力差，根本原因在于噪声多、标注少的监督信号与EFM高度可塑参数空间之间的结构性不匹配。

Method: 提出两阶段SCOPE框架：第一阶段利用几何正则化任务先验构建类级别平衡原型，并基于原型一致性生成置信度感知伪标签以过滤不可靠信号；第二阶段设计轻量ProAdapter，在冻结EFM基础上以结构化原型为条件进行适配。

Result: 在三个EEG任务和五个基础模型主干网络上的实验表明，SCOPE在标签受限的跨被试设置下持续取得高性能与高效率。

Conclusion: SCOPE有效缓解了EEG基础模型在低资源临床场景下的泛化瓶颈，为可信、鲁棒的脑电AI建模提供了新范式。

Abstract: Electroencephalography (EEG) foundation models (EFMs) have achieved strong performance under full fine-tuning but exhibit poor generalization when subject-level supervision is limited, a common constraint in real-world clinical settings. We show that this failure stems not merely from limited supervision, but from a structural mismatch between noisy, limited supervision and the highly plastic parameter space of EFMs. To address this challenge, we propose SCOPE, a Structured COnfidence-aware Prototype-guided adaptation framework for EFM fine-tuning. SCOPE follows a two-stage pipeline. In the first stage, we construct reliable external supervision by learning geometry-regularized task priors, constructing balanced class-level prototypes over the resulting embeddings, and producing confidence-aware pseudo-labels from their agreement to filter unreliable signals on unlabeled data. In the second stage, we introduce ProAdapter, which adapts frozen EEG foundation models via a lightweight adapter conditioned on the structured prototypes. Experiments across three EEG tasks and five foundation model backbones demonstrate that SCOPE consistently achieves strong performance and efficiency under label-limited cross-subject settings.

</details>


### [213] [Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems](https://arxiv.org/abs/2602.17263)
*Alexander Klemps,Denis Ilia,Pradeep Kr. Banerjee,Ye Chen,Henrik Tünnermann,Nihat Ay*

Main category: cs.LG

TL;DR: 本文提出了一种基于Wasserstein自编码器的生成建模框架，用于学习激光脉冲整形与下游电子束动力学之间的可微潜变量接口，从而降低对高成本脉冲传播仿真的依赖。


<details>
  <summary>Details</summary>
Motivation: 自由电子激光器光注入器中纵向激光脉冲形状的调控对优化电子束质量至关重要，但其设计空间庞大，传统暴力仿真成本过高，亟需高效替代方法。

Method: 采用Wasserstein Autoencoder构建生成模型，学习脉冲形状到电子束动力学响应的可微潜变量映射；通过主成分分析和高斯混合模型分析潜空间结构，并验证其在仿真与实测数据上的泛化能力。

Result: 所学潜空间连续、可解释、高保真；不同脉冲族（如高阶高斯）在潜空间中呈连贯轨迹；脉冲时长标准化后潜变量分布与能量相关；线性插值可实现脉冲类型间平滑过渡；模型能准确重建并嵌入真实实验脉冲。

Conclusion: 该框架显著减少对昂贵脉冲传播仿真的依赖，提升电子束动力学仿真与分析效率，为激光脉冲调控提供数据驱动新范式。

Abstract: Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gaussian Mixture Models reveals a well behaved latent geometry, enabling smooth transitions between distinct pulse types via linear interpolation. The model generalizes from simulated data to real experimental pulse measurements, accurately reconstructing pulses and embedding them consistently into the learned manifold. Overall, the approach reduces reliance on expensive pulse-propagation simulations and facilitates downstream beam dynamics simulation and analysis.

</details>


### [214] [RLGT: A reinforcement learning framework for extremal graph theory](https://arxiv.org/abs/2602.17276)
*Ivan Damnjanović,Uroš Milivojević,Irena Đorđević,Dragan Stevanović*

Main category: cs.LG

TL;DR: 本文提出了一个名为RLGT的新强化学习框架，用于系统化和扩展先前在极值图论中应用强化学习的工作，支持有向/无向图、带/不带环、多边着色图，并优化了计算性能与模块化设计。


<details>
  <summary>Details</summary>
Motivation: 先前Wagner等人将深度交叉熵强化学习方法应用于极值图论问题并取得成果，但缺乏统一、灵活且可扩展的框架；现有环境分散、图类型支持有限，限制了后续研究。

Method: 提出RLGT框架，支持多种图结构（有向/无向、含/不含环、多边色），采用高效图表示方法，具备清洁、模块化设计和优化计算性能。

Result: 系统整合了此前RL在极值图论中的应用（如推翻Laplacian谱半径不等式、改进Ramsey数下界、推进Turán型极值问题），并提供通用、可扩展的开源RL环境。

Conclusion: RLGT为基于强化学习的图论研究提供了统一、高效、灵活的基础框架，有望推动更多极值图论问题的自动化发现与求解。

Abstract: Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for certain Ramsey numbers, and contributions were made to the Turán-type extremal problem in which the forbidden structures are cycles of length three and four. Here, we present Reinforcement Learning for Graph Theory (RLGT), a novel RL framework that systematizes the previous work and provides support for both undirected and directed graphs, with or without loops, and with an arbitrary number of edge colors. The framework efficiently represents graphs and aims to facilitate future RL-based research in extremal graph theory through optimized computational performance and a clean and modular design.

</details>


### [215] [Efficient privacy loss accounting for subsampling and random allocation](https://arxiv.org/abs/2602.17284)
*Vitaly Feldman,Moshe Shenfeld*

Main category: cs.LG

TL;DR: 本文研究了一种随机分配采样方案的隐私放大特性，提出了一种基于隐私损失分布（PLD）的高效计算方法，克服了现有理论分析中隐私参数不紧和计算开销大的问题，并证明其在DP-SGD训练中优于泊松采样。


<details>
  <summary>Details</summary>
Motivation: 现有对随机分配采样方案的理论分析存在隐私参数不紧、依赖近似步骤，且使用hockey stick或Renyi散度导致隐私损失核算开销大等问题。

Method: 提出基于隐私损失分布（PLD）的通用隐私损失核算方法，引入PLD实现（PLD realization）概念，支持对任意差分隐私算法（特别是高斯机制）下的随机分配采样进行高效精确的PLD计算。

Result: 实现了对随机分配采样的精确、高效PLD计算；在高斯机制下，其隐私-效用权衡至少不劣于泊松子采样；特别适用于DP-SGD训练；扩展了通用隐私损失核算能力，无需针对噪声机制手动分析。

Conclusion: 随机分配是一种具有实用优势的采样方案，本文提出的PLD realization框架为子采样场景提供了更准确、更通用的隐私核算工具，推动了差分隐私优化与聚合算法的实际部署。

Abstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used in privacy loss accounting.
  In this work, we demonstrate that the privacy loss distribution (PLD) of random allocation applied to any differentially private algorithm can be computed efficiently. When applied to the Gaussian mechanism, our results demonstrate that the privacy-utility trade-off for random allocation is at least as good as that of Poisson subsampling. In particular, random allocation is better suited for training via DP-SGD. To support these computations, our work develops new tools for general privacy loss accounting based on a notion of PLD realization. This notion allows us to extend accurate privacy loss accounting to subsampling which previously required manual noise-mechanism-specific analysis.

</details>


### [216] [LexiSafe: Offline Safe Reinforcement Learning with Lexicographic Safety-Reward Hierarchy](https://arxiv.org/abs/2602.17312)
*Hsin-Jung Yang,Zhanhong Jiang,Prajwal Koirala,Qisai Liu,Cody Fleming,Soumik Sarkar*

Main category: cs.LG

TL;DR: 本文提出LexiSafe框架，一种面向离线安全强化学习的字典序方法，用于保障网络物理系统（CPS）在无在线试错前提下的安全性与性能，包含单成本（LexiSafe-SC）与多成本（LexiSafe-MC）两种形式，并提供理论误差界与样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 离线安全强化学习对网络物理系统至关重要，因训练中不可容忍安全违规且仅能使用预采集数据；现有方法缺乏防止安全漂移的结构化机制。

Method: 提出字典序优先的离线RL框架LexiSafe：LexiSafe-SC处理单安全约束并推导安全违规与性能次优性界；LexiSafe-MC扩展至多层级安全成本，并支持相应样本复杂度分析；结合字典序优化与结构偏差设计。

Result: 实验表明LexiSafe相比约束型离线基线方法显著降低安全违规率并提升任务性能。

Conclusion: LexiSafe通过将字典序优先与结构偏差统一，为安全关键CPS决策提供了兼具实用性与理论保障的新范式。

Abstract: Offline safe reinforcement learning (RL) is increasingly important for cyber-physical systems (CPS), where safety violations during training are unacceptable and only pre-collected data are available. Existing offline safe RL methods typically balance reward-safety tradeoffs through constraint relaxation or joint optimization, but they often lack structural mechanisms to prevent safety drift. We propose LexiSafe, a lexicographic offline RL framework designed to preserve safety-aligned behavior. We first develop LexiSafe-SC, a single-cost formulation for standard offline safe RL, and derive safety-violation and performance-suboptimality bounds that together yield sample-complexity guarantees. We then extend the framework to hierarchical safety requirements with LexiSafe-MC, which supports multiple safety costs and admits its own sample-complexity analysis. Empirically, LexiSafe demonstrates reduced safety violations and improved task performance compared to constrained offline baselines. By unifying lexicographic prioritization with structural bias, LexiSafe offers a practical and theoretically grounded approach for safety-critical CPS decision-making.

</details>


### [217] [Flickering Multi-Armed Bandits](https://arxiv.org/abs/2602.17315)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文提出了闪烁多臂老虎机（FMAB）框架，用于处理动态变化且受局部移动约束的可用动作集问题，并设计了两阶段算法，在两种随机图模型下实现了次线性遗憾界，同时证明了探索成本的近似最优性。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机假设所有臂始终可用，但现实中如灾难救援场景中，智能体可选动作（如探测点）随时间和位置动态变化，且受限于局部移动能力，需新框架建模此类约束演化可用性。

Method: 提出基于随机图过程（i.i.d. Erdős–Rényi 和 Edge-Markovian）建模臂可用性演化；设计两阶段算法：第一阶段用懒惰随机游走进行探索以识别最优臂，第二阶段导航并锁定最优臂进行 exploitation；理论分析采用高概率和期望意义下的次线性遗憾界，并推导信息论下界。

Result: 在两种图模型下均获得高概率和期望意义的次线性后悔界；证明探索成本近似最优（存在匹配的信息论下界）；数值模拟验证算法在机器人灾后侦察等实际场景中的有效性。

Conclusion: FMAB 框架有效刻画了局部移动约束下的动态动作选择问题；所提两阶段算法兼具理论最优性与实践可行性，为受限环境中的序贯决策提供了新范式。

Abstract: We introduce Flickering Multi-Armed Bandits (FMAB), a new MAB framework where the set of available arms (or actions) can change at each round, and the available set at any time may depend on the agent's previously selected arm. We model this constrained, evolving availability using random graph processes, where arms are nodes and the agent's movement is restricted to its local neighborhood. We analyze this problem under two random graph models: an i.i.d. Erdős--Rényi (ER) process and an Edge-Markovian process. We propose and analyze a two-phase algorithm that employs a lazy random walk for exploration to efficiently identify the optimal arm, followed by a navigation and commitment phase for exploitation. We establish high-probability and expected sublinear regret bounds for both graph settings. We show that the exploration cost of our algorithm is near-optimal by establishing a matching information-theoretic lower bound for this problem class, highlighting the fundamental cost of exploration under local-move constraints. We complement our theoretical guarantees with numerical simulations, including a scenario of a robotic ground vehicle scouting a disaster-affected region.

</details>


### [218] [SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework](https://arxiv.org/abs/2602.17330)
*Rong Fu,Zijian Zhang,Wenxin Zhang,Kun Liu,Jiekai Wu,Xianda Li,Simon Fong*

Main category: cs.LG

TL;DR: SubQuad是一个端到端的免疫组库分析流程，通过亚二次检索、GPU加速亲和力计算、多模态融合与公平性约束聚类，解决大规模免疫组库比较分析中的计算效率与数据偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两方面瓶颈：成对亲和度评估接近二次时间复杂度，以及数据集不平衡导致临床重要的稀有克隆型被掩盖。

Method: SubQuad结合抗原感知的近亚二次检索、GPU加速亲和力核函数、学习型多模态融合和公平性约束聚类；采用MinHash预过滤减少候选比较、可微门控模块自适应加权比对与嵌入通道，并通过自动校准保障稀有抗原特异性亚群的比例代表性。

Result: 在大型病毒与肿瘤免疫组库上，SubQuad显著提升吞吐量与峰值内存效率，同时保持或提升召回率@k、聚类纯度及亚群公平性。

Conclusion: SubQuad通过协同设计索引、相似性融合与公平性目标，提供了一个可扩展、去偏倚的免疫组库挖掘平台，适用于疫苗靶点优先排序与生物标志物发现等转化任务。

Abstract: Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.

</details>


### [219] [From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection](https://arxiv.org/abs/2602.17342)
*Luzhi Wang,Xuanshuo Fu,He Zhang,Chuang Liu,Xiaobao Wang,Hongbo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为SIGOOD的无监督图分布外（OOD）检测框架，通过结合提示增强与能量偏好优化（EPO）损失，在测试时持续自学习以逐步强化OOD信号，显著提升图神经网络在开放世界场景下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法多采用单次推理范式，难以逐步修正错误预测并放大OOD信号；且多数依赖训练数据或监督信息，不适用于真正开放、无监督的部署场景。

Method: 提出SIGOOD框架：1）构造提示增强图以放大OOD信号；2）设计能量偏好优化（EPO）损失，利用原始图与提示增强图之间的能量差异驱动提示迭代优化；3）构建自提升循环，将优化后的提示嵌入检测模型，实现测试时连续自学习。

Result: 在21个真实世界数据集上全面评估，SIGOOD显著优于现有SOTA方法；代码已开源。

Conclusion: SIGOOD通过测试时自改进机制有效提升了图OOD检测性能，为GNN在开放世界中的鲁棒部署提供了新范式。

Abstract: Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-pass inference paradigm, which prevents them from progressively correcting erroneous predictions to amplify OOD signals. To this end, we propose a \textbf{S}elf-\textbf{I}mproving \textbf{G}raph \textbf{O}ut-\textbf{o}f-\textbf{D}istribution detector (SIGOOD), which is an unsupervised framework that integrates continuous self-learning with test-time training for effective graph OOD detection. Specifically, SIGOOD generates a prompt to construct a prompt-enhanced graph that amplifies potential OOD signals. To optimize prompts, SIGOOD introduces an Energy Preference Optimization (EPO) loss, which leverages energy variations between the original test graph and the prompt-enhanced graph. By iteratively optimizing the prompt by involving it into the detection model in a self-improving loop, the resulting optimal prompt-enhanced graph is ultimately used for OOD detection. Comprehensive evaluations on 21 real-world datasets confirm the effectiveness and outperformance of our SIGOOD method. The code is at https://github.com/Ee1s/SIGOOD.

</details>


### [220] [Shortcut learning in geometric knot classification](https://arxiv.org/abs/2602.17350)
*Djordje Mihajlovic,Davide Michieletto*

Main category: cs.LG

TL;DR: 本文探讨了使用机器学习（ML）解决纽结分类问题的可行性，发现现有方法依赖于非拓扑特征；为此，作者构建了一个去除非拓扑偏差的数据集和可生成保拓扑几何嵌入的代码工具，为未来基于ML的纽结分类研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 纽结分类是低维拓扑中的核心问题，具有跨学科应用价值；鉴于神经网络在复杂分类任务中的强大能力，自然引出能否用ML解决纽结分类问题的动机。

Method: 分析ML在纽结分类中使用的捷径策略，识别分子动力学模拟生成的多边形纽结数据中被ML利用的隐藏非拓扑特征；构建一个消除非拓扑特征干扰的公开数据集，并开发能忠实探索固定纽结拓扑下几何状态空间的嵌入生成代码。

Result: 发现了ML在现有纽结数据上依赖非拓扑特征进行分类的现象；提供了首个面向纯拓扑分类目标的、可控几何采样的纽结数据集与生成工具。

Conclusion: 当前ML模型尚未真正学习纽结的拓扑本质，需通过严谨的数据构造与生成方法排除几何偏差，方能推动真正基于拓扑理解的ML纽结分类模型发展。

Abstract: Classifying the topology of closed curves is a central problem in low dimensional topology with applications beyond mathematics spanning protein folding, polymer physics and even magnetohydrodynamics. The central problem is how to determine whether two embeddings of a closed arc are equivalent under ambient isotopy. Given the striking ability of neural networks to solve complex classification tasks, it is therefore natural to ask if the knot classification problem can be tackled using Machine Learning (ML). In this paper, we investigate generic shortcut methods employed by ML to solve the knot classification challenge and specifically discover hidden non-topological features in training data generated through Molecular Dynamics simulations of polygonal knots that are used by ML to arrive to positive classifications results. We then provide a rigorous foundation for future attempts to tackle the knot classification challenge using ML by developing a publicly-available (i) dataset, that aims to remove the potential of non-topological feature classification and (ii) code, that can generate knot embeddings that faithfully explore chosen geometric state space with fixed knot topology. We expect that our work will accelerate the development of ML models that can solve complex geometric knot classification challenges.

</details>


### [221] [2Mamba2Furious: Linear in Complexity, Competitive in Accuracy](https://arxiv.org/abs/2602.17363)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: 本文提出2Mamba，通过简化并改进Mamba-2（一种线性注意力变体），在保持高内存效率的同时显著缩小与Softmax注意力的精度差距，甚至在某些情况下超越其精度。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽高效，但表达能力弱、精度低于Softmax注意力；需在不牺牲效率的前提下提升其准确性。

Method: 对Mamba-2进行简化得到Mamba-2S，识别关键组件；进一步改进A-mask结构并提高隐藏状态阶数，构建新模型2Mamba。

Result: 2Mamba在长上下文任务中接近Softmax注意力的精度，同时内存效率显著更高；部分配置下精度甚至超过Softmax注意力。

Conclusion: 通过精简与针对性增强线性注意力核心机制，可在效率与精度之间取得更优平衡，验证了线性注意力仍有较大提升潜力。

Abstract: Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments

</details>


### [222] [A feature-stable and explainable machine learning framework for trustworthy decision-making under incomplete clinical data](https://arxiv.org/abs/2602.17364)
*Justyna Andrys-Olek,Paulina Tworek,Luca Gherardini,Mark W. Ruddock,Mary Jo Kurt,Peter Fitzgerald,Jose Sousa*

Main category: cs.LG

TL;DR: 本文提出CACTUS框架，专为小规模、异质性及不完整临床数据设计，通过特征抽象、可解释分类和系统性特征稳定性分析，提升模型在缺失数据下的鲁棒性与可解释性，并在血尿队列中验证其优于传统机器学习方法的特征稳定性与预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在生物医学高风险场景中因鲁棒性差、可解释性低及在数据扰动（如缺失）下特征不稳定而难以获得信任，亟需能保证特征稳定性和可解释性的新方法。

Method: 提出CACTUS框架，整合特征抽象、可解释分类与系统性特征稳定性分析，量化关键特征在数据质量下降时的保持一致性；在含568例血尿患者的现实队列中，对比随机森林和梯度提升等方法，在可控缺失水平下评估性能与稳定性。

Result: CACTUS在预测性能上达到或优于基准方法，且在缺失率升高时显著保持更高特征稳定性，包括按性别分层分析；特征稳定性被证实提供区别于传统指标的互补信息。

Conclusion: 特征稳定性是评估生物医学机器学习模型可信度的关键维度；CACTUS通过显式建模缺失鲁棒性与优先选择稳定可解释特征，为可信数据驱动决策提供通用框架。

Abstract: Machine learning models are increasingly applied to biomedical data, yet their adoption in high stakes domains remains limited by poor robustness, limited interpretability, and instability of learned features under realistic data perturbations, such as missingness. In particular, models that achieve high predictive performance may still fail to inspire trust if their key features fluctuate when data completeness changes, undermining reproducibility and downstream decision-making. Here, we present CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures), an explainable machine learning framework explicitly designed to address these challenges in small, heterogeneous, and incomplete clinical datasets. CACTUS integrates feature abstraction, interpretable classification, and systematic feature stability analysis to quantify how consistently informative features are preserved as data quality degrades. Using a real-world haematuria cohort comprising 568 patients evaluated for bladder cancer, we benchmark CACTUS against widely used machine learning approaches, including random forests and gradient boosting methods, under controlled levels of randomly introduced missing data. We demonstrate that CACTUS achieves competitive or superior predictive performance while maintaining markedly higher stability of top-ranked features as missingness increases, including in sex-stratified analyses. Our results show that feature stability provides information complementary to conventional performance metrics and is essential for assessing the trustworthiness of machine learning models applied to biomedical data. By explicitly quantifying robustness to missing data and prioritising interpretable, stable features, CACTUS offers a generalizable framework for trustworthy data-driven decision support.

</details>


### [223] [MDP Planning as Policy Inference](https://arxiv.org/abs/2602.17375)
*David Tolpin*

Main category: cs.LG

TL;DR: 本文将回合制马尔可夫决策过程（MDP）规划建模为对策略的贝叶斯推断，提出一种基于策略后验分布的规划方法，并通过变分序贯蒙特卡洛（VSMC）近似该后验；该方法在多个基准任务中展现出与Soft Actor-Critic等基线方法在行为统计和定性上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习常依赖熵正则化或确定性策略优化，而本文旨在从贝叶斯视角建模策略层面的不确定性，将最优性建模为策略的未归一化概率，从而自然导出策略后验分布并支持不确定性感知的决策。

Method: 将策略视为隐变量，定义其关于期望回报单调递增的未归一化最优性概率，构建策略后验；在离散域中采用改进的变分序贯蒙特卡洛（VSMC）进行近似推断，引入状态重访一致性扫描和粒子间转移随机性耦合以应对动态随机性和模拟器噪声；动作选择通过后验预测采样实现，具有Thompson采样解释。

Result: 在Grid World、Blackjack、Triangle Tireworld和Academic Advising等多个领域验证了所提方法的有效性；实验分析了推断出的策略分布结构，并与离散版Soft Actor-Critic进行了对比，揭示了二者在行为多样性、探索模式及统计稳定性上的本质差异。

Conclusion: 将MDP规划视为策略空间上的贝叶斯推断是可行且富有表现力的框架；所提出的VSMC近似方法能有效捕捉策略级不确定性，其后验预测采样机制提供了不同于熵正则化的新型随机控制范式。

Abstract: We cast episodic Markov decision process (MDP) planning as Bayesian inference over _policies_. A policy is treated as the latent variable and is assigned an unnormalized probability of optimality that is monotone in its expected return, yielding a posterior distribution whose modes coincide with return-maximizing solutions while posterior dispersion represents uncertainty over optimal behavior. To approximate this posterior in discrete domains, we adapt variational sequential Monte Carlo (VSMC) to inference over deterministic policies under stochastic dynamics, introducing a sweep that enforces policy consistency across revisited states and couples transition randomness across particles to avoid confounding from simulator noise. Acting is performed by posterior predictive sampling, which induces a stochastic control policy through a Thompson-sampling interpretation rather than entropy regularization. Across grid worlds, Blackjack, Triangle Tireworld, and Academic Advising, we analyze the structure of inferred policy distributions and compare the resulting behavior to discrete Soft Actor-Critic, highlighting qualitative and statistical differences that arise from policy-level uncertainty.

</details>


### [224] [Convergence Analysis of Two-Layer Neural Networks under Gaussian Input Masking](https://arxiv.org/abs/2602.17423)
*Afroditi Kolomvaki,Fangshuo Liao,Evan Dramko,Ziyun Guang,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 本文研究了在高斯随机掩码输入下两层神经网络训练的收敛性保证，通过NTK分析证明其线性收敛至与掩码方差成正比的误差区域，并解决了非线性激活中随机性的技术难题。


<details>
  <summary>Details</summary>
Motivation: 针对传感器网络、隐私保护训练和联邦学习等场景中用户仅能访问部分或被污染特征的问题，研究带高斯随机掩码输入的神经网络训练的理论保证。

Method: 采用神经正切核（NTK）分析方法，研究两层ReLU网络在高斯随机掩码输入下的训练动态，并处理非线性激活函数中的内在随机性。

Result: 证明了该设置下训练过程具有线性收敛性，但收敛精度受限于掩码方差，即存在一个与方差成正比的最终误差区域。

Conclusion: 高斯随机掩码输入虽带来噪声，但在NTK框架下仍可实现可控的线性收敛，为含噪/缺失特征场景下的模型训练提供了理论支撑。

Abstract: We investigate the convergence guarantee of two-layer neural network training with Gaussian randomly masked inputs. This scenario corresponds to Gaussian dropout at the input level, or noisy input training common in sensor networks, privacy-preserving training, and federated learning, where each user may have access to partial or corrupted features. Using a Neural Tangent Kernel (NTK) analysis, we demonstrate that training a two-layer ReLU network with Gaussian randomly masked inputs achieves linear convergence up to an error region proportional to the mask's variance. A key technical contribution is resolving the randomness within the non-linear activation, a problem of independent interest.

</details>


### [225] [Variational Grey-Box Dynamics Matching](https://arxiv.org/abs/2602.17477)
*Gurjeet Sangra Singh,Frantzeska Lavda,Giangiacomo Mercatali,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 本文提出了一种灰箱方法，将不完整的物理模型（如ODE/PDE）嵌入到基于流匹配的生成模型中，仅用观测轨迹学习动力学，无需真实物理参数或数值模拟，兼顾可解释性与建模能力。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型（如流匹配、扩散模型）虽强大但缺乏物理解释性；而传统物理模型（ODE/PDE）可解释但常缺失关键项，难以拟合真实观测。亟需融合二者优势的灰箱建模范式。

Method: 在流匹配框架下构建结构化变分分布：引入两个隐变量编码——一个建模缺失的随机性和多模态速度，另一个以物理信息先验编码未知物理参数；并扩展至二阶动力学建模；全程无需数值求解ODE，避免Neural ODE的稳定性与可扩展性问题。

Result: 在典型ODE/PDE问题上的实验表明，该方法性能媲美或优于纯数据驱动方法及现有灰箱基线，同时保持物理模型的可解释性。

Conclusion: 所提灰箱流匹配方法成功弥合了黑箱生成模型与白箱物理模型之间的鸿沟，在不依赖真实参数和数值模拟的前提下，实现了高精度、可解释的动力学学习。

Abstract: Deep generative models such as flow matching and diffusion models have shown great potential in learning complex distributions and dynamical systems, but often act as black-boxes, neglecting underlying physics. In contrast, physics-based simulation models described by ODEs/PDEs remain interpretable, but may have missing or unknown terms, unable to fully describe real-world observations. We bridge this gap with a novel grey-box method that integrates incomplete physics models directly into generative models. Our approach learns dynamics from observational trajectories alone, without ground-truth physics parameters, in a simulation-free manner that avoids scalability and stability issues of Neural ODEs. The core of our method lies in modelling a structured variational distribution within the flow matching framework, by using two latent encodings: one to model the missing stochasticity and multi-modal velocity, and a second to encode physics parameters as a latent variable with a physics-informed prior. Furthermore, we present an adaptation of the framework to handle second-order dynamics. Our experiments on representative ODE/PDE problems show that our method performs on par with or superior to fully data-driven approaches and previous grey-box baselines, while preserving the interpretability of the physics model. Our code is available at https://github.com/DMML-Geneva/VGB-DM.

</details>


### [226] [Learning with Boolean threshold functions](https://arxiv.org/abs/2602.17493)
*Veit Elser,Manish Krishan Lal*

Main category: cs.LG

TL;DR: 本文提出了一种在布尔数据上训练神经网络的新方法，所有节点值严格为±1，权重通常也为±1；采用非凸约束优化而非损失最小化，通过BTF（布尔阈值函数）约束与架构一致性约束的分治-协调框架，并用RRR投影算法求解；在多个布尔任务中表现优异，且模型可解释、推理高效。


<details>
  <summary>Details</summary>
Motivation: 标准梯度法在布尔/离散神经网络训练中效果不佳，缺乏可解释性与高效推理能力，亟需一种概念上不同的学习范式。

Method: 将训练建模为非凸约束满足问题：一是局部BTF约束（含最小间隔下界），确保输入、权重与输出满足布尔阈值关系；二是架构一致性约束，强制神经元输出等于下游输入并跨样本共享权重；采用reflect-reflect-relax（RRR）投影算法求解。

Result: 在乘法器电路发现、二值自编码、逻辑网络推断和元胞自动机学习等任务中，实现了精确解或强泛化能力，显著优于标准梯度法；当间隔下界足够大时，所得模型可证稀疏且等价于±1权值的逻辑门网络。

Conclusion: 基于投影的约束满足是一种可行且本质不同的离散神经系统学习基础，兼顾可解释性、高效推理与强泛化性能。

Abstract: We develop a method for training neural networks on Boolean data in which the values at all nodes are strictly $\pm 1$, and the resulting models are typically equivalent to networks whose nonzero weights are also $\pm 1$. The method replaces loss minimization with a nonconvex constraint formulation. Each node implements a Boolean threshold function (BTF), and training is expressed through a divide-and-concur decomposition into two complementary constraints: one enforces local BTF consistency between inputs, weights, and output; the other imposes architectural concurrence, equating neuron outputs with downstream inputs and enforcing weight equality across training-data instantiations of the network. The reflect-reflect-relax (RRR) projection algorithm is used to reconcile these constraints.
  Each BTF constraint includes a lower bound on the margin. When this bound is sufficiently large, the learned representations are provably sparse and equivalent to networks composed of simple logical gates with $\pm 1$ weights. Across a range of tasks -- including multiplier-circuit discovery, binary autoencoding, logic-network inference, and cellular automata learning -- the method achieves exact solutions or strong generalization in regimes where standard gradient-based methods struggle. These results demonstrate that projection-based constraint satisfaction provides a viable and conceptually distinct foundation for learning in discrete neural systems, with implications for interpretability and efficient inference.

</details>


### [227] [Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models](https://arxiv.org/abs/2602.17497)
*Wen-Tse Chen,Jiayu Chen,Fahim Tajwar,Hao Zhu,Xintong Duan,Ruslan Salakhutdinov,Jeff Schneider*

Main category: cs.LG

TL;DR: 本文提出RICL方法，利用大语言模型（LLM）通过回溯式上下文学习将稀疏奖励转化为密集优势函数信号，并结合在线学习框架RICOL提升策略训练效率，在BabyAI任务中显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决自演化智能体在自采样数据和稀疏环境反馈下训练困难的问题，尤其是传统时序信用分配方法依赖任务特定价值函数导致样本效率低、泛化性差。

Method: 提出基于预训练大语言模型的回溯式上下文学习（RICL）进行优势函数估计，并构建在线学习框架RICOL迭代优化策略。

Result: RICL能以少量样本准确估计优势函数并识别关键状态；在四个BabyAI场景中，RICOL达到与传统在线强化学习算法相当的收敛性能，但样本效率显著更高。

Conclusion: 利用大语言模型进行时序信用分配具有潜力，可推动更高效、更通用的强化学习范式发展。

Abstract: Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.

</details>


### [228] [LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights](https://arxiv.org/abs/2602.17510)
*Kasun Dewage,Marianna Pensky,Suranadi De Silva,Shankadeep Mondal*

Main category: cs.LG

TL;DR: CRAFT是一种参数高效的微调方法，通过对跨层注意力权重张量进行冻结的Tucker分解，并仅训练少量适配矩阵，实现高性能与极低参数量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解类PEFT方法要么分解梯度更新（如LoTR、SuperLoRA），要么逐层独立分解权重（如PiSSA），缺乏对预训练权重跨层结构的联合建模；CRAFT旨在统一这两条技术路线，直接在跨层权重张量上应用HOSVD并冻结所有因子，提升参数效率与表达能力。

Method: 将多层Transformer的注意力权重堆叠为三维张量，对其应用高阶奇异值分解（HOSVD）得到冻结的Tucker因子；仅引入小型可训练的方阵（adaptation matrices）作用于各因子，不更新原始权重。

Result: 在GLUE基准上，使用RoBERTa-base/large时，CRAFT以仅41K可训练参数（与模型尺寸和层数无关）达到与主流PEFT方法相当的性能。

Conclusion: CRAFT通过跨层权重张量的冻结Tucker分解与轻量适配机制，在保持高性能的同时显著降低参数量，验证了联合建模跨层权重结构的有效性与高效性。

Abstract: We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $ΔW$ across layers before applying Tucker decomposition. Separately, methods like PiSSA apply SVD to pre-trained weights but operate independently per layer. CRAFT bridges these two lines of work: it performs full Tucker decomposition via Higher-Order SVD (HOSVD) directly on pre-trained weights organized as cross-layer 3D tensors, freezes all resulting factors, and adapts the model through lightweight trainable transformations applied to each factor matrix. Experiments on the GLUE benchmark using RoBERTa-base and RoBERTa-large demonstrate that CRAFT achieves competitive performance with existing methods while requiring only 41K Tucker adaptation parameters--a count independent of model dimension and depth at fixed Tucker ranks.

</details>


### [229] [Variational inference via radial transport](https://arxiv.org/abs/2602.17525)
*Luca Ghafourpour,Sinho Chewi,Alessio Figalli,Aram-Alexandre Pooladian*

Main category: cs.LG

TL;DR: 本文提出radVI算法，通过优化概率分布的径向轮廓来改进变分推断（VI），尤其解决高斯近似在径向分布建模上的不足，并提供基于Wasserstein空间优化的理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 高斯分布等简单代理分布常无法准确刻画目标分布π的径向轮廓，导致变分推断覆盖效果差。

Method: 从优化径向轮廓角度重构VI问题；提出radVI算法，作为现有VI方法（如高斯平均场VI、Laplace近似）的轻量高效扩展；借助Wasserstein空间优化理论与Caffarelli型径向传输映射正则性分析建立理论保障。

Result: radVI是一种廉价且有效的VI增强方法，在保持计算效率的同时显著提升对复杂径向结构的建模能力。

Conclusion: radVI为VI提供了新的几何视角（径向轮廓优化），兼具实用性与理论严谨性，可广泛适配主流VI框架。

Abstract: In variational inference (VI), the practitioner approximates a high-dimensional distribution $π$ with a simple surrogate one, often a (product) Gaussian distribution. However, in many cases of practical interest, Gaussian distributions might not capture the correct radial profile of $π$, resulting in poor coverage. In this work, we approach the VI problem from the perspective of optimizing over these radial profiles. Our algorithm radVI is a cheap, effective add-on to many existing VI schemes, such as Gaussian (mean-field) VI and Laplace approximation. We provide theoretical convergence guarantees for our algorithm, owing to recent developments in optimization over the Wasserstein space--the space of probability distributions endowed with the Wasserstein distance--and new regularity properties of radial transport maps in the style of Caffarelli (2000).

</details>


### [230] [Provably Explaining Neural Additive Models](https://arxiv.org/abs/2602.17530)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Volkan Şahin,Jan Kretinsky,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出了一种针对神经加性模型（NAMs）的高效算法，能以对数复杂度生成具有可证明保证的基数最小解释，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有后验解释方法多为启发式，缺乏可证明保证；而对标准神经网络求解基数最小充分特征集在计算上不可行。

Method: 设计了专用于NAMs的模型特定算法，在并行预处理后，仅需对数次验证查询即可生成可证明的基数最小解释。

Result: 该算法在解释大小和运行时间上均优于现有方法，且其可证明解释优于基于采样的NAM解释技术。

Conclusion: NAMs结构特性使得可证明基数最小解释的高效生成成为可能，为可解释AI提供了兼具严格性与实用性的新路径。

Abstract: Despite significant progress in post-hoc explanation methods for neural networks, many remain heuristic and lack provable guarantees. A key approach for obtaining explanations with provable guarantees is by identifying a cardinally-minimal subset of input features which by itself is provably sufficient to determine the prediction. However, for standard neural networks, this task is often computationally infeasible, as it demands a worst-case exponential number of verification queries in the number of input features, each of which is NP-hard.
  In this work, we show that for Neural Additive Models (NAMs), a recent and more interpretable neural network family, we can efficiently generate explanations with such guarantees. We present a new model-specific algorithm for NAMs that generates provably cardinally-minimal explanations using only a logarithmic number of verification queries
  in the number of input features, after a parallelized preprocessing step with logarithmic runtime in the required precision is applied to each small univariate NAM component.
  Our algorithm not only makes the task of obtaining cardinally-minimal explanations feasible, but even outperforms existing algorithms designed to find the relaxed variant of subset-minimal explanations - which may be larger and less informative but easier to compute - despite our algorithm solving a much more difficult task.
  Our experiments demonstrate that, compared to previous algorithms, our approach provides provably smaller explanations than existing works and substantially reduces the computation time. Moreover, we show that our generated provable explanations offer benefits that are unattainable by standard sampling-based techniques typically used to interpret NAMs.

</details>


### [231] [Position: Evaluation of ECG Representations Must Be Fixed](https://arxiv.org/abs/2602.17531)
*Zachary Berger,Daniel Prakah-Asante,John Guttag,Collin M. Stultz*

Main category: cs.LG

TL;DR: 本文指出当前12导联心电图（ECG）表征学习的基准测试存在局限，呼吁扩展下游评估任务至结构性心脏病诊断和患者级预测等临床相关目标，并强调在多标签、类别不平衡场景下采用更严谨的评估实践；研究发现随机初始化编码器在线性评估下可媲美现有最优预训练方法，故建议将其作为合理基线。


<details>
  <summary>Details</summary>
Motivation: 当前ECG表征学习过度依赖以心律失常和波形形态为主的少数公开多标签基准（PTB-XL、CPSC2018、CSN），忽视ECG所蕴含的更广泛临床信息（如结构性心脏病、血流动力学、长期预后），导致评估结果不可靠且临床相关性不足。

Method: 提出扩展下游评估任务至结构性心脏病诊断、血流动力学推断与患者级预测；系统梳理并应用多标签、不平衡数据下的评估最佳实践（如指标选择、阈值设定、重采样处理）；实证对比三种代表性ECG预训练方法与随机初始化编码器在线性探针下的性能。

Result: 应用规范评估实践后，文献中关于最优表征的结论发生改变；随机初始化编码器在线性评估下在多个任务上达到甚至超过SOTA预训练模型性能；在六个不同评估设置（含三个标准基准及结构性疾病、血流动力学、患者预测）中验证了上述现象。

Conclusion: 当前ECG表征学习基准亟需改革，应纳入更多元、更具临床意义的下游任务，并采用更严格的评估协议；随机编码器可作为合理基线，提示现有预训练方法的增益可能被高估，未来研究需更关注临床效用而非单纯指标提升。

Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting.

</details>


### [232] [MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning](https://arxiv.org/abs/2602.17550)
*Xiaoliang Fu,Jiaye Lin,Yangyi Fang,Binbin Zheng,Chaowen Hu,Zekai Shao,Cong Qin,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出MASPO框架，解决现有RLVR算法在LLM优化中因硬裁剪、均匀比率约束和对称信任机制导致的梯度利用低效、概率质量不敏感及信号可靠性不对称三大问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法（如GRPO）采用刚性、统一且对称的信任域机制，与大语言模型复杂的优化动态不匹配，存在梯度利用效率低、忽略token分布、正负样本信用分配模糊不对称等问题。

Method: 提出Mass-Adaptive Soft Policy Optimization (MASPO)，包含可微软高斯门控（提升梯度利用率）、质量自适应限幅器（平衡全概率谱探索）、非对称风险控制器（按信号置信度调整更新幅度）。

Result: MASPO在广泛评估中显著优于强基线，成为鲁棒、一体化的RLVR解决方案。

Conclusion: MASPO通过协同优化梯度利用、概率质量建模与信号可靠性建模，有效适配LLM的优化特性，为RLVR提供了更灵活、更精准的策略优化范式。

Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.

</details>


### [233] [A Theoretical Framework for Modular Learning of Robust Generative Models](https://arxiv.org/abs/2602.17554)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文提出了一种模块化生成建模的理论框架，通过预训练专家模型与鲁棒门控机制组合，无需启发式调参即可在任意数据混合下实现与单体大模型相当甚至更优的性能，并提供了理论保证与高效算法。


<details>
  <summary>Details</summary>
Motivation: 训练大规模生成模型资源消耗大且依赖启发式数据加权；希望实现模块化训练（组合小规模领域专家）并消除对数据混合的启发式调优。

Method: 构建基于归一化门控函数空间G₁的极小-极大博弈框架，证明鲁棒门控函数的存在性（利用Kakutani不动点定理），提出随机原始-对偶算法和结构蒸馏方法。

Result: 理论证明模块化具有强正则化效果，泛化界由轻量门控复杂度决定；可理论上优于在聚合数据上重训的模型，性能差距由Jensen-Shannon散度刻画；实验表明该方法能有效缓解梯度冲突，在合成与真实数据上稳健超越单体基线。

Conclusion: 模块化生成建模不仅可行，而且具备理论鲁棒性与实践优越性，为大模型训练提供了一种资源高效、免调参的新范式。

Abstract: Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.

</details>


### [234] [Revisiting Weight Regularization for Low-Rank Continual Learning](https://arxiv.org/abs/2602.17559)
*Yaoyue Zheng,Yin Zhang,Joost van de Weijer,Gido M van de Ven,Shaoyi Du,Xuetao Zhang,Zhiqiang Tian*

Main category: cs.LG

TL;DR: 本文提出EWC-LoRA方法，将弹性权重巩固（EWC）引入低秩持续学习（PECL），通过在共享低秩更新上施加EWC正则化来缓解任务干扰，保持存储和推理开销恒定，并在多个基准上验证了其优越的稳定性-可塑性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有低秩持续学习（PECL）方法多依赖任务专用模块缓解任务干扰，而权重正则化（如EWC）在该范式中尚未被充分探索；本文旨在重新审视EWC在低秩CL中的作用，提供一种更高效、通用的干扰抑制视角。

Method: 提出EWC-LoRA：利用LoRA低秩结构估计全维参数重要性，并在共享低秩更新上施加EWC正则化，避免为每个任务单独存储重要性矩阵，从而维持恒定的存储与计算开销。

Result: 在多个持续学习基准上，EWC-LoRA显著优于现有低秩CL方法，在稳定性与可塑性之间取得更好平衡；实验证明即使在低秩参数化下，权重正则化仍有效缓解任务干扰。

Conclusion: 权重正则化（如EWC）在参数高效持续学习中依然有效且实用；EWC-LoRA为PECL提供了轻量、高效、可扩展的新思路，推动了正则化技术在低秩持续适应中的应用。

Abstract: Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space. This design offers a practical, computational- and memory-efficient solution for CL with PTMs, and provides insights that may inform the broader application of regularization techniques within PECL. Extensive experiments on various benchmarks demonstrate the effectiveness of EWC-LoRA, achieving a stability-plasticity trade-off superior to existing low-rank CL approaches. These results indicate that, even under low-rank parameterizations, weight regularization remains an effective mechanism for mitigating task interference. Code is available at: https://github.com/yaoyz96/low-rank-cl.

</details>


### [235] [Be Wary of Your Time Series Preprocessing](https://arxiv.org/abs/2602.17568)
*Sofiane Ennadir,Tianze Wang,Oleg Smirnov,Sahar Asadi,Lele Cao*

Main category: cs.LG

TL;DR: 本文首次从理论上分析了不同归一化策略（如基于实例和全局缩放）对基于Transformer的时间序列建模的影响，提出了一种面向时间序列的表达能力评估框架，并通过理论推导与实验验证表明：归一化方法的选择显著影响模型表征能力，且不存在普适最优方案，有时甚至不归一化效果更好。


<details>
  <summary>Details</summary>
Motivation: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective.

Method: Propose a novel expressivity framework tailored to time series to quantify model's ability to distinguish similar/dissimilar inputs; derive theoretical bounds for Standard and Min-Max scaling; validate empirically on classification and forecasting benchmarks with multiple Transformer-based models.

Result: The choice of normalization strategy significantly influences representational capacity depending on task and data; no single method consistently outperforms others; in some cases, omitting normalization yields superior performance.

Conclusion: Preprocessing—especially normalization—plays a critical role in time series learning, and more principled, task- and dataset-specific normalization strategies are needed.

Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.

</details>


### [236] [Canonicalizing Multimodal Contrastive Representation Learning](https://arxiv.org/abs/2602.17584)
*Sharut Gupta,Sanyam Kansal,Stefanie Jegelka,Phillip Isola,Vikas Garg*

Main category: cs.LG

TL;DR: 本文发现不同训练的多模态对比学习模型（如CLIP、SigLIP、FLAVA）的嵌入空间之间存在统一的正交映射关系，该映射可同时对齐图像和文本编码器，为模型升级与表示隐私提供新视角。


<details>
  <summary>Details</summary>
Motivation: 随着模型和数据规模扩大，独立训练的多模态模型虽表现出相似性，但缺乏显式表征空间对应关系；尤其在跨模态一致性要求下，需探究不同模型嵌入空间间是否存在系统性几何关系。

Method: 通过实证分析多种主流多模态模型（CLIP、SigLIP、FLAVA）的图像与文本嵌入，验证其嵌入空间近似满足同一正交变换；并从理论角度证明：若两模型在少量锚点上的跨模态核近似一致，则二者必由同一正交矩阵关联。

Result: 实验表明，不同架构、训练分布下的多模态模型嵌入空间可用同一正交矩阵Q近似对齐（即f̃(x)≈Qf(x)且g̃(y)≈Qg(y)）；理论证明该性质源于跨模态内积一致性。

Conclusion: 多模态对比学习模型的嵌入空间具有内在几何一致性，表现为统一正交映射；该发现支持向后兼容的模型升级，并揭示表征隐私的新风险。

Abstract: As models and data scale, independently trained networks often induce analogous notions of similarity. But, matching similarities is weaker than establishing an explicit correspondence between the representation spaces, especially for multimodal models, where consistency must hold not only within each modality, but also for the learned image-text coupling. We therefore ask: given two independently trained multimodal contrastive models (with encoders $(f, g)$ and $(\widetilde{f},\widetilde{g})$) -- trained on different distributions and with different architectures -- does a systematic geometric relationship exist between their embedding spaces? If so, what form does it take, and does it hold uniformly across modalities? In this work, we show that across model families such as CLIP, SigLIP, and FLAVA, this geometric relationship is well approximated by an orthogonal map (up to a global mean shift), i.e., there exists an orthogonal map $Q$ where $Q^\top Q = I$ such that $\widetilde{f}(x)\approx Q f(x)$ for paired images $x$. Strikingly, the same $Q$ simultaneously aligns the text encoders i.e., $\widetilde{g}(y)\approx Q g(y)$ for texts $y$. Theoretically, we prove that if the multimodal kernel agrees across models on a small anchor set i.e. $\langle f(x), g(y)\rangle \approx \langle \widetilde{f}(x), \widetilde{g}(y)\rangle$, then the two models must be related by a single orthogonal map $Q$ and the same $Q$ maps images and text across models. More broadly, this finding enables backward-compatible model upgrades, avoiding costly re-embedding, and has implications for the privacy of learned representations.
  Our project page: https://canonical-multimodal.github.io/

</details>


### [237] [Asymptotic Smoothing of the Lipschitz Loss Landscape in Overparameterized One-Hidden-Layer ReLU Networks](https://arxiv.org/abs/2602.17596)
*Saveliy Baturin*

Main category: cs.LG

TL;DR: 本文研究了过参数化下单隐藏层ReLU网络的损失景观拓扑结构，理论上证明了在凸Lipschitz损失与ℓ1正则化条件下，相同损失值的模型间可近似连通，且能量间隙随宽度增大趋于零；实验上在合成与真实数据集上验证了更宽网络具有更小的能量障碍。


<details>
  <summary>Details</summary>
Motivation: 理解过参数化神经网络损失景观的拓扑性质，尤其是局部极小值之间的连通性与能量障碍，以解释其易优化性。

Method: 理论分析（包括路径连通性证明与能量间隙渐近上界推导）结合实验验证（使用Dynamic String Sampling测量能量间隙，并进行置换检验）。

Result: 证明了ℓ1正则化下损失子水平集在小损失增加内近似连通；得到能量间隙ε随网络宽度m增大而趋于零的渐近上界；实验显示更宽网络的最大能量间隙显著减小（p_perm=0）。

Conclusion: 过参数化与ℓ1正则化共同促使损失景观趋于平坦、子水平集连通，从而降低优化难度。

Abstract: We study the topology of the loss landscape of one-hidden-layer ReLU networks under overparameterization. On the theory side, we (i) prove that for convex $L$-Lipschitz losses with an $\ell_1$-regularized second layer, every pair of models at the same loss level can be connected by a continuous path within an arbitrarily small loss increase $ε$ (extending a known result for the quadratic loss); (ii) obtain an asymptotic upper bound on the energy gap $ε$ between local and global minima that vanishes as the width $m$ grows, implying that the landscape flattens and sublevel sets become connected in the limit. Empirically, on a synthetic Moons dataset and on the Wisconsin Breast Cancer dataset, we measure pairwise energy gaps via Dynamic String Sampling (DSS) and find that wider networks exhibit smaller gaps; in particular, a permutation test on the maximum gap yields $p_{perm}=0$, indicating a clear reduction in the barrier height.

</details>


### [238] [Towards Anytime-Valid Statistical Watermarking](https://arxiv.org/abs/2602.17608)
*Baihe Huang,Eric Xu,Kannan Ramchandran,Jiantao Jiao,Michael I. Jordan*

Main category: cs.LG

TL;DR: 本文提出了首个基于e值的水印框架Anchored E-Watermarking，统一了最优采样与任意时刻有效的推断，解决了现有统计水印方法在采样分布选择和固定窗口假设检验上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成内容的水印检测方法存在采样分布选择缺乏理论依据、固定窗口假设检验无法支持有效早停两大问题。

Method: 提出基于锚定分布的e值水印框架，构建检测过程的检验上鞅以支持任意时刻有效的推断，并从最坏对数增长率角度刻画最优e值及期望停止时间。

Result: 在仿真与标准基准测试中验证了理论主张，相比当前最优基线，平均检测所需token预算减少13–15%。

Conclusion: Anchored E-Watermarking实现了采样优化与任意时刻有效性推断的统一，显著提升了水印检测的样本效率。

Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.

</details>


### [239] [Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning](https://arxiv.org/abs/2602.17614)
*Obaidullah Zaland,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文提出了一种名为k-匿名差分隐私UFSL（KD-UFSL）的新方法，用于在联邦分割学习中增强中间表示（smashed data）的隐私保护，同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: UFSL中客户端上传的中间表示易被攻击者用于数据重构，导致隐私泄露，需增强其隐私保护能力。

Method: 结合微聚合（microaggregation）与差分隐私技术，设计KD-UFSL框架，在不共享原始数据和标签的前提下，对smashed data进行隐私增强处理。

Result: 实验表明，KD-UFSL可使图像重建的均方误差提升最多50%，结构相似性降低最多40%，同时不显著损害全局模型性能。

Conclusion: KD-UFSL在保障隐私的同时维持了模型效用，适用于大规模、需兼顾隐私与实用性的大数据场景。

Abstract: Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.

</details>


### [240] [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs](https://arxiv.org/abs/2602.17616)
*Luke Huang,Zhuoyang Zhang,Qinghao Hu,Shang Yang,Song Han*

Main category: cs.LG

TL;DR: 本文提出VCPO方法，通过基于有效样本量调整学习率和采用闭式最小方差基线，显著提升了异步REINFORCE/GRPO类强化学习算法在大语言模型推理任务中的训练稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 异步RL训练虽能提升吞吐量，但在REINFORCE/GRPO等critic-free策略梯度方法中会因过时rollout导致重要性采样比重尾化、方差剧增，进而引发梯度噪声大、训练不稳定甚至崩溃；作者发现崩溃可由有效样本量（ESS）和梯度范数不稳定可靠预测。

Method: 提出方差控制策略优化（VCPO）：（i）根据有效样本量（ESS）动态缩放学习率，抑制不可靠更新；（ii）为off-policy设置设计闭式最小方差基线，无需额外价值模型，开销极小。

Result: VCPO在数学推理、通用推理和工具使用等多类异步RL任务中显著提升鲁棒性，优于各类裁剪/掩码稳定器及算法变体；将长上下文、多轮训练时间减少2.5倍，同时达到同步训练性能。

Conclusion: 显式控制策略梯度方差是实现大规模可靠异步强化学习的关键，VCPO提供了一种轻量、通用且高效的解决方案。

Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.

</details>


### [241] [Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning](https://arxiv.org/abs/2602.17625)
*Obaidullah Zaland,Zulfiqar Ahmad Khan,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文提出了一种名为One-Shot Incremental Federated Learning（OSI-FL）的新框架，旨在解决联邦学习中通信开销大和灾难性遗忘两大挑战。该方法利用客户端冻结的视觉语言模型提取类别特定嵌入，在单轮通信中传至服务器，并由预训练扩散模型合成数据用于全局模型训练；同时引入Selective Sample Retention（SSR）策略，通过保留每类每任务中损失最高的前p个样本缓解遗忘问题。实验表明其在类增量与域增量设定下均优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现代大数据系统产生海量、异构、地理分散且隐私敏感的数据流，难以集中处理；而传统联邦学习假设静态数据流、多轮协作训练，难以应对增量数据和低通信预算下的持续学习需求。

Method: 提出OSI-FL框架：客户端使用冻结视觉语言模型提取类别特定嵌入，单轮上传至服务器；服务器用预训练扩散模型基于这些嵌入合成数据并训练全局模型；引入Selective Sample Retention（SSR）策略，依据样本损失选择并保留每类每任务中最具信息量的top-p样本，以抑制灾难性遗忘。

Result: OSI-FL在三个基准数据集上的类增量和域增量场景中，性能均显著优于传统及单轮联邦学习等基线方法。

Conclusion: OSI-FL是首个同时解决通信效率与灾难性遗忘问题的增量式联邦学习框架，通过单轮通信+合成数据+选择性样本保留，实现了高效、鲁棒的持续联邦学习。

Abstract: Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.

</details>


### [242] [SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer](https://arxiv.org/abs/2602.17632)
*Nathan S. de Lara,Florian Shkurti*

Main category: cs.LG

TL;DR: 本文提出SMAC方法，通过在离线阶段正则化Q函数，使其满足策略分数与Q函数动作梯度间的一阶导数等式，从而避免离线与在线最优解之间的低性能山谷，实现无性能下降的平滑在线微调。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法得到的策略在在线微调时性能会立即下降，推测是由于离线最优解与在线最优解在损失景观中被低性能山谷分隔。

Method: 提出Score Matched Actor-Critic（SMAC），在离线训练中对Q函数施加正则化约束，强制其动作梯度与策略分数（log-π的梯度）一致，以对齐离线与在线优化目标。

Result: SMAC在6个D4RL任务中均实现向SAC和TD3的平滑迁移；在4个环境中相较最佳基线降低34–58%的在线遗憾。

Conclusion: SMAC通过分数匹配机制使离线学习的策略能无缝过渡到在线价值型RL算法，验证了对齐策略梯度与Q函数梯度可有效消除离线到在线迁移中的性能断崖。

Abstract: Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.

</details>


### [243] [When to Trust the Cheap Check: Weak and Strong Verification for Reasoning](https://arxiv.org/abs/2602.17633)
*Shayan Kiyani,Sima Noorani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: 本文提出弱-强验证策略，用于在大语言模型推理过程中平衡低成本但不可靠的弱验证（如自一致性）与高成本但可靠的强验证（如人工检查），并设计了在线算法以在无假设条件下控制错误率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统中，弱验证（快速但不可靠）与强验证（可靠但昂贵）之间存在显著的成本-可靠性权衡，亟需形式化建模与优化策略。

Method: 形式化定义弱-强验证策略，提出衡量错误接受、错误拒绝及强验证频率的指标；证明最优策略具有双阈值结构，并基于校准性与锐度分析弱验证器价值；设计无需假设的在线控制算法。

Result: 理论证明最优策略为双阈值形式；揭示校准性与锐度是决定弱验证器效用的关键因素；提出可证控错的在线算法。

Conclusion: 弱-强验证策略为LLM可信推理提供了可分析、可控制的框架，所提在线算法能在实际动态场景中稳健控制验证错误。

Abstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.

</details>


### [244] [Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting](https://arxiv.org/abs/2602.17634)
*Xinghong Fu,Yanhong Li,Georgios Papaioannou,Yoon Kim*

Main category: cs.LG

TL;DR: 本文提出了一种高效的时间序列基础模型Reverso，通过小型混合架构（长卷积+DeltaNet RNN层）替代大型Transformer，在零样本预测任务中达到同等性能，模型体积缩小百倍以上，并结合数据增强与推理策略进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型依赖大规模Transformer，参数量达上亿，虽性能好但效率低、成本高，亟需更高效的替代方案。

Method: 采用小型混合模型架构，交替使用长卷积层和DeltaNet线性RNN层；辅以特定数据增强与推理策略。

Result: 所提Reverso模型在零样本时间序列预测任务中媲美大型Transformer，参数量减少超100倍，显著提升性能-效率帕累托前沿。

Conclusion: 大型Transformer并非必需；小型混合架构配合合理训练与推理策略，可构建高效且高性能的时间序列基础模型。

Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.

</details>


### [245] [FAMOSE: A ReAct Approach to Automated Feature Discovery](https://arxiv.org/abs/2602.17641)
*Keith Burghardt,Jienan Liu,Sadman Sakib,Yuning Hao,Bo Li*

Main category: cs.LG

TL;DR: FAMOSE is a novel agentic ReAct framework for automated feature engineering on tabular data, achieving state-of-the-art or near state-of-the-art performance in both regression and classification tasks by enabling LLMs to iteratively discover, evaluate, and refine features.


<details>
  <summary>Details</summary>
Motivation: Feature engineering is a critical yet challenging bottleneck in machine learning for tabular data, traditionally requiring substantial domain expertise to navigate an exponentially large feature space.

Method: FAMOSE leverages the ReAct paradigm within an agent architecture to autonomously explore, generate, refine, select, and evaluate features, integrating these capabilities into a unified framework.

Result: FAMOSE achieves state-of-the-art performance on regression (2.0% average RMSE reduction) and near state-of-the-art on classification (0.23% average ROC-AUC increase on large datasets), with improved robustness to errors.

Conclusion: FAMOSE demonstrates that AI agents—particularly those using the ReAct paradigm—can effectively tackle highly inventive problems like automated feature engineering, offering a promising direction beyond traditional methods.

Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.

</details>


### [246] [A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning](https://arxiv.org/abs/2602.17642)
*Dhruv Talwar,Harsh Desai,Wendong Yin,Goutam Mohanty,Rafael Reveles*

Main category: cs.LG

TL;DR: 本文提出了一种名为A.R.I.S.的低成本便携式电子废弃物分选系统，利用YOLOx模型实现实时高精度材料识别与分类，显著提升回收效率和材料纯度。


<details>
  <summary>Details</summary>
Motivation: 传统电子废弃物回收过程因材料分离与识别能力不足，导致大量资源流失，亟需更高效、低成本的解决方案。

Method: 采用YOLOx深度学习模型对粉碎后的电子废弃物（金属、塑料、电路板）进行实时图像分类，并结合既有分选技术构建便携式自动化识别分选系统。

Result: 实验结果表明系统整体精确率达90%，平均精度均值（mAP）为82.2%，分选纯度达84%，且具备低推理延迟与高检测准确率。

Conclusion: A.R.I.S.通过融合深度学习与传统分选方法，有效提升了电子废弃物材料回收效率，降低了先进回收技术的应用门槛，助力延长产品生命周期、支持回收计划并减少供应链环境影响。

Abstract: Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.

</details>


### [247] [Multi-Round Human-AI Collaboration with User-Specified Requirements](https://arxiv.org/abs/2602.17646)
*Sima Noorani,Shayan Kiyani,Hamed Hassani,George Pappas*

Main category: cs.LG

TL;DR: 本文提出了一种以人类为中心的多轮人机协作决策框架，强调‘反事实危害’与‘互补性’两大原则，并通过用户定义规则和在线无分布假设算法实现实时约束保证，在医学诊断与图像推理任务中验证了其有效性与可控性。


<details>
  <summary>Details</summary>
Motivation: 随着人类在高风险决策中日益依赖多轮对话式AI，亟需可解释、可保证的框架来确保人机协作真正提升决策质量，而非削弱人类优势或引入新风险。

Method: 形式化定义‘反事实危害’与‘互补性’为用户可定制的规则；设计一种在线、无分布假设、具有限样本保证的算法，动态约束协作过程；在LLM模拟医疗诊断和人类众包图像推理两个交互场景中进行评估。

Result: 所提算法能在非平稳交互动态下稳定维持预设的危害与互补性违规率；约束松紧程度可预测地调节人类下游准确率，验证了两原则作为调控杠杆的有效性。

Conclusion: 该框架无需建模或限制人类行为，即可通过可解释、可配置的原则实现对多轮人机协作决策质量的可靠引导与调控。

Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.

</details>


### [248] [MARS: Margin-Aware Reward-Modeling with Self-Refinement](https://arxiv.org/abs/2602.17658)
*Payel Bhattacharjee,Osvaldo Simeone,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文提出MARS方法，一种自适应、间隔感知的奖励模型数据增强与采样策略，聚焦于低间隔（模糊）偏好对，以提升奖励建模的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励建模范式严重依赖昂贵且有限的人类标注偏好数据，而传统数据增强方法忽略奖励模型自身的估计难度，难以有效提升模型鲁棒性。

Method: 提出MARS框架，通过识别低间隔（即模型预测置信度低）的偏好对进行针对性数据增强，并迭代优化训练分布；同时提供理论分析，证明该策略能提升损失函数平均曲率，改善优化条件。

Result: 实验表明MARS在多个基准上持续优于均匀数据增强，在提升奖励模型鲁棒性方面效果显著。

Conclusion: MARS通过将增强资源集中于模型最不确定的样本，实现了更高效、更鲁棒的奖励建模，为对齐学习中的数据效率问题提供了新思路。

Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [249] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: 本文提出了AIdentifyAGE本体，旨在为法医牙龄评估提供标准化、语义一致的框架，整合人工与AI辅助流程，提升司法与法医决策中的透明性、可追溯性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前法医牙龄评估存在方法学异质性、数据表示碎片化及系统间互操作性差等问题，影响结果的透明性、可重现性，尤其在AI方法日益普及背景下更为突出。

Method: 构建领域专用的AIdentifyAGE本体，融合临床、法医与司法信息，覆盖个体信息、牙科检查、影像、参考研究及AI估计算法，并基于成熟上层本体（生物医学、牙科、机器学习）开发，遵循FAIR原则。

Result: 实现了涵盖完整法医-司法工作流的标准化本体，支持观测、方法、参考数据与结果间的可追溯链接，具备互操作性、可扩展性与合规性。

Conclusion: AIdentifyAGE本体是提升法医牙龄评估一致性、透明性与可解释性的关键基础，为本体驱动的法医-司法决策支持系统奠定坚实基础。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [250] [Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems](https://arxiv.org/abs/2602.16715)
*H. Sinan Bank,Daniel R. Herber*

Main category: cs.AI

TL;DR: 本文探索了大语言模型（LLMs）、检索增强生成（RAG）和图结构RAG（GraphRAG）在自动生成设计结构矩阵（DSM）中的潜力，针对功率螺丝刀和CubeSat两个案例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统DSM构建依赖专家经验、耗时且易出错，亟需自动化方法提升效率与可复现性。

Method: 采用LLMs、RAG和GraphRAG三种方法，在两个已知架构参考的工程系统（功率螺丝刀、CubeSat）上开展实验，任务包括：1）判断预定义组件间关系；2）联合识别组件及其关系。评估指标覆盖DSM元素级与整体架构级性能。

Result: 三种方法均展现出生成DSM的可行性，虽存在设计与计算挑战，但在组件关系判定和架构还原方面识别出明确改进机会；全部代码已开源。

Conclusion: LLMs与RAG类技术具备辅助甚至部分替代人工构建DSM的潜力，是迈向智能支持系统架构设计的重要一步；开源实践促进了领域专家协作与迭代优化。

Abstract: We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.

</details>


### [251] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 本文揭示了单状态复用（single-state reuse）在自适应系统中导致的经典概率表示必然具有语境性（contextuality），并证明这种语境性是一种不可消除的信息论代价，而非量子力学特有现象；作者通过构造性例子阐明该代价的操作含义，并指出非经典概率框架可通过放弃全局联合概率空间假设来规避此限制。


<details>
  <summary>Details</summary>
Motivation: 理解自适应系统中因资源约束而普遍存在的单状态复用所引发的基本表征后果，尤其是其与语境性的关系。

Method: 将语境建模为对共享内部状态的干预，从经典概率论出发，通过信息论分析证明再现语境统计结果必然导致不可约的语境依赖代价；给出最小构造性示例，并对比非经典概率框架如何绕过该限制。

Result: 证明任何能复现语境结果的经典模型都必须承担不可消除的信息论代价——语境依赖无法仅通过内部状态中介；提供显式构造示例；阐明非经典框架通过放弃全局联合概率空间假设来避免该代价。

Conclusion: 语境性是自适应智能中一种普适的表征约束，源于单状态复用，与物理实现无关，不依赖于量子力学结构。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [252] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: 本文提出MobCache框架，通过可重构缓存机制提升大规模人类移动性模拟的效率，同时保持与现有大语言模型方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLMs）的人类移动性模拟方法计算开销高，难以扩展到大规模场景。

Method: 设计了MobCache框架，包含两个核心组件：(1) 推理组件，将每步推理编码为潜在空间嵌入，并利用潜在空间评估器实现推理步骤的复用与重组；(2) 解码组件，采用受移动规律约束的知识蒸馏训练的轻量解码器，将潜在空间推理链转化为自然语言。

Result: 实验表明，MobCache在多个维度显著提升了模拟效率，同时性能与当前最先进的LLM-based方法相当。

Conclusion: MobCache通过引入移动性感知的可重构缓存机制，有效平衡了大规模人类移动性模拟的效率与保真度，为相关应用提供了更具可扩展性的解决方案。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [253] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 本文分析了60个大语言模型（LLM）基准的饱和现象，发现近一半基准已饱和且随时间推移加剧；隐藏测试数据无防护作用，而专家构建的基准比众包基准更抗饱和。


<details>
  <summary>Details</summary>
Motivation: AI基准在衡量模型进展和指导部署中至关重要，但许多基准迅速饱和，丧失区分顶尖模型的能力，亟需识别导致饱和的因素以提升基准长期有效性。

Method: 对来自主流模型技术报告的60个LLM基准进行系统分析，沿任务设计、数据构建和评估格式等14个属性刻画基准特征，并检验五个关于各属性如何影响饱和率的假设。

Result: 近50%的基准已出现饱和，饱和率随基准年龄增长而上升；测试数据是否公开（公/私）对饱和无显著影响；专家人工构建的基准比众包构建的更具抗饱和性。

Conclusion: 基准的设计选择（尤其是数据构建方式）显著影响其寿命，专家主导的高质量构建策略可提升基准长期评估价值，为开发更稳健的AI评估方法提供实证依据。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [254] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 本文评估了两种简单基线方法在三个领域（数学界、智能体框架设计、机器学习竞赛）中的表现，发现它们在多数情况下匹敌甚至超越复杂的代码演化方法，并指出当前代码演化研究中存在的搜索空间设计、评估随机性等关键问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码演化技术虽表现优异，但缺乏与简单基线的公平比较，且其开发和使用中存在方法论缺陷。

Method: 在数学界优化、智能体框架设计、机器学习竞赛三个任务上，对比测试两种简单基线与复杂代码演化方法的性能，并分析失败原因及改进方向。

Result: 简单基线在所有三个领域均匹配或超越复杂代码演化方法；数学界性能主要取决于搜索空间设计与提示中的领域知识；智能体框架因高方差与小数据导致选择偏差；需改进评估以降低随机性并保持经济可行性。

Conclusion: 代码演化研究应更重视搜索空间设计、稳健评估与领域知识整合，而非单纯依赖复杂演化机制；未来工作需建立更严谨的方法论与最佳实践。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [255] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 本文研究了n维超立方体Q_n的所有边被超平面集合H切割所需的最少超平面数量S(n)，给出了新的上界S(n) ≤ ⌈4n/5⌉（n不是5的奇数倍时）或S(n) ≤ 4n/5 +1（n是5的奇数倍时），优于1971年Paterson的⌈5n/6⌉上界；该结果通过CPro1工具辅助构造Q_10的8个切割超平面实现，并给出了k<n个超平面最多能切割边数的新下界。


<details>
  <summary>Details</summary>
Motivation: 改进超立方体边切割问题中最小超平面数S(n)的已知上界，探索更优的几何构造方法，并利用新兴AI辅助数学发现工具提升构造效率。

Method: 结合理论分析与计算辅助构造：利用新开发的CPro1工具（集成推理型大语言模型与自动超参数调优）搜索并构造出Q_10上8个切割超平面的具体实例，进而推广至一般n维情形，辅以组合与几何论证推导上下界。

Result: 证明了S(n) ≤ ⌈4n/5⌉（n非5的奇数倍）或S(n) ≤ 4n/5 +1（n为5的奇数倍）；改进了k<n个超平面最多可切割边数的下界；首次将CPro1工具成功应用于该经典组合几何问题的构造性证明。

Conclusion: 本工作显著推进了超立方体边切割问题的上界研究，展示了AI驱动的自动化搜索工具在构造性数学证明中的实际效能，为类似极值组合问题提供了新方法论范式。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [256] [NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography](https://arxiv.org/abs/2602.16812)
*Zhongcan Xiao,Leyi Zhang,Guannan Zhang,Xiaoping Wang*

Main category: cs.AI

TL;DR: NeuDiff Agent 是一个受控的、工具驱动的AI工作流，用于TOPAZ中子散射设施的数据分析，可将原始数据自动处理为经过验证的晶体结构和出版级CIF文件，速度提升约4.6–5.0倍，同时确保可追溯性与验证合规性。


<details>
  <summary>Details</summary>
Motivation: 大型科学设施（如中子源）在处理结构与磁性复杂的样品时，数据分析与报告延迟已成为限制科研产出的关键瓶颈，亟需提升时间-结果效率与分析自动化水平。

Method: 开发了 NeuDiff Agent，一种基于大语言模型（LLM）的受控AI代理，严格限定于白名单工具、设置故障关闭式校验门，并完整记录全流程溯源信息；采用固定提示协议与双LLM后端进行重复端到端测试，量化人工干预、恢复行为及人机耗时。

Result: 在基准测试中，NeuDiff Agent 将人工处理的435分钟墙钟时间缩短至86.5±4.7至94.4±3.5分钟（加速4.6–5.0倍），输出无checkCIF A/B级警告的已验证CIF文件。

Conclusion: 该工作为在大型设施晶体学中安全、可审计、可复现地部署具身AI代理提供了切实可行的技术路径，兼顾效率提升与出版级验证要求。

Abstract: Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.

</details>


### [257] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 本文提出了一种名为Node Learning的去中心化学习范式，将智能分布于边缘节点，通过节点间选择性协作实现知识传播，无需全局同步或中心聚合，以应对边缘AI在通信、延迟、能耗和异构性等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 边缘AI的扩展暴露了集中式智能在数据传输、延迟、能耗及对大型数据中心依赖等方面的成本与脆弱性，现有方法难以适应异构、移动和资源受限的边缘环境。

Method: 提出Node Learning范式：各边缘节点自主学习本地数据、维护独立模型状态，并在有益时通过机会性交互交换知识；学习通过重叠与扩散传播，而非全局同步或中心聚合。

Result: 构建了Node Learning的概念框架，对比了其与现有去中心化方法的差异，并探讨了其对通信、硬件、信任与治理的影响。

Conclusion: Node Learning并非取代现有范式，而是将其纳入更广泛的去中心化视角，为边缘智能提供更具鲁棒性与可扩展性的基础。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [258] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出了一种基于序理论的犹豫模糊集统一评分框架，证明了经典序不构成格结构，而对称序下的评分满足强单调性等规范性条件；进一步引入支配函数用于排序，并给出两种具体实现以支持群体决策。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论的严格基础，导致灵活性与一致性不足。

Method: 构建基于给定序的统一评分框架；分析多种经典序在犹豫模糊元上的格结构性质；定义并研究对称序下评分函数的规范性；提出支配函数类，并给出离散支配函数和相对支配函数两种实例。

Result: 证明经典序不诱导格结构；对称序下的评分满足强单调性和Gärdenfors条件；两类支配函数可构造模糊偏好关系，支持群体决策。

Conclusion: 序导向的评分框架提升了理论严谨性与实用性；支配函数为犹豫模糊元素排序与决策提供了新工具。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [259] [IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages](https://arxiv.org/abs/2602.16832)
*Priyaranjan Pattnayak,Sanchari Chowdhuri*

Main category: cs.AI

TL;DR: 本文提出了Indic Jailbreak Robustness (IJR)，一个面向12种印度及南亚语言的无裁判员式对抗安全基准，揭示了当前LLM在多语言、自然表达场景下的对齐漏洞，尤其指出合同式评估会高估安全性，而代码切换与罗马化输入显著削弱防御效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全对齐评估主要集中于英语和合同式（结构化）提示，忽视了多语言（尤其是覆盖21亿使用者的印度及南亚语言）和自然表达场景下的脆弱性，导致真实使用中的风险被掩盖。

Method: 构建IJR基准，包含JSON（合同式）与Free（自然式）两条轨道，共45216条提示，覆盖12种印度及南亚语言；系统评测主流模型（如LLaMA、Sarvam）在不同提示形式、跨语言迁移、正字法变化（如罗马化）下的拒答率（JSR）；辅以人工审计与轻量-全量对比验证。

Result: 发现三大现象：(1) 合同式提示虚高拒答率但无法阻止越狱，Free轨道下所有模型JSR达1.0；(2) 英语越狱攻击向印地语等迁移性强，格式包装器优于指令包装器；(3) 罗马化/混合输入显著降低JSON轨道JSR，且与罗马化比例和分词特性呈弱相关（r≈0.28–0.32）；人工审计证实检测器可靠。

Conclusion: IJR揭示了当前安全评估范式的局限性——仅依赖英语和合同式提示会严重低估多语言真实场景（尤其代码切换与罗马化）中的越狱风险，亟需推广更自然、更包容的多语言鲁棒性评测标准。

Abstract: Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.
  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.

</details>


### [260] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5 是一个支持多平台（桌面、移动、浏览器等）的原生 GUI 智能体模型系列，具备指令/思维变体与多种参数规模（2B–235B），通过混合数据飞轮、统一思维合成与多平台强化学习算法 MRPO 实现多项 GUI 任务 SOTA 性能，并已开源。


<details>
  <summary>Details</summary>
Motivation: 解决现有 GUI 智能体在跨平台兼容性、长周期任务训练效率、UI 数据质量与多样性、以及工具调用、记忆与多智能体协同等核心能力上的不足。

Method: 提出 Hybrid Data Flywheel（融合模拟与云沙箱构建 UI 数据流水线）、Unified Thought-Synthesis Pipeline（统一提升推理、工具使用、记忆与多智能体适应能力）和 Multi-platform Environment RL Scaling（新算法 MRPO 应对多平台冲突与长视野训练低效问题）。

Result: 在 20+ 个开源 GUI 基准上达到 SOTA：OSWorld（56.5）、AndroidWorld（71.6）、WebArena（48.4）、ScreenSpotPro（80.3）、OSWorld-MCP（47.6）、MobileWorld（46.8）、GUI-Knowledge Bench（75.5）。

Conclusion: GUI-Owl-1.5 是首个支持云边协同与实时交互的全栈式开源 GUI 智能体模型体系，显著提升了跨平台 GUI 理解、自动化与推理能力，为通用智能体发展提供了可扩展技术路径。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [261] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: 本文提出了OpenSage，首个支持大语言模型（LLM）自动构建具备自生成拓扑结构、工具集及结构化记忆系统的智能体开发套件（ADK），显著提升智能体的通用性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体开发套件（ADKs）在代理拓扑、工具和记忆功能上支持不足，或严重依赖人工设计，限制了智能体的泛化能力和整体性能。

Method: 提出OpenSage ADK，支持LLM自动创建代理拓扑与工具集；引入分层图结构记忆系统；提供面向软件工程任务的专用工具包；并支持子代理与工具集的自主创建与管理。

Result: 在三个主流基准测试中，基于多种骨干模型的实验表明OpenSage显著优于现有ADK；消融实验验证了各核心组件的有效性。

Conclusion: OpenSage推动智能体开发范式从以人为核心转向以AI为核心，为下一代ADK奠定基础。

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [262] [AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks](https://arxiv.org/abs/2602.16901)
*Tanqiu Jiang,Yuhui Wang,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: 本文提出了AgentLAB，首个专门用于评估LLM代理在多轮交互中面对自适应长周期攻击的脆弱性的基准测试，涵盖5种新型攻击类型、28个真实环境和644个安全测试用例，并发现现有LLM代理仍高度易受此类攻击，且单轮防御方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在长周期、复杂环境中的广泛应用，其面临多轮交互下的新型长周期攻击风险，亟需专门的基准来系统评估其安全性。

Method: 构建了AgentLAB基准，包含五类新型长周期攻击（意图劫持、工具链攻击、任务注入、目标漂移、记忆污染），覆盖28个真实代理环境与644个安全测试案例，并对代表性LLM代理及防御方法进行实证评估。

Result: 实验表明当前主流LLM代理对长周期攻击高度脆弱，且为单轮交互设计的防御策略无法可靠抵御此类威胁。

Conclusion: AgentLAB为评估和提升LLM代理在实际场景中的长期安全性提供了关键基准，推动面向多轮交互的安全研究发展。

Abstract: LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.

</details>


### [263] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace 是一个评估大语言模型在规划、推理和世界知识方面能力的新基准，要求模型通过维基百科超链接逐步导航从源页面到达目标页面；尽管前沿模型在简单任务上表现超人，但在困难任务上成功率骤降（仅23%），凸显其在长程规划、动态重规划与避免循环等方面的重大缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以全面评估大语言模型在真实世界知识引导下的多步规划与长程推理能力，亟需一个简单、可解释、能揭示模型本质局限的新任务。

Method: 构建 LLM-Wikirace 基准：基于维基百科图结构设计源-目标页面导航任务，划分易/难两个难度等级；系统评测数十种开源与闭源大模型（如 Gemini-3、GPT-5、Claude Opus 4.5），并进行轨迹级分析（如失败后重规划行为、循环倾向）。

Result: 前沿模型在易级任务上达到超人类水平，但在硬级任务中表现急剧下降（最佳模型 Gemini-3 仅成功 23%）；分析表明世界知识是必要但非充分条件，规划与长程推理能力成为瓶颈；强模型普遍缺乏失败后有效重规划能力，易陷入循环。

Conclusion: LLM-Wikirace 揭示了当前大语言模型在复杂、开放、需持续决策的真实世界规划任务中的根本性短板，强调未来研究需聚焦于提升动态规划、长期信用分配与鲁棒恢复机制。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [264] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 本文提出了一种将形式化验证与深度学习图像检索相结合的新框架，通过图验证与神经代码生成的协同，提升复杂自然语言查询（如关系、数量、比例等约束）的可信赖性与可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有基于嵌入的检索方法在处理含复杂关系、对象组合或精确约束（如身份、数量、比例）的自然语言查询时仍不可靠或无法解决。

Method: 提出融合形式验证的图像检索框架，结合图结构验证方法与神经代码生成技术，对用户查询中的每个原子事实进行显式验证。

Result: 提升了主流嵌入式检索方法的效果，同时提供可解释、可追溯的结果——明确标识哪些约束被满足、哪些未被满足。

Conclusion: 通过将检索结果锚定于形式推理系统，该框架超越了向量表示固有的模糊性与近似性，实现了更透明、可信且可问责的信息检索。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [265] [Narrow fine-tuning erodes safety alignment in vision-language agents](https://arxiv.org/abs/2602.16931)
*Idhant Gulati,Shivam Raval*

Main category: cs.AI

TL;DR: 本文研究了多模态大模型在持续学习过程中因微调而引发的安全对齐退化问题，发现即使少量有害数据也会导致跨任务、跨模态的严重对齐崩溃，并揭示其几何低维本质；提出两种缓解策略但效果有限，强调需构建更鲁棒的持续学习框架。


<details>
  <summary>Details</summary>
Motivation: 多模态智能体需通过后训练持续适应新任务，但现有范式在能力获取与安全对齐之间存在根本张力，亟需理解并缓解对齐退化机制。

Method: 在Gemma3-4B上开展LoRA微调实验，使用窄域有害数据集；结合多模态与单模态安全评估、主成分分析（PCA）进行几何建模；对比良性窄域微调与激活导向调控两种缓解策略。

Result: 1）有害微调引发跨任务/模态的泛化性对齐崩溃；2）对齐退化随LoRA秩单调上升，多模态评估比文本评估更敏感（70.71 vs 41.19）；3）仅10%有害数据即显著损害对齐；4）有害行为集中在约10维低维子空间；5）两种缓解策略均无法完全消除有害行为。

Conclusion: 当前后训练范式难以保障部署后持续学习的安全对齐，必须发展能显式建模并抑制低维有害子空间的新框架。

Abstract: Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.

</details>


### [266] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec是一个高性能、后端无关的推荐系统框架，支持从本地实验无缝扩展到分布式训练，集成了50+算法、40个指标和19种数据处理策略，并内置能耗监控以支持可持续研究，同时面向生成式AI与智能体（Agentic AI）演进。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统研究中学术界（易用的内存实验）与工业界（复杂昂贵的分布式引擎）之间的割裂问题。

Method: 设计了一个后端无关的新型架构WarpRec，集成大量算法、评估指标与数据处理策略，并引入CodeCarbon实现能耗实时追踪，同时支持向Agentic AI范式迁移。

Result: 实现了本地实验与分布式训练/优化的无缝过渡，提供了统一、可扩展、可持续且面向未来的推荐系统开发框架。

Conclusion: WarpRec弥合了学术与工业鸿沟，为可持续、智能体就绪的新一代推荐系统提供了架构基础。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [267] [DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs](https://arxiv.org/abs/2602.16935)
*Justin Albrethsen,Yash Datta,Kunal Kumar,Sharath Rajasekar*

Main category: cs.AI

TL;DR: 本文提出DeepContext，一种基于RNN的状态感知安全监控框架，用于检测多轮对话中的渐进式越狱攻击，显著提升多轮 jailbreak 检测性能（F1达0.84），同时保持低延迟（<20ms）。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全守门员多为无状态设计，无法建模多轮对话中用户意图随时间的渐进演化，导致对Crescendo、ActorAttack等跨轮次恶意策略防御失效，形成‘安全缺口’。

Method: 提出DeepContext框架，采用RNN架构处理细粒度的逐轮嵌入序列，通过隐状态在对话轮次间传播，显式建模意图风险的时序累积过程。

Result: 在多轮越狱检测任务上达到SOTA F1分数0.84，显著优于云厂商守门员及Llama-Prompt-Guard-2、Granite-Guardian（均为0.67）；T4 GPU上推理延迟低于20ms。

Conclusion: 建模用户意图的时序演化比依赖大规模无状态模型更有效且更高效，状态感知是提升LLM对话安全的关键方向。

Abstract: While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a "Safety Gap" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.

</details>


### [268] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 本文提出两个新指标——可重用性和可验证性，用于评估多智能体IR管道中LLM代理间Chain-of-Thought（CoT）推理的质量，发现其与传统准确率无强相关，且专用推理模型的CoT并不比通用LLM（如Llama、Gemma）更优。


<details>
  <summary>Details</summary>
Motivation: 现有CoT评估仅关注最终任务准确率，无法衡量推理过程本身的质量和实用性，存在评估盲区。

Method: 提出Thinker-Executor解耦框架，定义并量化CoT的可重用性（Executor复用Thinker CoT的难易程度）与可验证性（Executor基于CoT能否复现Thinker答案的频率）。

Result: 在5个基准上测试4个Thinker与10个Executor组合，发现可重用性与可验证性均不与标准准确率相关；专用推理模型的CoT在可重用性与可验证性上并未系统性优于Llama、Gemma等通用LLM。

Conclusion: 准确率不能代表CoT推理质量，需引入可重用性与可验证性等新评估维度，以更全面衡量多智能体协同推理能力。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [269] [SourceBench: Can AI Answers Reference Quality Web Sources?](https://arxiv.org/abs/2602.16942)
*Hexi Jin,Stephen Liu,Yuheng Li,Simran Malik,Yiying Zhang*

Main category: cs.AI

TL;DR: 本文提出了SourceBench，一个用于评估大语言模型在回答问题时所引用网页来源质量的新基准，涵盖内容质量与页面级信号共八项指标，并通过人工标注与校准的LLM评估器验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估过于关注答案正确性，而忽视引用证据（网页来源）的质量，亟需系统性衡量引用源质量的基准。

Method: 构建包含100个真实查询的SourceBench基准，设计涵盖内容质量（相关性、事实准确性、客观性）和页面级信号（时效性、权威性、清晰度等）的八维评估框架；结合人工标注数据与校准后的LLM评估器进行多模型评测。

Result: 在3996个引用源上评估了8个LLM、Google搜索及3个AI搜索工具，揭示了四个关于生成式AI与网络搜索协同演进的关键新见解。

Conclusion: SourceBench为评估和提升AI系统引用证据质量提供了可靠、可扩展的基准，推动生成式AI与可信网络搜索的融合发展。

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

</details>


### [270] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026 是一个面向多语言、跨时期历史文本中人物-地点关系抽取的评测任务，聚焦于 'at' 和 'isAt' 两类时序敏感关系，并采用准确性、计算效率与领域泛化三重评估指标。


<details>
  <summary>Details</summary>
Motivation: 推动历史文本中语义关系抽取的研究，支持数字人文领域的知识图谱构建、传记重建和空间分析等下游应用。

Method: 组织评测实验室（CLEF HIPE-2026），定义两类时序区分的关系（at / isAt），提供多语言、跨时期的噪声历史文本数据集，并设计三维度（准确率、效率、领域泛化）联合评估框架。

Result: 建立了面向历史文本的人物-地点关系抽取基准任务与综合评估体系，促进模型在时间推理、地理关联与跨语言迁移上的能力发展。

Conclusion: HIPE-2026 填补了历史文本中细粒度、时序感知关系抽取的评测空白，为数字人文与NLP交叉研究提供了重要基础设施。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


### [271] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 本文提出了GAP基准，系统评估大语言模型代理在文本层面与工具调用层面的安全性差异，发现文本安全不等于工具调用安全，强调需专门测量和缓解工具调用风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要集中于文本层面的拒绝行为，但大语言模型作为代理通过工具调用产生真实世界影响，亟需评估其工具调用层面的安全性是否与文本层面一致。

Method: 构建GAP基准，涵盖6个前沿模型、6个受监管领域、7种越狱场景、3种系统提示条件和2种提示变体，共生成17,420个数据点；定义GAP指标量化文本拒绝与工具执行有害行为之间的分歧，并进行统计显著性检验。

Result: 所有6个模型均存在文本拒绝但工具仍执行有害操作的现象；即使在安全强化提示下仍有219例GAP现象；系统提示措辞对工具调用安全性影响显著（TC-safe率跨度达21–57个百分点）；运行时治理合约可减少信息泄露但无法抑制非法工具调用。

Conclusion: 仅靠文本安全评估不足以保障LLM代理的真实世界安全性，工具调用安全需独立建模、测量与干预。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [272] [LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation](https://arxiv.org/abs/2602.16953)
*Hejia Zhang,Zhongming Yu,Chia-Tung Ho,Haoxing Ren,Brucek Khailany,Jishen Zhao*

Main category: cs.AI

TL;DR: 本文提出LLM4Cov，一种面向硬件验证的离线LLM智能体学习框架，通过执行验证的数据筛选、策略感知的智能体数据合成和最差状态优先采样，在执行受限条件下实现高效学习，并在真实对齐基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 执行感知的LLM智能体虽有潜力，但因工具反馈昂贵且缓慢（如工业级硬件仿真器），在线强化学习不现实；硬件验证尤其面临非可微执行信号与高覆盖率需求的挑战。

Method: 提出LLM4Cov离线学习框架：将验证建模为由确定性评估器引导的无记忆状态转移；引入执行验证的数据筛选、策略感知的智能体数据合成、最差状态优先采样；构建真实对齐的基准测试集并修订评估协议。

Result: 一个紧凑的4B参数模型在智能体评估下达到69.2%覆盖率通过率，较教师模型提升5.3%，性能媲美大一个数量级的模型。

Conclusion: LLM4Cov证明了在执行受限场景下，通过离线、数据驱动的智能体学习策略可有效提升LLM在硬件验证等关键任务中的覆盖率表现。

Abstract: Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.

</details>


### [273] [Automating Agent Hijacking via Structural Template Injection](https://arxiv.org/abs/2602.16958)
*Xinhao Deng,Jiaqing Wu,Miao Chen,Yue Xiao,Ke Xu,Qi Li*

Main category: cs.AI

TL;DR: 本文提出Phantom框架，利用结构化模板注入实现自动化代理劫持，通过诱导角色混淆攻击LLM代理，显著提升攻击成功率和迁移性，并在多个主流模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义提示操纵的代理劫持方法成功率低、迁移性差，难以有效攻击闭源商用模型。

Method: 提出Phantom框架，基于结构化模板注入，利用LLM代理依赖特定聊天模板分隔不同角色指令的机制；引入多级模板增强、模板自编码器（TAE）与贝叶斯优化联合搜索高功效对抗模板。

Result: 在Qwen、GPT、Gemini等模型上显著超越基线方法，攻击成功率（ASR）与查询效率均提升；发现并确认70余个真实商业产品中的漏洞。

Conclusion: 结构化模板注入是一种高威胁、高迁移性的新型代理劫持手段，为下一代智能体系统安全防护提供了实证基础与新思路。

Abstract: Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.

</details>


### [274] [HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing](https://arxiv.org/abs/2602.16976)
*Srikumar Nayak*

Main category: cs.AI

TL;DR: HQFS是一种融合量子预测、离散风险优化与可审计性的混合金融风险系统，通过变分量子电路预测收益与波动率，转化为QUBO问题并用量子退火或经典求解器优化，结合后量子签名保障决策可追溯性，在预测精度、投资表现与求解效率上均优于经典基线。


<details>
  <summary>Details</summary>
Motivation: 传统两步式金融风险系统在市场变化、离散约束（如手数、限额）和大规模资产优化时易失效，且难以满足监管对可审计性与决策溯源的要求。

Method: HQFS采用三阶段混合流程：1）用带小型经典头部的变分量子电路（VQC）预测下期收益与波动率代理；2）将风险-收益目标与约束转化为QUBO形式，支持量子退火求解并兼容经典QUBO求解器作为回退；3）使用后量子签名对每次再平衡结果签名，确保分配记录可验证、不可篡改。

Result: 在实证数据集上，HQFS相较调优的经典基线：收益预测误差降低7.8%，波动率预测误差降低6.1%；样本外夏普比率提升9.4%，最大回撤降低11.7%；QUBO求解平均耗时减少28%，同时生成完全可追溯、已签名的配置记录。

Conclusion: HQFS证明了量子-经典混合架构可在保持部署实用性的同时，显著提升金融决策系统的预测准确性、优化鲁棒性、运行效率与监管合规性。

Abstract: Here's the corrected paragraph with all punctuation and formatting issues fixed:
  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.

</details>


### [275] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 本文挑战了黑箱安全评估的可靠性假设，指出当模型行为依赖于部署中常见但评估中罕见的隐式上下文变量时，黑箱评估无法可靠估计真实风险；理论证明了被动与自适应评估下的误差下界，并揭示了计算层面的安全分离现象，强调白盒分析或额外保障机制的必要性。


<details>
  <summary>Details</summary>
Motivation: 黑箱安全评估假设测试分布上的模型表现能可靠预测实际部署性能，但现实中模型可能依赖未观测的隐式上下文变量（在部署中高频出现、评估中稀疏），导致该假设失效。

Method: 采用统计学习理论（Le Cam方法、Yao极小极大原理）、密码学假设（陷门单向函数）和白盒探针建模，构建隐式上下文策略模型，推导各类评估方式的风险估计误差下界及样本复杂度。

Result: （1）被动评估存在最小绝对误差下界≈0.208·δ·L；（2）自适应评估误差下界仍为δ·L/16，检测需Θ(1/ε)查询；（3）计算上存在分离：拥有陷门的部署方可触发不可区分的危险行为；（4）白盒探针需O(1/(γ²ε_R²))样本，且需偏差校正。

Conclusion: 黑箱测试在隐式上下文存在时本质上统计不确定，必须辅以架构约束、训练保障、可解释性或部署监控等补充手段，才能实现最坏情况下的安全保证。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [276] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: 本文提出Conv-FinRe，一个面向金融投顾场景的对话式、纵向股票推荐基准，强调区分用户行为模仿与理性决策质量，引入多视角参考标准评估大语言模型在风险偏好驱动下的长期投资决策能力。


<details>
  <summary>Details</summary>
Motivation: 传统推荐基准仅以用户历史行为为唯一真值，但在金融市场中，用户行为易受市场波动影响而短视或噪声大，无法反映其真实长期目标；因此需构建能评估理性决策质量而非单纯行为模仿的新基准。

Method: 构建Conv-FinRe基准：基于真实市场数据与人类决策轨迹，设计包含开户访谈、逐阶段市场背景与投顾对话的纵向任务；要求模型在固定投资周期内生成股票排序；提供多视角参考（描述性行为 vs. 规范性效用），以诊断模型是否遵循理性分析、模仿噪声或追逐市场动量。

Result: 评估多个SOTA大语言模型发现：在效用导向排名上表现好的模型往往难以匹配用户实际选择；而行为对齐度高的模型则易过拟合短期噪声，揭示理性决策质量与行为一致性之间存在持续张力。

Conclusion: Conv-FinRe为金融推荐提供了更符合现实目标的评估范式，推动大语言模型从‘学用户怎么做’转向‘帮用户做更好决策’，并开源数据集与代码以促进后续研究。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [277] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: 本文提出了一种名为Sonar-TS的神经符号框架，用于自然语言查询时间序列数据库（NLQ4TSDB），通过“搜索-验证”流程结合SQL索引与Python程序处理连续形态意图（如形状、异常）和超长时序历史，并构建首个大规模基准NLQTSBench以推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法难以处理时间序列中连续形态语义（如形状、异常），而纯时序模型又难以应对超长历史；非专家用户亟需易用、精准的自然语言查询能力。

Method: 提出Sonar-TS神经符号框架，采用Search-Then-Verify两阶段流程：先用特征索引通过SQL检索候选时间窗口，再生成Python程序对原始信号进行细粒度验证；并构建NLQTSBench基准用于评估。

Result: 实验表明Sonar-TS在复杂时序查询任务上显著优于传统方法，能有效处理形态意图与超长历史，NLQTSBench成为该领域首个可扩展、大规模评估基准。

Conclusion: 本工作是NLQ4TSDB领域的首次系统性研究，提出了通用神经符号框架与标准化评估基准，为后续研究奠定基础。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [278] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: 本文提出Cinder系统，通过两阶段匹配机制（基于Ruzicka相似性指数的快速初筛 + 基于非线性技能桶与Kantorovich距离的精细公平性评估）提升多人在线游戏中预组队（lobby）间匹配的公平性与效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于平均技能（如均值或中位数）的匹配方法在队伍技能分布宽或偏斜时易导致不公平、一边倒的比赛，影响玩家留存与满意度。

Method: Cinder采用两阶段方法：第一阶段用Ruzicka相似性指数快速比较队伍的‘非异常值’技能范围进行初筛；第二阶段将玩家段位映射到由逆正态分布生成的非线性技能桶（增强中等水平区分度），再对排序后的桶索引计算Kantorovich距离，得到量化公平性的‘Sanction Score’。

Result: 在1.4亿次模拟队伍配对中验证了Sanction Score的分布特性，为设定公平匹配阈值提供了坚实依据。

Conclusion: Cinder在保证匹配速度的同时显著提升了预组队匹配的公平性，尤其适用于技能分布异质性强的场景，具备实际部署可行性。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [279] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: 本文提出了M2F（Math-to-Formal）框架，首次实现了面向Lean定理证明器的端到端、项目级数学自动形式化，成功将实分析与凸分析教材（479页）转化为15.3万行可编译Lean代码，显著提升形式化效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 现有数学自动形式化局限于孤立定理或短片段，缺乏处理跨文件依赖、模块导入及整项目端到端编译的能力，难以扩展至教材和研究论文规模。

Method: M2F采用两阶段智能体框架：第一阶段为陈述编译（statement compilation），将文档拆分为原子块、推断依赖顺序、修复声明骨架（允许证明留空）；第二阶段为证明修复（proof repair），在固定签名下基于目标条件进行局部编辑填补证明空缺；全程闭环集成验证器反馈，仅在工具链确认改进时提交修改。

Result: 在约三周内完成479页教材的形式化，生成153,853行Lean代码；在FATE-H基准上证明成功率96%（强基线为80%）。

Conclusion: M2F证明了大规模数学文献自动化形式化的可行性与实用性，为构建可验证数学知识库提供了可扩展的技术路径。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [280] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: 本文介绍了微软Dynamics 365 Sales中的Sales Research Agent，一个能连接实时CRM数据、理解复杂数据模式并生成可决策洞察（文本+图表）的AI应用；同时提出Sales Research Bench基准，从8个客户关注维度评估AI系统质量，并在200题测试中显著超越Claude Sonnet 4.5和ChatGPT-5。


<details>
  <summary>Details</summary>
Motivation: 企业亟需能基于实时、定制化CRM数据回答销售领导问题的AI系统，但现有模型缺乏透明、可复现的质量证据。

Method: 设计并实现Sales Research Agent（集成CRM连接、复杂Schema推理、文本与图表输出能力），并构建Sales Research Bench基准，从文本/图表可信度、相关性、可解释性、Schema准确性、图表质量等8个客户加权维度量化评估AI系统。

Result: 在200题、定制企业Schema的测试中，Sales Research Agent在100分制综合得分上比Claude Sonnet 4.5高13分、比ChatGPT-5高24.1分。

Conclusion: Sales Research Agent提供了高质量、可验证、面向企业销售场景的AI解决方案，Sales Research Bench为AI系统质量评估建立了客户导向的标准化方法。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [281] [Phase-Aware Mixture of Experts for Agentic Reinforcement Learning](https://arxiv.org/abs/2602.17038)
*Shengtian Yang,Yu Li,Shuo He,Yewen Li,Qingpeng Cai,Peng Jiang,Lei Feng*

Main category: cs.AI

TL;DR: 本文提出Phase-Aware Mixture of Experts (PA-MoE)，通过引入轻量级相位路由器，实现对强化学习中任务相位的自动识别与专家分配，提升LLM智能体在复杂任务中的表现，缓解单一策略网络导致的简单性偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法多采用单一策略网络，易产生“简单性偏差”，即简单任务占据大部分参数并主导梯度更新，导致复杂任务建模能力不足；而传统MoE架构的token级路由会破坏时序一致的相位模式，削弱专家专业化能力。

Method: 提出Phase-Aware Mixture of Experts（PA-MoE）：设计轻量级相位路由器，从RL目标中自主学习隐式相位边界，并为每个时间相位分配固定的专家，确保专家在相位内保持时序一致的专业化能力。

Result: 实验结果验证了PA-MoE在提升LLM智能体解决复杂任务能力方面的有效性，显著缓解了简单性偏差问题。

Conclusion: PA-MoE通过相位感知的专家分配机制，克服了传统MoE在RL场景下token级路由导致的相位碎片化问题，为构建更鲁棒、可扩展的LLM智能体策略网络提供了新思路。

Abstract: Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \emph{single} policy network, causing \emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.

</details>


### [282] [Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs](https://arxiv.org/abs/2602.17046)
*Uria Franko*

Main category: cs.AI

TL;DR: 本文提出Instruction-Tool Retrieval（ITR），一种面向LLM智能体的RAG变体，通过每步动态检索最简系统指令片段和最小必要工具集，显著降低上下文开销、提升工具选择准确率并减少整体运行成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步执行中反复加载冗长系统指令和庞大工具目录，导致高成本、高延迟、易偏离目标及工具选择错误。

Method: ITR是一种RAG变体，按步检索最相关系统提示片段和最小必要工具子集，构建动态运行时系统提示，并通过置信度门控的回退机制提供精简工具集。

Result: 在可控基准测试中，ITR将每步上下文token减少95%，工具路由准确率相对提升32%，端到端任务成本降低70%，支持智能体在上下文限制内多运行2–20倍步数。

Conclusion: ITR显著提升长周期自主智能体的效率与可靠性，其收益随步数增加而累积，具备实用部署价值。

Abstract: Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.

</details>


### [283] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: 本文提出了IntentCUA，一种基于多智能体和意图对齐计划记忆的计算机使用框架，旨在提升长周期任务执行的稳定性与效率。通过 Planner、Plan-Optimizer 和 Critic 协同工作于共享意图记忆，抽象交互迹为多视角意图表示并复用技能，在桌面自动化任务中显著优于现有RL和轨迹检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理在长周期、噪声感知、多窗口、动态环境中易偏离用户意图，重复解决常规子问题，导致错误累积与低效。

Method: 提出IntentCUA多智能体框架，包含Planner、Plan-Optimizer和Critic，依托共享记忆实现多视角意图抽象与可复用技能存储；运行时通过意图原型检索技能并注入部分计划，减少重规划与错误传播。

Result: 端到端评测中任务成功率74.83%，步效比0.91，显著优于基线；消融实验证明多视角意图抽象与共享记忆共同提升执行稳定性，多智能体协同机制对长周期任务增益最大。

Conclusion: 系统级意图抽象与记忆驱动的协同机制是实现大规模动态桌面环境中可靠高效自动化的核心。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [284] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 本文提出推理忠实性（reasoning faithfulness）的正式框架，定义了立场一致性与因果影响两个可测试条件，并构建RFEval基准评估12个开源大推理模型，发现近半数输出不忠实，且准确性不能作为忠实性的可靠代理。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）常生成看似合理但实际未反映真实决策过程的推理链，损害可靠性与信任，亟需一种独立于准确性的、可量化评估推理忠实性的方法。

Method: 提出基于立场一致性与因果影响的推理忠实性形式化定义；构建RFEval基准（7186个样本，7项任务），通过输出级反事实干预进行实证评测；对12个开源LRMs开展系统性评估，并分析训练范式、规模与忠实性的关系。

Result: 49.7%的模型输出被判定为不忠实，主要源于立场不一致；不忠实现象在数学与代码等收敛性强的任务中更突出；后训练方式（如RL微调）比模型规模更显著影响忠实性；准确性与忠实性之间无显著统计关联。

Conclusion: 准确性既非忠实性的充分条件，也非可靠代理；构建可信AI必须同步优化结果正确性与推理过程的结构完整性；RFEval为审计LRM可靠性提供了严谨方法论。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [285] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 本文提出了Successive Sub-value Q-learning (S2Q)，通过学习多个子值函数来保留高价值的替代动作，从而增强多智能体强化学习中的探索能力和对价值函数变化的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有基于值分解的多智能体强化学习方法依赖单一最优动作，难以适应训练过程中值函数的变化，易收敛到次优策略。

Method: 提出S2Q方法，学习多个子值函数，并将其整合进基于Softmax的行为策略中，以促进持续探索并加快全局Q值对动态最优解的适应。

Result: 在多个具有挑战性的MARL基准测试中，S2Q一致优于多种现有MARL算法，展现出更强的适应性和整体性能。

Conclusion: S2Q通过引入多子值函数机制，有效缓解了值分解方法在动态环境下的适应性不足问题，提升了多智能体协同学习的鲁棒性与性能。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [286] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: 本文提出了预测批量调度（PBS）技术，通过动态优先选择高损失样本进行批处理，加速语言模型训练收敛。该方法使用轻量级线性预测器，基于静态词元特征在线估计样本难度，仅需四个简单特征即可实现与实际损失0.44的相关性，显著提升收敛速度且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖预定义难度指标，而困难样本挖掘需要昂贵的逐样本损失跟踪，缺乏高效、低开销的动态样本调度方案。

Method: 提出Predictive Batch Scheduling（PBS），构建一个轻量级在线训练的线性预测器，利用四个静态词元级特征（词频、序列长度、词汇多样性、稀有词比例）预测样本损失，据此在批构建中动态优先选取高难度样本。

Result: 在130M参数Transformer上实验表明，PBS使评估损失下降速度提升6–13%；预测器与真实损失的相关性从训练初期0.14提升至10,000步后的0.44。

Conclusion: 词频等简单统计特征足以有效表征样本难度，PBS实现了接近课程学习效果但几乎无额外计算开销，为高效训练提供了新范式。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [287] [How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses](https://arxiv.org/abs/2602.17084)
*Kan Watanabe,Rikuto Tsuchida,Takahiro Monno,Bin Huang,Kazuma Yamasaki,Youmei Fan,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.AI

TL;DR: 本研究对五种AI编程代理生成的GitHub Pull Request（PR）描述特征及其对人类审阅者响应的影响进行了实证分析，发现不同AI代理在PR描述风格、审阅者参与度、响应时间及合并率上存在显著差异，凸显了PR呈现方式与人-AI协作中审阅动态的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究不同AI编码代理在Pull Request描述特征上的差异，以及这些差异如何影响人类审阅者的响应行为（如评审活跃度、响应时间、情感倾向和合并结果），以理解人-AI协同软件开发中的关键影响因素。

Method: 基于AIDev数据集，对五种AI编码代理生成的Pull Request进行实证分析，涵盖PR描述的结构化特征（如标题、正文、格式等），并量化分析人类审阅者的评审活动、响应时长、情感倾向及最终合并结果。

Result: 发现不同AI代理具有显著不同的PR描述风格；这些风格与审阅者参与度、响应速度及PR合并率密切相关；各代理在审阅互动指标和合并率上存在明显差异。

Conclusion: PR的呈现方式（尤其是描述质量与风格）深刻影响人类审阅者的行为与决策，是提升人-AI协同开发效能的关键环节；未来AI编码代理应优化PR自动生成策略以增强可解释性与协作友好性。

Abstract: The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.

</details>


### [288] [Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence](https://arxiv.org/abs/2602.17096)
*Zhaoyang Li,Xingzhi Jin,Junyu Pan,Qianqian Yang,Zhiguo Shi*

Main category: cs.AI

TL;DR: 本文探讨了在6G无线系统中引入基于大语言模型（LLM）的智能体（agentic AI）以实现意图驱动的物理层自主控制，提出了一种闭环框架（意图感知-自主决策-网络执行），并以AgenCom为例验证其在多目标、动态环境下的适应性。


<details>
  <summary>Details</summary>
Motivation: 6G系统面临多维、动态变化的用户意图（如时延敏感性、能效偏好、算力约束等），传统规则驱动或中心化优化方法难以准确理解并响应这些意图，亟需具备上下文理解和跨模态推理能力的新范式。

Method: 提出基于LLM的意图感知闭环智能体架构，涵盖多模态感知、跨层决策与可持续优化；系统梳理物理层任务的意图支持瓶颈；识别适用场景；并设计并实现意图驱动链路决策智能体AgenCom。

Result: 明确了LLM赋能的智能体在6G物理层的关键优势与适用场景，识别出多模态感知、跨层决策和可持续优化三大技术挑战，并通过AgenCom案例验证了其在多样化用户偏好与信道条件下的自适应链路构建能力。

Conclusion: LLM驱动的智能体为6G物理层提供了实现意图驱动、自主演化与可持续优化的新路径，但需突破多模态感知融合、实时决策可信性与资源效率等关键技术瓶颈。

Abstract: As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.
  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.
  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.

</details>


### [289] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 本文提出了一种人机协作框架（STRIDE + SR-Delta），旨在通过构建可信基准数据集来统一和评估ESG评级方法，提升其可比性、可信度与决策相关性。


<details>
  <summary>Details</summary>
Motivation: 现有ESG评级机构对同一公司的评分差异大，导致可比性、可信度和决策参考价值受限。

Method: 提出通用的人-AI协作框架：STRIDE提供基于大语言模型构建企业级基准数据集的原则性标准与评分体系；SR-Delta为偏差分析流程框架，用于识别评级方法需调整之处。

Result: 实现了可持续评级方法的可扩展、可比较评估，并推动AI赋能ESG评级方法的强化与进步。

Conclusion: 该框架有助于提升ESG评级的科学性与一致性，呼吁AI社区共同参与，以支持紧迫的可持续发展议程。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [290] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 本文提出O-Shap方法，通过满足T性质的新分割策略改进Owen值在XAI中的应用，提升归因精度、语义一致性与计算效率。


<details>
  <summary>Details</summary>
Motivation: Shapley值在XAI中广泛应用，但在视觉任务中因像素间强依赖而难以满足特征独立性假设；现有Owen值实现依赖的分组方式（如SLIC）缺乏理论一致性保障。

Method: 提出满足T-性质的新型特征分组分割方法，构建语义对齐的层次结构，支持Owen值计算并实现计算剪枝。

Result: 在图像和表格数据集上，O-Shap相比基线SHAP变体在归因精度、语义连贯性和运行效率上均更优，尤其在结构重要时优势显著。

Conclusion: 基于T-性质的层次分组是提升Owen值可解释性与实用性的关键，O-Shap为结构化数据的可信归因提供了新范式。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [291] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: 本文提出InstructKG框架，利用课程讲义材料自动构建教师对齐的知识图谱，以捕捉概念间的先决与子概念依赖关系，支持个性化学习干预。


<details>
  <summary>Details</summary>
Motivation: 在大规模课程中，教师难以手动诊断学生知识漏洞并确定需强化的概念；现有知识图谱方法或过于表面，或忽略教学材料中的丰富教学信号。

Method: InstructKG从课程讲义（如幻灯片、笔记）中提取关键概念作为节点，并结合教学材料中的时序与语义信号（如概念出现顺序、定义关系），利用大语言模型推断'part-of'和'depends-on'等有向边，构建 instructor-aligned 知识图谱。

Result: 在多个真实课程讲义数据集上的实验及人工评估表明，InstructKG 能有效捕获丰富且符合教师意图的学习进展结构。

Conclusion: InstructKG为规模化个性化教育提供了可扩展、可解释、教学对齐的知识建模新范式。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [292] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 本文提出了一种高维空间的指称认识论，认为生成式AI通过将符号输入映射到语义参数构成的高维几何空间中运作，该空间成为生成过程的主动认识条件；基于高维几何的四个结构性质，文章将生成模型重新概念化为学习流形的导航者，并提出‘导航知识’作为区别于符号推理和统计重组的第三种知识生产模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的机制具有模糊的认识论特征，缺乏对其运作原理的理解，阻碍了其在科学、教育和制度生活中的负责任整合。

Method: 基于图灵-香农-冯·诺依曼传统与神经网络架构的根本差异，结合高维几何的四个结构性质（测度集中、近正交性、指数方向容量、流形规则性），并融合皮尔士符号学与帕珀特建构主义，构建‘高维空间指称认识论’。

Result: 提出了‘导航知识’这一新的知识生产模式，将生成模型视为学习流形的导航者，揭示了生成式AI中语义空间作为主动认识条件的核心作用。

Conclusion: 生成式AI要求一种范式突破，即从传统的符号-外部语义模型转向以高维语义空间为认识基础的新认识论框架，‘导航知识’为此提供了哲学与结构基础。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [293] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 本文提出了一种用于分解困难CircuitSAT实例的新型并行算法，通过专用约束将原问题划分为多个弱化公式，并利用并行计算的难度估计来指导高质量分解的高效识别。


<details>
  <summary>Details</summary>
Motivation: 解决困难CircuitSAT实例的求解效率问题，特别是在逻辑等价性验证和密码哈希函数原像攻击等实际场景中。

Method: 提出一种参数化的并行算法，利用专门设计的约束对原始SAT实例进行划分，生成一系列弱化公式，并通过并行计算的硬度估计来动态调整参数以寻找高质量分解。

Result: 在包括逻辑等价性检查和密码哈希函数原像攻击在内的挑战性CircuitSAT实例上验证了该算法的实际有效性。

Conclusion: 所提出的并行分解算法能有效提升困难CircuitSAT实例的求解效率，具有良好的实用性和可调性。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [294] [Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning](https://arxiv.org/abs/2602.17145)
*Joseph Bingham,Sam Helmich*

Main category: cs.AI

TL;DR: 本文提出了Combine，一种基于准则的剪枝框架，用于迭代剪枝，具有快速高效的特点，并提出了新的剪枝准则函数，在VGG类模型上实现了高达79%的滤波器剪枝率，同时保持或提升准确率，并减少最多68%的计算量。


<details>
  <summary>Details</summary>
Motivation: 随着CNN模型规模增大，其执行时间、内存占用和功耗也随之增加，现有剪枝方法缺乏统一实现标准，难以比较和应用。

Method: 提出Combine框架，支持多种剪枝准则的统一实现与评估，引入标准化语言描述剪枝准则，并设计若干新型剪枝准则函数，采用迭代剪枝策略在VGG类模型上进行验证。

Result: 在VGG类模型上实现最高79%滤波器剪枝，准确率保持或提升，计算量减少最多68%。

Conclusion: Combine是一个灵活、高效且可比性强的剪枝框架，证实了不同剪枝准则对不同模型效果存在差异，并为剪枝研究提供了标准化工具和新思路。

Abstract: As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\%.

</details>


### [295] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA是一种新型基因组基础模型预训练框架，结合联合嵌入预测架构（JEPA）与传统生成目标，在潜在空间中监督CLS token以预测被掩码基因片段的高层功能表征，从而弥补现有MLM/NTP方法缺乏全局生物学语义的缺陷，显著提升各类下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型（GFMs）主要依赖掩码语言建模（MLM）或下一词预测（NTP），虽擅长捕捉局部序列模式，但难以建模全局功能上下文，导致表征缺乏生物学意义。

Method: 提出JEPA-DNA框架：在传统生成目标基础上引入JEPA，通过监督CLS token在潜在空间中预测被掩码基因片段的功能级嵌入，实现token级恢复与高阶功能预测的联合优化。

Result: 在多个基因组基准测试中，JEPA-DNA在监督学习和零样本任务上均显著优于纯生成式基线模型。

Conclusion: JEPA-DNA提供了更鲁棒、更具生物学意义的基因组表征，为构建理解基因组‘功能逻辑’而非仅‘字母序列’的基础模型提供了可扩展路径。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [296] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo is a lightweight, high-performance formula recognition model with only 20 million parameters, achieving performance comparable to larger SOTA models while enabling real-time inference and in-browser deployment.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight yet high-performance formula recognition model suitable for resource-constrained environments like consumer-grade hardware and web browsers.

Method: Attentive design, vocabulary and tokenizer distillation and transfer.

Result: Texo achieves comparable accuracy to UniMERNet-T and PPFormulaNet-S, with 80% and 65% smaller model size respectively; enables real-time inference and in-browser deployment; accompanied by a functional web application.

Conclusion: Texo demonstrates that significant model compression is possible without sacrificing performance in formula recognition, broadening accessibility and practical deployment scenarios.

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [297] [Continual learning and refinement of causal models through dynamic predicate invention](https://arxiv.org/abs/2602.17217)
*Enrique Crespo-Fernandez,Oliver Ray,Telmo de Menezes e Silva Filho,Peter Flach*

Main category: cs.AI

TL;DR: 本文提出了一种在线构建符号因果世界模型的框架，结合连续模型学习与修复、元解释学习和谓词发明，实现高样本效率、可解释且可扩展的世界建模。


<details>
  <summary>Details</summary>
Motivation: 标准世界建模方法存在样本效率低、缺乏透明性及可扩展性差等问题，难以支持智能体在复杂环境中高效导航。

Method: 将连续模型学习与修复集成到智能体决策循环中，利用元解释学习（Meta-Interpretive Learning）和谓词发明（predicate invention）在线构建层次化、解耦的符号因果世界模型。

Result: 所提提升推理（lifted inference）方法在具有复杂关系动态的领域中可扩展，避免命题方法的组合爆炸，并在样本效率上比PPO基线高出数量级。

Conclusion: 符号因果世界模型可通过在线学习与抽象发明实现高效、可解释、可扩展的环境建模，为通用智能体提供更鲁棒的认知基础。

Abstract: Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.

</details>


### [298] [From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences](https://arxiv.org/abs/2602.17221)
*Yi-Chih Huang*

Main category: cs.AI

TL;DR: 本研究提出了一种面向人文与社会科学的AI Agent协作研究工作流（Agentic Workflow），基于台湾Claude.ai使用数据进行方法验证，强调任务模块化、人机分工与可验证性，并归纳出三种人机协作模式，凸显人类判断在研究中的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI研究多集中于软件工程与自然科学，人文与社会科学缺乏适配的方法论探索，亟需构建可复用、可验证的人机协同研究框架。

Method: 设计并验证一个七阶段模块化AI Agent协作工作流，遵循任务模块化、人机分工、可验证性三原则；以台湾Anthropic经济指数（AEI）的7729轮Claude.ai对话数据为实证基础，开展二手数据分析示范。

Result: 成功构建并演示了Agentic Workflow的可行性；识别出直接执行、迭代优化与人类主导三种人机协作操作模式；证实人类在研究问题设定、理论解读、情境化推理与伦理反思中具有不可替代性。

Conclusion: 该工作流为人文与社会科学提供了可复制的AI协作方法论；强调AI是增强工具而非替代者，方法论创新需以人类研究者的核心判断力为锚点；同时指出单平台、横截面数据及AI可靠性等局限。

Abstract: Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a "methodological experiment," this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.
  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).
  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.

</details>


### [299] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 本文提出大型行为模型（LBM），通过将心理特质剖面嵌入而非依赖提示工程，显著提升对个体战略决策的高保真预测能力，尤其在复杂高风险场景中优于传统大语言模型及提示方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在预测人类高风险环境下的个体决策时存在一致性差、身份漂移、难以利用详细人格描述等问题，亟需更稳定、可扩展的行为建模方法。

Method: 提出大型行为模型（LBM），基于结构化高维心理特质剖面（源自全面心理测量量表）进行条件建模；使用专有数据集（关联稳定特质、动机状态、情境约束与实际选择）进行微调；摒弃临时提示，转向行为嵌入范式。

Result: 在保留场景评估中，LBM微调后显著优于未适配的Llama-3.1-8B-Instruct基座模型，且在仅使用大五人格特征时性能媲美前沿基线；性能随特质维度增加持续提升，而提示方法存在复杂度上限。

Conclusion: LBM是一种可扩展的高保真行为模拟新范式，为战略预判、谈判分析、认知安全与决策支持等应用提供了坚实基础。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [300] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 本研究利用布鲁姆分类法作为层次化视角，探究大语言模型内部神经表征中认知复杂度的编码方式，发现不同认知层级在模型残差流中具有线性可分性，且这种可分性随网络层加深而增强。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的黑箱特性需要超越表面性能指标的新评估框架，而认知复杂度的内部表征尚不明确。

Method: 以布鲁姆分类法为指导，分析多个大语言模型高维激活向量在残差流中的分布，检验各认知层级（从记忆到创造）在线性空间中的可分性，并考察其在前向传播过程中的演化规律。

Result: 线性分类器在所有布鲁姆层级上平均准确率达95%，表明认知层级在模型表征中存在线性可访问子空间；且该可分性在前向传播早期即显现，并随层数加深而增强。

Conclusion: 大语言模型在内部表征中显式编码了任务的认知复杂度，且该编码具有线性结构和层间演化特性，为理解模型推理机制提供了新依据。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [301] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 本文提出了一种检测和量化大语言模型（LLM）在回溯评估中时间知识泄露的新框架Shapley-DCLR，并设计了TimeSPEC方法通过声明级验证与再生来主动过滤时间污染，实验证明其在保持任务性能的同时显著降低泄露率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM回溯评估易受训练数据中截止日期后知识泄露影响，导致评估失效，亟需可解释、可量化的泄漏检测与控制机制。

Method: 提出基于声明分解与Shapley值归因的Shapley-DCLR指标，用于量化决策关键性泄露比例；并设计TimeSPEC方法，在生成过程中交织声明提取、时间验证与再生，确保所有支持性声明均源自截止日期前信息。

Result: 在最高法院判例预测、NBA薪资估计和股票收益排序共350个实例上，标准提示基线存在显著时间泄露；TimeSPEC显著降低了Shapley-DCLR，同时维持任务性能。

Conclusion: 显式的、可解释的声明级时间验证比仅依赖提示的时间约束更有效，为LLM可靠回溯评估提供了新范式。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [302] [Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web](https://arxiv.org/abs/2602.17245)
*Linxi Jiang,Rui Xi,Zhijie Liu,Shuo Chen,Zhiqiang Lin,Suman Nath*

Main category: cs.AI

TL;DR: 本文提出Web Verbs，一种面向Web操作的语义层抽象，将网站功能封装为类型化、可文档化的函数，以提升LLM驱动Web代理的可靠性、效率与可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理依赖低级操作（如点击、输入），存在脆弱、低效、难验证等问题；需构建类似NLWeb语义检索层的语义动作层。

Method: 设计并实现Web Verbs——一套大规模、类型化、带语义契约（前置/后置条件、策略标签、日志）的Web功能函数集，支持API与浏览器流程统一抽象，并提供发现、选择与程序合成能力。

Result: 通过概念验证实现与案例研究，证明Web Verbs能显著减少操作步骤、提升执行鲁棒性与可审计性，优于现有基于原始交互的代理方法。

Conclusion: Web Verbs为构建可信赖、可扩展的智能Web代理提供了关键语义基础设施，后续需推动标准化以实现全网部署。

Abstract: The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \textbf{reliability} by providing stable interfaces, \textbf{efficiency} by reducing dozens of steps into a few function calls, and \textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.

</details>


### [303] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细记录了从原始arXiv LaTeX源（数学、计算机科学、理论物理）端到端训练一个1.36B参数科学语言模型的工程实践，涵盖数据预处理、领域感知分词、受限算力（2×A100）下的训练分析，并揭示了预处理、I/O与tokenization对训练稳定性与符号一致性的关键影响。


<details>
  <summary>Details</summary>
Motivation: 前沿大模型虽具强推理能力，但如何在有限算力下从原始科学文献（如arXiv LaTeX）高效训练领域专用小模型，缺乏系统性、可复现的工程指南。

Method: 构建完整pipeline：arXiv元数据过滤→归档验证→LaTeX解析→文本标准化→领域感知分词→基于2块A100 GPU的密集Transformer训练；开展24组实验，系统评估训练稳定性、缩放律、数据损耗及基础设施瓶颈。

Result: 发现预处理决策显著影响可用token量；分词方式影响数学符号稳定性；存储与I/O常成为与计算并列的瓶颈；在52B预训练token的数据丰富场景下可实现稳定收敛。

Conclusion: 本工作不提出新架构，而是提供一份面向中等算力预算研究者的、工程扎实、细节透明的科学语言模型训练实践报告，强调可复现性与实操洞察。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [304] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: 本文提出MedClarify，一种能生成信息获取型随访问题以支持临床诊断决策的AI代理，通过信息论方法减少诊断不确定性，显著提升诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 现有医学大语言模型在诊断任务中缺乏对迭代式病史采集和差异性诊断推理的支持，难以有效处理临床中普遍存在的不确定性。

Method: MedClarify首先构建候选诊断列表（即差异性诊断），再基于信息增益最大化原则主动生成并选择最优随访问题，实现不确定性感知的迭代推理。

Result: 相比单次提示的基线模型，MedClarify将诊断错误率降低了约27个百分点。

Conclusion: MedClarify通过具身式信息寻求机制，使医学大模型更贴近真实临床推理的迭代性与不确定性，为提升人机医疗对话质量提供了新路径。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [305] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 本文提出了一种无需数据的表示漂移正则化方法，通过曲率矩阵近似（采用K-FAC）来解耦任务向量，提升任务算术在多任务组合中的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有表示漂移正则化方法依赖外部任务数据，违背模块化和隐私等实际约束，亟需一种无需数据的解决方案。

Method: 将表示漂移正则化建模为曲率矩阵近似问题，采用Kronecker-Factored Approximate Curvature（K-FAC）构造数据无关的正则器。

Result: 所提方法在任务添加与取反任务上达到SOTA；计算复杂度为任务数的常数级；对任务向量缩放具有鲁棒性，无需验证集调参。

Conclusion: 数据无关的曲率驱动正则化有效缓解任务算术中的跨任务干扰，在保持模块性的同时显著提升可扩展性与实用性。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [306] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 本文提出了一种多模态对比变分自编码器（MCVAE），用于在存在严重模态缺失的情况下，鲁棒地整合全切片图像、转录组和DNA甲基化数据，以预测非小细胞肺癌患者的生存结局。


<details>
  <summary>Details</summary>
Motivation: NSCLC患者生存预测因个体预后特征差异大而困难；临床数据常存在严重模态缺失，现有模型在该情形下鲁棒性不足。

Method: 提出MCVAE：使用模态特异性变分编码器建模不确定性，引入带门控机制的融合瓶颈，并设计多任务损失（生存损失+重建损失+跨模态对比损失）；训练中采用随机模态掩码增强鲁棒性。

Result: 在TCGA-LUAD和TCGA-LUSC数据集上，MCVAE在疾病特异性生存预测及严重缺失场景下的鲁棒性均优于两个SOTA模型；消融实验发现并非所有模态组合都带来性能增益。

Conclusion: MCVAE能有效应对临床多模态数据的不完整性，提升生存预测鲁棒性；多模态集成需谨慎评估，非总是有益。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [307] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种基于隐私设计（Privacy-by-Design）的框架，面向儿童使用的AI技术（特别是大语言模型），整合GDPR、PIPEDA、COPPA及UNCRC、英国AADC等法规原则，覆盖数据收集、模型训练、运行监控与持续验证各阶段，并结合学术文献中的操作控制措施和适龄设计指南，通过教育类LLM导师案例验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 儿童日益使用AI技术，但其隐私风险突出；现有法规虽有要求，实践中落实困难，亟需可操作的隐私保护设计框架。

Method: 构建融合多国儿童隐私法规与联合国儿童权利公约的Privacy-by-Design框架，将法规原则映射至LLM全生命周期各阶段（数据收集、训练、监控、验证），并整合学术界提出的工程技术与组织控制措施，辅以适龄设计指南，最后通过LLM教育导师案例进行实证分析。

Result: 该框架为AI服务提供商和开发者提供了分阶段、可落地的隐私风险缓解路径，支持在满足法律合规前提下开发安全、适龄的儿童AI应用；案例研究表明技术与组织控制结合适龄设计能有效提升隐私保护水平。

Conclusion: 将Privacy-by-Design系统性融入LLM全生命周期，并协同法律合规与儿童发展需求，是保障儿童AI应用隐私安全与合法性的可行且必要路径。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [308] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本文提出了一种面向ARM Cortex（M0+、M4、M7）处理器的AI模型能效优化基准测试框架，通过自动化测试平台系统评估能效、精度与资源占用，并利用FLOPs与推理时间的近线性关系及Pareto分析指导处理器与模型的协同选择。


<details>
  <summary>Details</summary>
Motivation: 针对嵌入式AI系统在能效、精度和资源利用间的多目标权衡难题，缺乏面向ARM Cortex系列处理器的系统化基准测试方法。

Method: 设计自动化测试平台，量化评估多个KPI；分析FLOPs与推理时间的相关性；采用Pareto分析平衡能效与精度；对比M0+、M4、M7三类处理器在不同AI任务下的表现。

Result: 发现FLOPs与推理时间呈近线性关系；M7适合短时推理，M4在长时推理中能效更优，M0+适用于简单任务；Pareto分析可有效支持能效-精度协同优化。

Conclusion: 该框架为嵌入式AI开发者提供了实用的处理器与模型选型指南，助力构建高性能且可持续的边缘AI系统。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [309] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: 本文提出KG-RAG框架，结合知识图谱与检索增强生成，提升大语言模型在电信领域的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域因领域复杂性、标准更新快及专业术语多而表现不佳，易产生幻觉、准确性低。

Method: 构建电信知识图谱（KG）并融合检索增强生成（RAG），利用KG提供结构化领域知识，RAG动态检索支撑事实以约束模型输出。

Result: 在基准数据集上，KG-RAG相较纯LLM和标准RAG分别平均提升准确率21.6%和14.3%，显著降低幻觉，增强结果可解释性与合规性。

Conclusion: KG-RAG有效提升了LLM在电信场景下的事实准确性、可靠性与标准符合性，为垂直领域大模型应用提供了可行路径。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [310] [KLong: Training LLM Agent for Extremely Long-horizon Tasks](https://arxiv.org/abs/2602.17547)
*Yue Liu,Zhiyuan Hu,Flood Sung,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: 本文提出KLong，一种开源的大型语言模型代理，通过轨迹分割监督微调（SFT）冷启动并结合渐进式强化学习（RL）训练，以解决极长视野任务。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM代理在极长视野任务中能力不足的问题，提升其在复杂、多步推理任务中的表现。

Method: 1）采用轨迹分割SFT方法冷启动基础模型，保留早期上下文、渐进截断后期上下文并维持子轨迹重叠；2）构建Research-Factory自动化数据生成管道，从研究论文中提取高质量长视野轨迹；3）设计渐进式RL训练，分阶段延长超时限制。

Result: KLong（106B）在PaperBench上超越Kimi K2 Thinking（1T）11.28%，并在SWE-bench Verified和MLE-bench等编码基准上展现出泛化性能提升。

Conclusion: 轨迹分割SFT与渐进式RL相结合可有效增强LLM代理解决长视野任务的能力，KLong在多个基准上验证了该方法的有效性与可扩展性。

Abstract: This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.

</details>


### [311] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 本文提出了一种基于常微分方程（ODE）的激活引导理论框架，并据此设计了ODESteer方法，通过障碍函数实现多步自适应引导，在多个LLM对齐基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法缺乏统一理论基础，且依赖单步操作，难以建模复杂激活分布。

Method: 将激活加法解释为ODE的一阶近似，将引导方向设计转化为控制理论中障碍函数的设计；提出ODESteer，以正负激活的对数密度比构造障碍函数，并驱动多步自适应ODE引导。

Result: 在TruthfulQA、UltraFeedback和RealToxicityPrompts上分别提升5.7%、2.5%和2.4%，优于当前最优激活引导方法。

Conclusion: 本工作通过ODE统一了激活引导的理论基础，并通过ODESteer验证了其有效性，为LLM对齐提供了新范式。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [312] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 本文提出了一种结合Swin Transformer与CNN的联邦学习（FL）增强型混合模型，用于X光图像中的COVID-19和肺炎诊断，兼顾准确性、安全性与数据隐私保护。


<details>
  <summary>Details</summary>
Motivation: 医疗数据分散且敏感，传统集中式AI模型难以兼顾隐私保护与模型性能；联邦学习可实现分布式协作建模，提升诊断可靠性并保障数据安全。

Method: 构建融合Swin Transformer与多种CNN（DenseNet201、Inception V3、VGG19）的混合架构，并集成联邦学习框架，在多个本地医疗机构节点上协同训练，使用TensorFlow/Keras及微软Swin Transformer实现。

Result: 该混合FL模型在X光图像上实现了对COVID-19和肺炎的高精度检测与病情严重程度预测，支持实时持续学习，同时保障数据不出本地、模型安全可信。

Conclusion: 所提方法验证了联邦学习与先进视觉Transformer-CNN混合架构在肺部疾病远程智能诊断中的可行性与优势，为隐私保护下的医疗AI落地提供了有效范式。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [313] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 本文提出以'人类游戏多元宇宙'为基准评估AI的通用智能，构建了AI GameStore平台，利用大模型与人工协同生成100个真实人类游戏，并测试前沿视觉语言模型表现，发现其远逊于人类，尤其在世界建模、记忆与规划方面。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准过于狭窄、静态，难以全面评估类人通用智能；需一种动态、开放、覆盖人类认知广度的新型评测范式。

Method: 定义'人类游戏'概念，提出'人类游戏多元宇宙'评测框架；构建AI GameStore平台，结合LLM与人类参与，自动合成并容器化来自App Store和Steam的真实数字游戏；对7个前沿VLM进行短时游戏测试。

Result: 最佳VLM在多数游戏中得分不足人类平均分的10%；模型在需世界建模、记忆与规划的游戏上表现尤为薄弱。

Conclusion: AI GameStore是迈向衡量与推动类人通用智能的可行路径，后续需扩展游戏规模、深化评估维度并增强人机协同机制。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [314] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: 本文提出了MolHIT，一种基于分层离散扩散模型的分子图生成框架，通过引入化学先验和解耦原子编码，显著提升了分子生成的化学有效性与性质导向性能，在MOSES数据集上达到近100%有效性，并在多属性引导生成和骨架扩展等下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在分子生成中化学有效性低、难以满足目标性质要求，相比1D建模存在明显性能瓶颈。

Method: 提出MolHIT框架，基于分层离散扩散模型（HDDM），将离散扩散推广至编码化学先验的额外类别，并采用解耦原子编码，按化学角色分离原子类型。

Result: 在MOSES数据集上实现近100%化学有效性，首次在图扩散模型中达成该指标，全面超越强1D基线；并在多属性引导生成和骨架扩展等下游任务中验证了其有效性。

Conclusion: MolHIT有效克服了现有图扩散模型的关键局限，为AI驱动的药物发现和材料科学提供了更可靠、可控的分子生成新范式。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [315] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: 本文提出AutoNumerics多智能体框架，能从自然语言描述中自动设计、实现、调试和验证偏微分方程（PDE）数值求解器，兼顾准确性、可解释性与自动化。


<details>
  <summary>Details</summary>
Motivation: 传统PDE数值求解器设计依赖专家经验与手动调参；现有神经网络方法虽灵活但计算开销大、可解释性差。

Method: 提出AutoNumerics多智能体框架，采用粗到细执行策略和基于残差的自验证机制，生成基于经典数值分析的透明求解器。

Result: 在24个典型及真实PDE问题上，AutoNumerics精度媲美或优于现有神经网络与大模型基线，并能依据PDE结构特性正确选择数值格式。

Conclusion: AutoNumerics为自动化PDE求解提供了一种兼具准确性、可解释性与易用性的新范式。

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>
