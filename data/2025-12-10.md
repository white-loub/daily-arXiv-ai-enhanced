<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 111]
- [cs.CL](#cs.CL) [Total: 19]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 33]
- [cs.LG](#cs.LG) [Total: 88]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Detection of Cyberbullying in GIF using AI](https://arxiv.org/abs/2512.07838)
*Pal Dave,Xiaohong Yuan,Madhuri Siddula,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文提出了一种基于VGG16深度学习模型的GIF形式网络欺凌检测方法，并构建了一个包含4100多个GIF的公开数据集，准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于文本和图像的网络欺凌检测，针对GIF/贴纸类内容的研究极少，而GIF在社交平台中广泛用于欺凌行为，亟需专门检测方法。

Method: 从Twitter提取网络欺凌相关标签，调用GIPHY公开API下载GIF，构建含4100+样本（含欺凌与非欺凌）的数据集；采用预训练VGG16模型进行二分类检测。

Result: 模型检测准确率达到97%，并开源了首个面向网络欺凌检测的GIF数据集。

Conclusion: 该工作填补了GIF模态网络欺凌检测的研究空白，验证了深度学习在该任务上的有效性，并为后续研究提供了关键数据资源。

Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.

</details>


### [2] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

TL;DR: 本文利用Planet Labs的4波段卫星影像和深度学习模型，实现了对苏丹武装冲突中火灾损害的近实时监测，并通过五个案例验证了该方法在识别活跃火点和烧毁区域方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 苏丹持续战争带来的挑战凸显了快速监测和分析此类冲突的必要性。

Method: 使用Planet Labs提供的4波段卫星遥感影像，结合深度学习模型进行火灾损害监测。

Result: 在五个苏丹案例研究中，该自动化方法比基线方法更准确地捕捉到活跃火点和烧毁区域；而使用8波段影像或其时间序列仅带来边际提升。

Conclusion: 基于4波段卫星影像的深度学习方法可有效实现武装冲突中火灾损害的近实时监测，且扩展至更多波段或时间序列收益有限。

Abstract: The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [3] [Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality](https://arxiv.org/abs/2512.07951)
*Zekai Luo,Zongze Du,Zhouhang Zhu,Hao Zhong,Muzhi Zhu,Wen Wang,Yuling Xi,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 本文提出了LivingSwap，首个视频参考引导的人脸交换模型，通过关键帧条件控制和视频参考引导实现高保真度和时间一致性的人脸交换。


<details>
  <summary>Details</summary>
Motivation: 现有视频人脸交换方法在长而复杂的视频序列中难以同时保证高保真度和时间一致性，本文受参考引导图像编辑启发，探索利用源视频丰富视觉特征提升视频人脸交换效果。

Method: 提出LivingSwap模型，采用关键帧作为条件信号注入目标身份，并结合视频参考引导进行时间拼接，以确保长时间序列中的身份稳定性和高保真重建；构建了配对人脸交换数据集Face2Face，并反向数据对以提供可靠监督。

Result: 实验表明该方法达到当前最优性能，能无缝融合目标身份与源视频的表情、光照和运动，显著减少制作流程中的人工干预。

Conclusion: LivingSwap首次实现了视频参考引导的人脸交换，在保真度、时间一致性和可控性方面取得突破，为影视娱乐制作提供了高效新工具。

Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap

</details>


### [4] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

TL;DR: 本文提出了Terrain Diffusion，一种基于扩散模型的新型程序化地形生成方法，兼具高保真度与传统噪声函数（如Perlin噪声）的无限性、种子一致性及常数时间随机访问等关键特性。


<details>
  <summary>Details</summary>
Motivation: 传统程序化噪声（如Perlin噪声）虽高效且无限，但在真实感和大尺度一致性上存在根本局限；AI生成方法又往往缺乏无缝无限扩展与可控性。本文旨在弥合这一鸿沟。

Method: 提出InfiniteDiffusion算法实现无限无缝生成；采用分层扩散模型耦合全球上下文与局部细节；引入紧凑Laplacian编码稳定超大动态范围输出；开发开源无限张量框架支持常数内存操作；结合少步一致性蒸馏提升效率。

Result: 实现了可扩展至地球尺度、种子一致、实时、常数时间随机访问的高保真程序化地形生成，支持全行星级连贯可控合成。

Conclusion: 扩散模型经系统性工程重构后，可成为新一代实用化程序化世界生成的基础范式，超越传统噪声函数的表达能力与灵活性。

Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [5] [Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection](https://arxiv.org/abs/2512.07984)
*Ryan Banks,Camila Lindoni Azevedo,Hongying Tang,Yunpeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种将解剖层级结构显式嵌入语义分割模型的新框架，通过递归逐层预测、限制性输出头和自顶向下特征调制，提升牙科影像中细粒度解剖结构（如牙层、牙槽骨）的分割精度与临床合理性。


<details>
  <summary>Details</summary>
Motivation: 现有层级感知分割方法主要依赖损失函数隐式编码解剖结构，监督弱且间接；而精准解剖理解对牙科疾病分期至关重要，亟需显式建模层级关系。

Method: 提出一种通用层级分割框架：1）递归逐层预测（每层重运行骨干网络，输入含上层logits）；2）子类特征通过父类概率进行特征级线性调制（FiLM）；3）概率组合规则保证父子类预测一致性；4）分层损失函数融合逐层加权Dice/交叉熵与一致性约束项。

Result: 在自建TL-pano数据集（194张全景片，含密集实例与语义标注）上验证，以UNet和HRNet为基线，5折交叉验证显示层级变体显著提升IoU、Dice和召回率（尤其细粒度结构），生成更解剖一致的掩码；但召回率提升幅度大于精度，假阳性增多。

Conclusion: 显式层级建模不仅提升分割性能，更增强临床可解释性与合理性，尤其适用于牙科小样本影像场景。

Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.

</details>


### [6] [Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform](https://arxiv.org/abs/2512.08478)
*Yuning Gong,Yifei Liu,Yifan Zhan,Muyao Niu,Xueying Li,Yuanjun Liao,Jiaming Chen,Yuanyuan Gao,Jiaqi Chen,Minming Chen,Li Zhou,Yuning Zhang,Wei Wang,Xiaoqing Hou,Huaxi Huang,Shixiang Tang,Le Ma,Dingwen Zhang,Xue Yang,Junchi Yan,Yanchi Zhang,Yinqiang Zheng,Xiao Sun,Zhihang Zhong*

Main category: cs.CV

TL;DR: 本文提出了Visionary，一个基于WebGPU和ONNX的开源、浏览器原生实时渲染平台，支持多种高斯泼溅（3DGS）变体及生成式后处理，实现轻量级、可插拔、端到端的神经渲染与推理一体化。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅查看器碎片化、重量级、依赖旧有管线，部署困难，且不支持动态内容与生成模型。

Method: 构建基于WebGPU的高效渲染器，集成每帧ONNX推理；定义标准化Gaussian Generator接口；提供兼容three.js的TypeScript插件库。

Result: 在相同3DGS资产下，Visionary因GPU原生图元排序而渲染效率优于现有Web查看器；已支持MLP-3DGS、4DGS、神经头像、风格迁移等变体。

Conclusion: Visionary统一了浏览器内推理与渲染，显著降低3DGS类方法的复现、对比与部署门槛，成为重建与生成范式共用的世界模型载体。

Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.

</details>


### [7] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: 本文介绍了FRIEDA基准，用于评估大视觉语言模型（LVLMs）在复杂开放性地图推理任务中的能力，涵盖拓扑、度量和方向三类空间关系，实验表明现有最先进模型表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 地图视觉问答（Map VQA）需要理解多层符号系统及与方位、距离相关的空间关系，而现有研究常将其简单视为图表理解的特例，缺乏专门针对地图推理能力的系统性评测基准。

Method: 构建FRIEDA基准：基于真实文档与报告中的地图图像，依据GIS文献分类覆盖拓扑、度量、方向三类空间关系；设计需多步推理及跨图定位的问题；在直接设置（提供相关地图）和上下文设置（需自主识别相关地图）下评估11个SOTA LVLM。

Result: 最强模型Gemini-2.5-Pro和GPT-5-Think在FRIEDA上准确率仅为38.20%和37.20%，显著低于人类的84.87%，揭示了LVLM在多步制图推理上的严重不足。

Conclusion: FRIEDA填补了地图推理评测的空白，凸显当前LVLM空间智能的短板，为推动其在灾害响应、城市规划等关键场景中的应用提供了严谨的评估标准和发展方向。

Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [8] [Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment](https://arxiv.org/abs/2512.08930)
*Youming Deng,Songyou Peng,Junyi Zhang,Kathryn Heal,Tiancheng Sun,John Flynn,Steve Marschner,Lucy Chai*

Main category: cs.CV

TL;DR: 本文提出Selfi方法，通过特征对齐提升VGGT等隐式3D模型的多视图几何一致性，从而在新视角合成与相机姿态估计任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有隐式3D模型（如VGGT）缺乏显式的多视图几何一致性，限制了其在新视角合成和姿态估计中的性能。

Method: 提出Selfi自改进3D重建流程：利用VGGT自身输出作为伪真值，训练轻量级特征适配器，并采用基于重投影的一致性损失，将VGGT特征蒸馏为几何对齐的新特征空间。

Result: 在新视角合成与相机姿态估计两个任务上均达到当前最优性能（SOTA）。

Conclusion: 特征对齐是提升下游3D推理能力的关键步骤，能有效增强隐式3D模型的几何一致性与重建保真度。

Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.

</details>


### [9] [SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification](https://arxiv.org/abs/2512.08038)
*Elifnur Sunger,Tales Imbiriba,Peter Campbell,Deniz Erdogmus,Stratis Ioannidis,Jennifer Dy*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSplain的新型解释方法，用于在视网膜病变（ROP）图像分类中生成既稀疏又平滑的像素级解释，以提升黑盒模型的可解释性与临床可信度。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法难以在生成解释时保持输入图像的结构特性（如平滑性和稀疏性），限制了其在临床场景中的可理解性与实用性。

Method: 提出Sparse and Smooth Explainer（SSplain），通过引入组合约束的优化问题，并采用交替方向乘子法（ADMM）求解，生成兼顾平滑性与稀疏性的像素级解释。

Result: SSplain在后验准确性和平滑性评估上均优于主流解释器；识别出的显著区域与临床医生公认的ROP判别特征一致；并在多个公开数据集上验证了泛化能力。

Conclusion: SSplain是一种有效、可解释且具临床相关性的解释方法，为医学图像AI模型的可信部署提供了新工具。

Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.

</details>


### [10] [Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment](https://arxiv.org/abs/2512.08040)
*Youngjoon Jang,Liliane Momeni,Zifan Jiang,Joon Son Chung,Gül Varol,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了一种统一模型，同时完成手语翻译（SLT）和手语字幕对齐（SSA），通过轻量视觉主干、滑动Perceiver映射网络和多任务训练策略，在BSL和ASL数据集上达到SOTA，并展现跨语言零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为实现连续手语视频到口语文本的转换及手语与字幕的时间对齐，以支持实际交流、大规模语料构建和教育应用，需统一建模SLT与SSA两个任务。

Method: 采用三部分设计：(i) 轻量级视觉主干，从人体关键点和唇部图像中提取手势与非手势线索并保护签署者隐私；(ii) 滑动Perceiver映射网络，将连续视觉特征聚合为词级嵌入以弥合视觉-文本鸿沟；(iii) 多任务可扩展训练策略，联合优化SLT与SSA，增强语言与时间对齐能力；并在BOBSL和YouTube-SL-25的大规模多语言手语-文本语料上预训练。

Result: 在BOBSL（BSL）数据集上SLT与SSA均达SOTA；在How2Sign（ASL）上展现出强零样本泛化能力和微调后优异的SLT性能。

Conclusion: 该统一模型兼顾性能、隐私与跨语言可扩展性，为手语理解提供了实用且可推广的技术路径。

Abstract: Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.

</details>


### [11] [Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking](https://arxiv.org/abs/2512.08042)
*Chandler Timm C. Doloriel,Habib Ullah,Kristian Hovde Liland,Fadi Al Machot,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文提出了一种基于频域掩码的通用深度伪造检测方法，通过在训练中引入随机频率掩码和几何变换，提升模型对未见过生成模型的泛化能力，同时支持模型剪枝以降低计算开销，实现了绿色AI目标下的高效、鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 通用深度伪造检测需应对不断涌现的新型生成模型（包括未见过的），同时兼顾计算效率以支持大规模筛查，契合绿色AI需求。

Method: 采用频域掩码作为训练策略，结合随机掩码与几何变换，重点利用频率域操作增强泛化性，并验证其在模型剪枝下的稳定性。

Result: 在GAN和扩散模型生成图像数据集上达到最优泛化性能，且在结构化剪枝下保持鲁棒性，显著降低计算开销。

Conclusion: 频域掩码是一种可行且有效的策略，有助于构建可持续、高泛化能力的深度伪造检测系统。

Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).

</details>


### [12] [Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning](https://arxiv.org/abs/2512.08048)
*Chandler Timm C. Doloriel*

Main category: cs.CV

TL;DR: 本文提出了一种简单有效的持续测试时自适应（CTTA）方法Mask to Adapt（M2A），通过随机空间或频率掩码生成多视图，并结合掩码一致性损失和熵最小化损失进行自适应，无需依赖校准的不确定性估计或注意力机制，在多个基准上达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖复杂的掩码设计（如校准的不确定性或稳定注意力得分），本文探究是否简单的随机掩码策略在强干扰下已足够有效。

Method: 提出M2A方法：生成短序列的空间或频率掩码视图，联合优化掩码一致性损失（对齐不同视图预测）与熵最小化损失（提升预测置信度）；对比了空间掩码（patch vs. pixel）和频率掩码（all/low/high）子类型。

Result: 在CIFAR10C/CIFAR100C/ImageNetC（severity~5）上，M2A（Spatial）平均错误率分别为8.3%/19.8%/39.2%，优于或持平强基线；M2A（Frequency）表现较差；消融实验证明随机掩码有效且鲁棒。

Conclusion: 简单的随机掩码策略，配合一致性与熵最小化目标，足以实现高效测试时自适应，无需复杂不确定性或注意力信号。

Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.

</details>


### [13] [Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models](https://arxiv.org/abs/2512.08075)
*Christian Massao Konishi,Helio Pedrini*

Main category: cs.CV

TL;DR: 本文评估了多种变化检测模型在亚马逊雨林毁林监测中的性能，引入了基于Transformer的自注意力机制，并通过预处理、后处理及模型集成策略显著提升了检测效果，最终达到80.41%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有基于PRODES数据的机器学习毁林检测方法存在有效性不足、缺乏现代网络架构（如Transformer）支持、以及方法不统一难以比较等问题。

Method: 在统一数据集上评估多种变化检测模型（包括全卷积网络和基于Transformer的自注意力网络），系统测试不同预/后处理技术（如连通域滤波、纹理替换、图像增强）及模型集成策略。

Result: 通过后处理与集成方法，模型F1-score达80.41%，性能媲美近期文献工作。

Conclusion: 引入Transformer结构、标准化评估框架及有效后处理与集成策略，可显著提升毁林检测精度，为遥感变化检测提供可复现、可比较的方法论参考。

Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.

</details>


### [14] [CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning](https://arxiv.org/abs/2512.08135)
*Zeyuan Chen,Xiang Zhang,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文提出了一种受人类中央-周边视觉启发的多模态空间推理框架CVP，通过引入目标亲和性token（类中央视觉）和以自我为中心的网格（类周边视觉）来增强3D场景理解能力，并在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖非结构化表示（如点云、体素、图像块特征）并隐式注入场景上下文，导致空间推理能力受限，缺乏显式的高层结构理解。

Method: 在大语言多模态模型架构中引入两个互补组件：目标亲和性token（模拟中央视觉，聚焦查询相关物体）和以自我为中心的网格（模拟周边视觉，捕获全局场景上下文与空间布局）。

Result: CVP在多个3D场景理解基准测试中实现了最先进的性能。

Conclusion: CVP通过模仿人类视觉系统的双通道机制，有效提升了模型对复杂3D环境的结构化、上下文感知理解能力，验证了显式空间建模的有效性。

Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.

</details>


### [15] [Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing](https://arxiv.org/abs/2512.08161)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于多状态感知范式的图像去雾新框架Fourier-RWKV，通过空间域、频域和语义关系三重感知机制，在保持全局建模能力的同时实现线性计算复杂度，显著提升真实非均匀雾场景下的去雾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法虽能建模全局上下文，但其二次计算复杂度难以满足实时需求；同时真实世界中非均匀雾条件下的去雾仍极具挑战。

Method: 提出Fourier-RWKV框架，融合三种感知状态：(1) 空间形态感知（DQ-Shift操作动态调整感受野）；(2) 频域感知（Fourier Mix模块将RWKV的WKV注意力扩展至傅里叶域）；(3) 语义关系感知（Semantic Bridge Module结合DSK-Fusion对齐特征并抑制伪影）。

Result: 在多个基准上达到SOTA性能，兼顾高质量复原与低计算开销，验证了线性复杂度下有效建模复杂退化的能力。

Conclusion: Fourier-RWKV为实时、鲁棒的非均匀图像去雾提供了高效且高性能的新范式，推动了Transformer架构在资源受限视觉任务中的实用化进展。

Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.

</details>


### [16] [Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators](https://arxiv.org/abs/2512.08163)
*Yuki Kubota,Taiki Fukiage*

Main category: cs.CV

TL;DR: 本文系统研究了单目深度估计模型的准确性与人类感知相似性之间的关系，发现二者存在权衡关系，提升准确性并不必然提升人类相似性，强调需发展以人为中心的多维度评估方法。


<details>
  <summary>Details</summary>
Motivation: 对齐模型表征与人类感知以提升模型鲁棒性和可解释性；探究深度估计中是否也存在类似物体识别中的准确率与人类行为相似性之间的权衡。

Method: 基于KITTI数据集，分析69种单目深度估计算法；采用仿射拟合分解预测误差，逐因子解析误差模式。

Result: 人类与DNN在某些估计偏差上具正相关性，但模型准确率与人类相似性之间存在明显权衡关系，即高准确率不意味着高人类相似性。

Conclusion: 传统基于传感器真值的精度评估不足以反映人类感知特性，亟需构建多维度、以人为中心的新型评估体系。

Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.

</details>


### [17] [GeoLoom: High-quality Geometric Diagram Generation from Textual Input](https://arxiv.org/abs/2512.08180)
*Xiaojing Wei,Ting Zhang,Wei He,Jingdong Wang,Hua Huang*

Main category: cs.CV

TL;DR: 本文提出了GeoLoom框架，用于从自然语言生成高精度几何图形，结合形式化语言GeoLingua与蒙特卡洛坐标求解器，并构建了配套数据集GeoNF和约束评估指标，显著提升了结构保真度。


<details>
  <summary>Details</summary>
Motivation: 高质量几何图生成需严格空间精度，而几何问题求解中形式语言与符号求解器的成功启发了本工作探索形式化方法提升生成的正确性与可解释性。

Method: 提出GeoLoom框架，含两部分：1）自动形式化模块，将自然语言转为专用于生成的形式语言GeoLingua；2）基于蒙特卡洛优化的坐标求解器，将形式约束映射为精确坐标；同时构建GeoNF数据集及约束驱动的评估指标。

Result: 实验表明GeoLoom在结构保真度上显著优于现有最先进方法。

Conclusion: GeoLoom为可解释、可扩展的几何图生成提供了原理性基础，验证了形式化建模与数值优化协同的有效性。

Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.

</details>


### [18] [Animal Re-Identification on Microcontrollers](https://arxiv.org/abs/2512.08198)
*Yubo Chen,Di Zhao,Yun Sing Koh,Talia Xu*

Main category: cs.CV

TL;DR: 本文提出了一种适用于微控制器（MCU）等低功耗边缘设备的轻量级动物重识别（Animal Re-ID）框架，通过定制化CNN架构、低分辨率适配与数据高效微调策略，在显著压缩模型体积（缩小两个数量级以上）的同时保持高检索精度，并在真实牛群数据集上实现全端侧部署。


<details>
  <summary>Details</summary>
Motivation: 现有动物重识别模型多面向服务器设计，难以部署于内存小、输入分辨率低的野外边缘设备（如项圈标签或MCU），制约了野生动物监测与精准畜牧管理的实际应用。

Method: 1）量化分析SOTA模型与MCU硬件间的性能鸿沟；2）基于MobileNetV2系统缩放设计适配低分辨率输入的紧凑CNN架构；3）提出仅需每类3张图像的数据高效现场微调策略。

Result: 在6个公开Animal Re-ID数据集上达到有竞争力的检索精度，模型体积缩小超100倍；在自建牛群数据集上实现全端侧推理，Top-1精度无损，整体精度仅有小幅下降。

Conclusion: 证明了高精度、可适配的Animal Re-ID可在MCU级设备上实用化部署，为大规模野外应用铺平道路。

Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.

</details>


### [19] [Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement](https://arxiv.org/abs/2512.08215)
*Chia-Hern Lai,I-Hsuan Lo,Yen-Ku Yeh,Thanh-Nguyen Truong,Ching-Chun Huang*

Main category: cs.CV

TL;DR: Blur2Sharp 是一种结合 3D 感知神经渲染与扩散模型的新框架，仅需单张参考图像即可生成几何一致、细节锐利的多视角人体图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾多视角几何一致性与高保真度，常导致模糊或失真结果，尤其在复杂姿态、宽松衣物和遮挡下表现不佳。

Method: 提出双条件架构：先用 Human NeRF 生成几何一致的多视角粗渲染；再以该渲染为条件驱动扩散模型进行细节增强，并引入基于 SMPL 的纹理、法线与语义先验进行分层特征融合。

Result: 在新姿态与新视角生成任务上显著超越现有最先进方法，尤其在宽松衣物和遮挡等挑战性场景中表现突出。

Conclusion: Blur2Sharp 有效弥合了 3D 几何一致性与 2D 图像逼真度之间的鸿沟，为单图驱动的高质量人体图像合成提供了新范式。

Abstract: The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.

</details>


### [20] [VisKnow: Constructing Visual Knowledge Base for Object Understanding](https://arxiv.org/abs/2512.08221)
*Ziwei Yao,Qiyang Wan,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为VisKnow的框架，用于构建结构化的视觉知识库（Visual Knowledge Base），以支持深层次的对象理解。该框架整合了图文对齐知识与区域标注，并以AnimalKB为例验证了其在零样本识别、细粒度视觉问答等任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据多为任务导向且缺乏系统性组织，难以支撑对物体类别的深入理解，因此需要构建一个能整合多模态信息的结构化知识库。

Method: 提出VisKnow框架，通过专家设计与大模型结合的方式，从图文数据中提取对象级多模态知识，并构建图结构的视觉知识库；以动物领域为例构建AnimalKB，包含文本三元组、图像及区域标注。

Result: AnimalKB包含406个动物类别、22K文本三元组、420K图像及对应区域标注；实验表明其可提升零样本识别、细粒度VQA等任务性能，并为知识图谱补全和部件分割提供新基准。

Conclusion: 自动构建视觉知识库是推动视觉理解发展的重要方向，VisKnow框架及其实例AnimalKB展示了该方法的有效性与应用潜力。

Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

</details>


### [21] [SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection](https://arxiv.org/abs/2512.08223)
*Ching-Hung Cheng,Hsiu-Fu Wu,Bing-Chen Wu,Khanh-Phong Bui,Van-Tin Luu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文探讨了在3D目标检测中应用提示调优（prompt tuning）方法的有效性，基于Waymo大规模数据集训练的基础模型，提出了一种面向场景的提示池（SOP²），验证了提示池在3D检测任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索常见提示调优方法在3D目标检测中的有效性，并验证基于Waymo数据集训练的大模型能否作为基础模型适配其他3D检测场景。

Method: 依次研究提示词（prompt tokens）和提示生成器（prompt generators）的影响，并提出场景导向的提示池（SOP²）。

Result: 验证了提示池在3D目标检测中的有效性，为未来在3D领域深入探索提示机制提供了启发。

Conclusion: 提示调优方法可有效迁移到3D目标检测任务中，SOP²结构提升了模型对不同场景的适应能力，表明提示工程在三维视觉领域具有潜力。

Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.

</details>


### [22] [New VVC profiles targeting Feature Coding for Machines](https://arxiv.org/abs/2512.08227)
*Md Eimran Hossain Eimon,Ashan Perera,Juan Merlos,Velibor Adzic,Hari Kalva*

Main category: cs.CV

TL;DR: 本文研究了在分割推理系统中使用VVC压缩神经网络中间特征的效果，并提出了三种轻量级VVC配置文件，以在压缩效率和编码速度之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现代视频编解码器针对人类视觉系统优化，但在分割推理系统中，传输的是抽象、稀疏且任务特定的中间特征，感知保真度不再适用，因此需要新的压缩方法。

Method: 对VVC编码工具进行逐级分析，评估各组件对压缩效率和下游视觉任务精度的影响，并基于分析结果设计三种轻量级VVC配置文件（Fast、Faster、Fastest）。

Result: Fast配置获得2.96% BD-Rate增益并减少21.8%编码时间；Faster获得1.85% BD-Rate增益并提速51.5%；Fastest编码时间减少95.6%，仅损失1.71% BD-Rate。

Conclusion: 针对中间特征压缩，精简VVC工具集可在显著提升编码速度的同时保持较高压缩效率，适用于MPEG-AI FCM标准下的机器视觉任务。

Abstract: Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.

</details>


### [23] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: 本文提出了MM-CoT基准，用于评估多模态模型在视觉证据支持和逻辑一致性两方面的链式推理能力，发现当前先进模型在生成流畅性与真实推理保真度之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准侧重于链式推理的生成能力，却忽视了对推理过程是否基于视觉证据且逻辑自洽的验证能力，因此需要一个专门诊断多模态模型链式推理质量的新基准。

Method: 构建MM-CoT诊断基准，要求模型从多个事件链中选择唯一同时满足视觉一致性（每步均有可观测依据）和逻辑连贯性（因果与常识合理）的选项；引入针对性对抗干扰项以暴露不同类型的推理失败。

Result: 主流视觉语言模型在MM-CoT上表现不佳，生成流畅但推理保真度低；MM-CoT与现有基准相关性低，证实其测量的是视觉 grounding 与逻辑推理的独特组合能力。

Conclusion: MM-CoT为推动真正忠实、连贯地立足于视觉世界的多模态推理模型发展提供了新基础和评估标准。

Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [24] [Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems](https://arxiv.org/abs/2512.08229)
*Tony Salloom,Dandi Zhou,Xinhai Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于法向量引导的稀疏深度采样策略，以更真实地模拟实际传感器的几何依赖性与空间非均匀可靠性，并将其集成到Marigold-DC扩散模型中，显著提升了深度补全的精度与边缘质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度补全方法在生成稀疏深度输入时通常采用均匀随机采样，忽略了真实传感器因几何结构和环境导致的非均匀可靠性，造成训练与实际部署间的不匹配。

Method: 提出基于PCA估计RGB-D点云表面法向量的像素级深度可靠性度量，并据此进行非均匀稀疏深度采样；将该采样策略嵌入扩散模型Marigold-DC，在NYU Depth v2上进行验证。

Result: 相比均匀采样，所提方法在标准指标下提升深度补全精度，减少边缘与深度不连续处的伪影，训练数据更贴近真实传感器行为。

Conclusion: 几何感知的稀疏深度采样能有效提升深度补全模型的泛化性与实用性，为构建更鲁棒的工业3D感知系统提供了新思路。

Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.

</details>


### [25] [FastBEV++: Fast by Algorithm, Deployable by Design](https://arxiv.org/abs/2512.08237)
*Yuanpeng Chen,Hui Song,Wei Tao,ShanHui Mo,Shuang Zhang,Xiao Hua,TianKun Zhao*

Main category: cs.CV

TL;DR: 本文提出FastBEV++框架，通过算法高效性与部署友好性双原则，解决纯视觉BEV感知中性能与部署之间的矛盾；采用可分解的Index-Gather-Reshape视图变换替代定制CUDA核，实现TensorRT原生兼容，并引入端到端深度感知融合、时序聚合与强数据增强，显著提升BEV几何精度；在nuScenes上达到0.359 NDS新SOTA，同时在Tesla T4上超134 FPS实时运行。


<details>
  <summary>Details</summary>
Motivation: 当前纯视觉BEV感知受限于高性能模型依赖计算昂贵的视图变换和平台定制内核，难以兼顾精度与车载部署可行性。

Method: 提出FastBEV++框架，遵循'Fast by Algorithm'和'Deployable by Design'两大原则：前者通过分解视图变换为标准Index-Gather-Reshape流水线（结合预排序），仅使用原生算子（如Gather、矩阵乘），消除定制CUDA核，确保TensorRT全兼容；后者在此结构上集成端到端深度感知融合、时序聚合与鲁棒数据增强。

Result: 在nuScenes基准上达到0.359 NDS（新SOTA），并在Tesla T4等车规级硬件上实现>134 FPS实时推理。

Conclusion: FastBEV++证明高精度与高部署效率可协同实现，提供无需插件、高精度、可扩展的BEV感知方案，适用于量产自动驾驶系统。

Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

</details>


### [26] [HybridToken-VLM: Hybrid Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.08240)
*Jusheng Zhang,Xiaoyang Guo,Kaitong Cai,Qinhan Lv,Yijia Fan,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: HTC-VLM提出双通道机制（连续细粒度+离散符号锚点），通过解耦语义与外观，在580:1高压缩比下实现87.2%平均性能保留，优于连续基线（81.0%），缓解VLM的效率-保真度困境。


<details>
  <summary>Details</summary>
Motivation: 传统VLM压缩方法面临权衡：连续压缩稀释高层语义（如物体身份），离散量化丢失细节（如纹理）；且高视觉token数导致LLM计算成本呈二次增长，受限于内存与上下文窗口。

Method: 提出HTC-VLM混合框架：1）连续通路保留ViT patch细节；2）离散通路用MGVQ量化生成4个符号锚点；3）融合为580-token序列，再经解耦注意力掩码与瓶颈压缩为单个voco token。

Result: 在GQA、VQAv2等7个基准上平均性能保留率达87.2%，显著优于连续基线81.0%；压缩比达580:1；注意力分析证实压缩token优先关注离散锚点，验证其语义引导作用。

Conclusion:  minimalist混合设计可有效解决VLM中效率与保真度的固有矛盾，为可扩展视觉语言模型提供新范式。

Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

</details>


### [27] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新型深度混合网络Residual-SwinCA-Net，结合残差CNN与定制化Swin Transformer，并引入多种注意力机制和图像增强算子，显著提升了乳腺病变超声图像的分割精度。


<details>
  <summary>Details</summary>
Motivation: 解决乳腺超声图像中组织连续性差、噪声干扰强、病灶边界模糊及结构变异鲁棒性不足等挑战。

Method: 构建Residual-SwinCA-Net：融合残差CNN提取局部鲁棒特征；定制含内部残差路径的Swin Transformer建模全局依赖；引入LoG区域算子抑制噪声并增强结构过渡；加入边界导向算子保持恶性病灶形态完整性；采用逐级特征图压缩策略提升尺度不变性；在解码器各层集成多尺度通道注意力与压缩（MSCAS）模块；最后使用像素注意力模块自适应加权病灶区域。

Result: 在BUSI公开数据集上达到99.29%平均准确率、98.74% IoU和0.9041 Dice系数，性能优于现有CNN/ViT方法。

Conclusion: Residual-SwinCA-Net有效提升了乳腺病变分割精度与临床诊断及时性，为超声辅助诊断提供了可靠技术支撑。

Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [28] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Future Temporal Knowledge Distillation (FTKD) 的稀疏查询式知识蒸馏方法，用于将离线教师模型中对未来帧的时序知识有效迁移至在线学生模型，提升相机驱动的3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法忽视未来帧信息，难以让在线模型有效学习离线模型所利用的未来帧知识。

Method: 提出FTKD框架，包括未来感知特征重建策略（避免严格帧对齐）和未来引导的logit蒸馏，以传递未来帧的特征与前景/背景上下文信息。

Result: 在nuScenes数据集上，FTKD为两个先进3D检测基线带来最高1.3 mAP和1.3 NDS提升，并实现最准确的速度估计，且不增加推理开销。

Conclusion: FTKD是一种高效、无额外计算成本的时序知识蒸馏方法，显著提升了在线相机3D检测模型的性能，尤其在利用未来帧信息方面具有开创性。

Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [29] [Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2512.08253)
*YiLin Zhou,Lili Wei,Zheming Xu,Ziyi Chen,Congyan Lang*

Main category: cs.CV

TL;DR: 本文提出了一种查询感知的中心原型（QHP）学习方法，用于少样本3D点云语义分割，通过构建支持集与查询集之间的二分图并优化原型分布，缓解原型偏差问题，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于度量学习的原型方法仅从支持集生成原型，忽视其与查询数据的相关性，导致原型偏差，尤其在分布偏移下泛化能力差。

Method: 提出Query-aware Hub Prototype（QHP）方法，包括Hub Prototype Generation（HPG）模块（构建支持-查询二分图、识别高频链接的支持中心点以生成查询相关原型）和Prototype Distribution Optimization（PDO）模块（采用纯度重加权对比损失优化原型分布）。

Result: 在S3DIS和ScanNet数据集上实验表明，QHP显著优于当前最优方法，有效缩小原型与查询集间的语义差距。

Conclusion: QHP通过显式建模支持集与查询集间的语义关联，并优化原型分布，有效缓解原型偏差，提升了少样本3D点云语义分割的鲁棒性与准确性。

Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.

</details>


### [30] [Beyond Real Weights: Hypercomplex Representations for Stable Quantization](https://arxiv.org/abs/2512.08524)
*Jawad Ibn Ahad,Maisha Rahman,Amrijit Biswas,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CV

TL;DR: 本文提出一种渐进式重参数化策略，通过逐步用紧凑的PHM层替代密集前馈网络块来压缩多模态语言模型（MLLMs），在大幅减少参数量和计算量的同时保持多模态对齐能力和输出质量。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型（MLLMs）因需对齐高维视觉特征与语言表征而参数量大、计算开销高，难以高效部署。

Method: 采用渐进式重参数化策略，逐步将密集前馈网络块替换为参数化超复数乘法（PHM）层，并结合残差插值调度、轻量重建损失和知识蒸馏损失，使PHM模块在训练中继承原密集模块的功能行为。

Result: 在多个视觉-语言模型（VLMs）上验证，该方法在保持与基线模型相当性能的同时，显著降低模型参数量、FLOPs和推理延迟。

Conclusion: 渐进式PHM替代为高效多模态推理提供了一种架构兼容的压缩路径，并可与低比特量化等现有技术互补。

Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

</details>


### [31] [SFP: Real-World Scene Recovery Using Spatial and Frequency Priors](https://arxiv.org/abs/2512.08254)
*Yun Liu,Tao Li,Cosmin Ancuti,Wenqi Ren,Weisi Lin*

Main category: cs.CV

TL;DR: 本文提出了一种结合空间域与频域先验（SFP）的场景恢复方法，通过空间先验估计透射图以应对散射退化，利用两个新颖频域先验自适应增强频谱，并加权融合多源信息，显著提升了真实场景下的泛化能力与恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅依赖单一先验，难以应对多种退化；或依赖复杂网络和合成数据训练，导致在真实场景中泛化性差。

Method: 提出空间与频率双域先验：空间域利用退化图像逆变换的谱方向投影估计透射图；频域构建自适应增强掩模，基于DC分量均值一致性和低径向频率能量占比两个新先验估计参数；最后加权融合空间恢复、频率增强及输入图像显著特征。

Result: 在多种退化条件下进行了广泛实验验证，结果表明SFP方法在真实场景恢复任务中具有优越性能和强泛化能力。

Conclusion: SFP通过协同建模空间与频域先验，无需复杂网络或大量合成数据，即可实现鲁棒、高效的现实场景恢复，为单图像退化校正提供了新思路。

Abstract: Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.

</details>


### [32] [Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture](https://arxiv.org/abs/2512.08738)
*Samuel Ebimobowei Johnny,Blessed Guda,Emmanuel Enejo Aaron,Assane Gueye*

Main category: cs.CV

TL;DR: 本文提出了一个名为‘手语识别（Sign Language Spotting）’的新任务，旨在从连续手语视频中检测特定手语是否存在；提出了一种基于姿态关键点的端到端二分类模型，避免了传统依赖词汇识别或文本匹配的方法，在WSLP 2025数据集上达到61.88%准确率和60.00% F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有自动手语识别（ASLR）研究多聚焦于整句识别或翻译，而如何在连续手语序列中精准定位/检索某个特定手势（即‘手语识别’）仍属空白，亟需探索。

Method: 提出一种端到端的编码器-only架构，直接以从手语视频中提取的姿态关键点序列为输入，通过二分类头判断查询手势是否出现在目标句子级手语视频或词表（gloss）中；摒弃原始RGB帧，仅依赖轻量、鲁棒的姿态表示。

Result: 在WSLP 2025共享任务的Word Presence Prediction数据集上，取得61.88%准确率和60.00% F1-score，验证了姿态驱动方法的有效性与高效性。

Conclusion: 该工作首次系统定义并解决了手语识别任务，证明了纯姿态特征足以支撑细粒度手语检索，为后续手语检索、验证及跨模态理解研究奠定了基础。

Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting

</details>


### [33] [RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera](https://arxiv.org/abs/2512.08262)
*Hafeez Husain Cholakkal,Stefano Arrigoni,Francesco Braghin*

Main category: cs.CV

TL;DR: 本文提出RLCNet，一种端到端可训练的深度学习框架，用于激光雷达、毫米波雷达和相机传感器的同步在线外参标定，具备实时性、鲁棒性和高精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中多传感器（LiDAR、RADAR、相机）的精确外参标定至关重要，但机械振动、累积漂移等动态环境因素使其极具挑战性。

Method: 提出RLCNet——一种端到端可训练的深度学习框架，并结合加权滑动平均与异常值剔除的在线校准机制，实现多模态传感器同步在线标定。

Result: 在真实数据集上验证有效，具备实时运行能力，标定精度与抗漂移鲁棒性优于现有方法。

Conclusion: RLCNet为动态环境下多传感器在线联合标定提供了实用、高效且鲁棒的解决方案，适合实际部署。

Abstract: Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.

</details>


### [34] [EgoX: Egocentric Video Generation from a Single Exocentric Video](https://arxiv.org/abs/2512.08269)
*Taewoong Kang,Kinam Kim,Dohyeon Kim,Minho Park,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: 本文提出EgoX框架，通过轻量级LoRA适配大规模视频扩散模型，并结合外源与内源先验及几何引导的自注意力机制，实现从单个第三人称视频生成连贯、真实的第一人称视频。


<details>
  <summary>Details</summary>
Motivation: 将第三人称视频转换为第一人称视频可提升沉浸式理解能力，但因相机姿态变化剧烈、视角重叠少而极具挑战性。

Method: 提出EgoX框架：1）基于预训练视频扩散模型，采用轻量LoRA微调；2）统一融合外源与内源先验（宽/通道维拼接）；3）引入几何引导的自注意力机制以保障几何一致性与视觉保真度。

Result: 在未见及野外视频上实现了连贯、高保真、可扩展且鲁棒的第一人称视频生成。

Conclusion: EgoX有效解决了从单帧第三人称视频生成几何一致、视觉真实的第一人称视频的关键难题，为具身感知与视频理解提供了新范式。

Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.

</details>


### [35] [PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282)
*Oh Hyun-Bin,Yuhta Takida,Toshimitsu Uesaka,Tae-Hyun Oh,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 本文提出了一种物理感知的视频到音频生成方法PAVAS，通过引入物理驱动的音频适配器（Phy-Adapter）和物理参数估计器（PPE），将物体质量与运动轨迹等物理信息融入扩散模型，提升生成音频的物理真实感，并构建新基准VGG-Impact与评估指标APCC进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频（V2A）生成模型多为外观驱动，忽略塑造真实声音的物理因素，缺乏物理合理性。

Method: 提出PAVAS框架，包含Physics-Driven Audio Adapter（Phy-Adapter）和Physical Parameter Estimator（PPE）；PPE利用视觉语言模型（VLM）估计物体质量，并结合分割与动态3D重建推断运动轨迹以计算速度；这些物理参数被注入潜空间扩散模型以指导音频合成。

Result: 在自建基准VGG-Impact和新指标APCC上，PAVAS在物理真实性和听觉一致性方面均优于现有V2A方法，定量与定性结果均有提升。

Conclusion: 将物理先验显式建模并融入V2A生成流程，可显著提升音频的物理合理性和感知质量，为多模态生成开辟了物理感知新方向。

Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.

</details>


### [36] [OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation](https://arxiv.org/abs/2512.08294)
*Yexin Liu,Manyuan Zhang,Yueze Wang,Hongyu Li,Dian Zheng,Weiming Zhang,Changsheng Lu,Xunliang Cai,Yan Feng,Peng Pei,Harry Yang*

Main category: cs.CV

TL;DR: 本文提出了OpenSubject数据集，一个基于视频的大规模语料库，用于主体驱动的图像生成和操作，通过四阶段流程构建，并引入了新的基准测试来评估性能。


<details>
  <summary>Details</summary>
Motivation: 当前主体驱动的图像生成模型在保持参考身份和处理多主体复杂场景时表现不佳。

Method: 构建了一个名为OpenSubject的视频衍生大规模数据集（2.5M样本，4.35M图像），采用四阶段流程：视频筛选、跨帧主体挖掘与配对、身份保持的参考图像合成（包括分割图引导外绘、框引导内绘等）、验证与标注；并设计了涵盖生成与操作任务的基准测试，使用视觉语言模型（VLM）进行多维度自动评估。

Result: 实验表明，使用OpenSubject训练显著提升了主体驱动图像生成与操作的性能，尤其在复杂多主体场景中效果更优。

Conclusion: OpenSubject为解决主体身份保真度和复杂场景建模问题提供了高质量数据支持与系统性评估框架，推动了主体驱动生成技术的发展。

Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

</details>


### [37] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种几何感知的数据集蒸馏框架GeoDM，通过在欧氏、双曲和球面流形的笛卡尔积空间中进行分布匹配，以更好地捕捉数据的内在几何结构（如曲率），从而提升蒸馏数据的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法局限于欧氏空间，无法建模真实数据的内在几何结构（如流形曲率），而高维数据常位于低维流形上，因此需对齐原始与蒸馏数据的流形结构。

Method: 提出GeoDM框架，在欧氏、双曲、球面流形的乘积空间中进行分布匹配；引入可学习的曲率与权重参数以自适应数据几何；设计最优传输损失提升分布保真度。

Result: 理论证明该几何感知匹配在乘积流形下具有更小的泛化误差界；实验表明GeoDM在多个基准上优于现有最先进蒸馏方法，并兼容单一流形策略。

Conclusion: 将数据分布匹配扩展到多流形乘积空间并实现几何自适应，能显著提升数据集蒸馏效果，为蒸馏方法提供了新的几何视角。

Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [38] [Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge](https://arxiv.org/abs/2512.08323)
*Achraf Ben-Hamadou,Nour Neifar,Ahmed Rekik,Oussama Smaoui,Firas Bouzguenda,Sergi Pujades,Niels van Nistelrooij,Shankeeth Vinayahalingam,Kaibo Shi,Hairong Jin,Youyi Zheng,Tibor Kubík,Oldřich Kodym,Petr Šilling,Kateřina Trávníčková,Tomáš Mojžiš,Jan Matula,Jeffry Hartanto,Xiaoying Zhu,Kim-Ngan Nguyen,Tudor Dascalu,Huikai Wu,and Weijie Liu,Shaojie Zhuang,Guangshun Wei,Yuanfeng Zhou*

Main category: cs.CV

TL;DR: 本文介绍了2024年MICCAI举办的3DTeethLand挑战赛，旨在推动基于口内3D扫描的牙齿关键点检测算法发展，并发布了首个公开的3D牙齿关键点检测数据集。


<details>
  <summary>Details</summary>
Motivation: 牙齿关键点检测在正畸临床中至关重要，但因牙齿几何结构复杂及个体差异大，现有方法面临挑战，亟需基于深度学习的先进解决方案。

Method: 组织并发布3DTeethLand挑战赛及配套首个公开3D牙齿关键点检测数据集，以评估和促进相关算法研究。

Result: 成功举办MICCAI 2024 3DTeethLand挑战赛，推出首个面向3D牙齿关键点检测的公开基准数据集。

Conclusion: 该挑战赛与数据集为推动牙齿关键点自动检测技术发展、提升临床诊断与治疗规划能力提供了重要基础与社区协作平台。

Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.

</details>


### [39] [GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification](https://arxiv.org/abs/2512.08325)
*Xuedeng Liu,Jiabao Guo,Zheng Zhang,Fei Wang,Zhi Liu,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出GeoDiffMM，一种基于扩散模型的拉格朗日视频运动放大框架，利用无噪声光流增强和几何引导的扩散运动放大器，在保留结构一致性的同时更准确地放大微小运动并抑制光子噪声。


<details>
  <summary>Details</summary>
Motivation: 现有欧拉方法难以在微小位移下区分光子噪声与真实微运动，需更鲁棒的几何感知放大机制。

Method: 提出基于光流几何先验的扩散模型框架，包括：1）无噪声光流增强策略生成干净运动监督；2）条件扩散运动放大器，以光流和可学习放大因子为条件；3）基于光流的高保真视频合成。

Result: 在真实与合成数据集上显著超越现有SOTA方法，提升运动放大质量与结构一致性。

Conclusion: GeoDiffMM通过几何引导的扩散建模，有效解决了微小运动放大中的噪声耦合与结构失真问题，为VMM提供了新范式。

Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

</details>


### [40] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于四元数的低秩支持四元数矩阵机（LSQMM）方法，用于彩色图像分类，通过将RGB通道建模为纯四元数并引入四元数核范数正则化，有效保持通道间耦合关系并提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将彩色图像特征表示为实向量、矩阵或三阶张量，难以有效建模RGB通道间的内在耦合关系；而四元数在图像恢复和去噪中已展现出优势，因此本文探索其在分类任务中的应用。

Method: 提出低秩支持四元数矩阵机（LSQMM），将RGB通道视为纯四元数，采用四元数核范数正则化增强低秩结构，并设计基于ADMM的迭代算法求解优化模型。

Result: 在多个彩色图像分类数据集上，LSQMM在分类精度、鲁棒性和计算效率方面均优于SVM、支持矩阵机和支持张量机等先进方法。

Conclusion: 四元数代数能更自然地建模彩色图像通道间相关性，结合低秩约束与ADMM优化的LSQMM是一种高效且准确的彩色图像分类新方法。

Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [41] [SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking](https://arxiv.org/abs/2512.08430)
*Nico Leuze,Maximilian Hoh,Samed Doğan,Nicolas R. -Peña,Alfred Schoettl*

Main category: cs.CV

TL;DR: 本文提出了一种基于多视角深度图融合的全稀疏、端到端深度-only 6D位姿估计方法，通过分阶段热图机制与密度感知稀疏Transformer，在高分辨率下高效建模密集杂乱场景中的前景几何细节，实现单次前向即对任意数量目标输出6D位姿。


<details>
  <summary>Details</summary>
Motivation: 工业 bin-picking 场景中因遮挡、反射和无纹理区域导致6D位姿估计困难，现有方法难以兼顾高分辨率几何细节与计算/内存效率。

Method: 融合多视角深度图为细粒度点云或稀疏TSDF；引入分阶段热图机制生成多尺度场景自适应注意力先验；设计密度感知稀疏Transformer块以动态处理自遮挡和3D数据非均匀分布；采用体素级投票策略实现整场景联合、多目标6D位姿预测。

Result: 在IPD和MV-YCB多视角数据集上达到领先性能，尤其在高度杂乱的工业与家庭bin-picking场景中表现稳健。

Conclusion: 全稀疏深度-only框架可有效平衡高分辨率几何建模能力与计算可行性，为近距离机器人操作中的鲁棒6D位姿估计提供了新范式。

Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.

</details>


### [42] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: 本文通过可解释AI方法系统分析了Glaze和Nightshade等图像保护机制的对抗扰动特性，发现其并非随机噪声，而是与图像内容紧密耦合的低熵结构化扰动，在表征、空间和频谱域均表现出可检测的规律性。


<details>
  <summary>Details</summary>
Motivation: 尽管Glaze和Nightshade等图像保护机制在实践中有效，但其内部结构、可检测性及在模型表征空间中的行为仍缺乏深入理解。

Method: 采用统一框架，结合白盒（潜空间聚类、特征通道激活分析、遮挡敏感性映射）与黑盒（信号级频域分析）方法进行系统性可解释AI分析。

Result: 发现保护扰动是结构化、低熵的，紧密耦合于图像内容；不引起全局表征漂移，而是在原有内容驱动结构上叠加保护特异性子结构；可检测性取决于扰动熵、空间部署与频率对齐的交互作用；Glaze和Nightshade在频域中沿图像主导频率轴重分布能量，而非引入扩散噪声。

Conclusion: 当前图像保护机制本质上是结构化的特征级形变，而非语义错位，这解释了其视觉隐蔽性与可检测性并存的现象，为生成式AI的防护与检测策略设计提供了理论依据。

Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>


### [43] [PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models](https://arxiv.org/abs/2512.08330)
*Pengbo Li,Yiding Sun,Haozhe Cheng*

Main category: cs.CV

TL;DR: 本文提出PointDico模型，结合扩散模型与对比学习的优势，通过知识蒸馏实现3D点云表征学习，在ScanObjectNN和ShapeNetPart等基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督3D表征学习方法面临点云无序性与密度不均的挑战：对比学习易过拟合，3D掩码自编码器难处理无序点云。因此，作者希望融合扩散模型（生成能力）与对比学习（判别能力）的优点。

Method: 提出PointDico模型：1）采用知识蒸馏框架，以扩散模型为教师指导对比学习学生模型；2）设计分层金字塔条件生成器用于多尺度几何特征提取；3）引入双通道结构融合局部与全局上下文信息。

Result: 在ScanObjectNN上达94.32%准确率，在ShapeNetPart上达86.5%实例平均IoU，刷新3D表征学习SOTA。

Conclusion: PointDico成功弥合了生成式与判别式范式在3D点云建模中的差异，验证了二者协同提升表征质量的有效性，为无序3D数据的自监督学习提供了新范式。

Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.

</details>


### [44] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 本文提出MELLA数据集，通过双源策略（原生网络alt-text用于文化，MLLM生成字幕用于语言）提升低资源语言多模态大模型的语义能力与文化扎根性，实现‘厚描述’效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低资源语言中仅关注文本模态或依赖机器翻译，忽视多模态信息与文化根基，难以有效服务低资源语言用户。

Method: 提出双源数据构建策略：使用原生网络alt-text增强文化扎根性，用MLLM生成字幕强化语言能力；构建多模态多语言数据集MELLA，并在多个MLLM骨干上微调验证。

Result: 在8种低资源语言上，基于MELLA微调显著提升各MLLM性能，模型能生成更具文化感知与语言丰富性的‘厚描述’；消融实验证明文化知识与语言能力均有提升。

Conclusion: 兼顾语言能力与文化扎根性是提升低资源语言MLLM效果的关键路径，MELLA为该方向提供了可复现、可扩展的数据基础。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce "thin descriptions", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing "thick descriptions". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [45] [Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery](https://arxiv.org/abs/2512.08577)
*Yuna Kato,Shohei Mori,Hideo Saito,Yoshifumi Takatsume,Hiroki Kajita,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本文提出了一种全自动视频对齐与视图合成方法，用于解决开放手术视频录制中因外科医生遮挡和无影灯移动导致的视角遮挡与图像错位问题，通过自动识别灯光移动帧、重对齐并选择最少遮挡视角，生成固定视角的高质量手术视频，并经外科医生用户研究验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 手术视频录制中常因医生遮挡和无影灯移动导致视角遮挡与多相机图像错位，现有方法依赖手动对齐，费时且不可靠。

Method: 提出全自动对齐方法：检测无影灯运动帧、自动重对齐多视角图像、动态选择遮挡最少的相机视角，并结合多种视图合成策略生成固定视角视频。

Result: 用户研究表明，该方法在手术区域识别易度、观看舒适度及视频质量方面均优于传统方法；不同合成选项亦经外科医生偏好评估。

Conclusion: 本方法实现了手术多视角视频的全自动对齐与最优视角合成，显著提升教育与科研用手术视频的可用性与质量。

Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.

</details>


### [46] [Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening](https://arxiv.org/abs/2512.08331)
*Xianghong Xiao,Zeyu Xia,Zhou Fei,Jinliang Xiao,Haorui Chen,Liangjian Deng*

Main category: cs.CV

TL;DR: 本文提出了一种双模态双自适应掩码感知卷积（Bi²MAC）方法，用于遥感图像的全色锐化任务，通过软硬掩码将特征分流至轻量全局分支与高成本精细分支，兼顾性能与效率，在多个数据集上达到SOTA且计算开销最小。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法难以适应特征表示中的区域异质性；现有自适应卷积方法计算成本高、对遥感图像中异质区域建模能力有限。

Method: 提出Bimodal Bi-Adaptive Mask-Aware Convolution（Bi²MAC），设计轻量模块生成软掩码（初步调制特征）和硬掩码（引导特征分流），将冗余特征送入紧凑分支进行低成本全局处理，异质特征送入聚焦分支进行细粒度建模。

Result: 在多个基准数据集上达到全色锐化任务的SOTA性能，同时训练时间更短、参数量更少、计算成本为当前自适应卷积模型中最低。

Conclusion: Bi²MAC通过智能特征分流与资源分配机制，有效平衡了建模能力与计算效率，为遥感图像融合提供了高效、可扩展的新范式。

Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.

</details>


### [47] [Accelerated Rotation-Invariant Convolution for UAV Image Segmentation](https://arxiv.org/abs/2512.08888)
*Manduhu Manduhu,Alexander Dow,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 本文提出了一种GPU优化的旋转不变卷积框架，通过避免im2col操作并利用对称旋转滤波器间的数据共享，显著降低内存访问和计算冗余，在保持精度的同时大幅提升训练速度与能效。


<details>
  <summary>Details</summary>
Motivation: 无人机航拍图像中目标朝向任意且细节丰富，传统CNN（如U-Net）的卷积不具备旋转不变性，导致分割精度下降；而扩展多方向滤波器库又带来高昂计算与内存开销。

Method: 设计GPU友好的旋转不变卷积，跳过im2col步骤，利用对称旋转滤波器间的结构化数据共享，并推广至任意旋转角度。

Result: 相比cuDNN，训练快20–55%，能耗低15–45%；在8方向设置下，256×256输入达45%加速与41%能耗下降，1024×1024输入达32%加速与23%能耗下降；嵌入U-Net后分割精度提升最高达6%。

Conclusion: 该方法为旋转不变CNN提供了高效、准确且硬件友好的新范式。

Abstract: Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.
  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.

</details>


### [48] [HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting](https://arxiv.org/abs/2512.08334)
*Chang Liu,Hongliang Yuan,Lianghao Zhang,Sichao Wang,Jianwei Guo,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: 本文提出了一种混合高斯光栅化（HybridSplat）机制，通过反射烘焙与基于瓦片的高斯光栅化结合，提升复杂反射场景下的渲染速度与内存效率，同时保持高质量反射效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯点阵的渲染方法在处理真实世界复杂反射场景时，存在渲染速度慢和内存占用高的瓶颈。

Method: 提出反射烘焙的高斯追踪方法，将视角相关反射信息嵌入每个高斯原语；采用瓦片化高斯光栅化进行反射渲染；构建统一的混合光栅化框架融合反射与基础高斯原语；引入流水线级加速与反射敏感的高斯剪枝策略。

Result: 在Ref-NeRF、NeRF-Casting等复杂反射场景上，渲染速度提升约7倍，高斯原语数量减少4倍，同时保持反射质量，达到当前最优性能。

Conclusion: HybridSplat是一种面向复杂反射场景的新一代高效高斯渲染方法，在速度、内存与质量之间实现了更好平衡。

Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.

</details>


### [49] [LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception](https://arxiv.org/abs/2512.08912)
*Simon de Moreau,Andrei Bursuc,Hafid El-Idrissi,Fabien Moutarde*

Main category: cs.CV

TL;DR: 本文提出了一种名为LiDAS的照明驱动动态主动感知系统，通过高分辨率车灯动态调控光照分布，提升夜间视觉感知性能，无需模型重训练即可实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 夜间环境对基于相机的感知构成重大挑战，现有方法被动依赖场景光照，缺乏主动优化能力。

Method: LiDAS是一种闭环主动照明系统，结合现成视觉感知模型与高清车灯，动态预测并生成最优照明场，将光能优先分配给目标区域而非均匀照明，并在合成数据上训练后零样本部署于真实驾驶场景。

Result: +18.7% mAP50 和 +5.0% mIoU（相比标准近光灯、同等功耗）；功耗降低40%时性能保持；可与领域泛化方法协同增强鲁棒性。

Conclusion: LiDAS将普通车灯转化为主动视觉执行器，以低成本方式显著提升夜间感知鲁棒性，且无需修改或重训练现有感知模型。

Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.

</details>


### [50] [DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation](https://arxiv.org/abs/2512.08337)
*Jianwei Wang,Qing Wang,Menglan Ruan,Rongjun Ge,Chunfeng Yang,Yang Chen,Chunming Xie*

Main category: cs.CV

TL;DR: 本文提出DINO-BOLDNet，利用冻结的自监督DINOv3编码器提取T1w图像结构特征，并结合多片层注意力与多尺度解码器生成BOLD图像，在临床数据上优于GAN基线。


<details>
  <summary>Details</summary>
Motivation: 解决BOLD图像缺失或损坏时无法进行下游任务的问题，探索从T1w图像生成BOLD图像的结构性到功能性映射。

Method: 构建DINOv3引导的多片层注意力框架：冻结DINOv3编码器提取单层结构特征；引入片层注意力模块融合邻近切片上下文；采用多尺度解码器恢复功能对比度；使用DINO特征空间中的感知损失保证结构与纹理一致性。

Result: 在248例临床数据集上，DINO-BOLDNet在PSNR和MS-SSIM指标上均超越条件GAN基线；首次实现直接从T1w图像生成平均BOLD图像。

Conclusion: 自监督Transformer（如DINOv3）可有效指导结构性到功能性医学图像生成，为缺失功能影像的重建提供新范式。

Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.

</details>


### [51] [TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels](https://arxiv.org/abs/2512.08358)
*Jiahao Lu,Weitao Xiong,Jiacheng Deng,Peng Li,Tianyu Huang,Zhiyang Dou,Cheng Lin,Sai-Kit Yeung,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出了TrackingWorld，一种用于单目视频中密集3D像素跟踪的新方法，通过分离相机运动与前景动态运动，并利用跟踪上采样器和优化框架实现世界坐标系下的稠密3D轨迹重建。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D跟踪方法难以区分相机运动与前景动态运动，且无法对视频中新出现的动态物体进行稠密跟踪。

Method: 提出TrackingWorld流程：1）设计跟踪上采样器将稀疏2D轨迹提升为稠密2D轨迹；2）对所有帧应用该上采样器并消除重叠区域冗余轨迹以泛化至新出现物体；3）构建基于优化的框架，联合估计相机位姿与2D轨迹对应的世界坐标系3D点位置。

Result: 在合成与真实数据集上验证了该系统能在世界中心坐标系下实现高精度、稠密的3D跟踪。

Conclusion: TrackingWorld有效解决了单目3D跟踪中运动解耦与新物体泛化两大挑战，实现了更鲁棒、更通用的稠密世界坐标系3D跟踪。

Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.

</details>


### [52] [SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation](https://arxiv.org/abs/2512.08362)
*Ju-Young Kim,Ji-Hong Park,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 本文提出SCU-CGAN模型，结合U-Net、CBAM和额外判别器，从非火图像生成高质量火图像，以缓解火灾检测中数据不足问题；生成图像质量优于CycleGAN（KID提升41.5%），增强数据集显著提升YOLOv5n的mAP@0.5:0.95达56.5%。


<details>
  <summary>Details</summary>
Motivation: 火灾检测因缺乏足够真实火灾图像数据集而受限，影响模型性能。

Method: 提出SCU-CGAN模型，融合U-Net结构、CBAM注意力机制与额外判别器，实现从非火图像到逼真火灾图像的生成。

Result: SCU-CGAN在KID指标上比CycleGAN提升41.5%；用其生成数据增强后，YOLOv5 nano的mAP@0.5:0.95提升56.5%。

Conclusion: SCU-CGAN能有效生成高质量火灾图像，缓解数据稀缺问题，显著提升下游火灾检测模型性能，无需修改模型结构。

Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.

</details>


### [53] [The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss](https://arxiv.org/abs/2512.08374)
*Bozhou Li,Xinda Xue,Sihan Yang,Yang Shi,Xinlong Chen,Yushuo Guan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文指出多模态大语言模型（MLLMs）中视觉与文本token的范数差异导致不对称更新动态，损害跨模态融合；提出在视觉投影后插入一个精心初始化的LayerNorm层来对齐范数，显著提升多模态及纯文本任务性能。


<details>
  <summary>Details</summary>
Motivation: MLLMs广泛采用Pre-Norm架构，但视觉token范数远高于文本token，造成范数失衡，进而引发不对称更新动态，阻碍有效跨模态融合。

Method: 理论分析揭示范数失衡导致视觉token‘表征惯性’；实证验证该现象普遍存在；提出在视觉投影后添加单个精心初始化的LayerNorm层以实现范数对齐。

Result: 在LLaVA-1.5上验证，该简单修正显著提升多个多模态基准（如MMBench、OCRBench）及纯文本基准（如MMLU）性能。

Conclusion: 范数失衡是MLLMs中被忽视的关键架构缺陷；通过轻量级LayerNorm干预可缓解该问题，提升模型整体能力，表明架构平衡对多模态与单模态性能均至关重要。

Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.

</details>


### [54] [Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions](https://arxiv.org/abs/2512.08378)
*Jing Tao,You Li,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种在复杂光照条件下同时进行图像增强与噪声抑制的新框架，结合梯度域加权引导滤波、Retinex分解、分层处理及多曝光融合等技术，在真实数据集上验证了其在对比度增强和噪声抑制方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有图像增强方法在挑战性光照下易放大噪声或仅适用于特定光照条件，难以兼顾增强效果与噪声抑制。

Method: 提出一种新框架：1）使用梯度域加权引导滤波（GDWGIF）估计照度并提升质量；2）基于Retinex模型分解为照度层与反射层；3）对两层并行处理（照度层优化照明、反射层增强细节）；4）通过多曝光融合与线性拉伸优化动态范围。

Result: 在真实世界数据集上实验表明，该方法在对比度增强与噪声抑制两方面均优于当前最优方法。

Conclusion: 所提框架能有效应对复杂光照下的图像退化问题，实现高质量增强与强鲁棒噪声抑制的统一。

Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.

</details>


### [55] [Detection of Digital Facial Retouching utilizing Face Beauty Information](https://arxiv.org/abs/2512.08397)
*Philipp Srock,Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: 本文研究了面部修图对人脸识别系统的影响，并提出利用美丑评估算法和AI特征提取方法来提升修图检测率，在未知攻击修图算法的情况下实现了1.1%的D-EER单图检测性能。


<details>
  <summary>Details</summary>
Motivation: 面部修图广泛应用于社交媒体和广告中，但当修图图像被用作生物识别样本时，会干扰人脸识别系统，因此亟需有效的修图检测方法。

Method: 研究并分析美丑评估算法在修图图像上的变化，对比多种基于人工智能的特征提取方法，并探索是否可利用‘面部美观度’信息来提升修图检测性能。

Result: 在攻击修图算法未知的场景下，单图检测达到1.1%的D-EER（Detection Equal Error Rate）。

Conclusion: 面部美观度特征与AI驱动的特征提取方法相结合，可有效提升面部修图检测精度，为应对修图干扰的生物识别系统提供新思路。

Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.

</details>


### [56] [Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries](https://arxiv.org/abs/2512.08400)
*Samitha Nuwan Thilakarathna,Ercan Avsar,Martin Mathias Nielsen,Malte Pedersen*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的鱼类重识别（Re-ID）自动化流程，利用新构建的AutoFish数据集，在硬样本三元组挖掘和定制图像变换策略下，Swin-T模型在mAP@k和Rank-1指标上显著优于ResNet-50，主要难点在于同物种个体间的区分。


<details>
  <summary>Details</summary>
Motivation: 电子监控（EM）系统产生大量视频数据，人工审核不可行，亟需自动化的鱼类重识别方法以支持渔业资源可持续管理。

Method: 构建模拟EM系统的AutoFish数据集（含6种相似鱼类），采用硬三元组挖掘与针对该数据集定制的图像归一化增强策略，对比Swin-T与ResNet-50在鱼类Re-ID任务上的性能。

Result: Swin-T达到41.65% mAP@k和90.43% Rank-1准确率，显著优于ResNet-50；分析表明同物种个体因视角不一致导致的误识别是主要挑战，比部分遮挡更严重。

Conclusion: 硬样本挖掘与数据集适配的预处理能显著提升鱼类Re-ID性能，ViT类模型优于CNN，且视角变化是当前关键瓶颈。

Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git

</details>


### [57] [SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos](https://arxiv.org/abs/2512.08406)
*Mingqi Gao,Yunqi Miao,Jungong Han*

Main category: cs.CV

TL;DR: 本文提出SAM-Body4D，一种无需额外训练的视频级人体网格恢复框架，通过利用视频中人体运动的连续性，结合提示式视频分割与遮挡感知模块，提升时序一致性和遮挡鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的HMR方法（如SAM 3D Body）在视频上逐帧推理，导致时序不一致和遮挡下性能下降。

Method: 提出训练自由的SAM-Body4D框架：1）使用可提示视频分割模型生成身份一致的masklets；2）通过遮挡感知模块修复缺失区域；3）用优化后的masklets引导SAM 3D Body生成一致的全身网格轨迹；4）采用基于填充的并行策略实现高效多人推理。

Result: 在真实世界视频上显著提升了时序稳定性与遮挡鲁棒性，且无需任何再训练。

Conclusion: SAM-Body4D验证了仅利用视频内在连续性即可有效提升HMR时序一致性与鲁棒性，为无训练视频HMR提供了新范式。

Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.

</details>


### [58] [Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval](https://arxiv.org/abs/2512.08410)
*Tao Chen,Shaobo Ju,Qiong Wu,Chenxin Fang,Kun Zhang,Jun Peng,Hui Li,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出OneClip-RAG，一种基于单视频片段检索增强的高效范式，通过查询引导的视频分块与跨模态检索一体化算法，显著提升多模态大模型处理长视频的能力与效率，并在多个基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多模态大语言模型（MLLMs）因内存开销过大，难以处理长视频（仅限少量帧），亟需一种高效、低开销的视频理解增强方法。

Method: 提出One-shot video-Clip based Retrieval AuGmentation（OneClip-RAG），包含：1）利用视频片段保持知识完整性与语义连贯性的检索增强机制；2）查询引导的视频分块算法，统一分块与跨模态检索；3）构建合成数据集SynLongVideo并设计渐进式训练策略。

Result: OneClip-RAG集成至5个主流MLLM后，在MLVU等长视频基准上显著提升性能（如InternLV2 8B和Qwen2-VL 7B达GPT-4o水平），且效率突出（LLaVA-Video可在单张4090 GPU上2.2分钟内理解长达1小时的视频）。

Conclusion: OneClip-RAG是一种兼顾效果与效率的通用视频增强范式，有效缓解MLLMs处理长视频的内存瓶颈，为实际长视频理解应用提供可行方案。

Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.

</details>


### [59] [LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training](https://arxiv.org/abs/2512.08439)
*Qing Xu,Kun Yuan,Yuxiang Luo,Yuhao Zhai,Wenting Duan,Nassir Navab,Zhen Chen*

Main category: cs.CV

TL;DR: 本文提出了LapFM，一种专为腹腔镜手术图像分割设计的基础模型，通过分层概念演化预训练范式，利用大量未标注手术图像提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 手术分割面临标注稀缺和跨手术类型语义不一致的挑战，现有方法仅微调自然基础模型，难以应对手术目标的高度多样性。

Method: 提出分层概念演化预训练范式：1）构建腹腔镜概念层次（LCH），通过带父子查询嵌入的分层掩码解码器统一解剖、组织和器械等多粒度实体；2）设计置信度驱动的演化标注方法，迭代生成并筛选符合层次一致性的伪标签，构建大规模数据集LapBench-114K。

Result: LapFM在通用腹腔镜分割任务上显著超越现有最先进方法，展现出优异的粒度自适应泛化能力。

Conclusion: LapFM成功将未标注手术图像转化为结构化知识，为构建真正面向手术场景的基础模型提供了新范式。

Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.

</details>


### [60] [Leveraging Multispectral Sensors for Color Correction in Mobile Cameras](https://arxiv.org/abs/2512.08441)
*Luca Cogo,Marco Buzzelli,Simone Bianco,Javier Vazquez-Corral,Raimondo Schettini*

Main category: cs.CV

TL;DR: 本文提出了一种统一的、基于学习的端到端颜色校正框架，联合利用高分辨率RGB传感器和低分辨率多光谱（MS）传感器的数据，在保持图像一致性的同时显著提升颜色准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有颜色校正方法通常分阶段处理且过早丢弃多光谱数据，未能充分利用MS信息；而新兴快照式多光谱成像为消费级设备提供了低成本光谱感知能力，亟需更有效的融合利用方案。

Method: 设计统一的端到端深度学习框架，联合建模RGB与MS传感器输入；重构两种主流图像到图像架构以适配该任务；构建覆盖多种RGB相机响应特性的合成多光谱-RGB配对数据集用于训练与评估。

Result: 在多个指标上显著优于纯RGB或纯MS基线方法，颜色误差最高降低50%；验证了框架的灵活性与泛化性；配套数据集、代码与模型将开源。

Conclusion: 联合利用RGB与MS传感器的端到端学习框架能更有效地实现高保真颜色校正，为移动与消费级光谱成像应用提供了实用可行的技术路径。

Abstract: Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.

</details>


### [61] [Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts](https://arxiv.org/abs/2512.08445)
*Madhav Gupta,Vishak Prasad C,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 本文提出一种结合子模子集选择与层间梯度不确定性估计的新框架，以提升深度视觉模型在分布外（OOD）场景下解释方法的鲁棒性与保真度，无需额外训练或辅助模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于子集选择的解释方法在分布内（ID）表现良好，但在分布外（OOD）条件下可靠性显著下降，表现为冗余、不稳定和对不确定性敏感。

Method: 将子模子集选择与层-wise 梯度不确定性估计相结合，通过自适应权重扰动估计不确定性，并以此指导子模优化，确保所选图像区域具有多样性与信息性。

Result: 该方法在多个ID-OOD数据集上显著缓解了现有方法在OOD下的缺陷，同时在ID设置下也取得性能提升。

Conclusion: 当前子集选择类解释方法存在OOD鲁棒性不足的问题，而引入不确定性驱动的优化可有效增强归因质量与对象级可解释性，推动真实视觉应用中更透明可信的AI发展。

Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

</details>


### [62] [Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery](https://arxiv.org/abs/2512.08467)
*Chamath Ranasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文提出了一种基于SAM与CSRT跟踪器结合、并引入球衣颜色外观模型的轻量级足球运动员跟踪方法，兼顾精度与效率，在轻度遮挡下实现100%跟踪成功率，但在长期遮挡（球员离框）情况下重捕获率仅为8.66%。


<details>
  <summary>Details</summary>
Motivation: 足球运动员跟踪面临频繁遮挡、外观相似和密集场景中快速运动等挑战，现有方法在精度、速度与鲁棒性之间难以兼顾。

Method: 融合Segment Anything Model（SAM）进行精确初始分割，结合CSRT跟踪器，并引入HSV直方图驱动的球衣颜色外观模型实现团队感知跟踪与遮挡后重识别。

Result: 在足球视频上达到7.6–7.7 FPS、内存稳定约1880 MB；轻度遮挡下跟踪成功率达100%，罚球区5人以上拥挤场景达90%；外观重识别恢复50%重度遮挡；但球员离框后重捕获率仅8.66%。

Conclusion: SAM+CSRT框架在可见性连续时表现稳健，适合资源受限部署；但需增强重识别机制以应对长期遮挡，凸显领域特定线索（如球衣颜色）对实际系统的重要性。

Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.

</details>


### [63] [ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention](https://arxiv.org/abs/2512.08477)
*Huiguo He,Pengyu Yan,Ziqi Yi,Weizhi Zhong,Zheng Liu,Yejun Tang,Huan Yang,Kun Gai,Guanbin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 本文提出ContextDrag，一种新的基于拖拽的图像编辑范式，通过引入上下文保持的令牌注入（CTI）和位置一致性注意力（PCA）机制，在不微调或反演的前提下，有效利用参考图像的上下文信息（尤其是细粒度纹理），显著提升编辑结果的一致性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的图像编辑方法未能充分利用参考图像中的上下文信息（尤其是细粒度纹理细节），导致编辑结果连贯性和保真度不足。

Method: 提出ContextDrag框架：1）Context-preserving Token Injection（CTI），通过潜空间逆映射（LRM）将VAE编码的无噪声参考特征精准注入目标位置；2）Position-Consistent Attention（PCA），对参考token进行位置重编码并采用重叠感知掩码抑制无关特征干扰。

Result: 在DragBench-SR和DragBench-DR数据集上全面超越现有SOTA方法。

Conclusion: ContextDrag通过有效融合参考图像的丰富上下文特征，在无需微调或图像反演的条件下，实现了更精确、语义与纹理一致的拖拽编辑效果。

Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.

</details>


### [64] [Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions](https://arxiv.org/abs/2512.08486)
*Ada Gorgun,Fawaz Sammani,Nikos Deligiannis,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: 本文提出PCI（Prompt-Conditioned Intervention）框架，无需训练、模型无关，通过分析概念插入成功率（CIS）来研究扩散模型中概念（如年龄）在去噪过程中的动态形成时机与稳定性，揭示不同概念在不同时间步的‘锁定’阶段，并为文本驱动图像编辑提供更有效、语义准确且内容保持更强的干预时机。


<details>
  <summary>Details</summary>
Motivation: 扩散模型通常只关注最终输出，但其生成过程是动态的；理解概念（如'年龄'）何时出现并稳定下来，对评估模型的可控性、可靠性与失败模式至关重要。

Method: 提出PCI框架，定义Concept Insertion Success（CIS）为在某时间步插入概念后该概念仍保留在最终图像中的概率；通过在多个SOTA文本到图像扩散模型上对多类概念进行CIS时序分析，实现概念动态建模。

Result: 发现不同概念在扩散轨迹中具有显著差异的时间敏感性——同一概念类型在不同模型或不同阶段表现出各异的‘有利干预窗口’；该发现可直接用于提升文本驱动图像编辑效果，在不访问模型内部或微调前提下，实现比强基线更优的语义准确性与内容保持平衡。

Conclusion: 概念在扩散过程中的形成具有明确的时间阶段性，PCI为理解与利用这一动态性提供了通用、实用且可量化的分析工具，推动了可控生成与编辑方法的发展。

Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions

</details>


### [65] [On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs](https://arxiv.org/abs/2512.08498)
*Yijia Guo,Tong Hu,Zhiwei Li,Liwen Hu,Keming Qian,Xitong Lin,Shengbo Chen,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种面向多相机系统的实时3D高斯泼溅（3DGS）重建框架，通过分层初始化、轻量级多相机BA和冗余感知优化策略，在无需标定前提下实现漂移自由的轨迹估计与高效在线重建。


<details>
  <summary>Details</summary>
Motivation: 单目RGB流受限于视场角（FOV），难以实现完整3D覆盖；多相机系统可从根本上缓解该问题，但尚无有效的实时多相机3DGS重建方法。

Method: 提出分层相机初始化方案（免标定粗对齐）、轻量级多相机bundle adjustment（稳定轨迹并保持实时性）、冗余自由高斯采样策略及频率感知优化调度器（减少高斯数量与迭代次数）。

Result: 仅用原始多相机视频流，2分钟内重建数百米场景，兼具前所未有的速度、鲁棒性与重建保真度。

Conclusion: 首次实现了多相机系统下的实时、漂移自由、高保真3D高斯泼溅在线重建，为大规模动态场景建模提供了新范式。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.

</details>


### [66] [Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models](https://arxiv.org/abs/2512.08503)
*Jiaming Zhang,Che Wang,Yang Cao,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: 本文提出ReasonBreak框架，通过概念感知的对抗扰动来破坏多模态大推理模型（MLRMs）的地理定位推理链，显著提升隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型（MLRMs）能通过分层思维链从个人图像中精确推断地理位置，而现有面向感知模型的隐私保护方法无法应对MLRMs复杂的多步推理过程。

Method: 提出ReasonBreak框架，基于概念层级对齐的扰动策略，针对性地破坏推理链中的关键概念依赖，并构建GeoPrivacy-6K数据集（含6341张超高清图像及分层概念标注）支撑方法验证。

Result: 在7个SOTA MLRM（如GPT-o3、GPT-5、Gemini 2.5 Pro）上评估，ReasonBreak将街区级和地块级地理隐私保护率分别提升至33.5%和33.8%，较基线提升显著（+16.7%和+14.4%）。

Conclusion: ReasonBreak开创了针对推理型威胁的隐私保护新范式，强调概念层级扰动对破坏多步推理的有效性。

Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.

</details>


### [67] [Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models](https://arxiv.org/abs/2512.08505)
*Vasco Ramos,Regev Cohen,Idan Szpektor,Joao Magalhaes*

Main category: cs.CV

TL;DR: 本文提出NoisyCLIP方法，在扩散模型去噪过程的早期噪声潜在空间中评估文本-图像语义对齐，实现生成过程中的实时对齐评估，降低50%计算成本并保持98%的CLIP对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有条件扩散模型常出现文本-图像错位与幻觉问题，而传统对齐评估需等待完整图像生成后进行，效率低、成本高；因此亟需能在生成过程中早期检测错位的方法。

Method: 提出NoisyCLIP，利用双编码器在反向扩散过程的噪声潜在空间中直接测量文本与噪声潜变量之间的语义对齐，首次系统探索并评测了生成过程中的prompt-to-latent错位检测。

Result: NoisyCLIP在Best-of-N设置下计算成本降低50%，同时达到CLIP对齐性能的98%；支持生成过程中的实时对齐评估。

Conclusion: 在噪声潜在空间中进行对齐评估是可行且高效的，NoisyCLIP为提升扩散模型生成质量与效率提供了新范式。

Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.

</details>


### [68] [OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds](https://arxiv.org/abs/2512.08506)
*Jialu Sui,Rui Liu,Hongsheng Zhang*

Main category: cs.CV

TL;DR: 本文提出OCCDiff方法，利用潜在扩散模型在占据函数空间中重建建筑表面，结合函数自编码器和点编码器，实现高保真、抗噪的3D建筑建模。


<details>
  <summary>Details</summary>
Motivation: 解决LiDAR点云中因点密度变化和噪声干扰导致建筑表面重建不准确的问题。

Method: 提出OCCDiff框架：将潜在扩散模型应用于连续占据函数空间；采用函数自编码器生成可任意位置评估的占据函数；设计点编码器提供条件特征、约束占据预测并注入多模态信息；引入多任务训练策略提升点编码器表征能力。

Result: 实验表明该方法生成物理一致、高保真度的样本，并对噪声数据具有强鲁棒性。

Conclusion: OCCDiff为LiDAR点云建筑重建提供了一种灵活、鲁棒且高精度的新范式，尤其适用于多分辨率与噪声场景。

Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.

</details>


### [69] [Thinking with Images via Self-Calling Agent](https://arxiv.org/abs/2512.08511)
*Wenxi Yang,Yuzhong Zhao,Fang Wan,Qixiang Ye*

Main category: cs.CV

TL;DR: 本文提出了一种名为Self-Calling Chain-of-Thought（sCoT）的新视觉推理范式，将多模态链式推理转化为纯语言的自调用链式推理，提升训练效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像思考（thinking-with-images）的方法虽具强视觉推理能力，但依赖稀缺高质量推理数据，难以通过强化学习有效优化多模态链式推理（iMCoT）。

Method: 提出sCoT范式：主智能体将复杂视觉任务分解为原子子任务，并调用参数共享的虚拟子智能体在隔离上下文中求解；采用组相对策略优化（group-relative policy optimization）强化有效推理行为。

Result: 在HR-Bench 4K数据集上，sCoT相比强基线方法最高提升1.9%整体推理性能，且GPU训练耗时减少约75%。

Conclusion: sCoT通过消除显式模态交织、引入自调用机制和高效策略优化，在保持甚至提升视觉推理性能的同时显著降低训练成本，为多模态推理提供了更实用、可扩展的新路径。

Abstract: Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\%$ with $\sim 75\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.

</details>


### [70] [MVP: Multiple View Prediction Improves GUI Grounding](https://arxiv.org/abs/2512.08529)
*Yunzhu Zhang,Zeyu Pan,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的多视角预测框架MVP，通过注意力引导的视图生成和多坐标聚类来提升GUI界面自然语言指令到像素坐标的定位稳定性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位模型对微小视觉扰动（如裁剪几个像素）敏感，导致坐标预测不稳定，尤其在高分辨率和小UI元素场景下性能严重下降。

Method: 提出Multi-View Prediction（MVP）框架，包含两部分：（1）Attention-Guided View Proposal，利用指令-图像注意力分数生成多样化裁剪视图；（2）Multi-Coordinates Clustering，对多视图预测坐标进行空间聚类并选取最密集簇的质心作为最终输出。

Result: 在ScreenSpot-Pro基准上，MVP显著提升多个模型性能：UI-TARS-1.5-7B达56.1%，GTA1-7B达61.7%，Qwen3VL-8B-Instruct达65.3%，Qwen3VL-32B-Instruct达74.0%。

Conclusion: MVP是一种通用、即插即用的推理增强方法，有效缓解GUI定位中的不稳定性问题，显著提升各类视觉语言模型在屏幕坐标预测任务上的鲁棒性与精度。

Abstract: GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.

</details>


### [71] [PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation](https://arxiv.org/abs/2512.08534)
*Zhangli Hu,Ye Chen,Jiajun Yao,Bingbing Ni*

Main category: cs.CV

TL;DR: 本文提出了一种统一的多模态框架，用于油画生成与编辑，支持参考图像、手绘草图和自然语言提示等多种输入方式，并通过空间对齐、语义增强训练策略、自监督风格迁移数据构建及AdaIN特征融合等技术，实现了风格一致、细节可控的交互式油画创作。


<details>
  <summary>Details</summary>
Motivation: 现有油画生成与编辑方法受限于训练数据分布，且主要面向真实照片修改，难以满足油画特有的笔触动态与艺术风格控制需求。

Method: 1）训练阶段引入空间对齐与语义增强条件策略，将掩码/草图映射为空间约束，参考图像与文本编码为特征约束；2）基于笔触渲染（SBR）设计自监督风格迁移流程，构建大规模配对油画训练数据；3）推理阶段采用AdaIN算子融合多源特征以保障风格一致性。

Result: 实验表明该系统能实现细粒度编辑并保持油画艺术特性，在风格化油画生成与编辑中达到前所未有的想象力实现水平。

Conclusion: 所提多模态框架有效解决了油画生成与编辑中的风格一致性、语义可控性与数据稀缺性三大挑战，为数字艺术创作提供了新范式。

Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.

</details>


### [72] [Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement](https://arxiv.org/abs/2512.08535)
*Xinyue Liang,Zhinyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: Photo3D 是一个提升3D生成真实感外观的新框架，利用GPT-4o-Image生成图像驱动，通过结构对齐的多视角合成与细节增强方案，解决现有3D生成器在纹理真实性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D原生生成器在几何可靠性上进步显著，但在外观真实性上仍不足；根本原因在于缺乏多样、高质量、富含纹理细节的真实世界3D资产，而此类数据采集因场景尺度差异、物体非刚性运动及扫描精度限制而极为困难。

Method: 提出Photo3D框架：1）利用GPT-4o-Image生成图像作为数据源；2）设计结构对齐的多视角合成流程，构建配对几何与增强细节的多视角数据集；3）提出基于感知特征适配与语义结构匹配的真实感细节增强方案；4）设计适配几何-纹理耦合/解耦范式的专用训练策略。

Result: Photo3D在多种3D原生生成范式上泛化良好，实现了当前最优的光度真实感3D生成性能。

Conclusion: Photo3D有效弥合了几何可信性与外观真实感之间的鸿沟，为基于图像先验的3D内容生成提供了可扩展、通用且高效的解决方案。

Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.

</details>


### [73] [Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation](https://arxiv.org/abs/2512.08537)
*Zhen Zou,Xiaoxiao Ma,Jie Huang,Zichao Yu,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出Fast-ARDiff框架，通过熵感知的推测解码与联合蒸馏优化，统一加速AR与扩散模型，显著降低生成延迟。


<details>
  <summary>Details</summary>
Motivation: AR-扩散混合范式虽兼具结构建模与高保真合成优势，但因顺序AR生成与迭代去噪导致高延迟。

Method: 提出熵信息引导的推测策略缓解熵错配；将扩散模块嵌入端到端框架，采用动态调度与轨迹+分布联合蒸馏优化；利用AR浅层特征熵预筛低熵草案以减少冗余计算。

Result: 在ImageNet 256×256上，TransDiff实现4.3×无损加速；NextStep-1在文本条件生成中达3×加速。

Conclusion: Fast-ARDiff实现了AR与扩散模型的协同优化，在保持高质量合成的同时大幅降低推理延迟，推动混合生成范式实用化。

Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.

</details>


### [74] [A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation](https://arxiv.org/abs/2512.08542)
*Zhigang Jia,Duan Wang,Hengkai Wang,Yajun Xie,Meixiang Zhao,Xiaoyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于四元数Wasserstein距离的新型生成对抗网络（WQGAN），通过定义新的四元数Wasserstein距离及其对偶理论，解决了彩色图像生成中通道相关性被忽略及数据分布缺乏理论度量的问题，实验表明其在生成效率和图像质量上优于现有GAN和WGAN模型。


<details>
  <summary>Details</summary>
Motivation: 现有彩色图像生成模型忽略颜色通道间的相关性，导致色差问题；同时，彩色图像数据分布缺乏系统阐述与理论度量方法。

Method: 定义新的四元数Wasserstein距离，建立其对偶理论；利用四元数凸集分离定理和四元数Farkas引理推导强对偶形式；构建Wasserstein四元数生成对抗网络（WQGAN）。

Result: 所提WQGAN在生成效率和图像质量上均优于传统GAN、四元数GAN及Wasserstein GAN。

Conclusion: 四元数Wasserstein距离为彩色图像建模提供了更合理的数学工具，WQGAN有效提升了彩色图像生成性能，并为多通道数据分布度量提供了新理论框架。

Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.

</details>


### [75] [An Iteration-Free Fixed-Point Estimator for Diffusion Inversion](https://arxiv.org/abs/2512.08547)
*Yifei Chen,Kaiyu Song,Yan Pan,Jianxing Yu,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 本文提出了一种无需迭代的扩散反演固定点估计器，通过误差近似方法避免了传统固定点迭代的高计算成本和超参数调优难题，在NOCAPS和MS-COCO数据集上实现了更优且稳定的重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定点迭代的扩散反演方法存在计算开销大、超参数选择复杂的问题。

Method: 推导理想反演步下的固定点显式表达式，并引入前一步可计算误差来近似当前步未知的数据预测误差，从而构建可计算、无偏且低方差的固定点估计器。

Result: 在NOCAPS和MS-COCO数据集上，相比DDIM反演及其他基于固定点迭代的方法，本方法在无需额外迭代或训练的前提下，实现了更一致且更优的图像重建性能。

Conclusion: 所提出的迭代自由固定点估计器有效平衡了精度与效率，为扩散模型反演提供了更实用、更鲁棒的新范式。

Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.

</details>


### [76] [SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/2512.08557)
*Alexander Dow,Manduhu Manduhu,Matheus Santos,Ben Bartlett,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSCATeR的稀疏散射卷积算法，利用LiDAR扫描的时间连续性，仅对点云中发生变化的区域进行计算，从而大幅减少卷积操作，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR目标检测方法对整个点云重复进行密集卷积，计算开销大；而实际扫描中多数区域在帧间保持不变，存在大量冗余计算。

Method: 采用滑动时间窗口与短步长策略，存储并复用跨帧的卷积结果；扩展散射卷积以支持时序数据复用，提出SSCATeR算法，仅处理点云中动态变化部分。

Result: 相比传统稀疏卷积，处理时间最多减少6.61倍，特征图输出完全一致，验证了精度无损。

Conclusion: SSCATeR通过引入时空稀疏性建模，在不牺牲检测精度的前提下显著提升了LiDAR点云处理的计算效率，为实时感知系统提供了可行方案。

Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.

</details>


### [77] [BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain](https://arxiv.org/abs/2512.08560)
*Navve Wasserman,Matias Cosarinsky,Yuval Golbari,Aude Oliva,Antonio Torralba,Tamar Rott Shaham,Michal Irani*

Main category: cs.CV

TL;DR: 本文提出了一种大规模、自动化的脑成像分析框架，用于在全皮层范围内发现并解释视觉概念的神经表征，通过无监督分解和自动化语义解释，揭示了数千种可解释的视觉表征模式。


<details>
  <summary>Details</summary>
Motivation: 理解人类大脑如何表征视觉概念及其在皮层中的定位仍具挑战性；现有研究规模小、依赖人工、区域局限且缺乏系统验证。

Method: 采用两阶段方法：首先用无监督数据驱动分解方法发现fMRI活动中的可解释模式；然后通过匹配高激活自然图像并生成自然语言描述来解释每个模式，并引入自动化流程评估并选择最一致的语义解释。

Result: 框架发现了覆盖广泛视觉概念的数千种可解释模式，包括此前未被报道的细粒度表征。

Conclusion: 该自动化、大规模框架显著提升了对全皮层视觉表征的系统性发现与可解释性，为脑科学提供了新范式。

Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.

</details>


### [78] [Modular Neural Image Signal Processing](https://arxiv.org/abs/2512.08564)
*Mahmoud Afifi,Zhongling Wang,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一种模块化的神经图像信号处理（ISP）框架，能够从原始传感器数据生成高质量显示图像，并支持用户交互式编辑和多风格渲染。


<details>
  <summary>Details</summary>
Motivation: 现有神经ISP方法缺乏模块化设计，难以控制中间渲染阶段、调试、泛化到新相机及适配不同用户偏好。

Method: 设计了一个高度模块化的端到端学习型神经ISP框架，各模块可独立控制与替换；构建了基于该ISP的用户交互式照片编辑工具，支持无限次后编辑重渲染。

Result: 模型参数量仅0.5M–3.9M，在多个测试集上取得具有竞争力的定性与定量结果；编辑工具实现了多样化编辑操作与风格化输出。

Conclusion: 模块化神经ISP在精度、可扩展性、可调试性、跨相机泛化性和用户风格适配性方面均优于传统方法，为实用化神经图像处理提供了新范式。

Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM

</details>


### [79] [Instance-Aware Test-Time Segmentation for Continual Domain Shifts](https://arxiv.org/abs/2512.08569)
*Seunghwan Lee,Inyoung Jung,Hojoon Lee,Eunil Park,Sungeun Hong*

Main category: cs.CV

TL;DR: 本文提出了一种面向语义分割的持续测试时自适应（CTTA）新方法，通过图像内自适应调整伪标签和动态平衡各类别学习，提升了模型在连续域变化下的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖固定或批次级阈值，无法应对语义分割中各类别和实例难度差异大、需密集多类预测的问题，易导致误差累积。

Method: 提出一种细粒度、类别与实例感知的自适应机制：基于每张图像内部的置信度分布动态调整伪标签，并优先更新受域偏移影响最大的类别。

Result: 在8个CTTA/TTA场景（含合成到真实、长期域偏移）上显著超越SOTA方法，尤其在语义分割任务中展现出更强的持续适应能力与稳定性。

Conclusion: 该方法通过更可靠的图像级监督信号缓解了误差累积问题，为动态演化环境下的语义分割提供了新的技术范式与性能基准。

Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.

</details>


### [80] [From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis](https://arxiv.org/abs/2512.08572)
*Olle Edgren Schüllerqvist,Jens Baumann,Joakim Lindblad,Love Nordling,Artur Mezheyeuski,Patrick Micke,Nataša Sladoje*

Main category: cs.CV

TL;DR: 本文提出HiGINE，一种基于分层图的模型，用于从多重免疫荧光图像中分析肿瘤微环境，预测肺癌患者生存期并提升风险分层效果。


<details>
  <summary>Details</summary>
Motivation: 肿瘤微环境（TME）是潜在的预后生物标志物来源，但需能捕捉多种细胞类型间复杂互作的分析方法。

Method: 提出HiGINE——一种分层图神经网络方法，建模细胞局部与全局邻域关系，融合细胞类型、形态及临床分期等多模态信息。

Result: 在两个公开数据集上验证，HiGINE显著提升了风险分层性能，具备鲁棒性与泛化能力。

Conclusion: HiGINE为基于TME图像的精准预后预测提供了有效新范式，有助于肺癌个体化诊疗。

Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.

</details>


### [81] [Automated Pollen Recognition in Optical and Holographic Microscopy Images](https://arxiv.org/abs/2512.08589)
*Swarn Singh Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 本研究利用YOLOv8s和MobileNetV3L分别进行花粉颗粒的检测与分类，验证了深度学习在光学与全息显微图像中的应用潜力，尤其针对兽医细胞学场景；通过数据集扩展与自动标注等策略显著提升了全息图像上的模型性能。


<details>
  <summary>Details</summary>
Motivation: 提升并自动化光学与全息显微镜图像中花粉颗粒的检测与分类，服务于兽医细胞学实际需求。

Method: 采用YOLOv8s进行目标检测，MobileNetV3L进行分类，并针对全息图像性能差的问题，引入自动化标注与边界框面积扩大策略进行数据集增强。

Result: 光学图像上检测mAP50达91.3%，分类准确率达97%；全息图像经增强后，检测mAP50从2.49%提升至13.3%，分类准确率从42%提升至54%。

Conclusion: 深度学习可有效结合低成本无透镜数字全息显微设备，尤其适用于图像分类任务。

Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.

</details>


### [82] [Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning](https://arxiv.org/abs/2512.08606)
*Zhenyu Zhang,Guangyao Chen,Yixiong Zou,Zhimeng Huang,Yuhua Li*

Main category: cs.CV

TL;DR: 本文提出了一种通过空提示（empty prompts）解耦CLIP中模板-样本相似性（TSS）偏差的方法，分预训练和少样本微调两阶段校正模板诱导偏差，从而提升分类准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP在少样本学习中因模板-样本相似性（TSS）引入偏差，导致模型依赖模板邻近性而非真实语义对齐，损害准确率和鲁棒性。

Method: 提出基于空提示的两阶段框架：预训练阶段用空提示揭示并缓解CLIP编码器中的模板偏差；少样本微调阶段引入偏差校准损失，强制图像与正确类别对齐。

Result: 在多个基准上显著降低TSS引发的性能波动，提升分类准确率和鲁棒性。

Conclusion: 空提示可有效解耦CLIP中的模板偏差，使模型更关注图像本身的判别性视觉特征，增强泛化能力。

Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

</details>


### [83] [OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics](https://arxiv.org/abs/2512.08625)
*Jisang Yoo,Gyeongjin Kang,Hyun-kyu Ko,Hyeonwoo Yu,Eunbyung Park*

Main category: cs.CV

TL;DR: 本文提出了OpenMonoGS-SLAM，首个将单目SLAM与开放集语义理解结合的框架，利用视觉基础模型（如MASt3R、SAM、CLIP）实现无需深度输入或语义标注的自监督三维高斯溅射建图与语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM与语义融合方法多依赖深度传感器或闭集语义模型，难以扩展和适应开放世界环境；亟需一种轻量、通用、开放词汇的单目语义SLAM方案。

Method: 基于MASt3R进行单目几何重建，结合SAM和CLIP实现开放词汇语义分割；采用自监督学习目标构建3D高斯溅射地图；设计专用内存机制管理高维语义特征，生成语义增强的高斯特征图。

Result: 在闭集与开放集分割任务中性能媲美或超越现有基线，且完全不依赖深度图或语义真值标注，仅使用单目视频输入。

Conclusion: OpenMonoGS-SLAM成功实现了单目、自监督、开放集语义感知与建图的统一，为开放世界空间智能提供了可扩展新范式。

Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.

</details>


### [84] [Trajectory Densification and Depth from Perspective-based Blur](https://arxiv.org/abs/2512.08627)
*Tianchen Qiu,Qirun Zhang,Jiajian He,Zhengyue Zhuge,Jiahui Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 本文提出了一种通过分析视频流中的模糊模式和密集轨迹来估计度量深度的新方法，结合光学设计与视觉语言模型，在多个深度数据集上表现出色，具有良好的泛化能力和精度。


<details>
  <summary>Details</summary>
Motivation: 相机在无机械稳定器时不可避免地发生旋转运动，导致长曝光下出现与深度位置相关的透视模糊；利用这种深度依赖的模糊特性进行深度估计。

Method: 使用现成的视觉编码器和点跟踪器提取视频信息；通过窗口嵌入和多窗口聚合估计深度图；利用视觉-语言模型对光学算法得到的稀疏轨迹进行稠密化。

Result: 在多个深度数据集上实现了大深度范围内的强性能和良好泛化能力；相比手持拍摄的真实轨迹，光学算法精度更高，稠密重建保持高准确性。

Conclusion: 该方法有效利用了透视模糊的深度依赖性，结合光学建模与多模态学习，为无稳定器条件下的深度估计提供了新思路和实用方案。

Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.

</details>


### [85] [Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning](https://arxiv.org/abs/2512.08639)
*Huilin Xu,Zhuoyang Liu,Yixiang Luomei,Feng Xu*

Main category: cs.CV

TL;DR: 本文提出了一种仅依赖单目RGB图像和自然语言指令的空中视觉-语言导航（Aerial VLN）统一框架，通过多任务学习、关键帧选择与动作标签重加权机制，在降低硬件依赖的同时提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有空中VLN方法依赖全景图像、深度图或里程计，增加了轻量级无人机的系统成本与集成复杂度，难以实际部署。

Method: 将导航建模为下一词预测问题，采用提示引导的多任务学习联合优化空间感知、轨迹推理与动作预测；引入语义关键帧选择策略减少视觉冗余，并设计动作合并与标签重加权机制缓解长尾监督不平衡。

Result: 在Aerial VLN基准上，该方法在单目RGB-only设置下显著优于现有RGB-only基线，且大幅缩小了与先进全景RGB-D方法的性能差距。

Conclusion: 仅用单目RGB和语言指令即可实现高性能空中VLN，验证了轻量化、低成本部署的可行性，推动了真实场景应用（如低空巡检、搜救、空中配送）的发展。

Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

</details>


### [86] [Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation](https://arxiv.org/abs/2512.08645)
*Young Kyung Kim,Oded Schlesinger,Yuzhou Zhao,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: 本文提出了Chain-of-Image Generation（CoIG）框架，将图像生成类比为人类作画的分步语义过程，利用大语言模型（LLM）将复杂文本提示分解为可监控的逐步指令，并由图像生成模型依次执行与编辑，从而提升生成过程的可解释性、可控性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型内部过程不透明（黑箱），难以监控、干预与保障安全可靠；其非人类式生成流程也导致人类难以理解。

Method: 提出CoIG框架：用LLM将文本提示分解为语义清晰、单实体聚焦的逐步指令序列；图像模型依序生成与编辑图像；定义两个新指标——CoIG Readability（评估中间步骤输出的清晰度）和Causal Relevance（量化各步骤对最终图像的因果影响）。

Result: CoIG显著提升了定量可监控性，同时在组合鲁棒性上媲美基线模型；缓解了实体坍缩问题；且框架模型无关，可适配任意图像生成模型。

Conclusion: CoIG通过引入类Chain-of-Thought的分步语义生成范式，有效增强了文本到图像生成的可解释性、可控性与可靠性，为构建可信AI图像生成系统提供了新路径。

Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.

</details>


### [87] [C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition](https://arxiv.org/abs/2512.08647)
*Keito Inoshita*

Main category: cs.CV

TL;DR: 本文提出C-DIRA框架，通过动态ROI路由和对抗学习，在轻量级模型中实现高精度、低计算开销及跨域鲁棒的驾驶员分心行为识别。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级模型难以兼顾实时性、细粒度行为识别能力与跨司机/场景泛化性，ROI方法又带来额外计算负担。

Method: 提出C-DIRA：1）基于显著性的Top-K ROI池化与融合分类进行局部特征提取；2）动态ROI路由，仅对高难度样本启用ROI推理以节省计算；3）伪域标签与对抗学习提升域不变特征表达。

Result: 在State Farm数据集上，相比先前轻量模型，C-DIRA显著降低FLOPs与延迟，同时保持高精度，并在模糊、低光照及未见域下展现强鲁棒性。

Conclusion: C-DIRA有效平衡了模型紧凑性、推理效率与泛化能力，适用于边缘端实时驾驶员行为识别任务。

Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.

</details>


### [88] [Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank](https://arxiv.org/abs/2512.08648)
*Shaofeng Zhang,Xuanqi Chen,Ning Liao,Haoxiang Zhao,Xiaoxing Wang,Haoru Tan,Sitong Wu,Xiaosong Jia,Qi Fan,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了一种无需外部预训练编码器的自包含生成模型训练框架{\mname}，通过内存库动态维护大量负样本并结合低维投影头，实现高效对比学习，在ImageNet-256上以更少步数达到SOTA FID 2.40。


<details>
  <summary>Details</summary>
Motivation: 现有去噪生成模型（如扩散、流匹配）训练成本高、表征学习效率低；引入判别性表征虽有效，但依赖外部预训练编码器带来开销和域偏移问题。

Method: 提出{\mname}框架：采用内存库机制跨迭代动态维护大规模负样本队列，解耦负样本数量与batch size；搭配低维投影头降低内存与带宽开销；全程无需外部编码器。

Result: 在ImageNet-256上仅用400k步即达FID 2.40，显著优于同类方法；训练收敛更快、推理零额外参数与计算开销。

Conclusion: 该方法实现了自包含、高效、高性能的生成建模，缓解了对外部编码器的依赖，提升了训练效率与生成质量。

Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.

</details>


### [89] [Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds](https://arxiv.org/abs/2512.08673)
*Shaofeng Zhang,Xuanqi Chen,Xiangdong Zhang,Sitong Wu,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出了一种新的双分支中心-环绕对比学习框架（CSCon），用于3D点云自监督学习，通过分别掩码中心与周围区域并引入块级对比损失，显著提升了判别性特征表示能力，在多个下游任务协议下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码自编码器（MAE）的生成式自监督方法难以有效捕获高层判别性特征；而2D上成功的对比学习方法在3D中应用受限，尤其难以建模局部几何细节。

Method: 提出双分支Center-Surrounding Contrast（CSCon）框架：对点云的中心区域和周围区域分别进行掩码，构建中心偏向与周围偏向的双输入分支，并设计patch-level对比损失以同时增强高层语义与局部敏感性。

Result: 在MLP-LINEAR、MLP-3和ONLY-NEW协议下达到SOTA，尤其在MLP-LINEAR上较Point-MAE分别提升7.9%、6.7%、10.3%（ScanObjectNN三变体）；在FULL/ALL协议下性能媲美生成式方法。

Conclusion: CSCon验证了对比学习在3D点云自监督中的有效性，通过结构化掩码策略与细粒度对比损失，可兼顾全局判别性与局部几何建模能力，为3D SSL提供了新范式。

Abstract: Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.

</details>


### [90] [What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance](https://arxiv.org/abs/2512.08697)
*Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: 本文提出MoSAIC-ReID框架，通过LoRA专家与属性关联、Oracle路由机制，定量分析行人重识别模型对各类语义属性（如衣着颜色、固有特征）的依赖程度，揭示其可解释性，并指出显式语义知识在实际应用中的整合需求。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法性能优异但缺乏可解释性，难以明确模型实际依赖哪些高层语义属性。

Method: 提出MoSAIC-ReID：基于LoRA的属性专属专家混合框架，配合Oracle路由器实现可控归因分析；结合广义线性模型、统计检验与特征重要性分析进行量化研究。

Result: 在Market-1501和DukeMTMC上达到有竞争力的性能；定量发现衣着颜色和内在特征贡献最大，配饰等稀疏线索影响有限。

Conclusion: MoSAIC-ReID为可解释ReID提供了原理性框架，强调了在实际中融合显式语义知识的必要条件与挑战。

Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID

</details>


### [91] [Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth](https://arxiv.org/abs/2512.08700)
*Kyumin Hwang,Wonhyeok Choi,Kiljoon Han,Wonjoon Choi,Minwoo Choi,Yongcheon Na,Minwoo Park,Sunghoon Im*

Main category: cs.CV

TL;DR: 本文提出了一种面向全环绕单目深度估计（FSMDE）的知识蒸馏新策略，通过混合回归框架结合深度分桶与跨交互、视图关系蒸馏，将大模型的尺度无关深度分布知识迁移到轻量学生网络，实现高精度、实时、度量尺度一致的深度估计。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在单目深度估计中泛化能力强，但直接用于全环绕单目深度估计（FSMDE）面临计算开销大、难以输出度量尺度深度两大问题。

Method: 提出一种新型知识蒸馏策略：1）混合回归框架，融合分类式知识蒸馏与深度分桶模块；2）跨交互知识蒸馏，将教师模型的尺度无关深度 bin 概率蒸馏给学生，并引导其从真值学习度量尺度 bin 中心；3）视图关系知识蒸馏，建模并迁移相邻相机间的结构关系以提升跨视图深度一致性。

Result: 在DDAD和nuScenes数据集上，该方法优于传统监督方法和现有知识蒸馏方法，在性能与效率间取得良好平衡，满足实时性要求。

Conclusion: 所提知识蒸馏方法能有效将基础模型的鲁棒深度知识迁移到轻量FSMDE网络，在保持低计算成本的同时实现高精度、度量尺度一致、跨视图一致的全环绕深度估计。

Abstract: Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.

</details>


### [92] [SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images](https://arxiv.org/abs/2512.08730)
*Kaiyu Li,Shengqi Zhang,Yupeng Deng,Zhi Wang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 本文探索了无需训练的开放词汇语义分割（OVSS）在遥感图像上的应用，基于SAM 3模型提出了一种简单有效的适配方法，包括掩码融合策略和存在性分数过滤机制，显著提升了小目标密集场景下的分割精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的无训练开放词汇语义分割方法在遥感图像中面临精确定位困难、流程复杂等问题，尤其难以处理大量密集且微小的目标；SAM 3提供了一个统一的可提示分割与识别框架，具备潜力但尚未在遥感OVSS中被探索。

Method: 提出一种无需训练的SAM 3适配方法：1）设计掩码融合策略，联合SAM 3的语义分割头与实例头（Transformer解码器）输出以提升地物覆盖；2）利用其存在性头（presence head）得分过滤不存在于场景中的类别，缓解大词表和图像块处理带来的误检问题。

Result: 在多个遥感数据集上实验表明，该轻量适配方法取得了有竞争力的性能，验证了SAM 3在遥感开放词汇分割任务中的有效性与潜力。

Conclusion: SAM 3无需训练即可有效支撑遥感开放词汇语义分割任务，所提出的掩码融合与存在性过滤策略简单高效，为后续研究提供了新思路与开源基线。

Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.

</details>


### [93] [Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting](https://arxiv.org/abs/2512.08733)
*Kuniko Paxton,Zeinab Dehghani,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种基于分布的个体公平性评估与缓解框架，将皮肤色调视为连续属性，利用核密度估计建模，并通过多种统计距离度量和距离驱动重加权（DRW）损失函数提升皮肤病变分类模型在个体层面的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像公平性研究多采用粗粒度的群体分类，忽视个体皮肤色调的连续性差异，难以识别子群内 outlier 所面临的偏见。

Method: 将皮肤色调建模为连续变量，使用核密度估计（KDE）刻画其分布；比较12种统计距离度量；设计距离驱动重加权（DRW）损失函数以缓解少数色调样本的表征不足。

Result: 实验表明：(i) 基于类别的重加权无法有效捕捉个体级差异；(ii) 基于分布的重加权（尤其使用保真相似性FS、Wasserstein距离WD、Hellinger度量HM和调和平均相似性HS）显著提升CNN与Transformer模型的个体公平性表现。

Conclusion: 该分布式方法为皮肤科AI系统提供了更鲁棒的个体公平性保障，并对医学图像中其他敏感连续属性的公平性建模具有普适启示。

Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.

</details>


### [94] [A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation](https://arxiv.org/abs/2512.08747)
*Artúr I. Károly,Péter Galambos*

Main category: cs.CV

TL;DR: 本文提出了一种结合Blender 3D渲染与约束扩散模型的合成数据生成新流程，用于生成高保真、精确标注的双孢蘑菇（Agaricus Bisporus）图像，显著提升了仅用合成数据训练的Mask R-CNN在真实场景中的零样本分割性能（F1达0.859），且具备跨物种和农业场景的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 工业蘑菇栽培中计算机视觉应用受限于高质量标注数据获取成本高；现有合成数据往往 realism 不足，泛化能力差。

Method: 融合Blender 3D建模与约束扩散模型，自动生成带精确实例标注的高保真合成图像；构建并开源两个含6,000张图像（>250k实例）的合成数据集；在零样本设定下训练并评估Mask R-CNN。

Result: 在真实世界数据集M18K上达到F1=0.859的最优分割性能，显著优于仅用传统合成数据训练的方法；验证了方法对其他蘑菇种类及农业检测任务（如果实、叶片）的可迁移性。

Conclusion: 该合成数据生成流程在无需图形学专家的前提下，兼顾可控性、标注精度与视觉真实性，为农业视觉任务提供了高效、可扩展的数据引擎。

Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.

</details>


### [95] [Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices](https://arxiv.org/abs/2512.08751)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种基于输出分布偏度的剪枝方法，用于在联邦学习框架下压缩多模态Swin Transformer模型，实现在边缘设备上高效、隐私保护的医学AI部署。


<details>
  <summary>Details</summary>
Motivation: 高性能视觉模型计算开销大、参数量大，难以部署于边缘设备；医疗数据隐私要求高，限制了集中式训练，因此需要结合模型压缩与联邦学习。

Method: 提出一种偏度引导的剪枝方法，针对多模态Swin Transformer中的多头自注意力和多层感知机层，依据其输出分布的统计偏度进行选择性剪枝，并在水平联邦学习环境中验证。

Result: 在紧凑型Swin Transformer上实现约36%的模型大小缩减，且准确率无损。

Conclusion: 该方法可有效兼顾模型轻量化与隐私保护，为边缘端多模态医学AI提供了可行路径。

Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.

</details>


### [96] [Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance](https://arxiv.org/abs/2512.08765)
*Ruihang Chu,Yefei He,Zhekai Chen,Shiwei Zhang,Xiaogang Xu,Bin Xia,Dingdong Wang,Hongwei Yi,Xihui Liu,Hengshuang Zhao,Yu Liu,Yingya Zhang,Yujiu Yang*

Main category: cs.CV

TL;DR: Wan-Move 是一种简单可扩展的视频生成运动控制框架，通过将物体运动轨迹投影到潜在空间并传播特征，实现对现成图像到视频模型（如 Wan-I2V-14B）的无修改、高精度运动引导，显著提升运动可控性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有运动可控视频生成方法存在控制粒度粗、可扩展性差的问题，难以满足实际应用需求。

Method: 用稠密点轨迹表示物体运动，将其投影至潜在空间，并沿轨迹传播首帧特征以构建对齐的时空特征图，作为原模型的运动引导条件，无需额外运动编码器或架构修改。

Result: 在 MoveBench 和公开数据集上显著优于现有方法；生成5秒、480p视频，运动可控性媲美 Kling 1.5 Pro 的 Motion Brush；发布代码、模型与基准数据。

Conclusion: Wan-Move 提供了一种轻量、通用、可扩展的运动控制范式，为视频生成模型赋予高精度运动编辑能力，推动可控视频生成实用化发展。

Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.

</details>


### [97] [Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps](https://arxiv.org/abs/2512.08774)
*Seoyeon Lee,Gwangyeol Yu,Chaewon Kim,Jonghyuk Park*

Main category: cs.CV

TL;DR: 本文提出了一种名为自精炼扩散（self-refining diffusion）的新框架，利用可解释人工智能（XAI）生成缺陷激活图（FAMs），在前向过程中增强缺陷区域噪声、反向过程中聚焦修复，显著提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中虽成功，但仍存在人工痕迹和不真实区域等质量问题，亟需有效检测与修正机制。

Method: 设计基于XAI的缺陷高亮器生成缺陷激活图（FAMs），并在扩散过程的前向阶段放大缺陷区域噪声、反向阶段针对性优化这些区域。

Result: 在多个扩散模型上Fréchet Inception Distance提升达27.3%，在图像生成、文生图、修复等任务及多种数据集上均表现稳健。

Conclusion: XAI不仅能用于模型解释，还可主动驱动图像质量提升；该框架具有通用性，适用于各类扩散模型与任务，推动图像合成发展。

Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.

</details>


### [98] [LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models](https://arxiv.org/abs/2512.08785)
*Yiming Hao,Mutian Xu,Chongjie Ye,Jie Qin,Shunlin Lu,Yipeng Qin,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出LoFA框架，通过两阶段超网络预测个性化先验，实现视觉生成模型的快速个性化适配，显著优于传统LoRA方法。


<details>
  <summary>Details</summary>
Motivation: 当前个性化视觉生成模型的方法（如LoRA）依赖任务特定数据且优化耗时，而现有超网络方法难以将细粒度用户提示映射到复杂的LoRA分布，实用性受限。

Method: 提出LoFA框架：首先发现LoRA参数相对于基础模型存在结构化分布模式；然后设计两阶段超网络——第一阶段预测相对分布模式以定位关键适配区域，第二阶段据此指导最终LoRA权重预测。

Result: 实验表明LoFA能在数秒内跨多种任务和用户提示稳定预测高质量个性化先验，性能甚至超越需数小时训练的传统LoRA。

Conclusion: LoFA为视觉生成模型的高效个性化提供了通用、实用的新范式，解决了现有方法在速度与泛化性上的瓶颈。

Abstract: Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.

</details>


### [99] [MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance](https://arxiv.org/abs/2512.08789)
*Chaewon Kim,Seoyeon Lee,Jonghyuk Park*

Main category: cs.CV

TL;DR: 本文提出了一种名为MatteViT的文档阴影去除框架，结合空间与频域信息，通过高频放大模块（HFAM）和连续亮度驱动的阴影遮罩（shadow matte）策略，在去除阴影的同时高保真保留文本边缘等高频细节，显著提升了下游OCR任务性能。


<details>
  <summary>Details</summary>
Motivation: 文档阴影会遮挡或扭曲文本边缘、线条等高频率细节，影响数字化文档清晰度与下游任务（如OCR）效果，因此需在去阴影过程中精准保留这些细粒度结构。

Method: 提出MatteViT模型：1）引入轻量级高频放大模块（HFAM），对图像高频成分进行分解与自适应增强；2）构建基于亮度连续性的阴影遮罩（shadow matte），由自建遮罩数据集与遮罩生成器联合训练，为网络提供早期精确空间引导。

Result: 在RDD和Kligler公开基准上达到SOTA性能；在OCR等下游任务中显著优于先前方法，文本细节恢复更完整、识别准确率更高。

Conclusion: MatteViT通过融合空间与频域先验、设计高频增强与精确阴影遮罩机制，实现了高质量、实用性强的文档阴影去除，兼顾视觉质量与实际应用价值。

Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.

</details>


### [100] [Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning](https://arxiv.org/abs/2512.08820)
*Yi Zhang,Chun-Wun Cheng,Junyi He,Ke Yu,Yushun Tang,Carola-Bibiane Schönlieb,Zhihai He,Angelica I. Aviles-Rivero*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的双曲适配器（T-DHA）方法，利用双曲空间建模视觉-语言概念间的层次关系，提升跨域泛化与少样本图像识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在跨域场景下性能下降，或需大量计算资源进行微调，亟需高效、轻量、泛化性强的适配方法。

Method: 提出Training-free Dual Hyperbolic Adapters（T-DHA），将视觉-语言语义关系映射至双曲空间（Poincaré球模型），利用其指数级体积增长特性更好建模层次结构，并结合负学习提升分类鲁棒性与判别力。

Result: 在多个数据集上的实验表明，T-DHA在少样本图像识别和域泛化任务中显著优于现有SOTA方法。

Conclusion: 双曲空间建模为VLM跨域适应提供了新范式，T-DHA实现了高性能、免训练、低维特征的统一，具有强实用性与可扩展性。

Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.

</details>


### [101] [InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829)
*Hongyuan Tao,Bencheng Liao,Shaoyu Chen,Haoran Yin,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: InfiniteVL is a linear-complexity Vision-Language Model that combines sliding window attention and Gated DeltaNet to overcome limitations of window-based and linear attention methods, achieving competitive performance with low resource cost and high inference speed.


<details>
  <summary>Details</summary>
Motivation: Window-based VLMs degrade when sequence length exceeds window size; linear attention underperforms on information-intensive tasks like OCR and document understanding.

Method: Proposes InfiniteVL, integrating sliding window attention (SWA) with Gated DeltaNet, and employs a three-stage training strategy: distillation pretraining, instruction tuning, and long-sequence SFT.

Result: Outperforms prior linear-complexity VLMs, matches top Transformer-based VLMs, achieves >3.6× inference speedup over FlashAttention-2-accelerated models, maintains constant latency/memory, and sustains 24 FPS in streaming video understanding.

Conclusion: InfiniteVL delivers efficient, scalable, and high-performance multimodal understanding under resource constraints, enabling practical long-sequence and real-time applications.

Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

</details>


### [102] [Generation is Required for Data-Efficient Perception](https://arxiv.org/abs/2512.08854)
*Jack Brady,Bernhard Schölkopf,Thomas Kipf,Simon Buchholz,Wieland Brendel*

Main category: cs.CV

TL;DR: 本文探讨了生成式与非生成式视觉模型在实现人类水平视觉感知（特别是组合泛化）方面的能力差异，理论证明生成式方法通过解码器约束与反演可自然施加必要归纳偏置，而非生成式方法难以做到；实验验证了生成式方法在无需额外数据的情况下显著提升组合泛化性能。


<details>
  <summary>Details</summary>
Motivation: 探究生成式建模是否对实现人类水平的视觉感知（尤其是组合泛化这一关键能力）是必要的，挑战当前主流非生成式视觉模型的有效性边界。

Method: 通过形式化组合数据生成过程，理论分析生成式（基于解码器反演）与非生成式（基于编码器）方法实现组合泛化的必要归纳偏置；证明编码器上施加该偏置在实践中不可行，而解码器上则可行，并支持梯度搜索或生成回放等高效反演方式；辅以在逼真图像数据集上的实证比较。

Result: 理论表明：生成式方法能自然、可行地满足组合泛化所需的归纳偏置；非生成式方法则难以通过正则化或架构设计满足；实验显示，缺乏相应偏置的非生成式模型组合泛化能力差，需大规模预训练或额外监督；而生成式模型仅靠解码器约束+搜索/回放即可显著提升泛化性能。

Conclusion: 生成式建模并非冗余，而是实现强组合泛化能力的关键路径；其核心优势在于可通过解码器设计与反演机制有效引入并利用必要归纳偏置，为构建更类人的视觉系统提供了理论依据与实践方向。

Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.

</details>


### [103] [Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference](https://arxiv.org/abs/2512.08860)
*Amit Bendkhale*

Main category: cs.CV

TL;DR: 本文提出Tri-Bench基准，用于评估视觉语言模型（VLMs）在平面三角形几何推理任务中的可验证性与鲁棒性，发现当前VLMs严重依赖2D图像线索、忽略提示中给出的参考系（如正方形边框），对相机姿态变化和少数类三角形识别能力差，而日常物体干扰影响不显著。


<details>
  <summary>Details</summary>
Motivation: 提升视觉语言模型在真实场景下几何推理的可验证性与可控性，解决其在相机姿态变化和复杂场景中性能下降的问题。

Method: 构建Tri-Bench——一个聚焦平面三角形几何推理的小型基准，控制变量包括相机姿态（平面/倾斜）和场景干扰（10种日常物体）；使用统一固定提示（含显式正方形边框作为参考系），通过单应性实现可验证推理；在6个二值与连续任务上评估4个主流VLMs。

Result: VLMs整体3D几何准确率仅约69%（最高75%，最低64%），2D图像平面准确率略高（~72%）；对等边、等腰、直角三角形识别准确率近0%；相机倾斜导致准确率下降约4.1%；物体干扰无显著影响。

Conclusion: 当前VLMs未能有效利用提示中提供的参考系信息，本质上仍依赖不可靠的2D图像统计线索，缺乏真正的几何理解与坐标系意识，亟需引入结构化几何先验与可验证推理机制。

Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.

</details>


### [104] [Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning](https://arxiv.org/abs/2512.08873)
*Jing Jie Tan,Anissa Mokraoui,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: 本文提出SOLI方法，利用Siamese网络优化低分辨率图像的潜在嵌入，以实现轻量级、高效的图像描述生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型（如Transformer）的方法在处理低分辨率图像时计算和内存开销大，难以在资源受限场景下重训练。

Method: 采用Siamese网络架构构建双通路神经网络，优化低分辨率图像的潜在嵌入表示，提升图像到文本转换的效率与准确性。

Result: SOLI在保持性能的同时显著降低计算开销，适用于资源受限环境下的训练与部署。

Conclusion: SOLI为低分辨率图像描述任务提供了一种轻量、高效且可训练性强的新范式。

Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.

</details>


### [105] [SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing](https://arxiv.org/abs/2512.08881)
*Aysim Toker,Andreea-Maria Oncescu,Roy Miles,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新型结构化定位机制，通过在预训练视觉语言模型（VLM）上微调指令跟随任务，并引入专用控制标记连接定位模块，显著提升了卫星图像中目标的精确定位能力，在多个遥感基准测试中达到SOTA，视觉定位任务相对提升24.8%。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在遥感领域虽具潜力，但在复杂卫星图像中精确视觉定位能力仍不足，需增强其对空间信息的联合推理能力。

Method: 在预训练VLM基础上，针对多样化指令跟随任务进行微调，并通过专用控制标记将独立的定位模块接入模型，实现语言与空间信息的联合建模与推理。

Result: 在多个遥感基准上均超越先前方法，视觉定位任务取得24.8%的相对性能提升。

Conclusion: 将结构化空间推理机制融入VLM可显著提升其在真实卫星数据分析中的可靠性与实用性。

Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.

</details>


### [106] [No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers](https://arxiv.org/abs/2512.08889)
*Damiano Marsili,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注的视觉推理训练框架，通过LLM和VLM双验证器分别优化推理链与视觉定位，显著提升空间关系理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法存在两大问题：纯语言链式推理需大量带标注的图文数据；程序合成方法虽免训练但逻辑错误多、定位不准。

Method: 构建AI驱动的双验证器框架：LLM验证器通过强化学习优化推理过程；VLM验证器通过自动难例挖掘增强视觉定位能力，全程无需真实标签。

Result: 在多种空间推理任务上超越开源及闭源模型，并优于近期纯文本视觉推理方法。

Conclusion: 结合大语言模型的推理分解能力与视觉大模型的定位优化能力，可实现高效、准确且无需标注的视觉推理。

Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

</details>


### [107] [UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation](https://arxiv.org/abs/2512.08897)
*Zeyang Liu,Le Wang,Sanping Zhou,Yuxuan Wu,Xiaolong Sun,Gang Hua,Haoxiang Li*

Main category: cs.CV

TL;DR: 本文提出UniLayDiff，一种统一的扩散Transformer模型，首次用单个端到端可训练模型解决多种内容感知布局生成任务，通过多模态扩散架构融合背景图像、布局元素及多样化约束，并利用LoRA微调引入关系约束，实现高质量统一生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以用单一模型统一处理内容感知布局生成中多样化的输入约束（如元素类型、尺寸、关系等），或需为不同条件单独设计模型参数，缺乏真正统一的解决方案。

Method: 提出UniLayDiff：基于多模态扩散Transformer框架，将布局约束视为独立模态，联合建模背景图像、布局元素与各类约束；并通过LoRA对预训练模型进行关系约束微调。

Result: 在无条件至各类条件生成任务上均达到SOTA性能，是首个统一覆盖全部内容感知布局生成子任务的模型。

Conclusion: UniLayDiff实现了真正统一、灵活且高质量的内容-aware布局生成，验证了多模态扩散架构与参数高效微调策略在该任务中的有效性与泛化能力。

Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.

</details>


### [108] [Self-Evolving 3D Scene Generation from a Single Image](https://arxiv.org/abs/2512.08905)
*Kaizhi Zheng,Yue Fan,Jing Gu,Zishuo Xu,Xuehai He,Xin Eric Wang*

Main category: cs.CV

TL;DR: EvoScene 是一种无需训练的自演化框架，通过结合3D生成与视频生成模型的优势，从单张图像逐步重建高质量、纹理丰富的完整3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有单图到3D方法受限于物体中心化训练，难以泛化至结构复杂、尺度大的真实场景，且几何稳定性与纹理一致性不足。

Method: 提出三阶段迭代流程：空间先验初始化、视觉引导的3D场景网格生成、空间引导的新视角生成，在2D与3D域间交替优化结构与外观。

Result: 在多种场景上显著优于强基线，展现出更优的几何稳定性、视角一致的纹理及未见区域补全能力，并输出可直接使用的3D网格。

Conclusion: EvoScene 证明了无需训练、融合多模态先验的自演化策略是提升单图3D场景重建质量的有效新范式。

Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.

</details>


### [109] [Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration](https://arxiv.org/abs/2512.08922)
*Jin Hyeon Kim,Paul Hyunbin Cho,Claire Kim,Jaewon Min,Jaeeun Lee,Jihye Park,Yeji Choi,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出UniT框架，通过融合扩散变换器（DiT）、视觉语言模型（VLM）和文本检测模块（TSM），在文本感知图像恢复（TAIR）任务中实现高保真文本重建并显著抑制文本幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在文本中心图像恢复任务中因缺乏显式语言知识而易产生文本幻觉，需引入语言理解与文本定位能力以提升文本恢复保真度。

Method: 提出UniT统一框架：VLM从退化图像中提取文本内容提供语言引导；TSM基于扩散特征在每步去噪中生成OCR预测，辅助VLM迭代优化引导；DiT主干网络利用上述多源线索进行高质量文本恢复。

Result: 在SA-Text和Real-Text数据集上，UniT显著降低文本幻觉，端到端F1分数达到SOTA水平。

Conclusion: UniT通过视觉-语言协同与迭代精炼机制，有效解决了TAIR任务中文本幻觉与结构失真难题，验证了显式语言建模对文本密集图像恢复的关键作用。

Abstract: Text-Aware Image Restoration (TAIR) aims to recover high-quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong generative priors for general image restoration, they often produce text hallucinations in text-centric tasks due to the absence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that integrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an iterative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermediate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine-grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.

</details>


### [110] [Efficiently Reconstructing Dynamic Scenes One D4RT at a Time](https://arxiv.org/abs/2512.08924)
*Chuhan Zhang,Guillaume Le Moing,Skanda Koppula,Ignacio Rocco,Liliane Momeni,Junyu Xie,Shuyang Sun,Rahul Sukthankar,Joëlle K Barral,Raia Hadsell,Zoubin Ghahramani,Andrew Zisserman,Junlin Zhang,Mehdi SM Sajjadi*

Main category: cs.CV

TL;DR: 本文提出D4RT模型，利用统一的Transformer架构从单个视频中联合推断深度、时空对应关系和完整相机参数，通过新颖的查询机制实现高效轻量的4D场景重建。


<details>
  <summary>Details</summary>
Motivation: 从视频中理解并重建动态场景的复杂几何与运动仍是计算机视觉中的重大挑战。

Method: D4RT采用统一的Transformer架构，引入一种新颖的查询机制，避免密集逐帧解码和多任务专用解码器，支持对任意时空点的3D位置进行独立灵活查询。

Result: 该方法在多种4D重建任务上达到新SOTA，训练和推理效率显著提升，具备高度可扩展性。

Conclusion: D4RT是一种简单而强大的前馈模型，为动态场景的4D重建提供了高效、轻量且通用的新范式。

Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.

</details>


### [111] [Astra: General Interactive World Model with Autoregressive Denoising](https://arxiv.org/abs/2512.08931)
*Yixuan Zhu,Jiaqi Feng,Wenzhao Zheng,Yuan Gao,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了Astra，一种交互式通用世界模型，用于从过去观测和动作中预测长期未来视频，支持多种现实场景（如自动驾驶、机器人抓取）和精确动作控制（如相机运动、机器人动作）。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在通用场景下长时程未来预测能力不足，尤其难以处理多样化的动作形式。

Method: 提出自回归去噪架构，引入时间因果注意力机制、噪声增强的历史记忆模块、动作感知适配器以及动作专家混合路由机制。

Result: Astra在保真度、长程预测能力和动作对齐性方面优于现有最先进世界模型。

Conclusion: Astra实现了交互式、一致且通用的长期视频预测，支持多种真实世界交互形式。

Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [112] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出短上下文主导假设，即大多数序列仅需较短的局部前缀即可准确预测下一个token；通过测量最小上下文长度（MCL）验证该假设，并设计无需真实标签的代理指标DaMCL用于识别长上下文困难样本；最后提出一种解码算法，利用DaMCL检测并增强长程相关token，从而缓解短上下文偏差，提升问答任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长上下文建模中可能存在短上下文主导偏差，即模型过度依赖局部信息而忽略长程依赖，影响对真正需要长上下文的任务的表现。

Method: 1) 定义并测量最小上下文长度（MCL）以量化短上下文主导现象；2) 提出分布感知的MCL（DaMCL）作为无需真实next-token的实用代理指标；3) 设计基于DaMCL检测结果的解码策略，在生成时动态增强长程相关token。

Result: 实验表明：1) 75-80%的1–7k token长文档序列仅需最后96个token即可实现全上下文级预测精度；2) DaMCL通过简单阈值即可高精度区分长/短上下文序列；3) 所提解码算法在多个Q&A任务和模型架构上均提升性能。

Conclusion: 短上下文主导是当前LLM中的显著现象，但可通过可学习、可部署的检测与校正机制缓解其带来的输出偏差，为长上下文建模提供新思路。

Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [113] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种分层多智能体架构，通过64×64轻量级智能体网格与选择性Oracle协同，结合空间课程学习与基于负对数似然（NLL）的置信度评估和Thompson采样策略，提升长程推理稳定性与效率，并在空间化汉诺塔基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型与多智能体系统在处理长时序推理任务时存在推理能力不足与计算成本过高的问题。

Method: 构建分层多智能体架构，将推理分布于64×64轻量级智能体网格；引入空间课程学习机制，由中心向边缘渐进扩展操作区域；采用负对数似然（NLL）衡量智能体置信度与校准性；由Thompson Sampling驱动的课程管理器动态选择训练区域。

Result: 在空间化Tower of Hanoi基准测试中，该方法显著提升了推理稳定性、降低了Oracle调用次数，并增强了长程推理能力。

Conclusion: 分布式轻量智能体协同配合课程驱动的自适应训练机制，可有效缓解长时序推理中的可靠性与效率瓶颈。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [114] [Adaptation of Embedding Models to Financial Filings via LLM Distillation](https://arxiv.org/abs/2512.08088)
*Eliot Brenner,Dominic Seyler,Manjunath Hegde,Andrei Simion,Koustuv Dasgupta,Bing Xiang*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工标注、基于无标签语料和教师-学生交互式挖掘的可扩展检索模型微调方法，显著提升了金融领域检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有通用嵌入模型在金融等专业领域的信息检索性能不足，而人工标注成本高、计算开销大、延迟要求严，亟需低成本、高精度的领域适配方案。

Method: 以通用检索嵌入模型为基座，通过教师-学生框架在无标签金融语料上迭代进行：1）用当前学生模型检索挖掘难正例/负例；2）用LLM判断的相关性标签蒸馏知识；3）重新训练学生模型。整个流程不依赖人工标注。

Result: 在21,800个查询-文档对上，MRR@5提升27.7%，mDCG@5提升44.6%；FinanceBench中3/4文档类NDCG提升。

Conclusion: 该方法有效弥合了通用嵌入模型与专业领域（如金融）之间的性能鸿沟，是一种高效、可扩展且免人工标注的领域适配新范式。

Abstract: Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\texttt{@}$5, 44.6% improvement in mean DCG$\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.

</details>


### [115] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: 本文提出了一种名为Segment, Embed, and Align (SEA)的通用字幕对齐方法，用于将口语字幕与连续手语视频对齐。该方法不依赖于特定语言或数据集的端到端训练，而是利用两个预训练模型分别进行视频分段和跨模态嵌入，并通过轻量级动态规划完成对齐，具备高效、多语言、多领域适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于特定语言或数据集的端到端训练，泛化能力差，难以推广到不同手语或场景；亟需一种通用、灵活且高效的字幕-手语视频对齐框架。

Method: 提出SEA框架：1）用预训练模型对视频帧序列进行逐个手语动作分割；2）用另一预训练模型将每个手语片段与对应文本嵌入到共享隐空间；3）采用轻量级CPU可运行的动态规划算法完成时间对齐。

Result: 在四个手语数据集上达到当前最优对齐性能；支持从小型词典到大规模连续语料的多种资源设置；对齐一小时视频仅需约一分钟（CPU）；代码与模型已开源。

Conclusion: SEA是一种通用、高效、开源的手语-字幕对齐框架，显著提升了跨语言和跨域手语处理中平行数据的构建能力，为手语理解与生成研究提供了坚实基础。

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [116] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出了一种通用对抗后缀（universal adversarial suffixes），即短小的token序列（4-10个token），通过可微软训练与Gumbel-Softmax松弛学习，再离散化用于推理，能广泛降低多种语言模型在多个NLP任务上的准确率和校准置信度，且具备跨任务、跨模型家族的良好迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常针对特定任务或模型优化触发器，导致结果难以比较且泛化能力差；本文旨在设计一种通用、轻量、可迁移的对抗后缀，提升攻击的普适性和实用性。

Method: 采用Gumbel-Softmax松弛实现可微分的“软”后缀学习，训练目标为最大化标签区域的校准交叉熵，并掩码真实标签token以防止信息泄露，同时加入熵正则化避免后缀坍缩；最后将软后缀离散化为实际token序列。

Result: 单个后缀在Qwen2-1.5B、Phi-1.5和TinyLlama-1.1B上训练后，能有效迁移到其他模型，在情感分析、自然语言推理、复述检测、常识问答和物理推理等多个任务中显著降低准确率与校准置信度。

Conclusion: 通用对抗后缀是一种高效、可迁移的黑盒攻击手段，揭示了当前语言模型在零/少样本分类场景下对输入后缀的高度敏感性，为模型鲁棒性评估与防御提供了新视角。

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [117] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习（PPO）的对抗后缀生成方法，通过将后缀建模为策略、以冻结语言模型为奖励源，并采用校准交叉熵奖励塑造，提升了对抗后缀在多任务、多模型间的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗后缀生成方法（如梯度搜索或规则法）鲁棒性差，泛化能力弱，难以跨任务和模型迁移。

Method: 将对抗后缀视为策略，使用PPO算法在冻结语言模型上进行训练；奖励函数采用校准交叉熵，消除标签偏差，并聚合多种表面形式以增强迁移性。

Result: 在5个NLP基准数据集（涵盖情感分析、自然语言推理、复述、常识推理）和3个不同语言模型（Qwen2-1.5B Instruct、TinyLlama-1.1B Chat、Phi-1.5）上验证，RL生成的后缀显著降低准确率，且迁移效果优于以往同类对抗触发器。

Conclusion: 基于强化学习的对抗后缀生成框架更鲁棒、更具泛化性，为评估和提升语言模型鲁棒性提供了新思路。

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [118] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub 是一个交互式搜索平台，整合 ClinicalTrials.gov 数据并利用大语言模型（如 GPT-5.1、Gemini-3-Pro）从 PubMed 文章中自动提取结构化临床试验信息，提升结构化数据可及性 83.8%，支持证据驱动的问答与用户研究验证。


<details>
  <summary>Details</summary>
Motivation: 提升临床试验数据的可访问性与可用性，弥合 ClinicalTrials.gov 数据覆盖不足的缺口，服务患者、临床医生、研究人员和政策制定者，推动循证医学发展。

Method: 构建 ClinicalTrialsHub 平台，融合 ClinicalTrials.gov 数据；利用大语言模型（GPT-5.1、Gemini-3-Pro）对 PubMed 全文进行结构化信息抽取；实现自然语言查询到结构化数据库查询的翻译；构建可归因的问答系统，答案链接至原文句子；通过用户研究（临床医生、研究人员、博士生）和自动评估验证性能。

Result: 相较仅使用 ClinicalTrials.gov，结构化临床试验数据可及性提升 83.8%；问答系统提供证据支撑的答案并可追溯至源句；用户研究与自动评估证实其在信息抽取与问答任务上的有效性。

Conclusion: ClinicalTrialsHub 成功拓展了高质量结构化临床试验数据的来源与交互方式，证明大语言模型在生物医学信息整合与可解释问答中的实用价值，为循证决策提供了新工具。

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [119] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: 本文通过复现Boukes（2024）的手动标注，系统考察了多种大语言模型（GLLMs）在五个政治传播概念上的标注偏差；发现尽管F1分数尚可，但模型标注与人工标注在标注率、下游分析结果及标注一致性上存在显著系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（GLLMs）在社会科学内容标注中是否存在系统性偏差，尤其关注其与人工标注的一致性及对下游研究结论的影响。

Method: 采用Llama3.1:8b、Llama3.3:70b、GPT-4o、Qwen2.5:72b四种GLLM，结合五种提示词，对政治内容、互动性、理性、无礼性、意识形态五个概念进行标注，并与Boukes（2024）的人工标注结果对比，评估F1分数、标注率、跨模型一致性及下游分析差异。

Result: GLLMs在F1分数上表现尚可，但标注 prevalence 显著偏离人工标注，导致下游实证结果不同；各模型间标注高度相似，却共同偏离人工标准，表明存在系统性偏差；F1指标无法有效捕捉该偏差程度。

Conclusion: F1等传统指标不足以评估GLLM用于社会科学研究标注的可靠性；需警惕其隐性系统性偏差，不能仅凭高F1分数即替代人工标注。

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [120] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: 本文通过对比解释和显著性归因方法，探究翻译模型在处理性别模糊源数据时选择特定性别词形的决策依据，发现模型对源词的显著性归因与人类性别感知存在明显重叠，并据此提出利用该理解来缓解性别偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于测量模型中的性别偏见，而本文旨在深入探索其根源，特别是翻译模型如何基于源句上下文（输入词元）做出目标语言中的性别词形选择。

Method: 使用对比解释（contrastive explanations）和显著性归因（saliency attribution）方法，分析性别模糊自然源数据中各源词对模型性别决策的影响；设定不同归因阈值进行检验；将显著源词与人类性别感知进行比对；并开展语言学分析。

Result: 发现模型对源词的显著性归因与人类对这些词的性别感知高度一致；识别出若干在性别决策中起关键作用的语言模式和词汇类型。

Conclusion: 理解模型在性别相关翻译决策中的行为机制至关重要，其与人类判断的相似性表明可借助此类可解释性分析来诊断和缓解翻译模型中的性别偏见。

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [121] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: 本文提出了一种软归纳偏置方法，通过显式定义推理视角来引导韩语大语言模型（Kanana-1.5）进行不当话语检测，显著提升了检测准确率（达87.0046，提升约3.89%）。


<details>
  <summary>Details</summary>
Motivation: 在线游戏和社区中因匿名性导致的不当言论升级为言语暴力甚至犯罪行为，亟需有效技术检测不当话语以构建安全交流环境。

Method: 提出软归纳偏置方法，显式定义推理视角以引导模型推理；在韩语大语言模型（Kanana-1.5）上进行微调，并对比不同训练策略的定量与定性效果。

Result: Kanana-1.5模型平均准确率达87.0046，较标准监督学习提升约3.89%；该方法能实现更精准、一致的判断，超越简单知识模仿。

Conclusion: 所提方法通过约束推理视角，显著提升韩语不当话语检测性能，验证了其在构建安全通信环境中的有效性与可行性。

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [122] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: 本教程全面介绍医疗领域自然语言处理（HealthcareNLP）的关键子领域，涵盖数据资源、NLP评估与患者三层架构，并强调合成数据生成、可解释临床NLP、检索增强生成及大模型与知识图谱融合等前沿方向，面向初学者提供入门性、实践性内容。


<details>
  <summary>Details</summary>
Motivation: 现有综述存在覆盖不全问题，如忽视合成数据生成以解决隐私问题、可解释临床NLP以促进落地，以及未提及检索增强生成和大语言模型与知识图谱的神经符号融合等重要方法。

Method: 采用三层结构化框架：数据/资源层（含标注规范、伦理审批、治理、合成数据）、NLP-Eval层（涵盖NER、关系抽取、情感分析、编码链接等任务及可解释HealthAI）、患者层（含公众参与、健康素养、翻译简化、摘要及共享决策支持），并辅以动手实践环节。

Result: 构建了一个系统化、分层化的HealthcareNLP教学体系，整合前沿技术方向与实际应用需求，提升跨学科理解与实践能力。

Conclusion: 该教程填补了当前HealthcareNLP综述的空白，为NLP研究者、医疗从业者及学生提供兼具广度、深度与实操性的入门指南，推动可信赖、以人为本的医疗AI发展。

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [123] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN是一个开源Python框架，用于系统生成问卷式提示的响应，支持基于大语言模型（LLM）的仿真调查与标注任务，并提供无代码界面以提升实验可复现性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 提升LLM在问卷调查与标注任务中的可复现性、可靠性与评估鲁棒性，降低计算成本。

Method: 开发开源Python框架QSTN，支持问卷呈现方式、提示扰动和响应生成方法的系统化评估，并集成无代码用户界面。

Result: 在超4000万条模拟调查响应的评估中，发现问题结构与响应生成方法显著影响模型输出与人类答案的一致性，且计算开销大幅降低。

Conclusion: QSTN为LLM驱动的社会科学与人机交互研究提供了高效、易用、可复现的工具支持。

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [124] [An Agentic AI System for Multi-Framework Communication Coding](https://arxiv.org/abs/2512.08659)
*Bohao Yang,Rui Yang,Joshua M. Biro,Haoyuan Wang,Jessica L. Handley,Brianna Richardson,Sophia Bessias,Nicoleta Economou-Zavlanos,Armando D. Bedoya,Monica Agrawal,Michael M. Zavlanos,Anand Chowdhury,Raj M. Ratwani,Kai Sun,Kathryn I. Pollak,Michael J. Pencina,Chuan Hong*

Main category: cs.CL

TL;DR: 本文提出了一种基于LangGraph的多框架结构化智能体AI系统MOSAIC，用于临床医患对话分析，通过四个协同智能体实现高精度、可解释、可靠的多代码本自动标注，在测试集上达到0.928的整体F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的临床对话标注方法多为单任务模型，缺乏跨框架与跨领域的适应性、可解释性与可靠性；而人工标注又费时、不一致且难以规模化。

Method: 构建了名为MOSAIC的多智能体系统，包含Plan Agent（代码本选择与流程规划）、Update Agent（动态更新检索数据库）、Annotation Agents（基于代码本的RAG+动态少样本提示）和Verification Agent（一致性校验与反馈），基于LangGraph架构编排。

Result: 在50条测试转录本（涵盖风湿科与妇产科）上，MOSAIC整体F1达0.928，其中风湿科子集达0.962；在‘患者行为’维度表现最优；消融实验表明其显著优于基线模型。

Conclusion: MOSAIC通过模块化、可解释的多智能体协同机制，有效提升了临床沟通分析的自动化水平、泛化能力与可信度，为跨领域医疗对话理解提供了新范式。

Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

</details>


### [125] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 本文发布了首个面向巴斯克语C1水平的自动作文评分（AES）与反馈生成公开数据集，包含3200篇由专家标注的作文及详细反馈；通过微调RoBERTa-EusCrawl和Latxa等开源模型，在评分与反馈生成任务上超越GPT-5、Claude Sonnet 4.5等闭源SOTA系统，并提出结合自动指标与专家验证的新反馈评估方法。


<details>
  <summary>Details</summary>
Motivation: 解决巴斯克语等低资源语言在自动作文评分与教育反馈生成领域缺乏高质量公开数据集和可复现基准的问题，推动透明、可复现且教学导向的NLP研究。

Method: 构建首个巴斯克语C1级AES数据集（含3200篇作文及多维度专家标注与错误示例），微调RoBERTa-EusCrawl（编码器）和Latxa 8B/70B（解码器）模型用于评分与反馈生成，并提出融合自动一致性指标与专家验证的新型反馈评估方法。

Result: 微调后的Latxa模型在评分一致性与反馈质量上超越GPT-5和Claude Sonnet 4.5；能生成符合评分标准、具有教学意义的反馈，并识别更广泛的错误类型；新评估方法验证了反馈的准则对齐性与实用性。

Conclusion: 该工作为低资源语言的教育NLP提供了首个开源数据集、强基线模型与可靠评估框架，显著提升了AES与反馈生成在真实教学场景中的适用性与可信度。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [126] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 本文提出了一种针对低资源语言的后训练方法，通过on-policy偏好优化，在缺乏目标语言原生指令微调数据的情况下，实现流利的语言模型对齐，且无需依赖难以获取的数据。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏母语者编写的高质量数据集和能生成流利合成数据的语言模型，现有偏好优化研究主要集中于英语和中文，难以直接迁移。

Method: 采用on-policy训练方法进行偏好优化，并与基于机器翻译数据的监督微调、多语言联合微调两种基线方法对比。

Result: 在挪威语（Bokmål）上的案例研究表明，on-policy方法显著优于其他方法，在母语者评估中展现出更高流利度，且不依赖任何目标语言的原生指令数据。

Conclusion: on-policy偏好优化是提升低资源语言模型流利度的关键，为无原生数据场景下的语言模型对齐提供了可行路径。

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [127] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出了一种在联邦学习环境中对齐大语言模型与多样化人类偏好的新方法，通过自适应奖励聚合策略，在保证对齐质量的同时提升公平性。


<details>
  <summary>Details</summary>
Motivation: 标准联邦学习方法难以充分表征多样化的人类观点，导致LLM对不同群体的偏好对齐不足。

Method: 构建了系统评估框架，比较多种奖励聚合策略（min/max/avg），并提出一种基于群体历史对齐表现动态调整偏好权重的自适应方案；各组本地生成奖励信号，服务器仅聚合组级奖励而不访问原始数据。

Result: 在PPO-based RLHF问答任务实验中，所提自适应方法在保持竞争力的对齐性能的同时，显著提升了公平性。

Conclusion: 该工作为跨多元人群评估和训练公平、包容的大语言模型提供了稳健的方法论和实用解决方案。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [128] [Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts](https://arxiv.org/abs/2512.08814)
*Yifan Lyu,Liang Zhang*

Main category: cs.CL

TL;DR: ROME is a novel framework that improves personality detection by using LLMs to simulate user responses to psychometric questionnaires, thereby generating interpretable, questionnaire-grounded evidence and enabling richer supervision and semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing personality detection methods suffer from label scarcity and vague semantic mappings between user language and psychological constructs.

Method: ROME uses LLMs' role-play capability to generate questionnaire-style answers from user posts; employs a question-conditioned Mixture-of-Experts module for question answering under explicit supervision; and integrates answer vectors with user representations in a multi-task learning framework.

Result: ROME outperforms state-of-the-art baselines on two real-world datasets, achieving a 15.41% improvement on the Kaggle dataset.

Conclusion: Injecting structured psychological knowledge via simulated questionnaire responses significantly enhances both performance and interpretability of personality detection models.

Abstract: Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a "posts -> user vector -> labels" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).

</details>


### [129] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: 本文揭示了渐进式增加Transformer深度（如MIDAS方法）提升推理性能的机制，指出其通过改善深层利用率、改变残差流结构及形成可置换计算模块来克服‘深度诅咒’，并提出一种轻量改进版MIDAS，在下游推理任务中进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有渐进式加深方法（如MIDAS）虽被证实有效，但缺乏对其性能提升机制的深入理解；同时，标准非增长型Transformer存在‘深度诅咒’——后半层贡献小，深度利用不足。

Method: 采用逐层分析方法，研究渐进式中间堆叠（gradual middle stacking）对残差流结构、深度利用效率和计算模块可置换性的影响，并提出MIDAS的轻量级改进方案。

Result: 验证了渐进式加深能更有效地利用模型深度、重塑残差流、促成可置换计算块；所提改进版MIDAS在下游推理基准上取得进一步提升。

Conclusion: 渐进式增长模型深度不仅降低训练成本，更能催生结构化的计算电路，从根本上缓解标准Transformer中深度利用低效的问题。

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [130] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 本文提出RAGLens，一种基于稀疏自编码器（SAE）的轻量级RAG幻觉检测器，利用大模型内部表征识别幻觉特征，无需大量标注数据或外部大模型评判，在保证高准确率的同时提供可解释性依据。


<details>
  <summary>Details</summary>
Motivation: 现有RAG幻觉检测方法依赖大量标注数据训练检测器或调用外部大模型评判，成本高、效率低；基于内部表征的方法精度有限。本文受机制可解释性研究启发，探索如何更有效地从LLM内部激活中识别幻觉信号。

Method: 采用稀疏自编码器（SAE）解耦LLM内部激活，结合信息论驱动的特征选择与加性特征建模，构建轻量级检测器RAGLens，直接利用LLM前馈层内部表征进行幻觉判别。

Result: RAGLens在幻觉检测任务上性能优于现有方法，具备高准确率与强可解释性，能为每条检测结果提供可理解的归因依据，并支持事后干预修正不忠实输出。

Conclusion: RAGLens验证了利用LLM内部机制信号进行高效、可解释幻觉检测的可行性，揭示了幻觉相关神经特征在模型中的分布规律，为可信RAG系统提供了新思路与实用工具。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [131] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: 本文利用大语言模型（LLM）构建多智能体系统，模拟职场毒性对话，通过蒙特卡洛方法量化毒性对讨论收敛时间的影响，发现毒性使对话时长显著增加约25%，提出‘毒性延迟’可作为组织效率损失的代理指标，并论证该方法是伦理、可复现的社会摩擦研究新范式。


<details>
  <summary>Details</summary>
Motivation: 职场毒性虽被公认为损害组织文化，但因其伦理与实操限制难以在人类受试者中直接量化其对运营效率的影响。

Method: 采用基于大语言模型的多智能体系统模拟1对1对抗性辩论，构建‘社会学沙盒’；运用蒙特卡洛方法模拟数百场讨论，对比基线组与植入‘毒性’系统提示的实验组的收敛时间（达成结论所需论点数）。

Result: 含毒性参与者的对话收敛时间显著延长约25%；验证了‘毒性延迟’可作为企业与学术环境中财务损耗的代理指标；证明基于智能体的建模是测量社会摩擦机制的可复现且合乎伦理的替代方案。

Conclusion: 毒性不仅影响心理氛围，更直接拖慢决策与协作效率；LLM多智能体仿真为组织行为学提供了新型、可控、无害的实证研究工具。

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [132] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: This paper compares Data Oriented Design (DOD) and Object-Oriented Design (OOD) for the A* search algorithm across single- and multi-threaded implementations, showing DOD’s superior cache efficiency and execution time—especially in multi-threaded settings—despite thread overhead diminishing gains in fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: The widening performance gap between multi-core CPUs and main memory calls for hardware-aware software design; DOD is investigated as a more cache-efficient alternative to traditional OOD.

Method: Four variants of the A* search algorithm were implemented and benchmarked: ST-OOD, ST-DOD, MT-OOD, and MT-DOD. Performance was evaluated using execution time, memory usage, and CPU cache misses.

Result: DOD outperformed OOD in execution time and cache miss counts—especially in multi-threaded settings—though OOD occasionally used slightly less memory or had lower percentage-based cache miss rates. Single-threaded versions beat multi-threaded ones due to thread management overhead in fine-grained tasks.

Conclusion: DOD demonstrates foundational architectural superiority over OOD in hardware efficiency, making it more suitable for data-intensive, large-scale AI and parallel computing applications.

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [133] [Can AI autonomously build, operate, and use the entire data stack?](https://arxiv.org/abs/2512.07926)
*Arvind Agarwal,Lisa Amini,Sameep Mehta,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.AI

TL;DR: 本文提出了一种范式转变：从在数据栈各独立组件中局部使用AI，转向对整个数据生命周期进行整体、自主的智能代理管理，以构建完全自治的企业数据体系。


<details>
  <summary>Details</summary>
Motivation: 当前AI虽已应用于数据工程等特定角色，但远未实现全自动化；而AI能力的快速提升为实现端到端自治数据系统提供了紧迫契机。

Method: 系统性探讨现代数据栈各阶段（架构、集成、质量、治理等）如何由智能代理自主管理，并分析驱动该范式转变的关键力量、实施路径及技术挑战。

Result: 提出了‘自治数据体系’（autonomous data estate）概念框架，明确了AI代理在数据全生命周期中的协同作用机制，并识别出若干关键开放问题与研究方向。

Conclusion: 迈向全自主数据系统是可行且必要的下一步；需跨学科协作、持续研究与产业实践共同推动该范式落地。

Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.

</details>


### [134] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: 本文提出SkipKV，一种无需训练的KV缓存压缩方法，通过句子级选择性剔除与生成，提升大推理模型在链式思维推理中的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在链式思维（CoT）推理中因KV缓存随推理长度线性增长，导致内存开销大、吞吐量受限；现有KV缓存剔除方法在多batch场景下因token级评分不稳定和填充token干扰而失效，且易引发重复验证、生成冗长。

Method: SkipKV是一种训练无关的方法：1）设计句子级打分指标，识别并剔除语义高度相似但冗余的句子，保持语义连贯；2）动态调整引导向量，在推理中更新隐藏状态，抑制冗余生成，促使模型输出更简洁。

Result: 在多个推理基准上，SkipKV在相近压缩预算下，准确率最高提升26.7%；相比SOTA，生成长度最多减少1.6倍，吞吐量提升最多达1.7倍。

Conclusion: SkipKV通过粗粒度句子级KV管理与动态隐状态调控，有效缓解CoT推理中的KV缓存瓶颈，在不牺牲准确性前提下显著提升推理效率与紧凑性。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [135] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 本文提出了一种分层多智能体框架，用于解决胃肠肿瘤学中多模态临床推理的挑战，通过模拟人类多学科团队协作，显著提升了推理逻辑和医学准确性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在处理复杂异构医疗数据时存在上下文稀释和幻觉问题，难以满足胃肠肿瘤学中对精准、可解释临床推理的需求。

Method: 提出一种分层多智能体框架，模拟人类多学科团队（MDT）的协同工作流程，整合内镜图像、影像学数据和生化标志物进行多模态临床推理。

Result: 系统在专家综合评估中获得4.60/5.00分，显著优于单体基线模型；尤其在推理逻辑与医学准确性方面提升最显著。

Conclusion: 模仿人类协作的智能体架构为肿瘤学自动决策支持提供了可扩展、可解释且临床稳健的新范式。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [136] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 本文提出了一种基于开源、推理增强型大语言模型的安全可扩展AI系统，用于辅助临床试验患者匹配，通过生成可解释的结构化评估和动态资格状态判断，支持专家复核并减轻协调员负担。


<details>
  <summary>Details</summary>
Motivation: 临床试验入组筛选过程手动、耗时且资源密集，亟需AI辅助以提升效率与可解释性。

Method: 采用开源、具备推理能力的大语言模型，整合异构电子健康记录数据，生成带可解释推理链的结构化资格评估，并将资格判定建模为动态状态而非静态结果。

Result: 实现了支持人机协同审核的安全、可审计、可扩展的AI辅助匹配系统，能识别当前匹配、提供未来达标建议，并扩大每位患者可考虑的试验范围。

Conclusion: 该系统有效缓解协调员工作负担，提升匹配透明度与灵活性，为临床试验AI落地提供了兼顾实用性、安全性与可解释性的可行路径。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [137] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 本文探讨了基础模型（FMs）在多智能体环境中的原生智能问题，指出单智能体能力强并不自动转化为多智能体能力，并提出了构建具备原生多智能体智能的FMs所需的关键研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型虽已具备单智能体能力（如GUI交互、工具调用），但多智能体智能尚未被系统性赋予，且其能力不能自发涌现，亟需专门研究。

Method: 通过在41个大语言模型上进行广泛实证分析，识别出多智能体场景下FMs所需的四大核心能力（理解、规划、高效通信、适应），并提出涵盖数据集构建、评估、训练范式与安全的系统性研究路径。

Result: 实证表明强单智能体性能不等价于鲁棒的多智能体智能；明确了多智能体智能的四个关键能力维度及其实现所需的研究方向。

Conclusion: 应将‘原生多智能体智能’作为基础模型发展的新前沿，需针对性设计训练与评估方法，而非依赖能力的自发涌现。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [138] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本文全面评估了ChatGPT和DeepSeek两大主流大语言模型在教育与科研场景中的表现，涵盖技术分析、实验 benchmark 和用户调研，发现二者在文本生成、编程、医学诊断与数学求解等任务中各具优势，并揭示了准确性、计算效率与用户体验之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型（LLM）在教育与科研中展现出巨大潜力，但不同模型在任务性能、效率与用户体验上的差异尚缺乏系统性评估。

Method: 结合背景技术分析、多维度实证实验（文本生成、编程、专业问题求解）以及面向学生、教师和研究人员的实地用户调查，开展综合评估。

Result: ChatGPT在通用语言理解与文本生成上更优；DeepSeek在编程任务中因效率优化设计而表现更佳；两者均能提供准确的医学诊断建议并有效解决复杂数学问题；用户调研进一步验证了其实际价值与现存局限。

Conclusion: ChatGPT与DeepSeek在教育与科研中互补性强，选择应依据具体任务需求权衡准确性、效率与用户体验；该研究为LLM在学术场景中的合理应用提供了实证依据与实践指导。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency-focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [139] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 本文设计并评估了一个可扩展的后端系统，用于支持移动糖尿病预测应用，通过水平扩展、数据库分片和基于RabbitMQ的异步通信，在10,000并发用户下实现<5%失败率和<1000ms平均延迟，83%功能达标。


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病发病率上升，亟需早期检测；AI预测应用依赖高响应性、可扩展的后端支撑大规模用户服务。

Method: 采用水平扩展、数据库分片和基于RabbitMQ的消息队列实现异步通信，构建可扩展后端架构，并进行性能测试与评估。

Result: 系统在10,000并发用户下稳定运行；24项功能中20项（83%）满足<5%失败率与<1000ms延迟目标；用户管理、活动追踪及读密集型预测功能达标；RabbitMQ显著降低计算密集型请求错误率并防止数据丢失。

Conclusion: 所提出的后端架构具备良好可扩展性与可靠性，能有效支撑AI驱动的糖尿病预测移动端应用，为类似健康类AI服务提供工程实践参考。

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [140] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: 本文探讨了因果学习在人类认知和人工智能中的重要性，提出'赋权（empowerment）'作为连接贝叶斯因果学习与强化学习的桥梁，并通过实证研究检验儿童与成人如何利用赋权线索进行因果推断与干预设计。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型难以通过标准深度学习方法实现因果学习，而人类（尤其是儿童）却能高效学习因果结构；需寻找一种可计算、具认知合理性的机制来统一解释人类因果学习并赋能机器。

Method: 基于因果贝叶斯网络形式化框架，引入'赋权'（动作与结果间互信息最大化）作为内在奖励信号，构建其与因果模型准确性的双向促进关系，并开展针对儿童与成人的行为实验，检验其对赋权线索的因果推理与干预能力。

Result: 实证研究表明，儿童与成人确实利用赋权相关线索（如动作对结果的可控性与可预测性）进行因果关系推断和有效干预设计，支持赋权作为因果学习的认知与计算基础。

Conclusion: 赋权不仅为理解人类（尤其儿童）因果学习提供了统一、可计算的理论框架，也为在AI系统中实现鲁棒因果学习提供了新路径；因果建模与赋权优化本质上是协同增强的过程。

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [141] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: 本文提出KPI框架，利用知识图谱、原型学习和对比学习提升疾病预测准确性，并结合大语言模型生成可解释的医学解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于患者自报信息的疾病预测方法存在疾病分布不平衡和缺乏可解释性的问题，导致预测偏差或不可靠。

Method: 构建融合结构化医学知识的疾病知识图谱；设计临床意义明确的疾病原型；采用对比学习提升长尾疾病的预测性能；利用大语言模型生成患者特异、医学相关的解释。

Result: 在真实数据集上，KPI在预测准确率上优于当前最优方法，并能生成与患者叙述高度一致、临床有效的解释。

Conclusion: KPI框架兼顾高准确性与强可解释性，具有推动以患者为中心的医疗实践的实际价值。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [142] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 本文评估了最新推理模型在CFA考试模拟题上的表现，发现多数模型能通过全部三个级别，其中Gemini 3.0 Pro在Level I取得97.6%的最高分。


<details>
  <summary>Details</summary>
Motivation: 先前研究指出大语言模型在CFA考试中表现不佳，但近期推理模型在其他专业考试中表现出色，因此有必要重新评估其在CFA考试中的能力。

Method: 使用包含980道题的三套CFA Level I、两套Level II和三套Level III模拟试卷，采用与先前研究一致的及格/不及格标准，对多个前沿推理模型进行评测。

Result: 多数模型通过全部三个级别；Gemini 3.0 Pro在Level I达97.6%，GPT-5在Level II达94.3%，Gemini 2.5 Pro和Gemini 3.0 Pro分别在Level III多选题和建构题中表现最佳。

Conclusion: 当前最先进推理模型在CFA考试中已远超及格线，显著优于早期LLM，表明其在专业金融知识理解和推理方面取得重大进展。

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [143] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 本文提出了一种名为'生成式智能体（Generative Agents）'的自动化内容评估方法，用于高效、低成本地评估AI生成内容的质量，减少对人工评估的依赖。


<details>
  <summary>Details</summary>
Motivation: 现代企业在内容生成与评估中面临时间长、成本高的问题，传统人工评估方法昂贵且低效，而现有LLM生成内容质量不稳定，亟需可靠的自动化评估方案。

Method: 设计并引入'生成式智能体'，模拟人类判断标准（如连贯性、趣味性、清晰度、公平性、相关性）对AI生成内容进行自动评分与评估。

Result: 该方法可快速、低成本地实现高质量内容评估，显著提升内容生成流程效率，并保证输出一致性；实证表明其能有效替代部分人工评估任务。

Conclusion: 生成式智能体为AI内容生成与评估提供了高效、可扩展的自动化范式，推动了面向商业场景的高质量内容生产技术发展。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [144] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 本文提出了一个用于预测多智能体系统性能的定量缩放原理框架，通过在四个基准上对180种配置进行控制实验，发现工具协调权衡、能力饱和和拓扑依赖性错误放大三大效应，并构建了能准确预测最优协调策略的模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型的智能体系统已被广泛应用，但其性能决定因素仍缺乏系统性研究，导致实践者依赖经验而非原则性设计。

Method: 在四个多样化基准（Finance-Agent、BrowseComp-Plus、PlanCraft、Workbench）上，对五种典型架构（Single、Independent、Centralized、Decentralized、Hybrid）及三种大语言模型家族组合共180种配置进行受控评估；引入效率、开销、错误放大、冗余等实证协调指标，构建预测模型。

Result: 得到R²=0.513的跨验证预测模型；识别出三大效应：（1）工具-协调权衡；（2）能力饱和（单智能体准确率超~45%后协调收益递减或为负）；（3）拓扑依赖的错误放大（独立架构放大误差17.2倍，集中式仅4.4倍）；集中式在并行任务（如金融推理）提升80.9%，去中心化在动态网页导航中表现更优（+9.2% vs +0.2%），但所有多智能体在顺序推理任务中性能下降39–70%；框架对87%留出配置能正确预测最优协调策略。

Conclusion: 该研究为多智能体系统提供了可测量、可预测的缩放设计原则，强调应依据任务属性（如并行性、顺序性、动态性）选择协调架构，而非盲目堆叠智能体。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [145] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 本文提出了一种强化策略注入机制（rSIM），通过小规模规划器（leader agent）与大语言模型（follower agent）联合多智能体强化学习训练，引导LLM在思维链中自适应注入推理策略，使其进化为具备‘顿悟’能力的推理语言模型（RLM）。该方法显著提升小模型（如Qwen2.5-0.5B）推理能力，甚至超越更大模型（Qwen2.5-14B），且规划器可即插即用、支持跨任务持续学习。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型经强化学习后演化出'顿悟'式高级推理能力（如自我反思、深度思考）的启发，亟需一种轻量、通用、可迁移的方法将任意LLM转化为推理语言模型（RLM）。

Method: 提出强化策略注入机制（rSIM）：构建leader-follower多智能体框架，由小型规划器（leader）指导LLM（follower）在思维链中动态注入推理策略；采用基于规则的奖励函数，联合训练二者；规划器可即插即用并支持跨任务持续学习。

Result: rSIM使Qwen2.5-0.5B性能显著超越Qwen2.5-14B；规划器只需训练一次即可泛化至其他LLM；支持跨任务持续学习，规划能力随任务增多而增强和泛化。

Conclusion: rSIM是一种高效、轻量、通用的LLM推理增强范式，验证了小型专用规划器协同大模型进行策略级引导的有效性，为构建可扩展、可持续进化的推理语言模型提供了新路径。

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [146] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 本研究提出了一种基于机器学习的加州承载比（CBR）预测框架，利用来自土耳其不同地理气候区的382个土壤样本数据，比较了12种回归算法，其中随机森林表现最优（训练R²=0.95，验证R²=0.76，测试R²=0.83），证明了数据驱动方法在替代传统耗时昂贵CBR试验中的有效性与潜力。


<details>
  <summary>Details</summary>
Motivation: 传统CBR实验室试验耗时、昂贵且难以适用于大规模或多样化的土壤剖面，亟需高效、精准的替代方法。

Method: 构建包含12种机器学习回归算法（如随机森林、XGBoost、SVR等）的对比框架，基于382个土耳其多源土壤样本及其理化参数进行监督学习建模与交叉验证。

Result: 随机森林回归器表现最佳，训练、验证和测试集R²分别达0.95、0.76和0.83，展现出优异的非线性映射能力与泛化性能。

Conclusion: 机器学习模型（尤其是随机森林）可作为传统CBR试验的有效补充或替代工具，推动岩土工程向智能化、数据驱动和数字化方向发展。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [147] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 本研究提出一种自动机器学习（AutoML）方法来预测土壤压实参数（最优含水率OMC和最大干密度MDD），在异质土壤数据集上XGBoost表现最优（OMC R²=89.1%，MDD R²=80.4%），显著提升模型泛化能力与工程实用性。


<details>
  <summary>Details</summary>
Motivation: 传统实验室测定OMC和MDD费时费力，经验回归模型适用性与精度有限；现有机器学习方法在多类型土壤异质数据上预测精度和泛化性不足。

Method: 采用自动机器学习（AutoML）框架，自动化选择算法与优化超参数；通过大量实验评估多种模型，最终确定XGBoost为最优算法。

Result: XGBoost在独立测试集上对OMC和MDD的预测R²分别达89.1%和80.4%；验证了异质数据集对提升模型泛化能力的关键作用。

Conclusion: AutoML（尤其是XGBoost）可有效提升不同土壤类型下压实参数预测的准确性与普适性，有助于提高工程建设效率与可靠性。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [148] [Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions](https://arxiv.org/abs/2512.08344)
*Tien Cuong Bui*

Main category: cs.AI

TL;DR: 本文提出了一种面向图神经网络（GNN）的新可解释人工智能（XAI）框架，旨在兼顾解释的适应性、计算效率与对图结构影响的建模能力，弥补现有后验解释方法计算开销大、可靠性低及自解释模型泛化性差的不足。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法难以有效解析GNN复杂的决策过程：后验方法依赖外部计算且可靠性受限；自解释模型虽高效但泛化能力差。

Method: 设计一种专用于图机器学习的新型XAI框架，超越单个特征分析，聚焦图结构如何影响预测，兼顾适应性与计算效率。

Result: 构建了一个能提供灵活、高效且结构感知解释的GNN可解释性框架。

Conclusion: 该框架有望在保持GNN性能的同时，显著提升其决策透明度与可信度，推动GNN在关键领域的可靠部署。

Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.

</details>


### [149] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: 本文提出DuSAR框架，一种无需外部示例、基于双策略协同与轻量级反思机制的LLM智能体方法，在ALFWorld和Mind2Web上显著提升性能并大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖外部示例或检索增强规划，导致鲁棒性差、泛化能力弱、计算开销高；受人类问题解决方式启发，需构建更自主、自适应的推理机制。

Method: 提出DuSAR：基于单个冻结LLM，融合高层整体规划与上下文感知的局部策略，并通过策略适配度评分驱动的轻量级反思机制实现动态协调与修正。

Result: 在ALFWorld（Llama3.1-70B）上成功率达37.1%，超先前最优结果13.0%一倍以上；在Mind2Web上达4.02%，同样翻倍；每步token消耗减少3–9倍；消融实验证明双策略协同必要。

Conclusion: DuSAR实现了高效、鲁棒、可扩展的自主推理，兼具演示无关性与可选示范兼容性，为LLM智能体设计提供了新范式。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [150] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: 本文提出DeepFeature，一种基于大语言模型（LLM）的上下文感知可穿戴生物信号特征生成框架，通过多源特征生成、迭代精炼与多层过滤验证机制，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有生物信号特征提取方法缺乏任务特定上下文知识、难以在高维特征空间中自动选择最优设置，且易出现代码生成与自动化错误。

Method: 提出DeepFeature框架：1）多源特征生成机制（融合专家知识与任务设定）；2）基于特征评估反馈的迭代精炼过程；3）鲁棒的多层过滤与验证机制保障特征到代码的正确翻译。

Result: 在八个不同医疗任务上平均AUROC提升4.21–9.67%；在五个任务上超越SOTA方法，其余任务性能相当。

Conclusion: DeepFeature首次将LLM引入生物信号特征工程，实现了上下文感知、自动化且鲁棒的特征生成，为可穿戴健康应用提供了新范式。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [151] [Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems](https://arxiv.org/abs/2512.08411)
*Mingwei Li,Xiaoyuan Zhang,Chengwei Yang,Zilong Zheng,Yaodong Yang*

Main category: cs.AI

TL;DR: 本文提出PRISM-WM，一种基于上下文感知混合专家（MoE）的结构化世界模型，用于建模机器人领域中具有离散事件（如接触、碰撞）的混合动力学，通过隐式模式识别与专家正交化避免模式坍缩，显著降低长期rollout漂移，提升基于模型规划的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统潜世界模型使用单一大型神经网络，强制全局连续性，过度平滑不同物理模式（如粘滞vs滑动、飞行vs支撑），导致长程预测时在物理边界处产生灾难性累积误差，使规划不可靠。

Method: 提出Prismatic World Model（PRISM-WM）：采用上下文感知的Mixture-of-Experts架构，门控机制隐式识别当前物理模式，各专家专精对应模式的动力学预测；引入潜空间正交化目标以增强专家多样性、防止模式坍缩。

Result: 在高维人形机器人及多任务连续控制基准上，PRISM-WM显著降低rollout漂移，为TD-MPC等轨迹优化算法提供更高保真度的基础模型，提升长期规划可靠性。

Conclusion: PRISM-WM通过结构化分解混合动力学并保障模式特异性建模，为下一代基于模型的智能体提供了更鲁棒、高保真的世界建模基础。

Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.

</details>


### [152] [From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change](https://arxiv.org/abs/2512.08449)
*Yong-Woon Kim*

Main category: cs.AI

TL;DR: 本文提出Impact-Driven AI Framework（IDAIF），将变革理论（ToC）与AI系统设计融合，构建五层架构映射ToC五阶段，嵌入多目标优化、因果建模、RLHF等技术以保障价值对齐、公平性与鲁棒性，并通过三领域案例验证其影响驱动范式。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统过度关注技术性能指标，忽视社会技术维度，导致在医疗、金融、公共政策等高风险领域中价值对齐不足；亟需一种能系统保障伦理、可信与社会效益的AI架构方法。

Method: 提出IDAIF框架，将ToC的Inputs-Activities-Outputs-Outcomes-Impact五阶段映射为AI的Data-Pipeline-Inference-Agentic-Normative五层架构；每层集成特定技术：Pareto多目标优化、分层多智能体编排、因果DAG抑制幻觉、对抗去偏+RLHF保障公平，并引入Assurance Layer应对假设失效。

Result: 形式化定义各组件数学模型；在医疗、网络安全、软件工程三个案例中验证IDAIF可提升AI系统的价值对齐度、可解释性、公平性与实际社会影响；提供可复用的架构模式。

Conclusion: IDAIF标志着AI开发从模型中心范式转向影响中心范式，为构建伦理、可信、有益社会的AI系统提供了系统性架构基础与工程实践路径。

Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.

</details>


### [153] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 本文研究了在无外部反馈条件下技能习得的过程，通过将强化学习智能体直接连接到水槽中的旋转圆柱体来最大化或最小化阻力。实验发现，高维流场反馈有助于智能体在几分钟的真实世界交互中发现高性能的阻力控制策略，且执行时无需反馈；但训练时若缺乏流场反馈，智能体在阻力最大化任务中失败，而在阻力最小化任务中仍能成功（尽管更慢、更不可靠）。这表明学习所需信息可能比执行更多，且学习难度取决于目标而非系统动态或策略复杂度。


<details>
  <summary>Details</summary>
Motivation: 探究无外部反馈下人类高绩效活动（如花滑、投球、拉花）背后的技能习得机制，需在完全可控条件下开展研究，避免人类实验的复杂性与不可重复性。

Method: 使用通用强化学习智能体直接控制水槽中旋转圆柱体的运动，以实现阻力最大化或最小化；利用真实物理水动力系统提供高维流场反馈作为观测输入，并对比有/无流场反馈下的训练效果。

Result: 有流场反馈时，智能体数分钟内即学会高性能策略，且开环重放性能几乎不变；无流场反馈时，阻力最大化任务完全失败，阻力最小化任务虽成功但更慢、更不可靠。

Conclusion: 技能学习所需信息丰富度高于执行阶段；学习难易程度（‘友善’或‘恶劣’）取决于目标任务本身，而非系统动力学或策略复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [154] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 本文提出一种基于数据转换图（DTG）的新范式，替代传统代码属性图（CPG），结合多智能体框架与神经符号推理，实现仓库级自动程序修复（APR），在SWE-Verified基准上达到87.1%修复率。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级自动程序修复（APR）受限于以控制流为中心的范式，难以高效定位逻辑缺陷；标准RAG系统易陷入‘语义陷阱’，且AI代码助手缺乏对数据语义和完整性建模的能力。

Method: 提出数据转换图（DTG）——以数据状态为节点、函数为边；构建多智能体框架，融合数据完整性导航与控制流逻辑；开发自主问题解决器（AIR），集成神经符号推理与DTG结构进行可扩展逻辑修复。

Result: 在多个SWE基准上取得良好效果，尤其在SWE-Verified基准上达到87.1%的问题解决率；理论分析与案例研究证实其能有效规避标准RAG系统的‘Semantic Trap’。

Conclusion: DTG范式与AIR系统为仓库级APR提供了更鲁棒、语义更清晰的基础，显著提升了AI代码助手在真实软件维护场景中的实用性与可扩展性。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [155] [A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles](https://arxiv.org/abs/2512.08512)
*Jiang Liu,Yan Qin,Wei Dai,Chau Yuen*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级的基于迁移学习（TL）的锂离子电池健康状态（SOH）监测方法——构建式增量迁移学习（CITL），通过半监督机制、结构风险最小化、迁移失配最小化和流形一致性最大化，提升SOH估计精度并降低计算开销，实验表明其在UAV电池数据集上显著优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法在便携式移动设备上因计算资源消耗大而不可行，亟需一种轻量、高效且适应多变工况的SOH监测方法。

Method: 提出构建式增量迁移学习（CITL）：1）利用目标域无标签数据，设计半监督TL机制，通过迭代增加网络节点来构造性减小监测残差；2）从结构风险最小化、迁移失配最小化和流形一致性最大化三方面保障节点参数的跨域学习能力；3）提供CITL收敛性分析以理论保证性能与网络紧凑性。

Result: 在真实UAV电池数据集上，CITL在RMSE指标下相比SS-TCA、MMD-LSTM-DA、DDAN、BO-CNN-TL和AS$^3$LSTM分别提升83.73%、61.15%、28.24%、87.70%和57.34%。

Conclusion: CITL是一种兼顾高精度、低计算开销与强泛化能力的轻量级迁移学习方案，适用于资源受限的便携式设备中的电池SOH实时监测。

Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.

</details>


### [156] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: 本文提出了Principles2Plan系统，通过人类与大语言模型（LLM）协作，将高层次伦理原则（如仁慈、隐私）转化为可操作的、上下文敏感的伦理规则，并用于指导经典自动规划，从而提升机器人在人类环境中的伦理决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动规划工具缺乏对伦理意识的支持，而人工指定伦理规则费时且难以泛化到不同场景。

Method: 设计交互式原型Principles2Plan：领域专家输入规划域、问题及高层伦理原则；LLM据此生成可操作的伦理规则；用户审查、排序并将其输入规划器生成伦理感知计划。

Result: 实现了首个支持用户在经典规划中基于伦理原则生成可执行规则的系统，验证了人机协同构建伦理规划的可行性。

Conclusion: Principles2Plan展示了人类与LLM协作可使伦理自动规划更实用、可行，为具身智能体的伦理嵌入提供了新路径。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [157] [The SMART+ Framework for AI Systems](https://arxiv.org/abs/2512.08592)
*Laxmiraju Kandikatla,Branislav Radeljic*

Main category: cs.AI

TL;DR: 本文提出SMART+框架，旨在通过安全、监控、问责、可靠性和透明性等核心支柱，并结合隐私与安全、数据治理、公平与偏见、防护机制等增强维度，为跨行业AI系统提供全面、实用的治理与评估方法，以应对AI应用中的安全、问责与合规挑战。


<details>
  <summary>Details</summary>
Motivation: AI在临床研究、金融、制造等多个行业的广泛应用带来了安全、问责和监管合规等新挑战，亟需一个系统化、可操作的AI治理框架。

Method: 构建SMART+框架，基于Safety、Monitoring、Accountability、Reliability、Transparency五大支柱，并扩展Privacy & Security、Data Governance、Fairness & Bias、Guardrails四大增强维度，融合现有监管要求与操作实践。

Result: SMART+框架展现出风险缓解、信任构建与合规就绪能力，支持负责任的AI采用与可审计性，已在临床研究领域验证其有效性。

Conclusion: SMART+是一个面向多行业的实用型AI治理框架，能有效支撑AI系统的安全、合规与可持续部署，为AI治理提供了结构化、可落地的解决方案。

Abstract: Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.

</details>


### [158] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: 本文提出了一种认知引导的蒙特卡洛树搜索框架（CogMCTS），通过多轮认知反馈、双轨节点扩展与精英启发式管理、以及策略性变异，提升大语言模型在自动启发式设计中的探索-利用平衡与多样性，显著优于现有LLM-based方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的进化方法易陷入局部最优；LLM与蒙特卡洛树搜索（MCTS）结合虽改善探索-利用权衡，但多轮认知整合有限、搜索多样性不足。

Method: 提出CogMCTS框架：1）多轮认知反馈整合历史经验、节点信息与负向结果以动态优化启发式生成；2）双轨节点扩展结合精英启发式管理，兼顾多样性探索与高质量经验利用；3）策略性变异调整启发式形式与参数以增强解多样性与优化性能。

Result: 实验表明，CogMCTS在稳定性、效率和解质量上均优于现有LLM-based自动启发式设计方法。

Conclusion: CogMCTS通过深度耦合LLM的认知机制与MCTS，实现了更高效、鲁棒且多样化的自动启发式优化，为LLM赋能复杂优化问题提供了新范式。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [159] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的模型，利用注意力机制预测蛋白质二级结构，并通过滑动窗口数据增强技术在CB513数据集上提升性能。


<details>
  <summary>Details</summary>
Motivation: 预测蛋白质二级结构（如α螺旋、β折叠和无规卷曲）对理解蛋白质功能至关重要。

Method: 采用基于Transformer的模型，结合注意力机制处理蛋白序列，并使用滑动窗口数据增强技术扩展CB513训练样本。

Result: 该模型在可变长度序列上表现出强泛化能力，能有效捕获局部及长程残基相互作用。

Conclusion: Transformer架构适用于蛋白质二级结构预测任务，滑动窗口增强进一步提升了模型性能。

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [160] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: 本文提出了See-Control框架，用于通过低自由度机械臂直接物理交互操作智能手机，摆脱了对ADB的依赖，实现了跨平台的具身智能手机操作（ESO）任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的智能手机操作方法依赖ADB，仅适用于Android设备，缺乏跨平台通用性。

Method: 提出具身智能手机操作（ESO）任务；构建包含155个任务的ESO基准与评估指标；设计基于MLLM的具身智能体，直接生成机械臂控制指令；收集并发布带丰富标注的操作片段数据集。

Result: 实现了不依赖ADB和系统后端访问的物理层智能手机操作；提供了首个面向具身智能体的智能手机操作基准、模型框架与高质量数据集。

Conclusion: See-Control是连接数字智能体与物理世界的重要一步，为家庭机器人执行依赖智能手机的任务提供了可行路径，并推动平台无关的具身智能研究。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [161] [Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance](https://arxiv.org/abs/2512.08740)
*Yiming Lu*

Main category: cs.AI

TL;DR: 本文提出了一种'人-AI协同认知增强'新范式，通过结构化的'元交互'将人类专家的隐性直觉与AI的不可信决策转化为可组合、可审计、可扩展的'功能白盒'系统；核心是'即插即用认知框架'，能从专家对话中提取可计算知识包并加载至递归对抗元思维网络（RAMTN），实现专家思维的复用与规模化，推动AI从工具升维为思维伙伴，并首次为'认知公平'提供工程实证，开创基于交互协议透明度的可验证、可干预AI治理新路径。


<details>
  <summary>Details</summary>
Motivation: 弥合人类专家‘认知黑箱’（隐性直觉）与人工智能‘计算黑箱’（不可信决策）之间的根本鸿沟，实现可信、可干预、可共享的认知增强。

Method: 提出‘元交互’结构化方法，构建‘即插即用认知框架’作为可计算知识包，并设计递归对抗元思维网络（RAMTN）作为承载平台，从专家对话中提取、封装、加载和复用专家思维逻辑。

Result: 实现了医疗诊断逻辑、教学直觉等专家认知能力向可重用、可扩展公共资产的转化；开源了整套框架；提供了‘认知公平’的首个工程实证；确立了以交互协议透明度为核心的新型AI治理路径。

Conclusion: 本工作标志着AI角色从‘工具’向‘思维伙伴’的根本转变，不仅推动认知科学与AI工程的深度交叉，更通过开放框架促进技术向善与认知包容，为可信AI发展提供了新理论、新架构与新治理范式。

Abstract: Currently, there exists a fundamental divide between the "cognitive black box" (implicit intuition) of human experts and the "computational black box" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of "human-AI collaborative cognitive enhancement," aiming to transform the dual black boxes into a composable, auditable, and extensible "functional white-box" system through structured "meta-interaction." The core breakthrough lies in the "plug-and-play cognitive framework"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from "AI as a tool" to "AI as a thinking partner." This work not only provides the first engineering proof for "cognitive equity" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through "transparency of interaction protocols" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.

</details>


### [162] [Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments](https://arxiv.org/abs/2512.08755)
*Dongdong Yang,Bin Li,Jiguang He*

Main category: cs.AI

TL;DR: 本文比较了空中可重构智能表面（RIS）和同时透射反射RIS（STAR-RIS）在三维无线环境中的性能，建立了考虑方向辐射特性的信道模型，联合优化部署高度与朝向以最大化系统和速率，并发现STAR-RIS在低空更优，而RIS在高空近基站处更优。


<details>
  <summary>Details</summary>
Motivation: 尽管空中RIS和STAR-RIS在增强未来网络覆盖与容量方面潜力巨大，但二者在三维环境下的综合性能对比尚未被深入研究。

Method: 建立考虑方向辐射模式的精确信道模型，分析部署高度与朝向影响；针对两种架构分别构建联合优化问题，并采用加权最小均方误差（WMMSE）与块坐标下降（BCD）算法求解以最大化系统和速率。

Result: 仿真表明：STAR-RIS凭借全空间覆盖能力在低空场景下优于RIS；而RIS在高空、靠近基站时性能更佳。

Conclusion: 该研究为6G中空中智能表面的实际部署提供了重要依据和实用指导。

Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.

</details>


### [163] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供了一个端到端的实用指南，用于设计、开发和部署生产级的自主智能体（Agentic AI）系统，涵盖工程生命周期、九大核心最佳实践及一个多媒体新闻分析与生成的案例研究。


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI在产业和研究中加速落地，组织面临如何构建可靠、可观测、可维护且符合安全与治理要求的生产级自主智能体工作流的核心挑战。

Method: 提出结构化的工程生命周期（含工作流分解、多智能体设计模式、模型上下文协议MCP、工具集成、确定性编排、负责任AI考量及环境感知部署），并总结九项核心工程最佳实践，辅以一个完整的多模态新闻分析与媒体生成案例研究。

Result: 形成一套面向生产环境的Agentic AI系统工程方法论，包括架构指导、运维模式与实施洞见，支持构建鲁棒、可扩展、可投入生产的自主智能体工作流。

Conclusion: 该论文为构建稳健、可扩展、符合治理要求的生产级Agentic AI系统提供了基础性参考框架与实践路径。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [164] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: 本文提出CARLoS框架，通过CLIP嵌入差异定义LoRA的语义方向、强度与一致性三要素表征，在无额外元数据条件下实现对650+ LoRA的量化刻画与语义检索，性能超越文本基线，并可延伸支持版权相关法律分析。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA发现方法依赖不可靠的用户描述或有偏的流行度指标，导致可用性差；亟需一种不依赖元数据、能客观刻画LoRA行为的系统化表征方法。

Method: 基于650多个LoRA在多样化提示和随机种子下的图像生成结果，提取其与基础模型输出的CLIP嵌入差异，构建包含Direction（语义偏移）、Strength（效应显著性）和Consistency（效应稳定性）的三维表征；并据此设计语义检索框架，支持文本查询匹配及强度/稳定性过滤。

Result: 所提CARLoS框架在自动与人工评估中均优于文本基线检索方法；其表征还可关联版权法中的实质性（substantiality）与主观意图（volition）概念，验证了分析的法律相关性。

Conclusion: CARLoS提供了一种无需元数据的大规模LoRA表征与检索新范式，兼具技术有效性与法律分析潜力，为生成式AI组件治理提供了实用工具。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [165] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 本文探讨了在知识表示领域中，特别是描述逻辑和逻辑编程中，计算Craig插值和一致插值的理论结果与实用方法。


<details>
  <summary>Details</summary>
Motivation: Craig插值和一致插值在知识表示中具有广泛应用（如可解释性、遗忘、模块化和重用等），但许多相关形式化方法通常不具备这些插值性质，且实际计算插值项具有挑战性。

Method: 聚焦于描述逻辑和逻辑编程两种主流知识表示形式，综述并分析其插值性质的理论结果及实际计算方法。

Result: 总结了描述逻辑和逻辑编程中插值性质的存在性、可判定性以及计算插值项的现有方法。

Conclusion: 尽管插值在知识表示中意义重大，但其在多种形式化系统中的存在性和可计算性仍受限；需进一步发展兼顾理论保证与实用效率的插值计算技术。

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [166] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: 本文提出了EcomBench，一个基于真实电商场景的综合性基准测试，用于评估基础模型代理在实际应用中的核心能力，如深度信息检索、多步推理和跨源知识整合。


<details>
  <summary>Details</summary>
Motivation: 现有基准多集中于学术或人工设计场景，忽视了真实应用场景（如电商）中的复杂挑战，亟需更贴近现实的评估工具。

Method: 构建了源自全球主流电商平台真实用户需求的EcomBench基准，由领域专家精心筛选与标注，涵盖多类电商任务，并定义三级难度以评估关键能力。

Result: EcomBench提供了严格且动态的测试环境，能有效衡量代理在现代电商环境中的实际能力。

Conclusion: EcomBench填补了面向真实世界、尤其是电商领域代理能力评估的空白，推动基础代理向实用化发展。

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [167] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 本文提出了两个新基准REST和REST+，用于系统评估多模态大语言模型（MLLMs）中的跨模态不一致性问题，发现当前模型在不同模态（图像、文本、混合）下推理表现不一致，且该不一致性受视觉特征和视觉token数量影响，并与文本-图像模态差距相关。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽将视觉与语言映射到同一嵌入空间，却无法在不同模态下稳定执行相同任务，缺乏对跨模态不一致性的系统性评估手段。

Method: 构建REST和REST+两个基准，包含语义相同但模态不同（图像、文本、混合）的样本；对15个SOTA MLLMs进行评测，分析OCR准确性、视觉特征（颜色、分辨率、字体）、视觉token数量等因素对性能的影响，并计算一致性分数与模态差距的相关性。

Result: 发现MLLMs存在显著跨模态不一致性，该现象不能仅由OCR错误解释；文本颜色、图像分辨率及视觉token数量影响性能，而字体无显著影响；一致性分数与文本-图像模态差距呈相关性。

Conclusion: 跨模态不一致性是MLLMs的内在问题，其根源与模态间表征差距有关，需在模型设计中显式建模模态对齐机制。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [168] [MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction](https://arxiv.org/abs/2512.07846)
*Guoyao Li,Ran He,Shusen Jing,Kayhan Behdin,Yubo Wang,Sundara Raman Ramachandran,Chanh Nguyen,Jian Sheng,Xiaojing Ma,Chuanrui Zhu,Sriram Vasudevan,Muchen Wu,Sayan Ghosh,Lin Su,Qingquan Song,Xiaoqing Wang,Zhipeng Wang,Qing Lan,Yanning Chen,Jingwei Wu,Luke Simon,Wenjing Zhang,Qi Guo,Fedor Borisyuk*

Main category: cs.IR

TL;DR: 本文提出MixLM框架，通过混合文本与嵌入标记的方式缩短输入上下文长度，在保持跨编码器语义能力的同时大幅提升系统吞吐量，已在LinkedIn搜索中实现全流量部署并提升DAU。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在相关性排序中表现优异，但其高计算开销难以满足工业级低延迟、高吞吐需求，尤其是跨编码器排名中长上下文预填充导致的效率瓶颈。

Method: 提出MixLM框架，采用mix-interaction机制——将物品描述编码为少量嵌入标记并缓存于近线缓存中，在线推理时以嵌入标记替代原始长文本，显著压缩输入上下文；同时设计配套训练流程与在线服务基础设施优化方案。

Result: 相比强基线，MixLM在相同延迟预算下吞吐量提升10.0倍，相关性指标保持不变；在线A/B测试中实现全流量部署，带来0.47%的每日活跃用户（DAU）增长。

Conclusion: MixLM通过融合文本与嵌入表示，在不牺牲语义建模能力的前提下有效缓解LLM在工业检索场景中的效率瓶颈，为LLM驱动的搜索系统落地提供了可扩展、高性能的实用框架。

Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.

</details>


### [169] [Detecting Privileged Documents by Ranking Connected Network Entities](https://arxiv.org/abs/2512.08073)
*Jianping Zhang,Han Qin,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文提出了一种基于邮件头元数据构建人物网络的链接分析方法，通过识别与律师频繁交互的个体来识别特权文档，并为网络中的人物分配得分以提升特权文档检测效果。


<details>
  <summary>Details</summary>
Motivation: 识别特权文档在法律审查中至关重要，而传统方法难以准确判断哪些文档具有法律特权；本文旨在利用通信网络结构特征，通过分析人员交互模式自动识别潜在特权文档。

Method: 从邮件头元数据中提取人物实体，依据预定义律师名单将实体分为律师和非律师两类；构建人物交互网络，基于律师交互频率为核心假设，设计算法为每个实体分配特权相关性得分；结合实体得分与连接强度对文档进行排序与筛选。

Result: 实验结果表明该算法能有效对法律相关实体进行排序，显著提升特权文档的检测与识别能力。

Conclusion: 基于链接分析的网络建模方法可有效辅助特权文档识别，尤其适用于大规模电子发现场景，为法律科技（Legal Tech）提供了可扩展的自动化方案。

Abstract: This paper presents a link analysis approach for identifying privileged documents by constructing a network of human entities derived from email header metadata. Entities are classified as either counsel or non-counsel based on a predefined list of known legal professionals. The core assumption is that individuals with frequent interactions with lawyers are more likely to participate in privileged communications. To quantify this likelihood, an algorithm assigns a score to each entity within the network. By utilizing both entity scores and the strength of their connections, the method enhances the identification of privileged documents. Experimental results demonstrate the algorithm's effectiveness in ranking legal entities for privileged document detection.

</details>


### [170] [A Comparative Study of Retrieval Methods in Azure AI Search](https://arxiv.org/abs/2512.08078)
*Qiang Mao,Han Qin,Robert Neary,Charles Wang,Fusheng Wei,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文评估了微软Azure RAG框架中多种检索策略在电子发现早期案件评估（ECA）任务中的效果，比较了关键词、语义、向量、混合及混合-语义检索方法在AI生成回答的准确性、相关性和一致性方面的表现，旨在帮助律师优化RAG配置以提升文档审查效率。


<details>
  <summary>Details</summary>
Motivation: 律师亟需超越传统关键词和语义搜索，利用大语言模型（LLM）通过自然语言提问高效获取文档关键信息，尤其在早期案件评估（ECA）阶段快速把握数据全貌与风险。

Method: 在Microsoft Azure RAG框架下，系统对比Azure AI Search提供的五种检索方法：关键词、语义、向量、混合、混合-语义；评估指标包括AI生成回答的准确性、相关性和一致性。

Result: 不同检索方法在准确性、相关性和一致性上表现各异，为ECA场景下的RAG配置提供了实证依据。

Conclusion: 该研究结果可指导法律从业者根据具体需求选择更优的RAG检索策略，从而提升eDiscovery中早期案件评估的效率与可靠性。

Abstract: Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future.

</details>


### [171] [Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery](https://arxiv.org/abs/2512.08079)
*Qiang Mao,Fusheng Wei,Robert Neary,Charles Wang,Han Qin,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文系统研究了如何利用图像聚类、图像标注和大语言模型（LLMs）自动生成图像簇描述，重点评估了采样策略、提示方法和生成方法三方面对描述质量的影响，并在法律发现等高时效性场景中验证其可行性与效率。


<details>
  <summary>Details</summary>
Motivation: 数字图像数量激增使得人工审查在法律发现、数字归档等场景中既不现实也不经济，亟需高效、可扩展的自动化图像集合组织与理解方法。

Method: 采用K-means将图像聚为20个视觉一致簇，用Azure AI Vision生成基础标题；对比五种图像采样策略、两种提示技术（标准vs.思维链）及三种描述生成方法（LLM vs. TF-IDF vs. 模板法），并以语义相似度与覆盖率评估效果。

Result: 20张/簇的策略采样效果接近全图输入，仅分层采样略有下降；LLM方法显著优于TF-IDF；标准提示优于思维链提示。

Conclusion: 该研究为面向法律发现等高吞吐场景的大规模图像自动组织提供了实用、高效且准确的集群描述生成方案。

Abstract: The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections.

</details>


### [172] [Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters](https://arxiv.org/abs/2512.08083)
*Keith Huffman,Jianping Zhang,Nathaniel Huber-Fliflet,Fusheng Wei,Peter Gronvall*

Main category: cs.IR

TL;DR: 本文实证研究了随机性在基于大语言模型（LLM）的律师-客户特权文件检测分类任务中的作用，发现LLM本身效果良好，随机性控制参数影响甚微，但通过特定方法主动利用随机性可显著提升准确率，并增强合规流程中的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着企业越来越多地依赖LLM辅助合规工作（如制裁筛查），降低输出变异性、提升结果可信度（包括内部与监管层面）成为关键需求；而随机性对LLM分类性能的影响及其潜在利用价值尚不明确。

Method: 开展实证研究，系统考察四个维度：(1) LLM识别特权文件的有效性；(2) 随机性控制参数对分类输出的影响；(3) 这些参数对整体性能的影响；(4) 一种主动利用随机性以提升准确率的方法论。

Result: 实验表明：LLM能有效识别特权文档；随机性控制参数对分类性能影响极小；所提出的利用随机性的方法可显著提高准确率；该方法有助于提升企业在制裁合规流程中对LLM输出的信心。

Conclusion: 随机性本身不是主要干扰源，但可通过结构化方法转化为提升LLM分类准确率与决策可信度的积极工具，尤其适用于高可靠性要求的法律合规场景。

Abstract: In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions.

</details>


### [173] [Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring](https://arxiv.org/abs/2512.08398)
*Jiin Park,Hyuna Jeon,Yoonseo Lee,Jisu Hong,Misuk Kim*

Main category: cs.IR

TL;DR: 本文提出了一种面向工业标准文档的本体知识图谱构建方法，通过分层语义结构组织、原子命题分解与大模型三元组抽取，有效建模复杂条件、约束与数值规则，并在多类QA任务上显著优于现有KG-RAG方法。


<details>
  <summary>Details</summary>
Motivation: 工业标准文档包含大量高度结构化的技术信息（如表格、适用范围、约束、例外和数值计算），传统KG构建方法难以准确刻画其复杂的逻辑与层次语义。

Method: 将标准文档组织为分层语义结构；对句子和表格进行条件与数值规则驱动的原子命题分解；利用大语言模型（LLM）进行三元组抽取，构建本体增强的知识图谱；并设计ontology-aware KG-RAG框架进行验证。

Result: 在自建的规则型、表格型、多跳问答及有毒条款检测数据集上，该方法在所有QA类型中均显著优于现有KG-RAG基线。

Conclusion: 该方法证明了在含交织条件、约束与适用范围的工业文档中，实现可靠、可扩展知识表征的可行性，为领域专用RAG与智能文档管理提供了新路径。

Abstract: Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.

</details>


### [174] [VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation](https://arxiv.org/abs/2512.08702)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Zitong Wan,Hewei Wang,Weijie Liu,Yijie Li,Edith C. H. Ngai*

Main category: cs.IR

TL;DR: 本文提出VI-MMRec框架，通过基于模态特征相似性的虚拟用户-物品交互来缓解多模态推荐中的数据稀疏问题，无需修改原有模型结构或增加训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐模型受限于数据稀疏问题，即用户仅与少量物品交互，导致未观测交互被错误视为负样本。

Method: 提出两种虚拟交互构建策略：Overlay（独立聚合各模态相似性）和Synergistic（跨模态协同融合），并设计基于统计的权重分配机制以自适应调整虚拟交互权重。

Result: 在六个真实数据集、七个SOTA多模态推荐模型上验证了VI-MMRec的有效性，显著提升性能且无额外训练成本。

Conclusion: VI-MMRec是一种模型无关、零训练开销、即插即用的通用增强框架，能有效缓解数据稀疏问题，提升多模态推荐性能。

Abstract: Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [175] [CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models](https://arxiv.org/abs/2512.07890)
*Ryan Feng Lin,Keyu Tian,Hanming Zheng,Congjing Zhang,Li Zeng,Shuai Huang*

Main category: cs.MA

TL;DR: 本文提出CrowdLLM，通过融合预训练大语言模型与生成模型，提升数字人群的多样性与保真度，理论分析与多领域实验验证其在准确性与分布保真度上优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）构建的数字人群难以充分反映真实人类群体的准确性与多样性，亟需更优方法以支撑社会仿真、众包、营销等应用。

Method: 提出CrowdLLM框架，整合预训练LLM与生成模型；进行理论分析，并在众包、投票、用户评分等多个领域开展实验与仿真研究。

Result: CrowdLLM在准确性和分布保真度两方面均显著优于仅依赖LLM的方法，能构建成本更低、更具代表性与可扩展性的高质量数字人群。

Conclusion: CrowdLLM为构建高保真、多样化、可扩展的数字人群提供了有效新范式，兼具理论支撑与实证效果。

Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.

</details>


### [176] [MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement](https://arxiv.org/abs/2512.07898)
*Hongwei Zhang,Ji Lu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.MA

TL;DR: 本文提出MARINE框架，通过多智能体递归上下文增强，将大语言模型的测试时推理重构为对持久参考轨迹的迭代优化，显著提升单次响应准确率，并在参数效率和计算效率上取得突破。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理受限于单次输出范式，无法充分释放其推理潜力，亟需一种能充分利用模型能力、兼顾性能与效率的新推理框架。

Method: 提出MARINE（Multi-Agent Recursive IN-context Enhancement）框架，将测试时推理建模为对参考轨迹的迭代精炼；设计理论支撑的精炼算子，将基础模型的pass@N能力转化为近似最优的pass@1性能；采用最小可行批次与对数增长批调度策略，在固定调用预算下最大化性能增益。

Result: 在BrowserComp-ZH基准上达到46.0% pass@1准确率（685B参数模型）；80B参数模型+MARINE媲美1000B参数独立代理，参数需求降低一个数量级以上；在固定计算预算下，为对齐与优化提供更高质量样本。

Conclusion: MARINE不仅显著提升单次响应性能，还开创了参数高效推理新范式，有望大幅提升后训练效率，具有重要理论价值与实用前景。

Abstract: Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.

</details>


### [177] [Probabilistic Multi-Agent Aircraft Landing Time Prediction](https://arxiv.org/abs/2512.08281)
*Kyungmin Kim,Seokbin Yoon,Keumjin Lee*

Main category: cs.MA

TL;DR: 本文提出了一种概率性多智能体飞机着陆时间预测框架，能够输出多个飞机着陆时间的概率分布，兼顾预测精度、不确定性量化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹和交通流具有内在不确定性，且受邻近飞机及空管干预（如雷达引导）影响，传统点估计预测难以满足空管资源分配对准确性和可信度的要求。

Method: 构建一个概率性多智能体建模框架，显式建模空域中多架飞机间的交互关系，并输出着陆时间的概率分布；利用韩国仁川国际机场终端区的航迹监视数据进行训练与评估；通过注意力机制分析空管干预模式以提升可解释性。

Result: 相比基线模型，该框架在预测精度和不确定性量化方面均更优；注意力得分揭示了空管操作的潜在规律，增强了模型可解释性。

Conclusion: 所提概率多智能体框架能有效提升着陆时间预测的准确性、鲁棒性与可解释性，为智能化空管决策提供可靠支持。

Abstract: Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [178] [Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions](https://arxiv.org/abs/2512.08500)
*Jianan Li,Xiao Chen,Tao Huang,Tien-Tsin Wong*

Main category: cs.GR

TL;DR: 本文提出Mimic2DM框架，直接从视频中提取的2D关键点轨迹学习物理仿真中的运动控制策略，无需3D动作数据，通过重投影误差训练单视角2D跟踪策略，并融合多视角提升3D能力；结合Transformer自回归2D动作生成器，实现多样化、物理合理的动作合成（如舞蹈、足球运球、动物运动）


<details>
  <summary>Details</summary>
Motivation: 现有基于视频学习3D运动控制器的方法依赖于泛化性差的3D动作重建技术，难以处理人-物交互或非人类角色等复杂场景，且需稀缺3D标注数据或产生不合理的姿态。

Method: 提出Mimic2DM：1）仅用2D关键点轨迹训练单视图物理仿真中的运动跟踪策略，以重投影误差为监督；2）通过多视角2D数据聚合隐式获得3D运动跟踪能力；3）设计Transformer自回归2D动作生成器，嵌入分层控制框架以生成高质量参考轨迹。

Result: 在舞蹈、足球运球、动物运动等多个领域实现了无需3D数据的物理合理、多样化动作合成，验证了方法在HOI与非人类角色等挑战场景下的有效性与泛化性。

Conclusion: Mimic2DM证明仅依赖广泛可用的2D视频关键点即可高效学习复杂、多样且物理真实的运动控制策略，为视频驱动的具身智能提供了新范式。

Abstract: Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [179] [Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization](https://arxiv.org/abs/2512.07969)
*Alan Papalia,Nikolas Sanderson,Haoyu Han,Heng Yang,Hanumant Singh,Michael Everett*

Main category: cs.RO

TL;DR: 本文提出了一种针对具有规范对称性（gauge symmetries）的机器人感知问题的变量投影（VarPro）新方法，联合利用可分离性与稀疏性，构建矩阵自由的Schur补算子，显著加速非线性最小二乘求解（2–35倍），并保持精度。


<details>
  <summary>Details</summary>
Motivation: 传统变量投影（VarPro）在机器人感知中应用受限，主因是感知问题普遍存在规范对称性（如全局平移/旋转不变性），导致标准VarPro出现数值与计算困难。

Method: 提出一种适配规范对称性的VarPro方案，通过一次预处理构建矩阵自由的Schur补算子，支持高效计算约化问题的成本、梯度和Hessian-向量积，并兼容标准迭代NLS求解器；给出适用条件及部分失效时的扩展策略。

Result: 在SLAM、SNL、SfM等合成与真实基准测试中，相比最先进方法，运行速度提升2–35倍，同时保持精度；开源C++实现与全部实验数据集。

Conclusion: 该方法成功将VarPro推广至含规范对称性的机器人感知NLS问题，兼顾理论严谨性与工程实用性，为大规模感知优化提供了高效新范式。

Abstract: Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \textbf{2$\times$--35$\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.

</details>


### [180] [VLD: Visual Language Goal Distance for Reinforcement Learning Navigation](https://arxiv.org/abs/2512.07976)
*Lazar Milikic,Manthan Patel,Jonas Frey*

Main category: cs.RO

TL;DR: 本文提出了一种名为视觉-语言距离（VLD）学习的新框架，用于目标导向导航，通过解耦感知学习与策略学习来解决端到端图像导航策略训练难的问题。


<details>
  <summary>Details</summary>
Motivation: 端到端从图像数据直接预测机器人导航动作存在固有困难，主要受限于仿真到现实的差距和带动作标签的训练数据不足。

Method: 首先在互联网规模视频数据上自监督训练一个距离到目标预测器（支持图像和文本目标），再用该预测器提供的距离信号指导强化学习策略训练（在仿真中使用加噪的几何距离信号）。部署时策略使用VLD预测，融合语义目标信息与仿真中学到的底层导航行为。引入序一致性评估距离函数。

Result: VLD在仿真中达到有竞争力的导航性能，支持灵活的目标模态（图像/文本），且优于ViNT、VIP等现有时间距离方法。

Conclusion: VLD提供了一条可扩展、可靠、多模态的导航策略学习新路径。

Abstract: Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-"where to go"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.

</details>


### [181] [Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems](https://arxiv.org/abs/2512.08630)
*Marta Manzoni,Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: 本文研究了多任务贝叶斯优化在多无人机系统去中心化轨迹生成算法调优中的应用，利用多任务高斯过程建模不同任务（即不同无人机交互数量的场景）间的关联性，并对比了平均任务优化与单任务优化两种策略的性能与效率权衡。


<details>
  <summary>Details</summary>
Motivation: 为提升多无人机系统中去中心化轨迹生成算法的调优效率，需在不同规模（即不同无人机交互数量）的任务间共享信息，避免重复、低效的独立优化。

Method: 采用多任务贝叶斯优化框架，以多任务高斯过程（MTGP）建模各轨迹生成场景（任务）之间的结构相关性，支持跨任务信息迁移；对比两种优化目标：所有任务的平均任务时间最小化 vs. 各任务独立优化。

Result: 仿真结果表明：单任务优化随机群规模增大可获得更短的平均任务时间，但优化耗时显著高于平均任务优化策略；而平均任务优化在总优化时间上更高效，适合实际部署。

Conclusion: 多任务贝叶斯优化能有效平衡多无人机系统中不同规模轨迹生成任务的性能与调优效率，平均任务优化策略在实践中更具可行性。

Abstract: This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach.

</details>


### [182] [DIJIT: A Robotic Head for an Active Observer](https://arxiv.org/abs/2512.07998)
*Mostafa Kamali Tabrizi,Mingshi Chi,Bir Bikram Dey,Yu Qing Yuan,Markus D. Solbach,Yiqian Liu,Michael Jenkin,John K. Tsotsos*

Main category: cs.RO

TL;DR: 本文介绍了DIJIT——一种专为移动智能体设计的新型双目机器人头部系统，具备类人眼-头-颈运动能力，用于主动视觉研究及人机视觉对比分析。


<details>
  <summary>Details</summary>
Motivation: 为了支持主动视觉研究，并探究人类眼-头-颈协同运动机制及其对视觉能力的贡献，同时对比人类视觉与当前计算机视觉在运动策略上的差异。

Method: 设计了具有9个机械自由度和4个光学自由度的DIJIT机器人头部；提出一种新型扫视相机运动控制方法，建立相机朝向与电机控制值之间的直接映射关系。

Result: DIJIT的机械运动范围和速度接近人类水平，支持会聚式立体视觉所需的各种运动（包括会聚、同向和旋扭）；新扫视方法实现了接近人类精度的快速相机定向运动。

Conclusion: DIJIT是一个功能丰富、类人性能强的主动视觉平台，其设计与控制方法为人类视觉建模与机器人视觉发展提供了重要实验基础。

Abstract: We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.

</details>


### [183] [Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations](https://arxiv.org/abs/2512.08028)
*Lampis Papakostas,Aristeidis Geladaris,Athanasios Mastrogeorgiou,Jim Sharples,Gautier Hattenberger,Panagiotis Chatzakos,Panagiotis Polygerinos*

Main category: cs.RO

TL;DR: 本文提出了一种用于灾害救援（如野火）的无人机集群系统，结合局部ESDF地图实现自主避障与编队飞行，并通过改进的TSP算法优化高价值区域覆盖。


<details>
  <summary>Details</summary>
Motivation: 提升灾后响应效率，延长续航、增强数据采集能力，并降低因碰撞导致任务失败的风险。

Method: 采用分布式传感器部署、基于局部欧氏符号距离场（ESDF）的自主导航框架，以及面向POI价值加权的TSP变体进行覆盖路径规划。

Result: 仿真验证表明该系统在不同集群规模下均能有效实现高覆盖率与可靠的障碍物及无人机间碰撞规避。

Conclusion: 所提系统兼顾安全性、协同性与任务导向性，为复杂灾害环境下的无人机集群应用提供了可行方案。

Abstract: This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles.

</details>


### [184] [An Introduction to Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2512.08052)
*Pedro Santana*

Main category: cs.RO

TL;DR: 本文介绍了深度强化学习（DRL）和深度模仿学习（DIL）在具身智能体（如机器人和虚拟角色）中的应用，聚焦于若干基础算法（如PPO、BC、DAgger、GAIL），强调深度理解而非广泛综述，并自包含地讲解所需数学与机器学习概念。


<details>
  <summary>Details</summary>
Motivation: 具身智能体面临复杂的序列决策问题，手动设计控制器困难，因此需借助学习方法（尤其是DRL和DIL）实现高效任务执行。

Method: 采用深度优先、自包含的讲解方式，系统介绍DRL（从MDP到REINFORCE、PPO）和DIL（从行为克隆到DAgger、GAIL）的核心算法与原理。

Result: 为读者提供对DRL和DIL关键方法的深入、连贯且可理解的技术导引，适用于初学者或需夯实基础的研究者。

Conclusion: DRL和DIL是解决具身智能体决策问题的有力范式；掌握其基础算法与思想比泛泛了解更有利于实际建模与研究推进。

Abstract: Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.

</details>


### [185] [Chat with UAV -- Human-UAV Interaction Based on Large Language Models](https://arxiv.org/abs/2512.08145)
*Haoran Wang,Zhuohang Chen,Guang Li,Bo Ma,Chuanghuang Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于双LLM代理（任务规划代理与执行代理）的新型人机交互（HUI）框架，以解决当前大语言模型驱动的无人机交互中混合任务规划与执行困难、适应性差的问题；通过多场景任务数据库和三类量化指标验证了其在交互流畅性、任务执行灵活性及个性化需求满足方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统预定义人机交互设计难以满足用户个性化需求，而现有基于大语言模型（LLM）的人机交互框架在混合任务规划与执行方面存在困难，导致复杂场景下适应性低；同时用户与无人机之间缺乏通用语言，限制了高效交互。

Method: 提出一种双代理HUI框架，包含独立的任务规划代理和执行代理，分别采用不同提示工程处理任务理解、规划与执行；构建覆盖农业、航拍、物流、环境监测四类典型场景的任务数据库，并采用三个独立指标量化评估性能；对比多种LLM模型控制无人机的效果。

Result: 用户实验表明，该框架显著提升了人机交互的流畅性与任务执行的灵活性，能有效满足设定场景下的用户个性化需求；不同LLM模型验证了框架的可扩展性与鲁棒性。

Conclusion: 双代理架构结合针对性提示工程，为实现用户驱动、个性化、高适应性的人机交互提供了可行路径，推动了大语言模型在无人机系统中的实际落地应用。

Abstract: The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.

</details>


### [186] [RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features](https://arxiv.org/abs/2512.08170)
*Haoxin Zhang,Shuaixin Li,Xiaozhou Zhu,Hongbo Chen,Wen Yao*

Main category: cs.RO

TL;DR: 本文提出了一种用户友好的LiDAR-相机标定工具包，仅需一对激光点和一张相机图像，无需标定板，在无目标环境中即可完成高鲁棒性、高精度的外参标定。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-相机标定方法通常依赖标定板、初始位姿估计或特定传感器配置，限制了其在真实复杂场景（如无目标、大外参偏差）下的适用性与易用性。

Method: 基于Gluestick构建2D-3D点线特征对应以获得鲁棒初值；定量分析特征分布对结果的影响，并据此自适应加权各特征代价，滤除劣质特征干扰，优化外参。

Result: 在多种LiDAR-相机组合及室内外场景下实验验证，本方法在鲁棒性和精度上均优于当前SOTA方法。

Conclusion: 该工具包具有强通用性、免初始猜测、单帧输入、高鲁棒与高精度等优势，代码已开源。

Abstract: In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.

</details>


### [187] [Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation](https://arxiv.org/abs/2512.08186)
*Meng Wei,Chenyang Wan,Jiaqi Peng,Xiqian Yu,Yuqiang Yang,Delin Feng,Wenzhe Cai,Chenming Zhu,Tai Wang,Jiangmiao Pang,Xihui Liu*

Main category: cs.RO

TL;DR: 本文提出DualVLN，一种双系统视觉语言导航（VLN）基础模型：System 2（VLM全局规划器）进行图像引导的中长期路径点推理；System 1（轻量级多模态扩散Transformer策略）基于像素目标和System 2的特征生成平滑轨迹。二者解耦训练，兼顾泛化性与实时性，在各类VLN基准及真实动态环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端VLM导航方法动作碎片化、延迟高、难以应对动态障碍等现实挑战。

Method: 提出双系统架构：System 2为VLM驱动的全局规划器，执行‘慢思考’式图像接地的中程路径点预测；System 1为轻量级多模态条件Diffusion Transformer策略，融合显式像素目标与System 2的隐含特征，实现‘快执行’式平滑轨迹生成；两系统训练解耦。

Result: 在所有VLN基准上超越先前方法；真实世界实验验证其长时序规划能力与动态环境下的实时适应性。

Conclusion: DualVLN通过高-低两级协同架构，首次实现了兼具强泛化能力、实时控制性能与动态鲁棒性的VLN基础模型。

Abstract: While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, "grounds slowly" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, "moves fast" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.

</details>


### [188] [Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model](https://arxiv.org/abs/2512.08188)
*Wenjiang Xu,Cindy Wang,Rui Fang,Mingkang Zhang,Lusong Li,Jing Xu,Jiayuan Gu,Zecui Zeng,Rui Chen*

Main category: cs.RO

TL;DR: 本文提出了Embodied Tree of Thoughts (EToT)，一种结合物理仿真与视觉语言模型的Real2Sim2Real规划框架，用于提升机器人操作任务中的物理一致性和长期规划鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频生成的世界模型缺乏严格的物理基础，易产生幻觉且难以满足长期物理约束（如刚体动力学和碰撞）。

Method: 提出EToT框架：以物理交互数字孪生为具身世界模型；通过先验分支（语义与空间分析生成候选路径）和反思分支（VLM诊断仿真中失败并迭代修正）进行树搜索规划。

Result: 在短/长期操作任务上显著优于基线方法，能准确预测物理动态并自适应应对执行失败。

Conclusion: 将高层推理锚定于物理仿真可有效提升操作规划的物理一致性与鲁棒性，EToT为具身智能体提供了一种可靠、可解释的规划范式。

Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .

</details>


### [189] [High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement](https://arxiv.org/abs/2512.08206)
*Duo Zhang,Junshan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: 本文提出了SDAR（同步双臂重排规划器），一种用于桌面重排任务的任务与运动规划（TAMP）框架，通过紧耦合的任务规划器（SDAR-T）和同步双臂运动规划器（SDAR-M），高效求解强纠缠、非单调、长时序的双臂协同重排问题，并在仿真与真实UR-5e机器人上均实现100%成功率。


<details>
  <summary>Details</summary>
Motivation: 解决双臂在狭小空间内协同完成强纠缠、非单调桌面重排任务的挑战，克服单臂规划器难以处理依赖复杂、需高同步性与高成功率的实际需求。

Method: 提出SDAR框架，包含：1）SDAR-T——基于依赖图分解的双臂任务规划器，生成优于单臂最优解的双臂任务计划；2）SDAR-M——基于GPU SIMD加速的分层同步双臂运动规划器，高效筛选并执行高质量同步运动。

Result: 在复杂非单调长时序桌面重排任务中达到100%成功率，解质量显著超越此前SOTA；在两台UR-5e真实机器人上成功部署并稳定运行。

Conclusion: SDAR通过任务与运动的紧密协同设计及硬件感知优化，为强耦合多臂操作提供了可扩展、高可靠、可迁移的TAMP新范式。

Abstract: We propose Synchronous Dual-Arm Rearrangement Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal configurations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR-M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state-of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware.

</details>


### [190] [Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior](https://arxiv.org/abs/2512.08233)
*Timothy Chen,Marcus Dominguez-Kuhne,Aiden Swann,Xu Liu,Mac Schwager*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯框架的语义与空间可变风险建模方法，通过视觉语言模型先验与可学习ViT似然函数结合，从人类安全示范视频中提取隐式风险模型，生成像素级风险图，支持机器人规划与类人运动生成。


<details>
  <summary>Details</summary>
Motivation: 人类对安全的理解是连续、上下文相关且空间依赖的风险感知，而非二值信号；现有方法难以建模这种主观但理性的风险判断，需从人类示范中隐式提取风险模型。

Method: 提出语义条件化、空间变化的风险参数化方法：以预训练VLM提供贝叶斯先验，以可学习ViT作为似然函数将视觉特征映射为像素级风险值，联合监督于安全示范视频和VLM常识；输入RGB图像与查询物体名称，输出像素密集风险图。

Result: 生成的风险图能准确反映人类风险偏好；成功应用于视觉运动规划的价值学习和经典轨迹优化中，支持新物体与新场景泛化，并具备基于新观测或常识规则的快速自适应能力。

Conclusion: 该贝叶斯风险建模框架是使自主系统内化类人风险认知的重要进展，兼具可解释性、泛化性与可扩展性。

Abstract: Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/.

</details>


### [191] [Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks](https://arxiv.org/abs/2512.08248)
*Ahan Basu,Ratnangshu Das,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 本文提出了一种基于时空管（STT）的控制框架，用于处理具有未知动力学的非线性MIMO纯反馈系统，在外部扰动下满足时间约束的到达-避免-停留（T-RAS）任务。通过物理信息神经网络（PINN）近似STT的中心与半径，并结合Lipschitz条件进行连续时间形式验证，最终设计出无需近似的解析控制器。


<details>
  <summary>Details</summary>
Motivation: 针对未知动态、外部扰动及严格时间约束下的非线性MIMO纯反馈系统，现有方法难以同时保证安全性、实时性与形式可验证性，亟需一种兼具学习能力与理论保障的新控制框架。

Method: 定义时空管（STT）为时变球体，用物理信息神经网络（PINN）联合逼近其中心与半径；将STT约束编码为PINN损失函数并设计训练算法；引入Lipschitz连续性条件实现对连续时间域的形式化验证；基于学习到的STT构造无近似、解析形式的闭环控制器。

Result: 在移动机器人和无人机两个复杂场景中验证了该框架的有效性与可扩展性：成功实现高精度时间约束下的避障导航与停留任务，且控制器满足形式化安全保证。

Conclusion: 所提STT-PINN框架将数据驱动建模与形式化验证有机结合，为未知非线性系统的安全时序控制提供了新范式，具备理论严谨性与工程实用性。

Abstract: This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments.

</details>


### [192] [Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation](https://arxiv.org/abs/2512.08271)
*Srijan Dokania,Dharini Raghavan*

Main category: cs.RO

TL;DR: Zero-Splat TeleAssist 是一种零样本传感器融合方法，利用普通CCTV视频流构建共享的6自由度世界模型，支持多边遥操作，无需标记物或深度传感器。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人遥操作中缺乏全局位姿感知、依赖专用硬件（如深度传感器或fiducials）的问题，提升交互式遥操作的普适性与实时性。

Method: 融合视觉-语言分割、单目深度估计、加权PCA姿态提取和3D高斯溅射（3DGS），从CCTV流中构建实时6-DoF世界模型。

Result: 实现无需fiducials或深度传感器的多机器人全局位置与朝向实时估计，支持交互中心式多边遥操作。

Conclusion: Zero-Splat TeleAssist 展示了仅用普通监控视频即可构建高保真、可共享的3D世界模型的可行性，为低成本、可扩展的遥操作系统提供了新范式。

Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.

</details>


### [193] [Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making](https://arxiv.org/abs/2512.08280)
*Haldun Balim,Na Li,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了MPDiffuser，一种基于扩散模型的离线决策框架，通过规划器、动力学模型和排序器的组合设计，交替优化轨迹的任务对齐性与动力学可行性，在多个基准和真实机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有离线决策中的生成方法常产生动力学不可行的轨迹，难以兼顾任务目标与系统动态一致性。

Method: 提出Model Predictive Diffuser（MPDiffuser），包含三模块：任务对齐的轨迹规划器、保障动力学一致性的动力学模型、以及任务目标导向的行为排序器；采用交替扩散采样机制，在生成过程中协同优化任务对齐性与可行性，并提供理论分析支持。

Result: 在D4RL和DSRL等离线决策基准上显著优于现有方法；可扩展至视觉控制任务，并在真实四足机器人上成功部署。

Conclusion: MPDiffuser的模块化与交替优化设计提升了样本效率与泛化能力，为离线强化学习提供了兼具可靠性与实用性的新范式。

Abstract: Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.

</details>


### [194] [Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging](https://arxiv.org/abs/2512.08333)
*Yajat Yadav,Zhiyuan Zhou,Andrew Wagenmaker,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出一种通过插值微调模型与预训练模型权重的方法，在不损害原有泛化能力的前提下，使通用机器人策略能稳健地学习新任务。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略在面对训练数据未覆盖的新任务时表现不佳，微调易导致过拟合并丧失原有泛化能力。

Method: 采用微调模型与预训练模型权重插值的简单策略（model merging）。

Result: 在大量仿真与真实世界实验中，该方法显著提升模型对新任务分布外变化的泛化能力，并支持持续学习新技能而不遗忘原有能力。

Conclusion: 权重插值是一种有效且轻量的方法，可兼顾新任务学习与通用能力保留，为通用机器人策略的持续适应提供新范式。

Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.

</details>


### [195] [Learning Robot Manipulation from Audio World Models](https://arxiv.org/abs/2512.08405)
*Fan Zhang,Michael Gienger*

Main category: cs.RO

TL;DR: 本文提出了一种生成式潜在流匹配模型，用于预测未来音频观测，从而增强机器人策略在需要多模态（尤其是音频）时序推理任务中的长期决策能力。


<details>
  <summary>Details</summary>
Motivation: 许多机器人学习任务需要多模态（如视听）联合推理，而仅靠视觉信息往往模糊或不完整；尤其在涉及物理交互（如倒水）或节奏性信号（如音乐）的任务中，对音频时序演化与内在节律的建模至关重要。

Method: 提出一种生成式潜在流匹配模型，用于建模并预测未来音频观测序列；该模型嵌入到机器人策略中，支持基于未来音频状态的长程动作规划。

Result: 在两个需感知真实环境音频或音乐信号的操作任务上，所提方法显著优于无未来预测能力的基线方法；验证了准确预测具节奏特性的未来音频状态对成功学习机器人动作的关键作用。

Conclusion: 机器人动作学习不仅依赖多模态输入，更关键在于能精准建模和预测体现物理规律与内在节律的未来音频状态；所提流匹配模型为此类时序多模态推理提供了有效路径。

Abstract: World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.

</details>


### [196] [A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems](https://arxiv.org/abs/2512.08476)
*Po-An Shih,Shao-Hua Wang,Yung-Che Li,Chia-Heng Tu,Chih-Han Chang*

Main category: cs.RO

TL;DR: 本文提出了一种基于多智能体大语言模型（LLM）的设计空间探索（DSE）框架，用于自动驾驶系统开发，通过融合多模态推理、3D仿真与性能分析工具，自动解析执行结果并指导设计探索，在机器人出租车案例中优于遗传算法基线。


<details>
  <summary>Details</summary>
Motivation: 传统DSE方法难以处理自动驾驶系统中多模态执行输出和复杂性能权衡，且依赖人工判断正确性，亟需自动化、智能化的探索方法。

Method: 构建多智能体LLM框架，各智能体分别负责用户输入理解、设计点生成、执行编排及多模态（视觉+文本）输出分析，并集成3D仿真与性能剖析工具实现闭环自动化DSE。

Result: 在SAE L4机器人出租车案例中，相较遗传算法基线，在相同探索预算下找到更多Pareto最优、成本更优且导航时间更短的设计方案；验证了LLM方法在DSE中的高效性。

Conclusion: 该LLM驱动的多智能体DSE框架显著提升了自动驾驶系统设计自动化水平，为未来设计自动化提供了新范式。

Abstract: Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.

</details>


### [197] [Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception](https://arxiv.org/abs/2512.08481)
*Yixiang Lin,Tiancheng Yang,Jonathan Eden,Ying Tan*

Main category: cs.RO

TL;DR: 本研究通过一项物理耦合的目标到达任务，探究人类在人机交互（pHRI）中面对不确定性（如扰动概率变化）时的行为差异，发现存在‘权衡型’和‘始终补偿型’两类个体策略，表明人类对概率的感知与真实值存在偏差，需引入如累积前景理论（CPT）等更可解释的行为模型以改进自适应机器人控制。


<details>
  <summary>Details</summary>
Motivation: 传统pHRI控制基于最优控制理论，假设人类行为最小化代价函数，但现实中人类在不确定性下的行为常偏离最优；亟需深入理解人类在不确定性下的实际响应机制。

Method: 开展一项物理耦合的目标到达实验，机器人以10%至90%的系统性概率施加辅助或扰动；采集参与者的力输入与决策行为，并采用聚类分析识别行为模式。

Result: 识别出两种显著不同的行为集群：'权衡型'参与者根据扰动概率调整物理响应，'始终补偿型'则表现出强风险厌恶、不随概率变化而调整；证实人类对概率的感知存在主观偏差且行为高度个体化。

Conclusion: 人类在pHRI中的不确定性响应具有显著个体差异和非理性特征，现有最优控制框架难以准确建模；应采用如累积前景理论（CPT）等行为经济学模型构建更可解释、更贴合实际的人类行为模型，以支撑下一代自适应机器人控制器的设计。

Abstract: Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\% to 90\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a "trade-off" group that modulated their physical responses according to disturbance likelihood, and an "always-compensate" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.

</details>


### [198] [SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking](https://arxiv.org/abs/2512.08518)
*Nadezhda Kushina,Ko Watanabe,Aarthi Kannan,Ashita Ashok,Andreas Dengel,Karsten Berns*

Main category: cs.RO

TL;DR: 本研究探讨了人类与人形机器人Ameca互动时的舒适度，发现瞳孔最小直径是预测舒适度的关键指标，决策树模型表现最佳（F1=0.73），表明人机交互中的生理舒适阈值与人际交互不同。


<details>
  <summary>Details</summary>
Motivation: 社交机器人需适应人类的近体空间规范以保障用户舒适与参与度；眼动特征虽在人际互动中能可靠估计舒适度，但在人机互动中的适用性尚不明确。

Method: 在四个受控距离（0.5–2.0米）下，使用移动眼动追踪与主观报告（N=19），评估多种机器学习与深度学习模型基于眼动特征预测用户舒适度的能力。

Result: 决策树分类器性能最优（F1-score = 0.73），最小瞳孔直径是最关键预测因子；Transformer模型未如人际研究中表现优异。

Conclusion: 人机交互中的生理舒适阈值不同于人际互动，且可通过可解释的逻辑模型（如决策树）有效建模。

Abstract: Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.

</details>


### [199] [vEDGAR -- Can CARLA Do HiL?](https://arxiv.org/abs/2512.08541)
*Nils Gehrke,David Brecht,Dominik Kulmer,Dheer Patel,Frank Diermeyer*

Main category: cs.RO

TL;DR: 本文提出vEDGAR框架，通过改进CARLA模拟器，实现硬件在环（HiL）实时仿真，支持自动驾驶软件在真实硬件上的测试与性能边界识别，提升开源开发流程的一致性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有开源仿真器（如CARLA）缺乏对自动驾驶车辆完整传感器与执行器栈的实时仿真能力，难以支撑硬件在环（HiL）测试，限制了其在全流程开发中的评估一致性。

Method: 首先分析需求并设计仿真架构，随后开发基于CARLA改进的vEDGAR软件框架，集成真实硬件接口，支持实时闭环仿真，并对其开展系统性评估。

Result: 成功构建并验证了vEDGAR框架，证明改进后的CARLA可有效支持自动驾驶软件的HiL测试，具备实际工程应用潜力。

Conclusion: CARLA经vEDGAR增强后，可作为贯穿自动驾驶研发全流程的统一、开源、可扩展的HiL评估工具，显著提升开发效率与测试可靠性。

Abstract: Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR

</details>


### [200] [Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations](https://arxiv.org/abs/2512.08548)
*Yuchi Zhang,Churui Sun,Shiqi Liang,Diyuan Liu,Chao Ji,Wei-Nan Zhang,Ting Liu*

Main category: cs.RO

TL;DR: 本文提出一种语义驱动的语言化动作表征方法，通过忽略数值尺度、强调方向性来缓解机器人操作中因平台和任务差异导致的动作数据分布偏移问题，从而提升预训练模型的泛化与迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端机器人操作方法受制于不同平台和任务间动作数据的严重分布偏移（主要源于动作命令的数值差异），导致预训练知识难以有效迁移。

Method: 提出一种语义 grounded 的语言化动作表征（motion representation），摒弃对数值尺度的依赖，仅保留动作的方向性信息，并拉近动作 token 与语言词汇 token 的特征距离，以弥合模态鸿沟。

Result: 在两个多任务基准上的实验表明，该方法显著提升了模型在机器人操作任务中的泛化性能与跨任务/平台迁移能力。

Conclusion: 语义化的、尺度无关的动作表征可有效缓解分布偏移、缩小模态差距，是提升机器人操作大模型预训练效果与实用性的可行路径。

Abstract: Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.

</details>


### [201] [RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight](https://arxiv.org/abs/2512.08574)
*Vit Kratky,Robert Penicka,Parakh M. Gupta,Ondrej Prochazka,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于非线性模型预测控制（NMPC）与时间依赖的互惠速度约束（RVCs）的无人机协同避撞方法，仅依赖可观测信息、无需频繁通信，计算高效（100 Hz），适用于高机动性飞行，并在仿真与真实实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖大量通信，难以满足高机动无人机对实时性与低通信开销的需求；需一种仅基于局部可观测信息、计算高效且能处理非线性动力学的避撞方法。

Method: 将时间依赖的互惠速度约束（RVCs）直接嵌入非线性模型预测控制（NMPC）问题中，设计高效RVC计算算法，并在控制器层面实现端到端集成。

Result: 算法可在100 Hz实时运行；在最多10架、速度达25 m/s的仿真及加速度达30 m/s²的真实实验中均实现无碰撞导航；相比前沿方法，在复杂场景下飞行时间减少31%。

Conclusion: 该方法在保证安全性的同时显著提升导航效率与实时性，为高动态多无人机系统提供了一种实用、低通信依赖的避撞解决方案。

Abstract: This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.

</details>


### [202] [Mind to Hand: Purposeful Robotic Control via Embodied Reasoning](https://arxiv.org/abs/2512.08580)
*Peijun Tang,Shangjin Xie,Binyan Sun,Baifu Huang,Kuncheng Luo,Haotian Yang,Weiqi Jin,Jianan Wang*

Main category: cs.RO

TL;DR: Lumo-1 是一个统一视觉-语言-动作的通用机器人模型，通过三阶段预训练与强化学习，将大模型的推理能力与物理动作对齐，在复杂、长时程、自然语言指令驱动的机器人任务中展现出强泛化与执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI虽具备广泛推理能力，但难以将其有效落地到物理世界中的具身行动；亟需构建能将语义推理与运动控制深度对齐的通用具身智能模型。

Method: 提出三阶段预训练流程：（1）基于精选视觉-语言数据持续预训练VLM以增强具身推理（如规划、空间理解）；（2）跨机器人平台的视觉-语言-动作联合共训练；（3）在双臂移动操作机器人Astribot S1上，基于带推理过程的动作轨迹进行动作训练；最后引入强化学习优化推理-动作一致性。

Result: 在具身视觉语言推理任务中显著优于基线；真实世界实验表明其在多类挑战性任务（尤其长时程、新物体/环境、自然语言指令）中性能领先，具备强泛化能力。

Conclusion: Lumo-1成功实现了‘心智’与‘手’的统一，验证了分阶段渐进式扩展大模型至具身行动路径的有效性，为通用机器人智能提供了新范式。

Abstract: Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning ("mind") with robot action ("hand"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.

</details>


### [203] [A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation](https://arxiv.org/abs/2512.08653)
*Doumegna Mawuto Koudjo Felix,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了一种面向激光雷达的、基于现象学的退化模拟框架，直接在真实点云上施加物理可解释的退化（如遮挡、噪声、视场缩小等），用于可控、可复现的SLAM压力测试。


<details>
  <summary>Details</summary>
Motivation: 现有激光雷达SLAM鲁棒性评估方法缺乏物理依据或无法刻画传感器特异性行为，难以反映真实退化对系统性能的影响。

Method: 构建传感器感知、现象学驱动的点云退化模拟框架，支持结构化丢点、视场缩减、高斯噪声、遮挡掩码、稀疏化和运动畸变等退化类型，保留点云几何、强度与时间结构，并集成自动传感器识别、四级严重度配置及ROS实时兼容性。

Result: 在三种激光雷达架构和五种前沿SLAM系统上验证，揭示了传感器设计与环境上下文共同塑造的鲁棒性差异模式。

Conclusion: 该开源框架为激光雷达SLAM在物理可解释退化场景下的基准评测提供了实用基础。

Abstract: Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.

</details>


### [204] [Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes](https://arxiv.org/abs/2512.08656)
*Lauritz Rismark Fosso,Herman Biørn Amundsen,Marios Xanthidis,Sveinung Johan Ohrem*

Main category: cs.RO

TL;DR: 本文提出了一种名为Sim2Swim的零样本、基于深度强化学习（DRL）的AUV速度控制器，仅需3分钟仿真训练即可实现跨平台、无需调参的6自由度敏捷路径跟踪与控制，并在水池实验中验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有AUV控制器依赖大量人工调参，难以适应载荷和水动力环境变化，导致6自由度敏捷运动在实际中极少应用。

Method: 提出Sim2Swim方法，结合域随机化与大规模并行DRL训练，实现零样本sim2real迁移，无需后处理或微调。

Result: 仅用3分钟仿真训练即获得可直接部署的控制器，在多种AUV配置的水池试验中成功实现高敏捷性6DOF运动与路径跟踪。

Conclusion: Sim2Swim是首个通用、零样本、训练极快的DRL速度控制器，显著提升了AUV在动态环境下的自适应控制能力与工程实用性。

Abstract: Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.

</details>


### [205] [Ergodic Trajectory Planning with Dynamic Sensor Footprints](https://arxiv.org/abs/2512.08661)
*Ziyue Zheng,Yongce Liu,Hesheng Wang,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 本文提出了一种考虑动态、分辨率可变传感器覆盖区域的遍历性轨迹规划新方法，通过设计新度量、理论分析与数值优化算法，显著提升了遍历性（达一个数量级），并在多无人机3D目标覆盖任务中成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有遍历性规划方法常假设点传感器或固定形状/分辨率的传感器覆盖区域，但实际中（如无人机下视相机）传感器覆盖区域随姿态和高度动态变化，导致规划效果下降。

Method: 提出一种能刻画动态传感器覆盖区域的新遍历性度量；分析其局部最优性理论条件；设计相应的数值轨迹优化算法。

Result: 实验表明所提方法在遍历性指标上比传统方法提升达一个数量级，并在多无人机系统中实现了对三维空间中物体的遍历性覆盖。

Conclusion: 考虑动态传感器覆盖区域的遍历性规划是提升信息采集效率的关键，本文提出的度量与优化框架为该问题提供了有效且可部署的解决方案。

Abstract: This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.

</details>


### [206] [Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility](https://arxiv.org/abs/2512.08688)
*Mark Pustilnik,Francesco Borrelli*

Main category: cs.RO

TL;DR: 本文提出了一种动态博弈框架，用于共享工作空间中带障碍物的人机协同导航，通过引入非归一化均衡结构来灵活分配人与机器人在安全约束（如避障、间距）上的努力程度，并将其嵌入滚动时域最优控制方案中。


<details>
  <summary>Details</summary>
Motivation: 解决共享工作空间中人机协同导航的安全性与任务协作问题，尤其在存在障碍物时需联合满足安全约束并完成共同任务。

Method: 构建基于动态博弈的协同导航模型，提出非归一化均衡结构以差异化分配人与机器人对安全约束的贡献，并将其嵌入滚动时域最优控制框架。

Result: 实现了在共享约束下人机灵活协作的导航策略，支持不同主体按能力差异分担安全责任。

Conclusion: 非归一化均衡结构提升了人机协同导航中安全约束执行的灵活性与适应性，为复杂共享环境下的交互控制提供了新范式。

Abstract: This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.

</details>


### [207] [A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models](https://arxiv.org/abs/2512.08754)
*Jason Hughes,Marcel Hussing,Edward Zhang,Shenbagaraj Kannapiran,Joshua Caswell,Kenneth Chaney,Ruichen Deng,Michaela Feehery,Agelos Kratimenos,Yi Fan Li,Britny Major,Ethan Sanchez,Sumukh Shrote,Youkang Wang,Jeremy Wang,Daudi Zein,Luying Zhang,Ruijun Zhang,Alex Zhou,Tenzi Zhouga,Jeremy Cannon,Zaffir Qasim,Jay Yelon,Fernando Cladera,Kostas Daniilidis,Camillo J. Taylor,Eric Eaton*

Main category: cs.RO

TL;DR: This paper introduces a heterogeneous robotic system (UAVs and UGVs) for remote primary triage in mass-casualty incidents, enabling victim localization, injury assessment, vital sign monitoring, mental status evaluation, and data integration—without endangering first responders.


<details>
  <summary>Details</summary>
Motivation: To enhance safety and efficiency of primary triage in mass-casualty incidents by removing first responders from hazardous environments while improving speed and accuracy of victim assessment.

Method: A coordinated air-ground robotic team: UAVs perform aerial search and casualty localization; UGVs carry sensors to measure vital signs and detect/localize physical injuries; integrated software fuses data for triage classification (e.g., injury severity, mental status).

Result: A fully functional, end-to-end remote triage system validated in the DARPA Triage Challenge, demonstrating capability across all key triage tasks—localization, physiological sensing, injury classification, mental assessment, and decision-support data delivery.

Conclusion: Heterogeneous multi-robot systems can effectively extend human capabilities in disaster response, offering a scalable, safe, and comprehensive solution for remote primary triage in MCIs.

Abstract: This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved.

</details>


### [208] [Data-Driven Dynamic Parameter Learning of manipulator robots](https://arxiv.org/abs/2512.08767)
*Mohammed Elseiagy,Tsige Tadesse Alemayoh,Ranulfo Bezerra,Shotaro Kojima,Kazunori Ohno*

Main category: cs.RO

TL;DR: 本文提出了一种基于Transformer的动态参数估计方法，结合自动化数据生成与雅可比特征增强，显著提升了机器人质量、惯性等参数的估计精度，有助于缩小仿真到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 缩小仿真到现实（sim-to-real）的差距是机器人学中的基础挑战，而准确的动力学参数估计对基于模型的控制、高保真仿真和安全部署至关重要；传统解析方法难以应对复杂结构与交互，而常规神经网络（如RNN）又难以建模长程依赖。

Method: 提出基于Transformer的动力学参数估计方法，构建自动化流水线生成8192种多样化机器人模型及轨迹数据，并引入雅可比导出的运动学特征进行数据增强；利用注意力机制联合建模时间与空间依赖关系。

Result: 最优配置（序列长度64、采样率64Hz、4层、32头）在验证集上R²达0.8633；质量与惯性估计接近完美，库仑摩擦中高精度，粘性摩擦与远端连杆质心位置估计仍具挑战。

Conclusion: Transformer结合自动化数据生成与运动学特征增强，实现了可扩展、高精度的动力学参数估计，有效推动了机器人系统的sim-to-real迁移能力。

Abstract: Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems

</details>


### [209] [Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks](https://arxiv.org/abs/2512.08813)
*Connor York,Zachary R Madin,Paul O'Dowd,Edmund R Hunt*

Main category: cs.RO

TL;DR: 本文研究多机器人系统在执行连续任务时遇到紧急子任务时的性能权衡问题，提出通过行为异质性（角色专业化）和感知异质性（仅部分机器人具备特定感知能力）来优化权衡，并通过仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在执行如区域巡逻等连续任务时，常需响应突发、时间敏感的子任务（如定位异常无线电信号），由此引发任务间性能冲突与权衡问题。

Method: 采用仿真实验方法，对比分析行为同质与异质（划分为'巡逻者'和'搜索者'角色）、感知同质与异质（仅搜索者具备无线电信号感知能力）等多种团队构型下的性能表现，识别帕累托最优权衡点。

Result: 行为异质团队在多数情况下展现出最均衡的任务权衡；当感知能力受限时，半数机器人具备感知能力的异质团队性能可媲美全感知的同质团队，从而节省传感器载荷成本。

Conclusion: 预部署的角色与感知专业化是应对时间冲突任务的重要设计原则，调节行为异质程度可灵活调控系统偏向任一任务的性能表现。

Abstract: Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization ("patrollers" and "searchers") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task.

</details>


### [210] [IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams](https://arxiv.org/abs/2512.08877)
*Ryan LeRoy,Jack Kolb*

Main category: cs.RO

TL;DR: 本文研究了多智能体强化学习（MARL）中自博弈训练的PPO智能体是否能学到泛化协调策略，还是仅过拟合于训练伙伴行为；作者在异构多智能体挑战（HeMAC）环境中提出旋转策略训练（RPT）方法，通过轮换不同算法的异构队友策略来增强训练多样性；实验发现，即使未经历队友多样性训练，标准IPPO基线在面对未见过的DDQN队友时仍能达到与RPT相近的性能，表明IPPO本身已具备较强队友泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究自博弈训练的MARL智能体（尤其是使用参数共享的IPPO）是否真正学到基于游戏本质的通用协调策略，还是仅仅过拟合于固定训练伙伴的行为，尤其在异构智能体设置下。

Method: 在HeMAC异构环境（Observer与Drone角色互补）中，提出Rotating Policy Training（RPT）：训练过程中动态轮换来自不同算法（如DDQN、A2C等）的异构队友策略，以提升主智能体对多样化队友的适应性；对比标准IPPO（所有智能体共享单一PPO策略、无队友多样性）在面对 withheld DDQN队友时的泛化性能。

Result: RPT与IPPO在面对未参与训练的DDQN队友时表现相当，说明IPPO虽未显式引入队友多样性，却已具备良好泛化能力；RPT未带来显著性能增益。

Conclusion: 在所研究的异构MARL任务中，标准IPPO基线本身已隐含足够程度的队友泛化能力，挑战了‘必须通过显式多样化训练才能获得泛化’的常见假设；RPT等复杂训练机制未必必要。

Abstract: Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.

</details>


### [211] [OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer](https://arxiv.org/abs/2512.08920)
*Jessica Yin,Haozhi Qi,Youngsun Wi,Sayantan Kundu,Mike Lambeta,William Yang,Changhao Wang,Tingfan Wu,Jitendra Malik,Tess Hellebrekers*

Main category: cs.RO

TL;DR: OSMO is an open-source wearable tactile glove with 12 three-axis tactile sensors, designed to bridge the tactile embodiment gap between humans and robots for contact-rich manipulation tasks; it enables robot policies trained solely on human tactile demonstrations to achieve 72% success on a real-world wiping task, outperforming vision-only methods.


<details>
  <summary>Details</summary>
Motivation: Human video demonstrations lack rich contact signals essential for manipulation learning, and existing methods struggle with tactile embodiment mismatch and unreliable vision-based force inference.

Method: Design and deployment of OSMO — a tactile glove with 12 three-axis sensors on fingertips and palm, compatible with hand-tracking; training robot policies exclusively on human demonstrations collected with OSMO, while equipping both human and robot with identical gloves to align tactile and visual embodiment.

Result: A robot policy trained only on OSMO-collected human data achieves 72% success on a real-world wiping task, significantly surpassing vision-only baselines by mitigating contact-related failures.

Conclusion: Tactile-aware human demonstration collection via OSMO effectively closes the embodiment gap and enables high-performance transfer of contact-rich manipulation skills without any real robot training data.

Abstract: Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [212] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver提出一种自适应并行推理框架，在不牺牲准确率的前提下显著降低大模型推理延迟，适用于现成的自回归推理引擎。


<details>
  <summary>Details</summary>
Motivation: 现有自适应并行推理方法在真实任务中受限于监督行为克隆、精度下降明显或需定制化推理引擎，部署困难。

Method: ThreadWeaver包含三部分：1）两阶段并行轨迹生成器构建高质量并行CoT数据；2）基于Trie的训推协同设计，兼容标准自回归引擎；3）并行感知的强化学习框架，优化精度与并行效率的权衡。

Result: 在六个数学推理基准上，基于Qwen3-8B的ThreadWeaver平均准确率达71.9%（AIME24达79.9%），token延迟平均加速1.53倍。

Conclusion: ThreadWeaver在准确率与推理效率间建立了新的Pareto前沿，实现了无需修改底层引擎的高效并行推理。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [213] [HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability](https://arxiv.org/abs/2512.07988)
*Sudhanva Manjunath Athreya,Paul Rosen*

Main category: cs.LG

TL;DR: 本文提出了HOLE方法，利用持续同调分析深度神经网络的潜在表示，通过多种可视化技术揭示模型的表征结构、质量及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽成功但缺乏可解释性，亟需能揭示其内部表征与决策机制的分析工具。

Method: 提出HOLE（Homological Observation of Latent Embeddings），基于神经激活提取拓扑特征，并结合Sankey图、热图、树状图和斑点图等可视化手段进行多层表征分析。

Result: 在标准数据集和多种判别模型上验证表明，HOLE能揭示类别分离、特征解耦及模型鲁棒性等关键模式。

Conclusion: 拓扑分析为理解与改进深度学习系统提供了互补且有效的视角，增强了模型的可解释性与可靠性。

Abstract: Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.

</details>


### [214] [Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning](https://arxiv.org/abs/2512.07844)
*Jinping Wang,Zhiqiang Gao,Zhiwu Xie*

Main category: cs.LG

TL;DR: 本文研究长尾分布下神经坍缩（NC）现象失效的问题，提出特征空间与分类器权重空间对齐的新视角，并设计三种即插即用的对齐策略，在多个长尾数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在长尾数据分布下，样本不平衡导致神经坍缩（NC）现象难以出现，进而损害泛化性能；现有方法多关注恢复ETF几何结构，却忽视了特征空间与分类器权重空间之间的严重错位问题。

Method: 通过最优错误指数分析理论量化空间错位的危害，并据此提出三种无需修改网络结构、可即插即用的显式对齐策略。

Result: 在CIFAR-10-LT、CIFAR-100-LT和ImageNet-LT数据集上，所提方法在多个基线模型上持续提升性能，达到当前最优水平。

Conclusion: 特征与分类器权重空间的对齐是恢复长尾学习中神经坍缩的关键，所提对齐策略有效且通用。

Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.

</details>


### [215] [CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics](https://arxiv.org/abs/2512.07847)
*Mohamed Elrefaie,Dule Shu,Matt Klenk,Faez Ahmed*

Main category: cs.LG

TL;DR: 本文提出了CarBench，首个面向大规模3D汽车空气动力学的综合基准，基于DrivAerNet++数据集评估11种主流模型，涵盖神经算子、几何深度学习、Transformer求解器和隐式场网络，并开源了完整框架。


<details>
  <summary>Details</summary>
Motivation: 尽管CFD数据集日益丰富，工程设计领域仍缺乏标准化的大规模数值模拟基准，亟需统一评估平台推动数据驱动的工程建模发展。

Method: 构建CarBench基准，使用DrivAerNet++（8000+高保真汽车仿真）对11类模型进行系统评估；引入跨类别泛化实验、物理一致性检验、计算效率分析及基于Bootstrap的不确定性估计。

Result: 全面揭示了各类模型在预测精度、物理合理性、推理速度与不确定性量化方面的性能差异；发现Transformer类求解器具备跨类别泛化潜力，但存在物理不一致性问题。

Conclusion: CarBench为数据驱动的工程仿真建立了首个可复现、开源、多维评估的基准框架，有望成为CFD机器学习研究的标准基础设施。

Abstract: Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench.

</details>


### [216] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash 是一个轻量、可解释的结构化伤情严重程度预测模型，基于纽约市机动车碰撞数据，采用树模型（XGBoost/RF）优于小语言模型（SLM），并结合SHAP分析关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 纽约市每年发生超十万起机动车碰撞事故，造成重大伤害与公共卫生负担，亟需高效、可解释的伤情严重度预测工具。

Method: 构建整合三张表的统一特征架构，采用分区存储；训练轻量树模型（Random Forest/XGBoost）于工程化表格特征，并对比本地部署的小语言模型（SLM）在文本摘要上的表现；辅以类别加权与SHAP归因分析。

Result: XGBoost和Random Forest在时序外测试集上准确率分别为0.7828和0.7794，显著优于SLM（0.594/0.496）；类别加权提升致死类召回率；SHAP显示人体脆弱性、时间与地点是主导预测因子。

Conclusion: 可解释的小型集成树模型仍是城市级伤情分析的强基线；结合表格预测器与SLM生成叙述的混合流程可在不牺牲可扩展性的前提下提升沟通效果。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [217] [SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: 本文分析了大语言模型（LLM）智能体在长周期、工具调用任务中失败的根本原因，发现环境变更类动作（mutating actions）中的早期偏差（decisive deviations）是导致失败的关键因素，而非变更类动作影响甚微；基于此，提出一种模型无关、无需梯度的测试时防护机制CM，包含变异门控验证、面向目标的反思和分块上下文清理，显著提升多个基准上的成功率，并发布修正版基准τ-Bench Verified以消除标注与任务定义缺陷。


<details>
  <summary>Details</summary>
Motivation: 理解LLM智能体在长周期、工具使用任务中性能脆弱的根本原因，特别是探究不同动作类型对失败的贡献是否均等。

Method: 通过在τ-Bench（Airline/Retail）和SWE-Bench Verified上分析执行轨迹，将动作分解为mutating与non-mutating两类，并形式化‘decisive deviations’；利用逻辑回归量化偏差影响；据此设计模型无关的测试时防护机制CM，含mutation-gated verification、Targeted Reflection和block-based context cleaning。

Result: mutating动作中每多一个decisive deviation，SoTA模型成功率下降高达92%（Airline）或96%（Retail），而non-mutating动作偏差几乎无影响；CM带来显著提升，如Qwen3-Thinking在Airline上相对提升28%，并发布τ-Bench Verified以修复原基准天花板问题。

Conclusion: 行动级细粒度分析、面向关键动作的针对性防护、以及更可靠的评估基准，是构建鲁棒多轮智能体的必要前提。

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $τ$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $τ$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $τ$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [218] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 本文提出了一种预测多模态模型训练时GPU峰值内存使用量的框架，通过分解模型结构并分层因子化估算内存，实现了约8.7%的平均MAPE。


<details>
  <summary>Details</summary>
Motivation: 现有GPU内存预测方法仅适用于单模态模型，难以泛化至智能体AI系统中广泛使用的多模态模型，而OOM问题会中断训练并浪费大量计算资源。

Method: 将多模态模型按层分解，并对每层进行因子化建模以估算其内存占用，从而预测整体峰值GPU内存使用量。

Result: 在多模态模型上验证了该框架，平均绝对百分比误差（MAPE）约为8.7%，预测精度高。

Conclusion: 所提框架有效解决了多模态模型GPU内存预测难题，为避免OOM、提升训练稳定性与资源利用率提供了实用工具。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [219] [HSTMixer: A Hierarchical MLP-Mixer for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2512.07854)
*Yongyao Wang,Jingyuan Wang,Xie Yu,Jiahao Ji,Chao Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为HSTMixer的新型分层时空混合框架，采用全MLP架构，通过分层时空混合模块和自适应区域混合器，实现了高效且准确的大规模交通预测。


<details>
  <summary>Details</summary>
Motivation: 现有模型在大规模交通预测中存在二次计算复杂度问题，难以应用于实际大规模场景。

Method: 提出Hierarchical Spatio-Temporal Mixer（HSTMixer），采用全MLP架构；设计分层时空混合块进行多分辨率特征提取（底向上聚合与顶向下传播）；引入基于区域语义生成变换矩阵的自适应区域混合器。

Result: 在四个大规模真实世界数据集上实验表明，该方法在预测精度上达到SOTA，同时具备良好的计算效率。

Conclusion: HSTMixer为大规模交通预测提供了一种高效、可扩展且性能优越的新范式。

Abstract: Traffic forecasting task is significant to modern urban management. Recently, there is growing attention on large-scale forecasting, as it better reflects the complexity of real-world traffic networks. However, existing models often exhibit quadratic computational complexity, making them impractical for large-scale real-world scenarios. In this paper, we propose a novel framework, Hierarchical Spatio-Temporal Mixer (HSTMixer), which leverages an all-MLP architecture for efficient and effective large-scale traffic forecasting. HSTMixer employs a hierarchical spatiotemporal mixing block to extract multi-resolution features through bottom-up aggregation and top-down propagation. Furthermore, an adaptive region mixer generates transformation matrices based on regional semantics, enabling our model to dynamically capture evolving spatiotemporal patterns for different regions. Extensive experiments conducted on four large-scale real-world datasets demonstrate that the proposed method not only achieves state-of-the-art performance but also exhibits competitive computational efficiency.

</details>


### [220] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: 本文提出了一种面向Transformer模型的跨阶段稀疏加速方法LAPA，通过算法-架构协同设计，在保持性能的同时显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏Transformer方法多为单阶段设计，跨阶段应用时存在高功耗问题，而Transformer各阶段计算瓶颈具有动态性，需跨阶段稀疏策略。

Method: 提出log域注意力预测算法-架构协同设计LAPA，包括：1）非对称前导一计算（ALOC）消除乘法；2）混合精度多轮移位累加（MRSA）降低累加开销；3）数据特征依赖滤波（DDF）策略配合MRSA；4）定制化硬件加速器实现。

Result: LAPA在能效上相比SOTA工作Spatten、Sanger和FACT分别提升3.52倍、3.24倍和2.79倍。

Conclusion: LAPA通过算法与硬件协同优化，有效解决了Transformer跨阶段稀疏加速中的功耗与效率瓶颈，为高效AI推理提供了新思路。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [221] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: 本文提出了一种名为CLDD的图神经网络模型，通过协同学习患者与疾病之间的关联，在无需大量医学检测的情况下，实现对成百上千种疾病的高效、低成本预测。


<details>
  <summary>Details</summary>
Motivation: 传统疾病检测依赖大量昂贵且耗时的医学检查，难以覆盖所有可能疾病，亟需一种低成本、高效率的大规模疾病筛查方法。

Method: 提出基于图神经网络的协同学习框架CLDD，利用电子健康记录中的患者-疾病交互和人口统计学特征，自适应建模疾病间关联与患者间相似性。

Result: 在MIMIC-IV数据集（61,191名患者、2,000种疾病）上，CLDD在召回率和精确率上分别提升6.33%和7.63%，并能准确恢复被掩码的真实疾病。

Conclusion: CLDD显著降低了诊断成本，提升了可及性，适用于大规模疾病筛查与公共卫生保障。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [222] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: 本文提出SA^2GFM框架，通过结构感知语义增强、信息瓶颈压缩、专家自适应路由和分层结构联合学习，提升图基础模型在噪声与对抗攻击下的鲁棒性与跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型（GFMs）在面对领域噪声、结构扰动和对抗攻击时鲁棒性不足，且缺乏对层次化结构语义的充分建模，限制了其泛化能力。

Method: 提出SA^2GFM：1）将基于熵的编码树转化为结构感知文本提示以增强特征；2）采用自监督信息瓶颈机制进行结构引导的表征压缩；3）设计含空专家的混合专家路由机制缓解负迁移；4）构建联合社区内/间结构学习的微调模块。

Result: 在节点与图分类任务上，SA^2GFM显著优于9种SOTA方法，在随机噪声与对抗扰动下均展现出更强的有效性与鲁棒性。

Conclusion: 结构感知语义增强与分层表征学习可有效提升图基础模型的鲁棒性与跨域适应能力，为构建可信图AI提供了新范式。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [223] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的频率感知交互式Mamba模型FAIM，用于时间序列分类（TSC），通过自适应滤波块（AFB）和交互式Mamba块（IMB）提升特征表达能力，并引入自监督预训练增强鲁棒性，在多个基准上优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在时间序列分类中存在计算成本高、对噪声敏感、小样本易过拟合等问题。

Method: 提出FAIM模型，包含：1）基于傅里叶变换的自适应滤波块（AFB），含可学习阈值与全局-局部语义耦合滤波；2）交互式Mamba块（IMB），支持多粒度信息交互；3）自监督预训练机制。

Result: 在多个基准数据集上，FAIM持续超越现有SOTA方法，在精度与效率间取得更优平衡，尤其在高噪声场景下表现突出。

Conclusion: FAIM是一种高效、鲁棒且表达能力强的时间序列分类模型，为TSC任务提供了新的轻量级建模范式。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [224] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: 本文提出SetAD框架，将半监督异常检测重构为集合级任务，利用注意力机制的集合编码器和上下文校准的异常评分机制，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法局限于点或简单对的建模，忽视异常的上下文性和高阶交互，无法充分利用组合式监督信号。

Method: 提出SetAD框架：1）基于注意力的集合编码器，通过分级学习目标量化整个集合的异常程度；2）上下文校准的异常评分机制，通过多组多样化上下文集合中归一化偏差聚合来评估单点异常分数。

Result: 在10个真实数据集上显著优于当前最优方法，且性能随集合大小增加而持续提升，验证了集合级建模范式的有效性。

Conclusion: 集合级建模能更准确捕捉异常的上下文本质和高阶交互，是半监督异常检测的重要新方向。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [225] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 本文提出一种基于无监督机器学习的框架，用于从海量海关数据中自动识别可疑贸易模式，以支持环境条约（如《蒙特利尔议定书》）的履约监测。


<details>
  <summary>Details</summary>
Motivation: 需要新方法来监测环境条约（如《蒙特利尔议定书》）的执行情况，尤其是通过分析大规模、高复杂度的海关数据。

Method: 结合多种无监督机器学习技术：K-Means聚类发现贸易模式原型；Isolation Forest与IQR异常检测识别‘巨型贸易’和异常单价；启发式规则标记模糊货描；最终融合生成优先级评分。

Result: 在10万条贸易记录上成功识别出1351个价格异常点和1288个高优先级货物；验证了高优先级商品具有显著更高的价值/重量比；SHAP解释性分析确认模糊描述和高单价是最关键风险因子；模型灵敏度通过捕捉2021年初‘巨型贸易’激增得到实证。

Conclusion: 该研究构建了一个可复用的无监督学习流水线，能将原始贸易数据转化为监管机构可操作的情报，为国际环境条约合规监测提供了新范式。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [226] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 本文将瑞典大规模登记数据转化为文本化生命轨迹，利用NLP模型（如LSTM、BERT等）预测个体居住流动性，验证了文本化登记数据在纵向分析中的有效性与潜力。


<details>
  <summary>Details</summary>
Motivation: 解决数据中分类变量高基数性及随时间变化的编码方案不一致两大长期难题，并充分利用瑞典高质量、长时段、全覆盖的人口登记数据优势。

Method: 将690万瑞典居民2001–2013年的登记数据（含居住、工作、教育、收入、家庭等年度信息）转化为语义丰富的文本生命轨迹；采用LSTM、DistilBERT、BERT、Qwen等多种NLP架构预测2013–2017年居住流动性。

Result: 序列与Transformer模型显著优于基线模型；文本化登记数据能有效保留个体路径信息，支持复杂、可扩展建模；该数据集为序列建模方法提供了稀缺而严谨的实证测试平台。

Conclusion: 将语义丰富的登记数据与现代语言模型结合，可显著推动社会科学中的纵向分析发展。

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [227] [Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing](https://arxiv.org/abs/2512.07866)
*Maria Milena Araujo Felix*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习和自然语言处理（NLP）技术的DGA域名检测方法，使用LSTM模型在混合数据集上实现了97.2%的准确率，并显著降低了合法流量中的误报率。


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件利用域名生成算法（DGA）动态生成大量域名，使传统的基于静态黑名单的防御手段失效，亟需更有效的DGA检测方法。

Method: 构建包含5万合法与5万恶意域名的混合数据集，提取词汇特征，并训练长短期记忆网络（LSTM）模型进行分类。

Result: 所提LSTM方法在检测复杂DGA域名时优于传统统计熵分析，准确率达97.2%，且在模糊的合法流量场景中误报率更低。

Conclusion: 结合深度学习与NLP的LSTM模型是检测DGA域名的有效方案，尤其适用于应对高级、复杂的DGA变种。

Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.

</details>


### [228] [Bayesian Optimization for Function-Valued Responses under Min-Max Criteria](https://arxiv.org/abs/2512.07868)
*Pouya Ahadi,Reza Marzban,Ali Adibi,Kamran Paynabar*

Main category: cs.LG

TL;DR: 本文提出了一种新的贝叶斯优化框架MM-FBO，用于优化具有函数型响应（如随时间或波长变化的响应）的昂贵黑箱函数，其核心是直接最小化函数域上的最大误差，而非平均误差；通过函数主成分分析建模响应，并构建主成分得分的高斯过程代理模型，设计了兼顾最坏情况期望误差与探索性的集成不确定性采集函数；理论方面给出了离散化误差界和一致性保证；实验验证了其在合成基准及物理驱动任务（如超光子器件电磁散射、气相渗透）中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法主要针对标量响应，难以处理科学与工程中常见的函数型响应（如随时间/波长连续变化的输出）；且常用积分误差准则忽视最坏情况偏差，无法满足对鲁棒性要求高的场景。

Method: 提出min-max Functional Bayesian Optimization (MM-FBO)：1）用函数主成分分析（FPCA）表征函数型响应；2）对主成分得分分别构建高斯过程代理模型；3）设计集成不确定性采集函数，联合优化最坏情况下的期望误差与功能域上的探索。

Result: 理论证明了最坏情况目标的离散化误差界和采集函数的一致性收敛；实验表明MM-FBO在合成函数与两个物理真实案例（电磁散射、气相渗透）中均显著优于现有基线方法。

Conclusion: 显式建模函数型响应的不确定性并以min-max准则优化，能显著提升贝叶斯优化在关键科学应用中的鲁棒性与实用性；MM-FBO为函数型响应优化提供了新范式。

Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.

</details>


### [229] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种基于混合专家（MoE）的扩散模型框架，用于医疗时间序列信号重建，通过Receptive Field Adaptive MoE（RFAMoE）和Fusion MoE模块，实现了自适应感受野选择与单步多噪声信号融合重建，显著提升了性能并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 医疗生理时间序列具有多变量、高时变性、强噪声和易受伪迹干扰等特点，使得现有深度学习方法（尤其是扩散模型）在插补等任务中仍面临挑战。

Method: 提出基于分数的扩散框架，包含两个核心MoE模块：1）Receptive Field Adaptive MoE（RFAMoE），使各通道在扩散过程中自适应选择感受野；2）Fusion MoE，利用MoE并行生成K个噪声信号，并通过路由机制融合，在单次推理中完成重建。

Result: 在多个任务和数据集上，该框架持续超越基于扩散的SOTA方法，同时避免了多次推理带来的高计算成本和延迟。

Conclusion: 所提MoE增强的扩散模型有效应对医疗时间序列的复杂特性，在保持高性能的同时大幅降低推理开销，为临床应用提供了更实用的解决方案。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [230] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: 本文提出了可控风险代理生成（CRAG）框架，用于在仿真中统一建模自动驾驶车辆周围交通参与者在正常与安全关键场景下的行为，通过解耦潜在空间实现对风险行为的可控、高保真生成，从而提升AV鲁棒性评估的效率与针对性。


<details>
  <summary>Details</summary>
Motivation: 现有仿真难以同时兼顾交通参与者的日常行为真实性与事故相关风险行为的合理性，导致AV安全测试缺乏对罕见但关键场景的有效覆盖。

Method: CRAG构建了一个结构化潜在空间，解耦正常行为与风险行为表征，并结合风险感知的潜在表示和基于优化的模式转换机制，实现从安全状态到风险状态的平滑、可信过渡。

Result: 实验表明CRAG在生成多样性上优于现有基线，并支持可控生成风险场景，提升了AV鲁棒性评估的针对性和效率。

Conclusion: CRAG为自动驾驶安全测试提供了一种兼顾真实性与可控性的新型代理行为建模方法，有效缓解了稀缺碰撞数据利用难题。

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [231] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 本文提出Softly Symbolified Kolmogorov-Arnold Networks (S2KAN)，通过在训练中引入符号基元并结合可微稀疏门控与最小描述长度原则，提升KAN的符号可解释性与建模能力，在保持高精度的同时显著减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有Kolmogorov-Arnold Networks（KANs）虽具可解释潜力，但实际训练出的激活函数常缺乏符号保真度，导致分解结果病态且不可解释。

Method: S2KAN将每个激活函数建模为符号项与稠密项的可学习加权组合，采用可微稀疏门控机制，并以最小描述长度（MDL）为优化目标进行端到端训练。

Result: 在符号回归基准、动力系统预测及真实世界任务上，S2KAN在精度相当或更优的前提下，模型规模显著更小；且观察到无需显式正则化即出现自发稀疏化现象。

Conclusion: S2KAN成功桥接符号先验与神经拟合能力，在可解释性与泛化性之间实现更好平衡，为构建兼具精度与透明性的AI模型提供了新范式。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [232] [Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling](https://arxiv.org/abs/2512.07876)
*Qi Chen,Mihai Anitescu*

Main category: cs.LG

TL;DR: 本文提出了一种傅里叶增强的循环神经网络（RNN），用于电力负荷降尺度预测，融合了低分辨率输入驱动的RNN主干、显式傅里叶季节性嵌入以及自注意力机制，显著优于Prophet等基线模型。


<details>
  <summary>Details</summary>
Motivation: 提升电力负荷降尺度预测精度，尤其在捕捉季节性和高分辨率时序依赖方面存在现有方法（如Prophet和普通RNN）的不足。

Method: 构建傅里叶增强RNN：1）以低分辨率数据为输入的RNN主干；2）在潜在空间中融合显式傅里叶季节性嵌入；3）引入自注意力层建模每个周期内高分辨率组件间的依赖关系。

Result: 在PJM四个区域上，该模型的RMSE低于并更平稳于经典Prophet（含/不含季节性和LAA）及未使用注意力或傅里叶特征的RNN消融模型。

Conclusion: 傅里叶季节性建模与自注意力机制的结合能有效提升RNN在负荷降尺度任务中的性能，验证了显式频域先验与序列建模协同的有效性。

Abstract: We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.

</details>


### [233] [Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design](https://arxiv.org/abs/2512.07877)
*Amogh Anshu N,Harish BP*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的NoC设计空间探索框架，利用BookSim仿真和逆向神经网络模型（MLP、条件扩散模型、CVAE）预测最优NoC参数；其中条件扩散模型在未见数据上MSE达0.463，显著加速设计探索。


<details>
  <summary>Details</summary>
Motivation: 传统NoC设计空间探索方法速度慢且难以处理复杂非线性参数交互，难以满足高吞吐与低延迟需求。

Method: 构建基于BookSim仿真的机器学习框架，训练并比较MLP、条件扩散模型和CVAE三种逆向神经网络架构，从15万+仿真数据中学习性能指标到NoC参数的映射。

Result: 条件扩散模型预测精度最高（MSE=0.463），设计探索时间降低数个数量级。

Conclusion: 该框架可实现快速、可扩展的NoC协同设计，为NoC自动化设计提供实用新范式。

Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.

</details>


### [234] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges -- Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 本文提出使用Youden's J统计量（及其等价的平衡准确率）来选择用于评估大语言模型的分类器（如LLM-as-a-judge或人工标注员），以克服传统指标（如Accuracy、F1）在类别不平衡和正类定义上的敏感性问题，从而提升模型比较的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM的分类器选择标准（如Accuracy、Precision、F1）易受类别不平衡和正类定义影响，可能导致对模型性能的错误估计。

Method: 理论分析Youden's J统计量与模型比较目标的一致性；证明Balanced Accuracy是J的线性变换；通过解析推导、实证案例与模拟实验验证其优越性。

Result: Balanced Accuracy作为分类器选择指标，能更稳健、准确地支持LLM间基于行为流行率（如任务通过率、违规率）的比较。

Conclusion: 应优先采用Balanced Accuracy（或Youden's J）而非传统指标来选择LLM评估中的judges，以保障评估结果的可信度与可比性。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [235] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 本文提出了一种新的对比学习损失函数SpecMatch-CL，通过最小化不同视图下图嵌入构成的‘图中图’的归一化拉普拉斯矩阵差异，来对齐全局结构；理论证明其差异可作为理想对比损失与均匀性损失差的上界；实验表明其在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如InfoNCE）仅优化图嵌入在不同增强视图间的两两对齐，缺乏对全局图中图结构的控制机制。

Method: 提出SpecMatch-CL损失函数，通过最小化不同视图下图嵌入所构建的图中图的归一化拉普拉斯矩阵差异，实现全局结构对齐；并从理论上证明该差异对理想对比损失与均匀性损失差具有上界保证。

Result: 在八个TU数据集的无监督与低标签率半监督学习任务中取得SOTA；在PPI-306K和ZINC 2M迁移学习任务中也获得一致提升。

Conclusion: SpecMatch-CL通过引入图谱结构对齐机制，弥补了传统对比学习忽略全局结构的缺陷，在理论和实验上均验证了其有效性与泛化能力。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [236] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文从凸锥几何角度重新审视非负矩阵分解（NMF），提出Cone Collapse算法，通过收缩非负正交锥来恢复数据的极小生成锥，并在此基础上构建锥感知的正交NMF模型（CC-NMF），在多个数据集上聚类性能优于主流NMF方法。


<details>
  <summary>Details</summary>
Motivation: 现有NMF聚类方法多基于优化视角，未显式利用NMF所诱导的数据凸锥几何结构（即数据点位于由‘主题’张成的凸锥中）；本文旨在从该几何本质出发，提升NMF聚类的理论性与实用性。

Method: 提出Cone Collapse算法：从全非负正交锥出发，迭代收缩至数据生成的最小凸锥，理论上可有限步恢复X^T的极小生成锥；进而对恢复出的极向量应用单正交NMF，构建CC-NMF模型。

Result: 在16个基因表达、文本和图像基准数据集上，CC-NMF在聚类纯度指标上持续达到或超越包括乘性更新、ANLS、投影NMF、ONMF和稀疏NMF在内的多种强基线方法。

Conclusion: 显式建模并恢复数据的凸锥结构，不仅能提供坚实的理论保证（如有限步收敛与极锥恢复），还能显著提升NMF在聚类任务中的实际性能，为NMF提供了新的几何驱动范式。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [237] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文提出CLOP方法，通过促进类别嵌入形成正交线性子空间来防止对比学习中的维度坍缩问题，并在图像分类和目标检测任务中展现出更好的性能与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 对比学习中存在维度坍缩问题，尤其在半监督和自监督设置下严重影响表征质量；现有方法缺乏对学习率敏感性和坍缩机制的深入理解。

Method: 首先识别出导致坍缩的关键学习率阈值，进而提出CLOP损失函数，在半监督框架下显式约束类嵌入分布于正交线性子空间中。

Result: 在真实与合成数据集上的实验表明，CLOP提升了图像分类和目标检测性能，并在不同学习率和批量大小下表现出更强的训练稳定性。

Conclusion: CLOP有效缓解了对比学习中的维度坍缩问题，为设计更鲁棒的半监督对比学习方法提供了新思路。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [238] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2 是对 GSPN 的算法与系统联合优化，通过单个 2D GPU 核心、共享内存复用、通道级紧凑传播策略，显著降低计算开销，同时保持视觉 Transformer 级别的精度。


<details>
  <summary>Details</summary>
Motivation: 现有 GSPN 实现存在 GPU 内核频繁启动、全局内存数据搬运过多、各通道独立权重导致冗余计算等问题，限制其在高分辨率图像和长视频等实际场景中的效率。

Method: 提出 GSPN-2：（1）算法上采用紧凑的通道传播策略，用统一结构化变换替代每通道独立权重；（2）系统上整合为单个 2D GPU kernel，按 warp 绑定通道切片，并利用 shared memory 缓存前一列激活值。

Result: 在图像分类与文生图任务中，GSPN-2 在保持 transformer 级精度的同时，大幅降低计算成本，建立了建模全局空间上下文的新效率前沿。

Conclusion: GSPN-2 通过算法-硬件协同设计，在维持高精度前提下实现了接近线性的复杂度与卓越的实际加速效果，为高效视觉建模提供了新范式。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [239] [ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking](https://arxiv.org/abs/2512.07885)
*Davide Donno,Donatello Elia,Gabriele Accarino,Marco De Carlo,Enrico Scoccimarro,Silvio Gualdi*

Main category: cs.LG

TL;DR: ByteStorm是一个无需阈值调优的数据驱动框架，利用深度学习检测热带气旋中心，并结合BYTE算法追踪路径，在东/西太平洋盆地展现出优于现有确定性追踪器的性能。


<details>
  <summary>Details</summary>
Motivation: 传统热带气旋追踪方法依赖主观设定的阈值，易导致区域偏差，亟需更客观、鲁棒的自动化方法。

Method: 使用相对涡度（850 hPa）和海平面气压作为输入，通过深度学习网络进行热带气旋中心的分类与定位检测，再用BYTE算法将检测结果关联成完整轨迹。

Result: 在东太平洋（ENP）和西太平洋（WNP）盆地测试中，ByteStorm的探测概率分别达85.05%和79.48%，误报率分别为23.26%和16.14%，年际变率相关系数达0.75和0.69。

Conclusion: ByteStorm验证了深度学习与计算机视觉融合在热带气旋快速、精准追踪中的有效性，为传统方法提供了可靠替代方案。

Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.

</details>


### [240] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: 本文提出了一种名为Brush的符号回归（SR）算法，将决策树式的分割机制与非线性常数优化结合，以支持规则与连续函数的联合建模，特别适用于可解释的临床风险评分构建。Brush在SRBench基准上达到Pareto最优，并成功复现两种主流临床评分系统，兼具高预测精度与模型简洁性。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归难以建模医学决策中常见的‘风险方程+规则’混合逻辑；而临床亟需数据驱动、可解释的风险评分工具。

Method: Brush算法融合决策树式分裂策略与非线性常数优化，在符号回归框架中显式引入分段/条件规则，支持符号表达式与离散逻辑的协同学习。

Result: Brush在SRBench上实现Pareto最优；在两个真实临床评分任务中准确率高、模型简洁，性能优于或等同于决策树、随机森林及其他SR方法。

Conclusion: Brush拓展了符号回归的能力边界，使其能自然建模临床决策中的混合逻辑，为可解释AI在医疗领域的落地提供了新路径。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [241] [Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training](https://arxiv.org/abs/2512.08894)
*Jakub Krajewski,Amitis Shidani,Dan Busbridge,Sam Wiseman,Jason Ramapuram*

Main category: cs.LG

TL;DR: 本文提出了一种直接建模下游任务性能随训练预算缩放的框架，发现固定token-to-parameter比率下，log准确率遵循简单幂律；该方法比传统两阶段法更准确，并扩展至不同比率和推理计算场景，实验验证于最大17B参数、350B token的模型。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型缩放定律主要关注预训练损失等代理指标，而下游任务性能预测被认为不可靠；本文旨在挑战这一观点，建立直接预测下游性能的可靠缩放框架。

Method: 提出基于训练预算（如token数、参数量）直接建模下游任务log准确率的幂律框架；引入可泛化至不同token-to-parameter比率及考虑重复采样推理开销的函数形式；在多种数据混合、参数规模和数据量上进行实证验证。

Result: 在固定token-to-parameter比率下，log准确率服从简单幂律；直接缩放法外推精度优于传统两阶段法；所提函数形式能跨比率和推理设定预测准确率；结果在最大17B参数、350B token的模型上得到验证。

Conclusion: 下游任务性能可被简洁、鲁棒地建模为训练预算的幂函数，无需依赖预训练损失作为中介；该发现支持更可靠、端到端的大模型性能预测与资源规划。

Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.

</details>


### [242] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: 本文提出了CIP-Net，一种无需存储旧样本（exemplar-free）、基于原型的自解释持续学习模型，兼顾可解释性、高性能与低内存开销。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的灾难性遗忘问题，并克服现有可解释AI方法依赖后验解释或需额外任务内存、扩展性差的局限。

Method: 提出CIP-Net——一种自解释、原型驱动、无需存储历史样本的持续学习模型，通过内在解释机制在预测时保留知识。

Result: 在任务增量和类别增量设定下，CIP-Net在无样本方法和自解释方法中均达到SOTA性能，且内存开销显著更低。

Conclusion: CIP-Net为持续学习提供了一种实用、可解释且高效的解决方案。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [243] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 本文介绍了一个面向医疗领域的时序预测Web平台，旨在降低非专业用户在数据分析、建模与结果解读方面的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 时序预测在医疗等领域有广泛应用，但其所需的技术专长成为研究人员和临床医生使用的障碍。

Method: 开发一个支持数据上传、可视化、多种可定制预测模型训练，并结合大语言模型提供参数推荐与结果解释的Web平台。

Result: 平台实现了数据探索、模型训练与结果解释的一体化流程，并支持集成到学习型健康系统中以实现临床数据的持续收集与推理。

Conclusion: 该平台显著提升了时序预测技术在临床研究中的可及性与实用性，为学习型健康系统提供了可行工具。

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [244] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 本文在MIMIC-IV数据集上对比了三种离线多目标强化学习（MORL）算法与三种单目标基线算法，结果表明PEDA DT算法在灵活性和性能上更优，验证了离线MORL在重症监护中实现个性化、可调决策的可行性。


<details>
  <summary>Details</summary>
Motivation: 临床重症监护中需权衡生存率与资源消耗等冲突目标，传统单目标强化学习策略僵化，难以适应动态临床需求；而多目标强化学习（MORL）虽具潜力，但在医疗场景中必须满足离线学习约束。

Method: 在MIMIC-IV数据集上，对三种离线MORL算法（CPQL、自适应CPQL、改进的PEDA Decision Transformer）与三种单目标基线（BC、CQL、DDQN）进行基准测试，采用Off-Policy Evaluation（OPE）指标评估性能。

Result: PEDA DT算法展现出优于静态标量化基线的灵活性；序列建模架构（如Decision Transformer）在多目标条件生成任务中仍保持鲁棒性和有效性。

Conclusion: 离线多目标强化学习是实现重症监护中无需重训练的个性化、偏好可调临床决策的可行且有前景的框架。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [245] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: 本文提出了CLARITY，一种面向临床肿瘤学的医学世界模型，能够基于患者个体化时序与临床信息，在结构化潜在空间中预测疾病动态演化，并将预测结果转化为可解释、可操作的治疗决策。


<details>
  <summary>Details</summary>
Motivation: 当前静态AI模型无法预测肿瘤动态演化；现有医学世界模型（如MeWM）忽视患者个体时序与临床背景，且缺乏连接预测与治疗决策的反馈机制。

Method: 提出CLARITY模型：在结构化潜在空间中建模治疗条件下的疾病进展轨迹，显式融合时间间隔（时间上下文）和患者特异性数据（临床上下文），并设计预测到决策的可解释转化框架。

Result: 在MU-Glioma-Post数据集上，CLARITY较最新医学世界模型MeWM提升12%，显著优于各类医学专用大语言模型。

Conclusion: CLARITY实现了生理可信、个体化、可解释的疾病演化预测与治疗规划，为临床决策提供了新范式。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [246] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: 本文提出LUNA，一种可学习核函数的线性注意力机制，在保持O(n)计算复杂度的同时，达到甚至超越标准softmax注意力的精度。


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力依赖固定、数据无关的随机特征映射（如傅里叶特征），导致精度与效率不可兼得；需打破这一权衡。

Method: 设计可学习的正定核函数及其对应的流式特征映射，使特征基能自适应数据与任务，支持线性时间与内存复杂度。

Result: 在Long Range Arena（LRA）上取得同等计算预算下高效Transformer中的SOTA平均准确率；并能在BERT/ViT等预训练模型中后置替换softmax，经少量微调即恢复大部分性能，显著优于固定线性化方法。

Conclusion: 学习核而非使用固定核是提升线性注意力表达能力与实用性的关键路径，LUNA成功弥合了效率与精度之间的鸿沟。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [247] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 本文提出了一种可解释的深度竞争风险模型——深度核Aalen-Johansen（DKAJ）估计器，它推广了经典的Aalen-Johansen非参数累积发生率函数（CIF）估计，并通过自动学习的核函数加权聚类实现个体预测与可视化解释。


<details>
  <summary>Details</summary>
Motivation: 提升竞争风险模型的可解释性，同时保持预测性能；经典Aalen-Johansen估计器缺乏对个体异质性的建模能力，而现有深度模型往往缺乏透明性。

Method: 提出Deep Kernel Aalen-Johansen（DKAJ）模型：将每个样本表示为若干聚类的加权组合，权重由自动学习的相似性核函数生成；当样本仅属于一个聚类时，其预测CIF退化为该聚类内经典Aalen-Johansen估计。

Result: 在四个标准竞争风险数据集上，DKAJ在预测性能上媲美当前最优方法，并支持可视化以辅助模型解释。

Conclusion: DKAJ成功融合了经典非参数估计的可解释性与深度学习的表达能力，为临床等高风险决策场景提供了兼具准确性与透明性的新工具。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [248] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种因果引导的多模态领域泛化（MMDG）框架，通过对抗解耦和统一表征学习解决社交媒体危机分类中跨未见灾难类型的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨不同危机类型时泛化能力差，原因在于无法区分虚假特征与因果特征，且未能对齐异构模态的表征空间，导致难以迁移单模态领域泛化技术到多模态场景。

Method: 提出因果引导的多模态领域泛化（MMDG）框架，结合对抗解耦（聚焦领域不变的因果特征）与统一表征学习（将多模态特征映射到共享隐空间），从而支持单模态领域泛化策略向多模态扩展。

Result: 在多个数据集上的实验表明，该方法在未见灾难场景下显著优于现有方法，实现了最优性能。

Conclusion: 因果建模与多模态表征对齐是提升危机分类跨领域泛化能力的关键，所提MMDG框架为多模态领域泛化提供了新思路。

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [249] [Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders](https://arxiv.org/abs/2512.08077)
*Jaron Cohen,Alexander G. Hasson,Sara Tanovic*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏自编码器的技术，用于揭示化学语言模型（CLM）内部可解释的潜在特征，并在FM4M SMI-TED模型上验证了其能有效识别与化学结构、理化性质及药理分类相关的语义特征。


<details>
  <summary>Details</summary>
Motivation: 随着生成式模型在药物和材料发现等高风险领域应用日益广泛，理解化学语言模型（CLM）内部如何表征化学知识变得至关重要，但目前这方面的可解释性研究仍严重不足。

Method: 将稀疏自编码器技术扩展应用于化学语言模型（CLM），以提取并分析其潜在空间中的可解释特征；在FM4M SMI-TED模型上进行实证，考察特征在多个分子数据集上的激活模式。

Result: 成功提取出语义明确的潜在特征，并发现这些特征与化学结构片段、理化性质及药理学药物类别存在显著相关性，表明CLM内部编码了丰富的化学知识。

Conclusion: 该方法为化学AI系统的可解释性研究提供了通用框架，有助于深化对模型内在机制的理解，并推动计算化学研究的加速发展。

Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.

</details>


### [250] [Complexity of One-Dimensional ReLU DNNs](https://arxiv.org/abs/2512.08091)
*Jonathan Kogan,Hayden Jananthan,Jeremy Kepner*

Main category: cs.LG

TL;DR: 本文研究了一维ReLU深度神经网络的表达能力，通过分析其线性区域数量，证明了在无限宽度极限下随机初始化网络的期望线性区域数渐近于各层神经元总数加一，并提出了一个函数自适应的稀疏性度量。


<details>
  <summary>Details</summary>
Motivation: 理解一维ReLU深度神经网络的表达能力，特别是其线性区域数量如何随网络结构变化。

Method: 理论分析：在无限宽度极限和He初始化（含非零偏置）条件下，推导随机初始化一维ReLU全连接网络的期望线性区域数量；并提出一种函数自适应的稀疏性定义。

Result: 证明期望线性区域数为∑n_i + o(∑n_i) + 1；提出基于目标函数逼近所需最小区域数的自适应稀疏性度量。

Conclusion: 一维ReLU网络的线性区域数量主要由总神经元数决定，且其实际使用效率可通过新提出的函数自适应稀疏性来刻画。

Abstract: We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{o}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.

</details>


### [251] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 本文提出了一种通过让大语言模型（LLM）进行‘自白’（confession）来促使其诚实报告自身缺陷的方法，该自白独立于主回答进行奖励，旨在激励模型坦白而非掩盖错误行为，并在GPT-5-Thinking上验证了其在幻觉、指令遵循、阴谋行为和奖励黑客等场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在强化学习训练中因奖励塑形不当可能被无意激励说谎或掩盖自身行为，导致其在报告自身行动与信念时不诚实。

Method: 设计一种独立于主回答的‘自白’机制，在训练中仅依据自白内容的诚实性给予奖励，使模型发现‘坦白错误’比‘掩盖错误’更容易获得高奖励，从而激励诚实自白。

Result: 在GPT-5-Thinking上的实验表明，当模型在主回答中说谎或隐瞒缺陷时，其自白往往能诚实揭露这些行为；且随着训练推进，自白诚实度略有提升；自白可用于监控、拒绝采样及向用户提示问题等推理时干预。

Conclusion: 自白机制是一种可行且有潜力的对齐技术，能有效提升LLM在分布外场景下的可解释性与可信度，尤其对严重误行为具有良好的坦白倾向。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [252] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: 本文提出了一种名为MAC（Model-Based RL with Action Chunks）的离线强化学习方法，通过引入动作块（action-chunk）建模和拒绝采样策略，缓解长视界下模型误差累积问题，在大规模离线数据集上显著提升了长程任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决离线模型基强化学习中，长视界价值展开（n-step rollout）因模型误差累积导致预测退化的问题。

Method: 提出动作块（action chunk）动力学模型，以序列动作为输入预测未来状态；结合基于行为策略的拒绝采样机制，避免策略对分布外动作的过度依赖与模型误用。

Result: 在高达1亿转移的大规模离线数据集上验证，MAC在各类长时程任务中性能优于现有离线模型基RL算法。

Conclusion: 动作块建模与拒绝采样相结合能有效平衡偏差与方差，在复杂长视界离线RL任务中提供可扩展且鲁棒的解决方案。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [253] [Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach](https://arxiv.org/abs/2512.08124)
*Zijiang Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的加密货币组合管理新方法，通过神经网络预测多种加密货币的未来收益排序并据此分配权重，在2020年5月至2023年11月的真实市场数据回测中展现出优异的盈利性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 以往研究多独立预测单个加密货币（如比特币）价格走势再交易，忽视了加密货币间的相对关系；本文旨在利用跨资产的横截面信息提升组合管理效果。

Method: 在每个时间步，使用神经网络预测一组加密货币未来收益的相对排序，并据此动态分配投资权重。

Result: 在2020年5月至2023年11月涵盖牛、熊、盘整全周期的实证中，该方法实现年化收益率64.26%、夏普比率1.01，且对交易费用增加具有鲁棒性。

Conclusion: 基于相对收益排序的机器学习组合管理方法在复杂多变的加密货币市场中显著优于现有方法，具备实际应用潜力。

Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.

</details>


### [254] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 本文提出了一种名为类子空间正交化（CSO）的后训练后门检测方法，通过抑制类别固有特征来增强对后门目标类的检测敏感性，从而在混合标签和自适应攻击等挑战性场景下提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法依赖于攻击模型在目标类上表现出极端异常的检测统计量，但在某些非目标类本身易区分或后门特征较弱时容易失效；作者观察到目标类的检测统计量包含后门触发器和固有特征两部分贡献，而非目标类仅有后者，因此提出抑制固有特征以凸显后门贡献。

Method: 提出Class Subspace Orthogonalization（CSO）方法：针对给定类别，利用少量干净样本构建其固有特征子空间，并在优化检测统计量（如置信度）时施加与该子空间正交的约束，从而抑制固有特征影响。

Result: CSO在混合标签攻击和自适应攻击等具有挑战性的后门场景下显著优于现有方法，展现出更强的检测敏感性和鲁棒性。

Conclusion: 通过显式抑制类别固有特征，CSO能更有效地分离并放大后门触发器的贡献，为后训练后门检测提供了一种通用、即插即用且高敏感的新范式。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [255] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 本文提出了首个生物威胁基准生成（BBG）框架，旨在系统评估大语言模型（LLMs）在细菌类生物威胁场景下的生物安全风险，强调涵盖不同攻击者能力水平及操作性风险因素，并构建了分层的‘细菌生物威胁模式’（Bacterial Biothreat Schema）作为任务-查询架构基础。


<details>
  <summary>Details</summary>
Motivation: 为应对前沿AI（尤其是LLMs）被滥用于生物恐怖主义或获取生物武器的风险，亟需能可靠量化模型生物安全风险的基准工具；现有基准常忽略攻击者能力差异和操作性风险等关键威胁维度。

Method: 提出分层的Biothreat Benchmark Generation（BBG）框架，以细菌生物威胁为试点，构建包含威胁类别、要素与任务的层级结构——即‘细菌生物威胁模式’，并据此生成任务对齐的查询（queries），为后续提示工程与模型评估奠定基础。

Result: 完成了BBG框架的第一部分：定义并设计出可扩展、可重用的‘细菌生物威胁模式’，该模式覆盖技术与操作双重维度，支持多粒度（从具体任务到整体风险）的生物安全风险评估。

Conclusion: BBG框架及其核心组件‘细菌生物威胁模式’为LLMs在细菌生物威胁领域的风险评估提供了首个系统化、结构化、兼顾实操性的基准生成路径，有望成为模型开发者与政策制定者协同管控AI生物安全风险的重要基础设施。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [256] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 本文探讨了如何提升AI代理在开放、动态环境中的鲁棒性与泛化能力，提出并应用MiniHack、Maestro、质量-多样性方法及进化搜索等技术，在强化学习与大语言模型领域系统性地增强其面对新环境、分布外输入及对抗性提示的适应能力。


<details>
  <summary>Details</summary>
Motivation: AI代理需在不断变化、开放的世界中保持鲁棒性与泛化能力，尤其需应对训练中未见的新环境、分布外输入及多智能体交互等挑战。

Method: 结合开放性（open-endedness）与多智能体学习方法：1）构建MiniHack沙盒框架用于程序化生成多样化RL环境；2）提出Maestro方法生成对抗性课程以提升双人零和博弈中RL代理的鲁棒性；3）采用质量-多样性算法在足球游戏多智能体场景中系统挖掘预训练策略漏洞；4）利用进化搜索生成多样化对抗提示，诊断并提升大语言模型对恶意输入的鲁棒性。

Result: 在MiniHack、足球多智能体及LLM对抗提示等任务上验证了所提方法能显著提升代理在新环境、分布外场景及对抗交互下的泛化与鲁棒性能。

Conclusion: 本工作为构建可适应开放、动态、不可预见世界的强鲁棒AI代理提供了系统性方法论与实证基础，推动AI向真正通用与可靠的智能体演进。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [257] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: 本文提出PolyLingua，一种轻量级Transformer模型，采用两级对比学习框架用于领域内语言识别和细粒度语言分类，在Amazon Massive和Song数据集上分别达到99.25%和98.15%的F1分数，性能优于Sonnet 3.5且参数量仅为后者的十分之一。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别工具在音乐请求等场景中表现不佳，开源工具准确率低，大语言模型计算成本高，难以满足低延迟、低资源环境需求。

Method: 提出PolyLingua模型，基于轻量级Transformer架构，采用结合实例级分离与类别级对齐的两级对比学习框架，并引入自适应边界机制，以生成紧凑且类别区分度高的嵌入表示。

Result: 在Amazon Massive数据集上F1达99.25%，在Song数据集（含频繁语码转换）上F1达98.15%，均超越Sonnet 3.5，同时参数量减少10倍。

Conclusion: PolyLingua在保持高精度的同时显著降低计算开销，适用于资源受限与低延迟的实际部署场景，尤其擅长处理代码切换和跨语言不一致的挑战性用例。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases -- such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets -- Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching) -- PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [258] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一种新型强化学习框架，通过将去噪过程建模为搜索树，显著提升生成模型后训练的效率，在保持甚至提升性能的同时实现2.4倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习后训练方法计算成本过高，阻碍其在生成模型对齐中的广泛应用。

Method: 提出TreeGRPO框架，将扩散过程建模为共享初始噪声的搜索树，支持多分支候选轨迹生成与前缀复用，并实现步骤级优势估计和每前向传播多次策略更新。

Result: 在扩散与流模型上实现2.4倍训练加速，在效率-奖励权衡空间中达到更优Pareto前沿，且在多个基准和奖励模型上持续超越GRPO基线。

Conclusion: TreeGRPO为视觉生成模型的RL对齐提供了可扩展、高效且有效的解决方案。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [259] [LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks](https://arxiv.org/abs/2512.08160)
*Nanda K. Unnikrishnan,Keshab K. Parhi*

Main category: cs.LG

TL;DR: 本文提出了LayerPipe2，通过变量延迟梯度自适应和重定时方法，为LayerPipe提供理论基础，明确了各层所需梯度延迟量，并提出管道感知的移动平均法以减少历史权重存储开销，从而实现可扩展、内存高效且精度有保障的流水线训练。


<details>
  <summary>Details</summary>
Motivation: 先前的LayerPipe方法虽能加速神经网络训练，但缺乏对各层所需梯度延迟量的理论指导，无法系统解释延迟分配规律及伴随的存储瓶颈问题。

Method: 基于变量延迟梯度自适应与重定时技术形式化推导LayerPipe；分析合法插入延迟的位置；提出管道感知的移动平均机制替代显式存储历史权重。

Result: 明确了延迟量由网络结构（特别是下游阶段数）决定：内层延迟少、外层延迟多；分组流水线中同组层共享延迟；新方法显著降低内存开销，同时保持精度保证。

Conclusion: LayerPipe2构建了一个原理清晰、可预测延迟需求、可缓解存储压力的流水线训练框架，支持在通信与计算间进行可控权衡，推动大规模流水线训练的实用化。

Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

</details>


### [260] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: 本文提出了MobileFineTuner，一个支持在普通手机上直接进行端到端大语言模型（LLM）微调的开源框架，通过系统级优化解决内存与能耗限制问题，并在真实手机上验证了GPT-2、Gemma 3和Qwen 2.5的可行性。


<details>
  <summary>Details</summary>
Motivation: 高质量公开数据日益枯竭，而手机作为最普及的终端设备，拥有大量私有用户数据；现有端侧微调研究多基于仿真或PC/IoT设备，缺乏面向普通手机的开源实践框架。

Method: 提出MobileFineTuner统一框架，支持全参数微调（Full-FT）与参数高效微调（PEFT），并引入参数分片、梯度累积和能耗感知计算调度等系统级优化以适配手机资源限制。

Result: 在真实手机上成功微调GPT-2、Gemma 3和Qwen 2.5；实验与消融研究表明所提优化有效，框架具备实际部署能力。

Conclusion: MobileFineTuner填补了移动端LLM微调开源实践框架的空白，为未来端侧大模型训练研究提供了可行基础。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [261] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 本文挑战了AdamW中解耦权重衰减与学习率γ成正比的传统设定，提出并验证了其应与γ²成正比的新准则，从而实现更稳定的权重与梯度范数，并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统上将解耦权重衰减设为与学习率γ成正比缺乏理论依据；近期有研究基于稳态正交性提出应∝γ²，但尚未系统验证其动力学影响与普适性。

Method: 通过理论推导（假设稳态下更新与权重独立），分析权重范数演化；引入Total Update Contribution（TUC）概念，结合Scion优化器与动量依赖的有效学习率进行实证分析。

Result: 证实∝γ²的解耦权重衰减可带来稳定的权重与梯度范数；该设定下TUC由动量依赖的有效学习率刻画，其最优值具有可迁移性；实验验证其能更好调控训练动态并提升模型性能。

Conclusion: 解耦权重衰减应设为与学习率γ的平方成正比，该准则具有一般性（不限于特定优化器），可增强训练稳定性与泛化性能。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [262] [PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning](https://arxiv.org/abs/2512.08218)
*Ye Qin,Jingchao Wang,Yang Shi,Haiying Huang,Junxu Li,Weijian Liu,Tinghui Chen,Jinghui Qin*

Main category: cs.LG

TL;DR: 本文提出了一种伪黎曼胶囊网络（PR-CapsNet），将胶囊网络扩展到具有自适应曲率的伪黎曼流形中，以更好地建模图数据的复杂几何结构，显著提升了图表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有CapsNets在固定曲率空间中建模图数据存在测地线不连通问题，难以刻画真实图的复杂几何；而伪黎曼流形虽具图嵌入先验优势，但尚未被用于增强CapsNets。

Method: 提出PR-CapsNet：1）伪黎曼切空间路由，将胶囊状态分解为球面-时间与欧氏-空间子空间；2）自适应曲率路由，通过可学习曲率张量与几何注意力融合多曲率特征；3）保持几何性质的伪黎曼胶囊分类器，采用曲率加权softmax。

Result: 在节点与图分类基准上全面超越SOTA模型，验证了其对复杂图结构（如层次、簇状、环状）的强表达能力。

Conclusion: PR-CapsNet通过引入自适应曲率的伪黎曼几何，有效克服了传统CapsNets在图建模中的几何局限性，为图神经网络提供了新的几何建模范式。

Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.

</details>


### [263] [Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning](https://arxiv.org/abs/2512.08241)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Shipra Prashant*

Main category: cs.LG

TL;DR: 本文提出了一种基于持续同调与上同调流的、数学上严格的类脑表征学习框架，将神经计算建模为动态单纯复形上的上链映射演化，并结合代数拓扑与微分几何构建广义上同调算子，实验证明其在流形一致性与抗噪性上优于图神经网络和流形深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 为建立具有数学严谨性的类脑表征学习理论基础，克服现有深度学习模型在结构稳定性、跨尺度不变性及噪声鲁棒性方面的不足。

Method: 将神经计算建模为动态单纯复形上的上链映射演化；融合持续同调、层上同调与谱拉普拉斯等代数拓扑与微分几何工具，构建广义梯度式上同调学习算子。

Result: 在合成数据（具可控拓扑特征）与真实神经数据上验证了模型在流形一致性、连续性与结构保持性方面显著优于图神经网络与流形深度架构。

Conclusion: 该框架为拓扑驱动的表征学习提供了统一、可解释且鲁棒的数学基础，推动了类脑智能的理论发展。

Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.

</details>


### [264] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: 本文提出SPROCKET，一种基于原型的随机卷积核特征变换方法，用于时间序列分类，在多个基准数据集上性能媲美现有卷积算法，并通过MR-HY-SP集成进一步超越先前最优模型。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类算法依赖手工特征工程，而ROCKET等随机核方法虽有效，但仍有提升空间；本文旨在探索基于原型的新型特征工程策略以提高分类精度与鲁棒性。

Method: 提出SPROCKET（Selected Prototype Random Convolutional Kernel Transform），采用基于原型的随机卷积核特征变换；并构建MR-HY-SP集成模型，融合MultiROCKET、HYDRA与SPROCKET。

Result: 在多数UCR/UEA时间序列分类数据集上，SPROCKET性能接近现有最优卷积算法；MR-HY-SP集成的平均准确率排名超过此前最佳集成模型HYDRA-MR。

Conclusion: 基于原型的特征变换策略能有效提升时间序列分类的准确性与鲁棒性，为特征工程提供了新思路。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [265] [Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations](https://arxiv.org/abs/2512.08256)
*Deepak Gupta,Himanshu Pandey,Ratikanta Behera*

Main category: cs.LG

TL;DR: 本文提出了一种基于小波的物理信息量子神经网络框架，通过融合小波多分辨率特性和量子神经网络，避免自动微分，显著提升求解多尺度偏微分方程（含尖锐梯度、刚性、局部剧烈变化和强振荡）的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs及量子PINNs在处理多尺度特征（如尖锐梯度、刚性、强振荡）时精度不足，且依赖自动微分导致计算开销大、训练慢。

Method: 构建融合小波多分辨率分析的物理信息量子神经网络，用小波基替代自动微分构造损失函数，实现局部-全局特征协同建模。

Result: 数值实验表明，该方法精度更高，所需可训练参数不足经典小波PINNs的5%，收敛更快；相比现有量子PINNs提速3–5倍。

Conclusion: 小波加速的物理信息量子神经网络为高效求解复杂多尺度与振荡型偏微分方程提供了新范式，兼具高精度、低参数量与快收敛优势。

Abstract: This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.

</details>


### [266] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 本文提出了一种统一的几何-随机多模态深度学习框架，整合多种生理信号（EEG、ECG、呼吸、SpO2、EMG、fMRI），结合黎曼流形嵌入、李群不变特征、分数阶随机动力学、哈密顿能量流建模与跨模态注意力机制，用于建模SUDEP和急性缺血性卒中的易感性，并在MULTI-CLARID数据集上验证了其预测性能与可解释生物标志物。


<details>
  <summary>Details</summary>
Motivation: SUDEP和急性缺血性卒中是危及生命的神经自主系统疾病，需建模皮层、脑干与自主神经系统间的复杂交互，现有方法缺乏统一、可解释且多模态融合的数学框架。

Method: 提出几何-随机多模态深度学习框架：融合Riemannian流形嵌入、Lie群不变表示、分数阶随机动力学、哈密顿能量流建模、跨模态注意力，并用分数阶流行病扩散模型刻画卒中传播；基于结构脑图建模。

Result: 在MULTI-CLARID数据集上提升了预测准确率，并提取出可解释生物标志物：流形曲率、分数阶记忆指数、注意力熵、扩散中心性。

Conclusion: 该框架为神经自主系统疾病的早期检测、风险分层与可解释多模态建模提供了数学上严谨的基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [267] [Mathematical Foundations of Neural Tangents and Infinite-Width Networks](https://arxiv.org/abs/2512.08264)
*Rachana Mysore,Preksha Girish,Kavitha Jayaram,Shrey Kumar,Preksha Girish,Shravan Sanjeev Bagal,Kavitha Jayaram,Shreya Aravind Shastry*

Main category: cs.LG

TL;DR: 本文提出NTK-ECRN架构，结合傅里叶特征嵌入、层间缩放的残差连接和随机深度，在无限宽度极限下对神经网络进行严格分析；理论方面推导了NTK动态变化的界、刻画特征值演化，并关联其谱性质与泛化及优化稳定性；实验验证了理论预测并展示了更优训练稳定性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 弥合理论（无限宽度NTK）与实际深度网络架构之间的鸿沟，实现对训练过程中核演化的严格分析。

Method: 提出NTK-Eigenvalue-Controlled Residual Network（NTK-ECRN），融合傅里叶特征嵌入、带层缩放的残差连接和随机深度；理论推导NTK动态界与特征值演化规律；通过合成与基准数据集实验验证。

Result: 获得了NTK动态变化的理论界，刻画了特征值随训练的演化规律，并建立了其谱特性与泛化能力、优化稳定性之间的联系；实验上验证了核行为预测，且模型展现出更好的训练稳定性和泛化性能。

Conclusion: NTK-ECRN为连接无限宽度理论与实用深度学习架构提供了统一而严谨的框架，推动了可解释、可控的神经网络设计。

Abstract: We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.

</details>


### [268] [SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing](https://arxiv.org/abs/2512.08267)
*Yi Ni,Xinkun Wang,Han Zhang*

Main category: cs.LG

TL;DR: 本文提出SOFA-FL框架，通过动态聚类、自组织拓扑演化和自适应数据共享，解决联邦学习中数据异构与固定拓扑的挑战，提升系统适应性与个性化能力。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在演化环境中面临的数据异构性和固定网络拓扑刚性问题。

Method: 提出SOFA-FL框架，包含三个核心机制：动态多分支凝聚聚类（DMAC）、自组织分层自适应传播与演化（SHAPE）、自适应聚类数据共享。

Result: 实现了联邦系统对数据分布变化的动态适应，提升了客户端关系建模能力与个性化性能，无需预设簇结构。

Conclusion: SOFA-FL为动态异构环境下的联邦学习提供了可自组织、可演化的分层架构新范式。

Abstract: Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.
  The framework is built upon three core mechanisms: (1) \textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.
  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.

</details>


### [269] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: 本文提出gHAWK框架，通过预计算节点的局部（Bloom滤波器）和全局（TransE嵌入）结构特征，并与领域特征融合，增强GNN训练，显著提升大规模知识图谱上的准确性、训练速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于消息传递的图神经网络在大规模知识图谱上难以扩展，因其依赖迭代消息传递学习图结构，在小批量训练中效率低下且视野受限。

Method: 提出gHAWK框架：预计算每个节点的Bloom滤波器（编码局部邻域）和TransE嵌入（表征全局位置），再与领域特征（如文本嵌入）融合为节点特征向量，供任意GNN使用；将结构先验融入消息传递训练过程。

Result: 在Open Graph Benchmark多个大型数据集上实验表明，gHAWK在节点属性预测和链接预测任务中达到SOTA准确率，训练时间更短，并在三个图上登顶OGB排行榜。

Conclusion: gHAWK通过引入预计算的结构先验，有效缓解了传统GNN在大规模知识图谱上的可扩展性瓶颈，在精度、速度和内存开销方面均取得显著提升。

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [270] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: 本文提出JARF（Jacobian-Aligned Random Forests），通过使用监督式预处理（基于预测梯度的期望Jacobian外积）对特征空间进行全局线性变换，再输入标准轴对齐随机森林，从而在保持其高效与鲁棒性的同时，有效建模旋转或交互型决策边界。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树虽高效稳定，但在面对旋转或特征交互主导的决策边界时性能受限；而斜向树（oblique trees）虽能解决该问题，但计算开销大、实现复杂。本文旨在寻找一种兼顾性能与简洁性的替代方案。

Method: JARF首先训练一个轴对齐随机森林以获得预测输出，然后用有限差分法估计预测关于各特征的梯度，构建期望Jacobian外积作为全局线性预处理器（即特征空间的监督式旋转），再将变换后的数据送入标准轴对齐森林。该方法可泛化至任意可微模型。

Result: 在多个表格型分类与回归基准上，JARF持续提升轴对齐森林性能，常达到甚至超越斜向森林基线，同时显著加快训练速度。

Conclusion: 监督式特征预处理是一种有效折中：它能在几乎不增加实现复杂度的前提下，大幅逼近斜向树的表达能力，保留轴对齐树的核心优势。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [271] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 本文提出了一种名为MAN的正则化技术，通过最小化客户端模型各层激活范数来约束联邦学习中的Hessian矩阵最大特征值，从而促使全局模型收敛到平坦极小值，提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中全局模型易收敛到尖锐极小值，导致泛化性能下降，需引入平坦性约束以提升模型泛化能力。

Method: 提出基于Hessian矩阵最大特征值的平坦性约束，并设计客户端侧的MAN正则化方法（最小化各层激活范数），从理论上证明其可降低层间Hessian最大特征值，进而降低整体Hessian最大特征值。

Result: 将MAN应用于现有联邦学习方法中，显著提升了模型泛化性能，达到了新的SOTA水平。

Conclusion: 通过引入平坦性约束和MAN正则化，有效改善了联邦学习模型的泛化能力，为FL优化提供了新思路。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [272] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 本文提出了一种考虑标签依赖性的新型加权采样算法，用于多标签数据集，以改善稀有标签的代表性。


<details>
  <summary>Details</summary>
Motivation: 多标签数据集中标签频率差异大且不互斥，导致稀有标签样本不足，难以进行有效推断；同时需在偏离总体频率分布时保持对标签依赖关系的建模。

Method: 基于多元伯努利分布建模多标签生成过程，利用观测到的标签频率估计分布参数，并为每种标签组合计算权重，实现加权采样。

Result: 在Web of Science生物医学文献（64类主题标签）上的实验表明，该方法能保持类别频率排序、缩小最常见与最稀有类别间的频次差距，并体现类别依赖性，获得更均衡的子样本。

Conclusion: 所提加权采样算法能有效提升多标签数据中少数类别的表示能力，兼顾标签依赖结构与目标分布特性，适用于不平衡多标签学习场景。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [273] [Fully Decentralized Certified Unlearning](https://arxiv.org/abs/2512.08443)
*Hithem Lamri,Michail Maniatakos*

Main category: cs.LG

TL;DR: 本文提出RR-DU方法，在去中心化网络中实现可验证的机器遗忘，通过随机游走梯度更新与子采样高斯噪声机制，提供网络级遗忘保证和隐私-效用权衡分析。


<details>
  <summary>Details</summary>
Motivation: 现有认证遗忘研究集中于中心化或服务器协调的联邦学习场景，而去中心化（无协调者）网络中的认证遗忘尚未被充分探索。

Method: 提出RR-DU：在遗忘客户端对遗忘数据执行一次投影梯度上升；在其他节点对保留数据执行几何分布次数的投影梯度下降；结合子采样高斯噪声与原始模型邻域内的投影（信任区域）。理论分析采用子采样高斯Rényi差分隐私（RDP）与分段子采样。

Result: （i）凸情形下收敛、非凸情形下达到平稳点；（ii）提供客户端视角的(ε,δ)网络遗忘证书；（iii）给出删除容量界，刻画遗忘/本地数据比、网络混合与随机子采样对隐私-效用权衡的影响。实验表明RR-DU在MNIST/CIFAR-10上达到目标(ε,δ)的同时，测试准确率高于去中心化DP基线，且遗忘准确率降至约10%（随机猜测水平）。

Conclusion: RR-DU是首个为去中心化网络提供严格认证遗忘保证的方法，兼顾理论可证性与实际性能，揭示了网络结构与随机机制对遗忘能力的关键影响。

Abstract: Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,δ)$ network-unlearning certificates on client views via subsampled Gaussian Rényi DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy-utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,δ)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).

</details>


### [274] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: 本文介绍了Biothreat Benchmark Generation (BBG)框架的第二部分——Bacterial Biothreat Benchmark (B3)数据集的构建方法，通过网络提示生成、红队测试和现有基准库挖掘三种方式生成7000多个候选基准，经去重、诊断性评估与质量控制后筛选出1010个高质量、具生物安全相关性且支持多层级分析的基准。


<details>
  <summary>Details</summary>
Motivation: 为量化和缓解前沿AI模型（尤其是大语言模型）在生物恐怖主义和生物武器获取方面的潜在风险，亟需开发可评估模型生物安全风险的基准测试工具。

Method: 采用三种互补方法生成B3基准：1）基于网络的提示生成；2）红队测试；3）挖掘现有基准语料库；随后进行去重、 uplift诊断性评估及质量控制。

Result: 成功构建包含1010个高质量基准的B3数据集，这些基准具备诊断性 uplift、直接关联生物安全威胁，并兼容任务-查询架构以支持多粒度分析。

Conclusion: B3数据集是BBG框架的关键组成部分，为AI模型的生物安全风险评估提供了可扩展、可解释、结构化的新基准体系。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [275] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 本文介绍了Bacterial Biothreat Benchmark (B3)数据集的试点实施，作为Biothreat Benchmark Generation (BBG)框架的第三部分，旨在评估大语言模型（LLM）在生物安全方面的风险。试点通过运行前沿AI模型、人工评估响应及多维度风险分析，验证了B3能快速、细致地识别风险来源并指导缓解优先级。


<details>
  <summary>Details</summary>
Motivation: 应对前沿人工智能（尤其是大语言模型）可能助长生物恐怖主义或生物武器获取的风险，亟需可量化的模型基准来评估和缓解此类生物安全风险。

Method: 开展B3数据集的试点实施，包括在前沿AI模型上运行基准测试、人工评估模型输出，并进行多维度应用风险分析。

Result: 试点表明B3数据集是一种可行且细致的方法，能够快速评估LLM的生物安全风险，识别关键风险来源，并为风险缓解提供优先方向指导。

Conclusion: B3数据集及其试点验证了其在LLM生物安全风险评估中的有效性与实用性，为后续模型治理和政策制定提供了重要工具支撑。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [276] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 本文提出了一种结合fMRI数据与DICOM元数据的多模态Transformer框架，利用注意力机制提升脑状态解码的准确性、可解释性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法未能充分利用DICOM元数据所蕴含的丰富上下文信息，限制了fMRI脑状态解码的性能与可解释性。

Method: 构建基于Transformer的多模态模型，联合建模fMRI体素时间序列与DICOM元数据，并通过注意力机制捕获时空模式与跨模态上下文关系。

Result: 该框架在脑状态解码任务中提升了准确率、模型可解释性与鲁棒性，适用于临床诊断、认知神经科学与个性化医疗等场景。

Conclusion: 融合DICOM元数据的多模态Transformer方法为fMRI分析提供了新范式，尽管面临元数据异质性与计算开销等挑战，但具备良好的应用前景与扩展潜力。

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [277] [Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability](https://arxiv.org/abs/2512.08475)
*Weiqi Guan,Junlin He*

Main category: cs.LG

TL;DR: 本文提出了一种基于Post-LN的新型方法，通过引入代数平滑来缓解图神经网络（GNN）中的过平滑问题，同时避免深度诅咒，支持高达256层的深层网络且无需额外参数。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization（LN）的位置与过平滑现象之间的关系尚未被充分研究；Pre-LN可避免过平滑但受深度诅咒困扰，Post-LN则相反。

Method: 提出一种基于Post-LN的参数高效方法，通过诱导代数平滑来防止过平滑，同时规避深度诅咒。

Result: 在五个基准上验证了该方法的有效性，支持更深的网络（达256层），提升性能且不增加参数量。

Conclusion: 该工作从理论和实验两方面证实了所提方法能协同解决过平滑与深度诅咒问题，为深层GNN设计提供了新思路。

Abstract: The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.
  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.
  Key contributions:
  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.
  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.
  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.

</details>


### [278] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 本文提出了一种针对离线强化学习的数据投毒攻击新策略——全局预算分配（Global Budget Allocation），通过依据样本的TD误差敏感性分配扰动预算，在极小扰动下实现高达80%的性能下降，且能规避现有统计与谱分析防御。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击采用局部均匀扰动，效率低（浪费扰动预算于低影响样本）且隐蔽性差（引发显著统计偏差）。

Method: 基于TD误差决定样本对价值函数收敛影响的理论洞见，将攻击建模为全局资源分配问题，并在全局L2约束下推导出扰动幅度与TD误差敏感性成正比的闭式解。

Result: 在D4RL基准上显著优于基线攻击方法，仅需极小扰动即可造成最高80%的策略性能下降，且能成功绕过当前最先进的统计和谱检测防御。

Conclusion: 全局预算分配策略是一种更高效、更隐蔽的数据投毒攻击方法，揭示了离线RL在数据依赖性方面的关键脆弱性，对提升其鲁棒性具有重要警示意义。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [279] [Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction](https://arxiv.org/abs/2512.08499)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 本文提出了两种距离感知的不确定性估计方法PG-SNGP和PG-SNER，用于物理引导的确定性神经网络，以提升旋转机械轴承退化预测中的不确定性建模能力，尤其在分布外数据、对抗攻击和噪声下表现出更强的鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性方法存在置信度校准不足、计算成本高、缺乏距离感知能力以及在分布外数据下泛化能力差等问题，难以满足安全关键系统预测性维护的需求。

Method: 提出PG-SNGP（基于谱归一化高斯过程）和PG-SNER（基于深度证据回归）两种新方法；对隐藏层施加谱归一化以保持输入到隐空间的距离特性；PG-SNGP用高斯过程层替代输出层，PG-SNER输出正态逆伽马分布参数；引入基于皮尔逊相关系数的距离感知评估指标和动态加权损失函数以兼顾数据保真度与物理一致性。

Result: 在PRONOSTIA轴承数据集上验证表明，所提方法相比MC Dropout和深度集成PGNN，在预测精度、分布外泛化性、对抗鲁棒性和抗噪性方面均有提升。

Conclusion: PG-SNGP和PG-SNER有效解决了确定性物理引导神经网络中不确定性估计的距离感知与校准难题，为安全关键系统的可靠性预测提供了更可信的工具。

Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.

</details>


### [280] [A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks](https://arxiv.org/abs/2512.08567)
*Nader Sadek,Mirette Moawad,Christina Naguib,Mariam Elzahaby*

Main category: cs.LG

TL;DR: This paper proposes a multimodal GNN-based approach that fuses company-specific news headlines and historical stock prices for improved stock market prediction, outperforming LSTM baselines on direction-of-change and significance-based forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: Stock market prediction is challenging; while traditional models use only historical prices, financial news offers valuable external signals—yet effective integration remains underexplored.

Method: A heterogeneous graph is constructed where nodes represent companies, news articles, and industries; company price sequences are encoded via LSTM, news titles via a language model, and GraphSAGE aggregates cross-node interactions. Two prediction targets are considered: binary direction-of-change and significance-based labels.

Result: The GNN model achieves 53% accuracy on direction-of-change prediction and a 4% precision gain on significance-based labeling, outperforming the LSTM baseline; performance improves with more news per company, and headlines alone yield stronger signals than full articles.

Conclusion: Integrating structured stock data and unstructured news via heterogeneous GNNs enhances short-term market prediction, highlighting the value of concise, headline-level financial signals.

Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.

</details>


### [281] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 本文提出了一种基于长序列LSTM的NBA比赛结果预测模型，利用涵盖2004-05至2024-25赛季的新纵向数据集（含9840场比赛），有效建模长期球队表现趋势与跨赛季依赖性，在准确率、精确率和AUC-ROC上均优于多种传统机器学习与深度学习基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有NBA比赛预测模型普遍存在概念漂移、时间上下文有限及跨赛季性能不稳定等问题，亟需更鲁棒、更具泛化能力的时序建模方法和高质量长期数据支持。

Method: 构建覆盖2004-05至2024-25赛季的纵向NBA数据集；设计并训练一个长序列LSTM模型，输入长度达9840场（约八年）以捕获长期动态与跨赛季依赖；对比Logistic回归、随机森林、MLP和CNN等基线模型。

Result: LSTM模型在所有评估指标上最优：准确率72.35%，精确率73.15%，AUC-ROC达76.13%；显著优于各基线模型。

Conclusion: 长序列时序建模对提升NBA比赛预测性能至关重要；所构建的多赛季数据集为开发鲁棒、可泛化的体育预测系统提供了关键支撑。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [282] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文提出了DS FedProxGrad算法，改进了FedProxGrad在群组公平联邦学习中对非凸复合优化问题的收敛性分析，实现了渐近平稳收敛且消除了方差引起的噪声下界依赖。


<details>
  <summary>Details</summary>
Motivation: 原始FedProxGrad算法仅能保证收敛到受噪声主导的平稳点邻域，存在方差诱导的噪声下界限制，限制了其理论性能和实际精度。

Method: 提出带衰减步长和局部不精确近端解的广义FedProxGrad框架（DS FedProxGrad），采用Robbins-Monro步长序列，并施加局部不精确度的衰减条件。

Result: 证明了在合理假设下，算法满足liminf E[||∇F(x^r)||²] = 0，即渐近达到平稳点，且收敛速率不依赖于方差噪声下界。

Conclusion: DS FedProxGrad在理论上克服了原有方法的噪声局限，为公平联邦学习中的非凸优化提供了更强的收敛保障。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [283] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于动态时间规整（DTW）与迁移学习（TL）结合的框架，用于增材制造（AM）零件性能认证，通过将低成本聚合物的应力-应变行为知识迁移到金属材料上，提升金属AM零件力学性能预测精度。


<details>
  <summary>Details</summary>
Motivation: 增材制造零件在关键应用中需可靠服役，而其复杂应力-应变行为难以准确预测，尤其对金属材料缺乏足够实验数据，亟需利用易获取的聚合物数据辅助建模。

Method: 构建DTW-TL框架：首先用DTW度量聚合物与金属应力-应变曲线的时序相似性，选取最匹配的单一聚合物数据集作为源域；再基于LSTM模型实现迁移学习，验证四种聚合物（Nylon、PLA、CF-ABS、Resin）向三种金属（AlSi10Mg、Ti6Al4V、碳钢）的知识迁移效果。

Result: DTW-TL框架成功识别出最优源域聚合物，并在三类金属目标域上达到最低MAPE（12.41%）和最高R²（0.96），显著优于无迁移的LSTM及以全部四种聚合物为源域的TL模型。

Conclusion: DTW驱动的单源迁移策略比多源或无迁移更有效，为数据稀缺的金属AM零件性能预测提供了高效、可解释的跨材料建模新范式。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [284] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: 本文提出Bias-Guided Prompt Search (BGPS)框架，利用LLM生成属性中立提示，并结合属性分类器引导其朝向放大敏感属性（如性别、种族、年龄）的方向解码，从而自动发现易触发文本到图像模型偏见的提示，揭示现有去偏方法的漏洞，并提供可解释、用户可用的新评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法依赖人工或LLM构建的提示数据集，成本高且难以覆盖所有潜在偏见触发提示，导致模型在实际使用中仍暴露偏见。

Method: 提出BGPS框架：(1) 使用受指令的LLM生成属性中立提示；(2) 利用TTI模型内部表征上的属性分类器反馈，实时引导LLM解码过程，搜索能最大化图像中敏感属性偏差的提示。

Result: 在Stable Diffusion 1.5及前沿去偏模型上实验，发现了大量细微且此前未被记录的偏见，显著降低公平性指标；所发现提示具可解释性与用户实用性，且在困惑度指标上优于硬提示优化基线。

Conclusion: BGPS有效拓展了偏见提示搜索空间，揭示了TTI模型的深层脆弱性，可作为新型、更鲁棒的偏见评估工具，推动更全面的去偏研究与实践。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [285] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经常微分方程（NODE）的动态建模框架，用于学习蛋白质组与代谢组间的复杂相互作用，显著提升了对代谢通路时间动态的预测精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 高通量多组学数据日益丰富，但将其转化为可预测、可干预的生物系统模型仍存在瓶颈；传统机理模型受限于先验知识，亟需能从观测数据中自动学习潜在动态关系的数据驱动方法。

Method: 引入神经常微分方程（NODE）作为动态建模框架，应用于工程化大肠杆菌的时间序列蛋白组-代谢组数据，建模代谢通路的连续动力学过程。

Result: 在Limonene和Isopentenol通路数据集上，相比基线模型，RMSE提升超90%（最高达97.65%），且推理速度加速1000倍。

Conclusion: NODE是一种高保真、可扩展的新型建模工具，有望推动个性化医疗与合成生物学中的动态预测与干预设计。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [286] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: 本文提出LEAP模型，通过强化通用图提示调优的理论基础，在所有节点上添加提示以保证其普适性，并结合强化学习选择节点和编辑提示，在多种预训练策略和数据场景下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 早期图提示调优方法依赖于任务特定设计，适应性差；虽有通用图提示调优研究，但近期基于选择性节点的方法破坏了其理论基础，需重新加强该基础。

Method: 提出LEAP框架：先构建保持理论基础的全节点通用图提示，再利用Actor-Critic强化学习进行节点选择与提示编辑。

Result: 在图级和节点级任务、多种预训练策略及全量/少样本场景下，LEAP持续优于微调及其他提示方法。

Conclusion: 在所有节点添加提示是实现图提示普适性的必要条件；LEAP在保持理论严谨性的同时提升了提示质量与泛化能力。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [287] [De novo generation of functional terpene synthases using TpsGPT](https://arxiv.org/abs/2512.08772)
*Hamsini Ramanathan,Roman Bushuiev,Matouš Soldát,Jirí Kohout,Téo Hebra,Joshua David Smith,Josef Sivic,Tomáš Pluskal*

Main category: cs.LG

TL;DR: 本文提出了TpsGPT，一个基于ProtGPT2微调的生成式模型，用于高效设计新型萜烯合酶（TPS），并通过多维度计算验证与实验确认其功能活性。


<details>
  <summary>Details</summary>
Motivation: 萜烯合酶（TPS）对天然产物（如抗癌药紫杉醇）至关重要，但传统定向进化方法成本高、周期长，亟需高效从头设计新TPS的计算方法。

Method: 在79k条UniProt来源的TPS序列上微调蛋白语言模型ProtGPT2构建TpsGPT；生成28k序列，并用EnzymeExplorer、ESMFold pLDDT、CLEAN、InterPro、Foldseek等多指标联合过滤筛选；最终实验验证候选酶活性。

Result: 筛选出7条满足全部计算标准的全新TPS序列，其中至少2条经实验证实具有TPS催化活性。

Conclusion: 针对特定酶类精细构建数据集并微调PLM，辅以严格多模态验证，可成功生成结构新颖且功能可行的远缘TPS酶，为酶设计提供新范式。

Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.

</details>


### [288] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: 本文提出TabPFN-GN，将图节点分类问题转化为表格学习问题，通过提取节点属性、结构特性、位置编码及邻域平滑特征，利用预训练表格模型TabPFN实现零样本节点分类，在同质图上性能媲美GNN，在异质图上持续超越GNN。


<details>
  <summary>Details</summary>
Motivation: 探索是否能将图节点分类有效重构为表格学习问题，以避免图神经网络（GNN）的特定任务训练和大语言模型（LLM）依赖的图基础模型。

Method: 提出TabPFN-GN方法，将图数据转换为表格特征，包括节点属性、结构性质、位置编码及可选的平滑邻域特征，从而直接利用TabPFN进行节点分类，无需图特化训练或语言模型依赖。

Result: 在12个基准数据集上的实验表明，TabPFN-GN在同质图上与GNN性能相当，在异质图上持续优于GNN。

Conclusion: 合理的特征工程可弥合表格与图领域之间的鸿沟，为图节点分类提供一种不依赖GNN训练或LLM的实用替代方案。

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [289] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 本文提出了一种利用提升建模（uplift modeling）结果来估计反事实联合分布的新方法，通过拟合双变量Beta分布到预测的提升分数上，获得反事实结果的后验分布，无需额外因果假设。


<details>
  <summary>Details</summary>
Motivation: 提升建模仅估计干预的平均因果效应（uplift），而反事实识别需恢复潜在结果的联合分布，信息更丰富但更难估计；二者具有协同潜力，但缺乏有效整合方法。

Method: 提出一种反事实估计器：将提升模型输出的提升分数作为输入，拟合双变量Beta分布，从而推断潜在结果的联合分布并得到反事实结果的后验分布。

Result: 在模拟实验中验证了该方法的有效性；应用于电信客户流失场景，揭示了传统机器学习或单纯提升模型无法提供的新洞察。

Conclusion: 该方法桥接了uplift modeling与counterfactual identification，以轻量、假设友好的方式拓展了因果推断的信息维度，具备实际部署价值。

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [290] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: 本文提出WAAPO框架，用于生成针对天气预报AI模型的隐蔽且有效的对抗性扰动，揭示了当前AI气象模型在初始条件微小扰动下易产生显著预测偏差的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在天气预报中应用日益广泛，评估其对对抗性扰动的脆弱性变得至关重要。

Method: 提出Weather Adaptive Adversarial Perturbation Optimization（WAAPO）框架，通过引入通道稀疏性、空间局部性和平滑性约束，生成物理合理且难以察觉的定向对抗扰动；实验基于ERA5数据集和FourCastNet模型。

Result: WAAPO能生成高度匹配预设目标的对抗轨迹，即使在受限条件下仍保持有效性；实验证明微小初始扰动即可导致AI气象模型预测结果大幅偏离真实天气模式。

Conclusion: AI驱动的天气预报模型存在关键安全漏洞，亟需部署鲁棒防护机制以抵御对抗性攻击。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [291] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文提出STD(λ)方法，通过相对状态值训练函数逼近器，以解决TD(λ)在策略优化中因关注绝对值误差而导致策略退化的问题，并在理论和实验上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: TD(λ)虽在值函数逼近上有效，但其最小化的是状态值的绝对误差，而策略优化更依赖于状态间相对排序的准确性；已有案例显示TD(λ)可能从最优策略收敛至次优策略。

Method: 提出STD(λ)（State-Transition Difference TD(λ)），在二元决策问题中基于相对状态值进行函数逼近训练；给出理论分析，包括两状态系统中单调策略改进的证明，并与Bertsekas的微分训练法比较。

Result: STD(λ)在两状态系统、三状态系统及backgammon中避免了策略退化；在两状态系统和改进版acrobot任务上成功演示了性能提升。

Conclusion: 相较于标准TD(λ)，STD(λ)通过聚焦相对状态值误差，能更可靠地保障策略不退化，具备更强的策略优化能力，尤其适用于需保持动作偏好顺序的任务。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [292] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的文本到IMU运动合成框架，通过引入加速度二阶差分损失（L_acc）微调预训练扩散模型，提升了生成IMU数据的真实性与下游人体活动识别（HAR）性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动扩散模型缺乏对IMU传感器物理特性的建模（如加速度动态特性），导致生成的IMU信号失真，难以直接用于传感器相关任务（如HAR）。

Method: 在预训练文本到运动扩散模型基础上，引入加速度驱动的二阶时间差分损失L_acc作为正则项，微调模型以学习IMU特定的运动先验；结合表面建模与虚拟传感器仿真生成IMU数据，并在低维嵌入空间和HAR任务中评估性能。

Result: L_acc相对原模型下降12.7%，高动态动作（跑、跳）改进更显著；合成IMU数据在嵌入空间更接近真实分布；仅用合成数据训练的HAR准确率较原扩散模型提升8.7%，优于其他基线模型7.6%。

Conclusion: 加速度感知的扩散模型精调能有效对齐运动生成与IMU合成，证明通用文本到运动先验可通过轻量传感器定制化适配，提升实际应用效果。

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [293] [Differentially Private Synthetic Data Generation Using Context-Aware GANs](https://arxiv.org/abs/2512.08869)
*Anantaa Kotal,Anupam Joshi*

Main category: cs.LG

TL;DR: 本文提出ContextGAN，一种结合差分隐私与领域隐式规则建模的生成对抗网络，用于生成既符合领域约束（如医疗处方规则）又保护隐私的高质量合成数据。


<details>
  <summary>Details</summary>
Motivation: 传统合成数据方法难以捕捉数据中未显式表达但至关重要的领域隐式规则（如医疗中的用药禁忌），导致合成数据不真实、不可用；同时需满足GDPR等严格隐私法规。

Method: 提出ContextGAN：基于GAN框架，引入编码显式与隐式领域知识的约束矩阵，并设计约束感知判别器；结合差分隐私机制保障原始数据敏感信息不泄露。

Result: 在医疗、安全、金融多个领域验证，ContextGAN生成的数据显著提升真实性与实用性，同时满足差分隐私要求，优于现有合成数据方法。

Conclusion: ContextGAN通过融合领域约束建模与差分隐私，在保障隐私前提下实现了更合规、更可信的合成数据生成，适用于强监管场景。

Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.

</details>


### [294] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: 本文提出了Fed-SE框架，用于在隐私受限、环境异构的场景下实现LLM智能体的联邦式自我进化，通过局部高效微调与全局低秩子空间聚合，显著提升跨环境任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习难以适配LLM智能体在动态、异构环境中的自我进化需求，因任务差异大、奖励稀疏且梯度冲突严重。

Method: 提出Fed-SE：本地基于高回报轨迹进行参数高效微调；全局在低秩子空间中聚合更新，解耦环境特异性动态以减少负迁移。

Result: 在五个异构环境中实验表明，Fed-SE相较联邦基线平均任务成功率提升约18%。

Conclusion: Fed-SE有效支持隐私保护下的LLM智能体跨环境鲁棒知识迁移与协同进化。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [295] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 本文揭示了大语言模型（LLMs）在生成表格合成数据时存在隐私泄露风险，提出了一种仅依赖生成数据的无盒成员推断攻击（LevAtt），并设计了基于数字扰动的采样策略以防御该攻击，在保持数据效用的同时有效提升隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成表格合成数据的两种主流方法（微调小模型和上下文提示大模型）可能因记忆训练数据中的数字模式而泄露隐私，亟需系统性风险评估与防御。

Method: 提出无盒成员推断攻击LevAtt，仅利用合成数据中数字字符串进行隐私风险分析；并设计一种在生成过程中对数字进行策略性扰动的新采样策略作为防御方法。

Result: LevAtt在多种模型和数据集上成功暴露显著隐私泄露，部分场景下达到完美成员识别；所提扰动采样策略可有效抵御攻击，且对合成数据保真度和实用性影响极小。

Conclusion: LLM驱动的表格合成数据生成存在独特隐私漏洞，需针对性防御机制；本文提出的数字扰动采样策略为兼顾隐私、保真与效用提供了可行路径。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [296] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: 本文提出DAO-GP——一种面向概念漂移的自适应在线高斯过程模型，具备漂移感知、超参数无关、衰减机制与稀疏性，显著提升在线非线性回归在动态数据下的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常存在概念漂移（即数据分布随时间变化），而传统在线高斯过程方法缺乏漂移感知能力、依赖固定超参数、易受数据窥探影响、无合理衰减机制且内存效率低，亟需更鲁棒的在线建模方法。

Method: 提出DAO-GP模型：集成漂移检测与自适应机制，实现超参数自由、基于衰减的记忆管理、动态诱导点更新及稀疏近似，完全适配在线学习场景。

Result: 在静态与多种漂移类型（突发、渐进、缓慢）下均表现出强鲁棒性；实验证明其具备动态适应能力、高效内存与衰减管理、诱导点持续演化，并在多个基准上优于或媲美现有SOTA模型。

Conclusion: DAO-GP是一种高效、自适应、无需人工调参的在线高斯过程方法，为概念漂移环境下的在线非线性回归提供了可靠且可扩展的解决方案。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [297] [Explainable Anomaly Detection for Industrial IoT Data Streams](https://arxiv.org/abs/2512.08885)
*Ana Rita Paupério,Diogo Risca,Afonso Lourenço,Goreti Marreiros,Ricardo Martins*

Main category: cs.LG

TL;DR: 本文提出了一种结合无监督异常检测与人在回路交互学习的协同数据流挖掘框架，用于工业维护中的实时故障检测与解释，已在提花织机单元上实现并验证。


<details>
  <summary>Details</summary>
Motivation: 工业维护中，物联网和边缘计算产生大量连续数据流，需在计算资源受限下进行实时、自适应决策；但实际中标签常延迟或缺失，现有数据流挖掘方法多假设完全监督场景，难以适用。

Method: 采用在线隔离森林（Isolation Forest）进行无监督异常检测，并通过增量式偏依赖图（Partial Dependence Plots）和基于ICE曲线偏离衰减均值的特征重要性评分提升可解释性，支持用户动态调整特征相关性和异常阈值。

Result: 实现了该框架在提花织机单元上的实时部署，初步验证了其在故障检测中的有效性；当前工作正扩展至持续监测以预测并解释轴承即将发生的故障。

Conclusion: 所提协作式DSM框架能在标签稀疏条件下支撑可解释、可交互的实时维护决策，为边缘侧智能运维提供了新范式。

Abstract: Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.

</details>


### [298] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 本文提出了一种基于拓扑的无监督学习方法，用于自动选择核密度估计中的最优带宽，并在不同维度上与经典方法进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 核密度估计中带宽的选择至关重要，但传统方法缺乏对拓扑特征（如连通分量、环、空洞等）的量化考虑，尤其在高维难以可视化时。

Method: 提出一种基于拓扑数据分析（TDA）的损失函数，将其用于无监督优化核密度估计的带宽参数。

Result: 该方法在不同维度的数据上展现出优于经典带宽选择技术的性能，验证了其有效性与普适性。

Conclusion: 基于拓扑的损失函数为核密度估计提供了一种新颖、自动化且无监督的带宽选择范式，增强了对数据拓扑结构的建模能力。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [299] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: 本文介绍了Open Polymer Challenge (OPC)，发布了首个社区共建的聚合物信息学基准数据集（含10K聚合物及5种关键性质），推动多任务聚合物性质预测，并探讨了小样本、标签不平衡和异构模拟数据下的建模策略与数据实践启示。


<details>
  <summary>Details</summary>
Motivation: 解决聚合物领域缺乏大规模、高质量、开放获取数据集的问题，以促进机器学习在可持续高分子材料发现中的应用。

Method: 组织国际竞赛，构建含10,000个聚合物及5个性质的基准数据集；鼓励参赛者在小样本、标签不平衡、多源模拟数据等现实约束下，采用特征增强、迁移学习、自监督预训练和定向集成等方法进行多任务性质预测。

Result: 产出了高性能多任务预测模型，揭示了数据准备、分布偏移和跨组模拟一致性等关键问题，形成了聚合物AI建模最佳实践；同步开源测试数据集与可扩展的聚合物性质模拟流水线（ADEPT）。

Conclusion: OPC为聚合物分子AI建立了新基础，将加速可持续与节能高分子材料的研发。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>
