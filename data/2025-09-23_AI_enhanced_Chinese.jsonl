{"id": "2509.16369", "categories": ["cs.IR", "cs.AI", "cs.CE", "H.4; H.5; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.16369", "abs": "https://arxiv.org/abs/2509.16369", "authors": ["Akshay Govind Srinivasan", "Ryan Jacob George", "Jayden Koshy Joe", "Hrushikesh Kant", "Harshith M R", "Sachin Sundar", "Sudharshan Suresh", "Rahul Vimalkanth", "Vijayavallabh"], "title": "Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction", "comment": "14 Pages, 8 Tables, 2 Figures. Accepted and to be published in the\n  proceedings of FinNLP, Empirical Methods in Natural Language Processing 2025", "summary": "Accurate and reliable knowledge retrieval is vital for financial\nquestion-answering, where continually updated data sources and complex,\nhigh-stakes contexts demand precision. Traditional retrieval systems rely on a\nsingle database and retriever, but financial applications require more\nsophisticated approaches to handle intricate regulatory filings, market\nanalyses, and extensive multi-year reports. We introduce a framework for\nfinancial Retrieval Augmented Generation (RAG) that leverages agentic AI and\nthe Multi-HyDE system, an approach that generates multiple, nonequivalent\nqueries to boost the effectiveness and coverage of retrieval from large,\nstructured financial corpora. Our pipeline is optimized for token efficiency\nand multi-step financial reasoning, and we demonstrate that their combination\nimproves accuracy by 11.2% and reduces hallucinations by 15%. Our method is\nevaluated on standard financial QA benchmarks, showing that integrating\ndomain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets,\nincluding keyword and table-based retrieval, significantly enhances both the\naccuracy and reliability of answers. This research not only delivers a modular,\nadaptable retrieval framework for finance but also highlights the importance of\nstructured agent workflows and multi-perspective retrieval for trustworthy\ndeployment of AI in high-stakes financial applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eMulti-HyDE\u548c\u4ee3\u7406AI\u7684\u91d1\u878d\u9886\u57df\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u67e5\u8be2\u751f\u6210\u548c\u591a\u6b65\u63a8\u7406\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u5355\u4e00\u68c0\u7d22\u7cfb\u7edf\u96be\u4ee5\u5e94\u5bf9\u91d1\u878d\u9886\u57df\u590d\u6742\u3001\u9ad8\u98ce\u9669\u4e14\u6570\u636e\u6301\u7eed\u66f4\u65b0\u7684\u95ee\u7b54\u9700\u6c42\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u77e5\u8bc6\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4ee3\u7406AI\u4e0eMulti-HyDE\u7cfb\u7edf\uff0c\u751f\u6210\u591a\u4e2a\u975e\u7b49\u4ef7\u67e5\u8be2\u4ee5\u589e\u5f3a\u4ece\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u91d1\u878d\u8bed\u6599\u4e2d\u7684\u68c0\u7d22\u6548\u679c\uff0c\u5e76\u96c6\u6210\u5173\u952e\u8bcd\u68c0\u7d22\u3001\u8868\u683c\u68c0\u7d22\u7b49\u5de5\u5177\uff0c\u4f18\u5316token\u6548\u7387\u548c\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6807\u51c6\u91d1\u878d\u95ee\u7b54\u57fa\u51c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad811.2%\uff0c\u5e7b\u89c9\u51cf\u5c1115%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u9002\u5e94\u7684\u91d1\u878dRAG\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u548c\u591a\u89c6\u89d2\u68c0\u7d22\u5728\u9ad8\u98ce\u9669\u91d1\u878dAI\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.16411", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16411", "abs": "https://arxiv.org/abs/2509.16411", "authors": ["Chong You", "Rajesh Jayaram", "Ananda Theertha Suresh", "Robin Nittka", "Felix Yu", "Sanjiv Kumar"], "title": "Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe", "comment": "NeurIPS 2025", "summary": "Dual encoder (DE) models, where a pair of matching query and document are\nembedded into similar vector representations, are widely used in information\nretrieval due to their simplicity and scalability. However, the Euclidean\ngeometry of the embedding space limits the expressive power of DEs, which may\ncompromise their quality. This paper investigates such limitations in the\ncontext of hierarchical retrieval (HR), where the document set has a\nhierarchical structure and the matching documents for a query are all of its\nancestors. We first prove that DEs are feasible for HR as long as the embedding\ndimension is linear in the depth of the hierarchy and logarithmic in the number\nof documents. Then we study the problem of learning such embeddings in a\nstandard retrieval setup where DEs are trained on samples of matching query and\ndocument pairs. Our experiments reveal a lost-in-the-long-distance phenomenon,\nwhere retrieval accuracy degrades for documents further away in the hierarchy.\nTo address this, we introduce a pretrain-finetune recipe that significantly\nimproves long-distance retrieval without sacrificing performance on closer\ndocuments. We experiment on a realistic hierarchy from WordNet for retrieving\ndocuments at various levels of abstraction, and show that pretrain-finetune\nboosts the recall on long-distance pairs from 19% to 76%. Finally, we\ndemonstrate that our method improves retrieval of relevant products on a\nshopping queries dataset.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u5c42\u6b21\u5316\u68c0\u7d22\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6cd5\u4ee5\u6539\u5584\u957f\u8ddd\u79bb\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u53cc\u7f16\u7801\u5668\u6a21\u578b\u7531\u4e8e\u5d4c\u5165\u7a7a\u95f4\u7684\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u9650\u5236\uff0c\u53ef\u80fd\u5f71\u54cd\u5176\u5728\u5c42\u6b21\u5316\u68c0\u7d22\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3-\u5fae\u8c03\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u5d4c\u5165\u8868\u793a\u3002", "result": "\u5728WordNet\u5c42\u6b21\u7ed3\u6784\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u957f\u8ddd\u79bb\u914d\u5bf9\u7684\u53ec\u56de\u7387\u4ece19%\u63d0\u5347\u81f376%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u65b9\u6848\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u8ddd\u79bb\u68c0\u7d22\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u8fd1\u8ddd\u79bb\u6587\u6863\u7684\u6027\u80fd\u3002"}}
{"id": "2509.16442", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16442", "abs": "https://arxiv.org/abs/2509.16442", "authors": ["Pranjal A. Chitale", "Bishal Santra", "Yashoteja Prabhu", "Amit Sharma"], "title": "Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval", "comment": "EMNLP 2025 (MAIN Conference)", "summary": "Compact dual-encoder models are widely used for retrieval owing to their\nefficiency and scalability. However, such models often underperform compared to\ntheir Large Language Model (LLM)-based retrieval counterparts, likely due to\ntheir limited world knowledge. While LLM-based data augmentation has been\nproposed as a strategy to bridge this performance gap, there is insufficient\nunderstanding of its effectiveness and scalability to real-world retrieval\nproblems. Existing research does not systematically explore key factors such as\nthe optimal augmentation scale, the necessity of using large augmentation\nmodels, and whether diverse augmentations improve generalization, particularly\nin out-of-distribution (OOD) settings. This work presents a comprehensive study\nof the effectiveness of LLM augmentation for retrieval, comprising over 100\ndistinct experimental settings of retrieval models, augmentation models and\naugmentation strategies. We find that, while augmentation enhances retrieval\nperformance, its benefits diminish beyond a certain augmentation scale, even\nwith diverse augmentation strategies. Surprisingly, we observe that\naugmentation with smaller LLMs can achieve performance competitive with larger\naugmentation models. Moreover, we examine how augmentation effectiveness varies\nwith retrieval model pre-training, revealing that augmentation provides the\nmost benefit to models which are not well pre-trained. Our insights pave the\nway for more judicious and efficient augmentation strategies, thus enabling\ninformed decisions and maximizing retrieval performance while being more\ncost-effective. Code and augmented datasets accompanying this work are publicly\navailable at https://aka.ms/DAGR.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6570\u636e\u589e\u5f3a\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5c0f\u89c4\u6a21LLM\u589e\u5f3a\u5373\u53ef\u8fbe\u5230\u4e0e\u5927\u89c4\u6a21\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u589e\u5f3a\u6548\u679c\u5728\u68c0\u7d22\u6a21\u578b\u9884\u8bad\u7ec3\u4e0d\u8db3\u65f6\u66f4\u4e3a\u663e\u8457\uff0c\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u7d27\u51d1\u578b\u53cc\u7f16\u7801\u5668\u6a21\u578b\u56e0\u6548\u7387\u9ad8\u88ab\u5e7f\u6cdb\u7528\u4e8e\u68c0\u7d22\uff0c\u4f46\u6027\u80fd\u5e38\u4e0d\u5982\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\uff0c\u90e8\u5206\u5f52\u56e0\u4e8e\u5176\u4e16\u754c\u77e5\u8bc6\u6709\u9650\u3002\u5c3d\u7ba1LLM\u6570\u636e\u589e\u5f3a\u88ab\u63d0\u51fa\u4ee5\u7f29\u5c0f\u5dee\u8ddd\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u8d85\u8fc7100\u79cd\u4e0d\u540c\u7684\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u68c0\u7d22\u6a21\u578b\u3001\u589e\u5f3a\u6a21\u578b\u548c\u589e\u5f3a\u7b56\u7565\u4e0bLLM\u589e\u5f3a\u7684\u6548\u679c\uff0c\u5206\u6790\u589e\u5f3a\u89c4\u6a21\u3001\u6a21\u578b\u5927\u5c0f\u548c\u591a\u6837\u6027\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u5bdf\u5176\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u589e\u5f3a\u80fd\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u4f46\u8d85\u8fc7\u4e00\u5b9a\u89c4\u6a21\u540e\u6536\u76ca\u9012\u51cf\uff1b\u5c0f\u89c4\u6a21LLM\u589e\u5f3a\u53ef\u8fbe\u5230\u4e0e\u5927\u89c4\u6a21\u6a21\u578b\u76f8\u5f53\u7684\u6548\u679c\uff1b\u589e\u5f3a\u5bf9\u9884\u8bad\u7ec3\u8f83\u5dee\u7684\u68c0\u7d22\u6a21\u578b\u63d0\u5347\u6700\u660e\u663e\u3002", "conclusion": "LLM\u6570\u636e\u589e\u5f3a\u53ef\u6709\u6548\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u4f46\u9700\u6743\u8861\u6210\u672c\u4e0e\u6536\u76ca\uff1b\u4f7f\u7528\u5c0f\u578bLLM\u8fdb\u884c\u9002\u5ea6\u589e\u5f3a\u662f\u66f4\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u7b56\u7565\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u589e\u5f3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.16446", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16446", "abs": "https://arxiv.org/abs/2509.16446", "authors": ["Ruohan Zhang", "Jiacheng Li", "Julian McAuley", "Yupeng Hou"], "title": "Purely Semantic Indexing for LLM-based Generative Recommendation and Retrieval", "comment": null, "summary": "Semantic identifiers (IDs) have proven effective in adapting large language\nmodels for generative recommendation and retrieval. However, existing methods\noften suffer from semantic ID conflicts, where semantically similar documents\n(or items) are assigned identical IDs. A common strategy to avoid conflicts is\nto append a non-semantic token to distinguish them, which introduces randomness\nand expands the search space, therefore hurting performance. In this paper, we\npropose purely semantic indexing to generate unique, semantic-preserving IDs\nwithout appending non-semantic tokens. We enable unique ID assignment by\nrelaxing the strict nearest-centroid selection and introduce two model-agnostic\nalgorithms: exhaustive candidate matching (ECM) and recursive residual\nsearching (RRS). Extensive experiments on sequential recommendation, product\nsearch, and document retrieval tasks demonstrate that our methods improve both\noverall and cold-start performance, highlighting the effectiveness of ensuring\nID uniqueness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7eaf\u8bed\u4e49\u7d22\u5f15\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u975e\u8bed\u4e49\u6807\u8bb0\u751f\u6210\u552f\u4e00\u4e14\u4fdd\u6301\u8bed\u4e49\u7684ID\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u8350\u548c\u68c0\u7d22\u7cfb\u7edf\u4e2d\u8bed\u4e49ID\u51b2\u7a81\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e3a\u8bed\u4e49\u76f8\u4f3c\u7684\u6587\u6863\u5206\u914d\u76f8\u540cID\u65f6\u4f1a\u4ea7\u751f\u51b2\u7a81\uff0c\u901a\u5e38\u901a\u8fc7\u6dfb\u52a0\u975e\u8bed\u4e49\u6807\u8bb0\u89e3\u51b3\uff0c\u4f46\u8fd9\u4f1a\u5f15\u5165\u968f\u673a\u6027\u5e76\u6269\u5927\u641c\u7d22\u7a7a\u95f4\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7eaf\u8bed\u4e49\u7d22\u5f15\u65b9\u6cd5\uff0c\u653e\u677e\u4e25\u683c\u7684\u6700\u8fd1\u8d28\u5fc3\u9009\u62e9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u6a21\u578b\u65e0\u5173\u7b97\u6cd5\uff1a\u7a77\u4e3e\u5019\u9009\u5339\u914d\uff08ECM\uff09\u548c\u9012\u5f52\u6b8b\u5dee\u641c\u7d22\uff08RRS\uff09\uff0c\u4ee5\u5b9e\u73b0\u552f\u4e00ID\u5206\u914d\u3002", "result": "\u5728\u5e8f\u5217\u63a8\u8350\u3001\u5546\u54c1\u641c\u7d22\u548c\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6574\u4f53\u6027\u80fd\u548c\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u786e\u4fddID\u552f\u4e00\u6027\u7684\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u80fd\u6709\u6548\u63d0\u5347\u751f\u6210\u5f0f\u63a8\u8350\u4e0e\u68c0\u7d22\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.16606", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16606", "abs": "https://arxiv.org/abs/2509.16606", "authors": ["Wei Duan", "Jie Lu", "Junyu Xuan"], "title": "Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning", "comment": "Accepted at NeurIPS 2025", "summary": "In networked multi-agent reinforcement learning (Networked-MARL),\ndecentralized agents must act under local observability and constrained\ncommunication over fixed physical graphs. Existing methods often assume static\nneighborhoods, limiting adaptability to dynamic or heterogeneous environments.\nWhile centralized frameworks can learn dynamic graphs, their reliance on global\nstate access and centralized infrastructure is impractical in real-world\ndecentralized systems. We propose a stochastic graph-based policy for\nNetworked-MARL, where each agent conditions its decision on a sampled subgraph\nover its local physical neighborhood. Building on this formulation, we\nintroduce BayesG, a decentralized actor-framework that learns sparse,\ncontext-aware interaction structures via Bayesian variational inference. Each\nagent operates over an ego-graph and samples a latent communication mask to\nguide message passing and policy computation. The variational distribution is\ntrained end-to-end alongside the policy using an evidence lower bound (ELBO)\nobjective, enabling agents to jointly learn both interaction topology and\ndecision-making strategies. BayesG outperforms strong MARL baselines on\nlarge-scale traffic control tasks with up to 167 agents, demonstrating superior\nscalability, efficiency, and performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u56fe\u7b56\u7565\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5BayesG\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u53d8\u5206\u63a8\u65ad\u5b66\u4e60\u7a00\u758f\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ea4\u4e92\u7ed3\u6784\uff0c\u5728\u5927\u89c4\u6a21\u4ea4\u901a\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u9759\u6001\u90bb\u5c45\u7ed3\u6784\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u6216\u5f02\u6784\u73af\u5883\uff1b\u800c\u96c6\u4e2d\u5f0f\u6846\u67b6\u4f9d\u8d56\u5168\u5c40\u72b6\u6001\u548c\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u968f\u673a\u5b50\u56fe\u7684\u7b56\u7565\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5728\u5176\u5c40\u90e8\u7269\u7406\u90bb\u57df\u4e0a\u91c7\u6837\u5b50\u56fe\u8fdb\u884c\u51b3\u7b56\uff1b\u5f15\u5165BayesG\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u5728\u81ea\u6211\u56fe\u4e0a\u901a\u8fc7\u8d1d\u53f6\u65af\u53d8\u5206\u63a8\u65ad\u5b66\u4e60\u6f5c\u5728\u901a\u4fe1\u63a9\u7801\uff0c\u4ee5\u6307\u5bfc\u6d88\u606f\u4f20\u9012\u548c\u7b56\u7565\u8ba1\u7b97\uff0c\u5e76\u8054\u5408\u4f18\u5316\u53d8\u5206\u5206\u5e03\u4e0e\u7b56\u7565\u3002", "result": "\u5728\u6700\u591a\u5305\u542b167\u4e2a\u667a\u80fd\u4f53\u7684\u5927\u89c4\u6a21\u4ea4\u901a\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cBayesG\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u6269\u5c55\u6027\u3001\u8fd0\u884c\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "BayesG\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e0b\u52a8\u6001\u3001\u7a00\u758f\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ea4\u4e92\u7ed3\u6784\u5b66\u4e60\uff0c\u4e3a\u7f51\u7edc\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16336", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16336", "abs": "https://arxiv.org/abs/2509.16336", "authors": ["Jan Philipp Schneider", "Pratik Singh Bisht", "Ilya Chugunov", "Andreas Kolb", "Michael Moeller", "Felix Heide"], "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing", "comment": null, "summary": "Learning editable high-resolution scene representations for dynamic scenes is\nan open problem with applications across the domains from autonomous driving to\ncreative editing - the most successful approaches today make a trade-off\nbetween editability and supporting scene complexity: neural atlases represent\ndynamic scenes as two deforming image layers, foreground and background, which\nare editable in 2D, but break down when multiple objects occlude and interact.\nIn contrast, scene graph models make use of annotated data such as masks and\nbounding boxes from autonomous-driving datasets to capture complex 3D spatial\nrelationships, but their implicit volumetric node representations are\nchallenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a\nhybrid high-resolution scene representation, where every graph node is a\nview-dependent neural atlas, facilitating both 2D appearance editing and 3D\nordering and positioning of scene elements. Fit at test-time, NAGs achieve\nstate-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR\nincrease compared to existing methods - and make environmental editing possible\nin high resolution and visual quality - creating counterfactual driving\nscenarios with new backgrounds and edited vehicle appearance. We find that the\nmethod also generalizes beyond driving scenes and compares favorably - by more\nthan 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS\nvideo dataset with a diverse set of human and animal-centric scenes.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u56fe\u8c31\u56fe\uff08NAGs\uff09\uff0c\u4e00\u79cd\u6df7\u5408\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u54082D\u5916\u89c2\u7f16\u8f91\u4e0e3D\u7a7a\u95f4\u5e03\u5c40\uff0c\u5b9e\u73b0\u5728\u52a8\u6001\u590d\u6742\u573a\u666f\u4e2d\u7684\u9ad8\u8d28\u91cf\u53ef\u7f16\u8f91\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u7f16\u8f91\u6027\u4e0e\u573a\u666f\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u795e\u7ecf\u56fe\u8c31\u96be\u4ee5\u5904\u7406\u591a\u7269\u4f53\u906e\u6321\uff0c\u800c\u573a\u666f\u56fe\u6a21\u578b\u7684\u9690\u5f0f\u4f53\u7d20\u8868\u793a\u96be\u4ee5\u4e00\u81f4\u5730\u7f16\u8f91\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u652f\u6301\u590d\u6742\u52a8\u6001\u573a\u666f\u53c8\u80fd\u5b9e\u73b0\u4fbf\u6377\u7f16\u8f91\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u56fe\u8c31\u56fe\uff08NAGs\uff09\uff0c\u6bcf\u4e2a\u56fe\u8282\u70b9\u4e3a\u89c6\u56fe\u76f8\u5173\u7684\u795e\u7ecf\u56fe\u8c31\uff0c\u7ed3\u5408\u4e86\u795e\u7ecf\u56fe\u8c31\u76842D\u7f16\u8f91\u80fd\u529b\u548c\u573a\u666f\u56fe\u76843D\u7a7a\u95f4\u5efa\u6a21\u80fd\u529b\uff1b\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u62df\u5408\uff0c\u652f\u6301\u9ad8\u5206\u8fa8\u7387\u52a8\u6001\u573a\u666f\u7684\u5206\u5c42\u8868\u793a\u4e0e\u7f16\u8f91\u3002", "result": "\u5728Waymo Open Dataset\u4e0aPSNR\u63d0\u53475 dB\uff0c\u5728DAVIS\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u6700\u65b0\u89c6\u9891\u62a0\u50cf\u548c\u7f16\u8f91\u65b9\u6cd57 dB\u4ee5\u4e0a\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u73af\u5883\u7f16\u8f91\u548c\u865a\u62df\u9a7e\u9a76\u573a\u666f\u751f\u6210\u3002", "conclusion": "NAGs\u5b9e\u73b0\u4e86\u52a8\u6001\u573a\u666f\u8868\u793a\u5728\u7f16\u8f91\u6027\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u590d\u6742\u6027\u4e4b\u95f4\u7684\u826f\u597d\u5e73\u8861\uff0c\u5177\u6709\u826f\u597d\u7684\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u521b\u610f\u7f16\u8f91\u7b49\u5e94\u7528\u3002"}}
{"id": "2509.16221", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16221", "abs": "https://arxiv.org/abs/2509.16221", "authors": ["Martin Prei\u00df"], "title": "Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement", "comment": null, "summary": "For the bachelor project 2021 of Professor Lippert's research group,\nhandwritten entries of historical patient records needed to be digitized using\nOptical Character Recognition (OCR) methods. Since the data will be used in the\nfuture, a high degree of accuracy is naturally required. Especially in the\nmedical field this has even more importance. Ensemble Learning is a method that\ncombines several machine learning models and is claimed to be able to achieve\nan increased accuracy for existing methods. For this reason, Ensemble Learning\nin combination with OCR is investigated in this work in order to create added\nvalue for the digitization of the patient records. It was possible to discover\nthat ensemble learning can lead to an increased accuracy for OCR, which methods\nwere able to achieve this and that the size of the training data set did not\nplay a role here.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u5b66\u4e60\u4e0eOCR\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5386\u53f2\u75c5\u5386\u624b\u5199\u8bb0\u5f55\u6570\u5b57\u5316\u7684\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u96c6\u6210\u5b66\u4e60\u80fd\u591f\u63d0\u5347OCR\u7684\u51c6\u786e\u7387\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5927\u5c0f\u5bf9\u6b64\u5f71\u54cd\u4e0d\u5927\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5386\u53f2\u75c5\u5386\u624b\u5199\u8bb0\u5f55\u6570\u5b57\u5316\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u9886\u57df\u5bf9\u9ad8\u7cbe\u5ea6\u7684\u9700\u6c42\u4e0b\uff0c\u63a2\u7d22\u96c6\u6210\u5b66\u4e60\u4e0eOCR\u7ed3\u5408\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7528\u4e8eOCR\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u96c6\u6210\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8OCR\u7684\u51c6\u786e\u6027\uff0c\u67d0\u4e9b\u7279\u5b9a\u65b9\u6cd5\u6548\u679c\u5c24\u4e3a\u7a81\u51fa\uff0c\u800c\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5927\u5c0f\u5e76\u672a\u663e\u8457\u5f71\u54cd\u7ed3\u679c\u3002", "conclusion": "\u96c6\u6210\u5b66\u4e60\u4e0eOCR\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u75c5\u5386\u6570\u5b57\u5316\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.16261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16261", "abs": "https://arxiv.org/abs/2509.16261", "authors": ["Shuocheng Yang", "Zikun Xu", "Jiahao Wang", "Shahid Nawaz", "Jianqiang Wang", "Shaobing Xu"], "title": "RaFD: Flow-Guided Radar Detection for Robust Autonomous Driving", "comment": null, "summary": "Radar has shown strong potential for robust perception in autonomous driving;\nhowever, raw radar images are frequently degraded by noise and \"ghost\"\nartifacts, making object detection based solely on semantic features highly\nchallenging. To address this limitation, we introduce RaFD, a radar-based\nobject detection framework that estimates inter-frame bird's-eye-view (BEV)\nflow and leverages the resulting geometric cues to enhance detection accuracy.\nSpecifically, we design a supervised flow estimation auxiliary task that is\njointly trained with the detection network. The estimated flow is further\nutilized to guide feature propagation from the previous frame to the current\none. Our flow-guided, radar-only detector achieves achieves state-of-the-art\nperformance on the RADIATE dataset, underscoring the importance of\nincorporating geometric information to effectively interpret radar signals,\nwhich are inherently ambiguous in semantics.", "AI": {"tldr": "\u63d0\u51faRaFD\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1\u5e27\u95f4BEV\u6d41\u5e76\u5229\u7528\u51e0\u4f55\u7ebf\u7d22\u63d0\u5347\u7eaf\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5728RADIATE\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u539f\u59cb\u96f7\u8fbe\u56fe\u50cf\u5e38\u53d7\u566a\u58f0\u548c\u201c\u9b3c\u5f71\u201d\u4f2a\u5f71\u5f71\u54cd\uff0c\u8bed\u4e49\u4fe1\u606f\u6a21\u7cca\uff0c\u5bfc\u81f4\u4ec5\u4f9d\u8d56\u8bed\u4e49\u7279\u5f81\u7684\u68c0\u6d4b\u56f0\u96be\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u76d1\u7763\u7684\u5149\u6d41\u4f30\u8ba1\u8f85\u52a9\u4efb\u52a1\uff0c\u4e0e\u68c0\u6d4b\u7f51\u7edc\u8054\u5408\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u4f30\u8ba1\u7684BEV\u6d41\u5f15\u5bfc\u524d\u4e00\u5e27\u7279\u5f81\u4f20\u64ad\u5230\u5f53\u524d\u5e27\u3002", "result": "\u5728RADIATE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7eaf\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u5f15\u5165\u51e0\u4f55\u4fe1\u606f\uff08\u5982BEV\u6d41\uff09\u5bf9\u63d0\u5347\u96f7\u8fbe\u4fe1\u53f7\u7684\u89e3\u6790\u80fd\u529b\u548c\u68c0\u6d4b\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.16226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16226", "abs": "https://arxiv.org/abs/2509.16226", "authors": ["Brian S. Lin", "Jiaxin Yuan", "Zihan Zhou", "Shouli Wang", "Shuo Wang", "Cunliang Kong", "Qi Shi", "Yuxuan Li", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations", "comment": "24 pages", "summary": "As large language models (LLMs) increasingly exhibit human-like capabilities,\na fundamental question emerges: How can we enable LLMs to learn the underlying\npatterns from limited examples in entirely novel environments and apply them\neffectively? This question is central to the ability of LLMs in inductive\nreasoning. Existing research on LLM-based inductive reasoning can be broadly\ncategorized based on whether the underlying rules are expressible via explicit\nmathematical equations. However, many recent studies in the beyond-equations\ncategory have emphasized rule design without grounding them in specific\nscenarios. Inspired by the parallels between inductive reasoning and human\nscientific discovery, we propose the task of LLM-Based Scientific Inductive\nReasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to\nevaluate the inductive reasoning abilities of LLMs in scientific settings. Our\nexperimental results show that current LLMs still struggle with this task,\nunderscoring its difficulty and the need for further advancement in this area.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u5f52\u7eb3\u63a8\u7406\u4efb\u52a1\u53ca\u65b0\u57fa\u51c6SIRBench-V1\uff0c\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u975e\u65b9\u7a0b\u5f0f\u573a\u666f\u4e0b\u7684\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u53d7\u4eba\u7c7b\u79d1\u5b66\u53d1\u73b0\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5168\u65b0\u73af\u5883\u4e2d\u4ece\u5c11\u91cf\u4f8b\u5b50\u4e2d\u5b66\u4e60\u5e76\u5e94\u7528\u6f5c\u5728\u89c4\u5f8b\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u65e0\u6cd5\u7528\u660e\u786e\u6570\u5b66\u65b9\u7a0b\u8868\u8fbe\u89c4\u5219\u7684\u79d1\u5b66\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u4efb\u52a1\u5b9a\u4e49\u548c\u57fa\u51c6SIRBench-V1\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d85\u8d8a\u65b9\u7a0b\u7684\u79d1\u5b66\u5f52\u7eb3\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u8868\u73b0\u51fa\u8f83\u5f31\u7684\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u573a\u666f\u4e0b\u7684\u975e\u65b9\u7a0b\u5f0f\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u4e9f\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0e\u63d0\u5347\u3002"}}
{"id": "2509.16215", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.NE", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16215", "abs": "https://arxiv.org/abs/2509.16215", "authors": ["Izavan dos S. Correia", "Henrique C. T. Santos", "Tiago A. E. Ferreira"], "title": "Discovering Software Parallelization Points Using Deep Neural Networks", "comment": "17 pages, 10 figures", "summary": "This study proposes a deep learning-based approach for discovering loops in\nprogramming code according to their potential for parallelization. Two genetic\nalgorithm-based code generators were developed to produce two distinct types of\ncode: (i) independent loops, which are parallelizable, and (ii) ambiguous\nloops, whose dependencies are unclear, making them impossible to define if the\nloop is parallelizable or not. The generated code snippets were tokenized and\npreprocessed to ensure a robust dataset. Two deep learning models - a Deep\nNeural Network (DNN) and a Convolutional Neural Network (CNN) - were\nimplemented to perform the classification. Based on 30 independent runs, a\nrobust statistical analysis was employed to verify the expected performance of\nboth models, DNN and CNN. The CNN showed a slightly higher mean performance,\nbut the two models had a similar variability. Experiments with varying dataset\nsizes highlighted the importance of data diversity for model performance. These\nresults demonstrate the feasibility of using deep learning to automate the\nidentification of parallelizable structures in code, offering a promising tool\nfor software optimization and performance improvement.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u53ef\u5e76\u884c\u5316\u7684\u5faa\u73af\u7ed3\u6784\u3002\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u751f\u6210\u72ec\u7acb\u548c\u6a21\u7cca\u5faa\u73af\u4ee3\u7801\uff0c\u4f7f\u7528DNN\u548cCNN\u8fdb\u884c\u5206\u7c7b\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\u4e14\u5177\u6709\u4f18\u5316\u8f6f\u4ef6\u6027\u80fd\u7684\u6f5c\u529b\u3002", "motivation": "\u81ea\u52a8\u8bc6\u522b\u4ee3\u7801\u4e2d\u53ef\u5e76\u884c\u5316\u7684\u5faa\u73af\u7ed3\u6784\u6709\u52a9\u4e8e\u63d0\u5347\u7a0b\u5e8f\u6027\u80fd\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u4ee3\u7801\u751f\u6210\u5668\u4ee5\u4ea7\u751f\u72ec\u7acb\u5faa\u73af\u548c\u6a21\u7cca\u5faa\u73af\u4ee3\u7801\uff1b\u5bf9\u751f\u6210\u7684\u4ee3\u7801\u8fdb\u884c\u5206\u8bcd\u548c\u9884\u5904\u7406\u540e\uff0c\u91c7\u7528DNN\u548cCNN\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc730\u6b21\u72ec\u7acb\u8fd0\u884c\u7684\u7edf\u8ba1\u5206\u6790\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "CNN\u6a21\u578b\u5e73\u5747\u8868\u73b0\u7565\u4f18\u4e8eDNN\uff0c\u4f46\u4e24\u8005\u53d8\u5f02\u6027\u76f8\u4f3c\uff1b\u6570\u636e\u96c6\u5927\u5c0f\u5b9e\u9a8c\u663e\u793a\u6570\u636e\u591a\u6837\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u8bc6\u522b\u4ee3\u7801\u4e2d\u7684\u53ef\u5e76\u884c\u5316\u7ed3\u6784\uff0c\u4e3a\u8f6f\u4ef6\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2509.16288", "categories": ["cs.AI", "05C22, 05C90, 68R10 05C22, 05C90, 68R10 05C22, 05C90, 68R10"], "pdf": "https://arxiv.org/pdf/2509.16288", "abs": "https://arxiv.org/abs/2509.16288", "authors": ["Shanookha Ali", "Nitha Niralda P C"], "title": "Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity", "comment": null, "summary": "Coronary heart disease (CHD) arises from complex interactions among\nuncontrollable factors, controllable lifestyle factors, and clinical\nindicators, where relationships are often uncertain. Fuzzy subgraph\nconnectivity (FSC) provides a systematic tool to capture such imprecision by\nquantifying the strength of association between vertices and subgraphs in fuzzy\ngraphs. In this work, a fuzzy CHD graph is constructed with vertices for\nuncontrollable, controllable, and indicator components, and edges weighted by\nfuzzy memberships. Using FSC, we evaluate connectivity to identify strongest\ndiagnostic routes, dominant risk factors, and critical bridges. Results show\nthat FSC highlights influential pathways, bounds connectivity between weakest\nand strongest correlations, and reveals critical edges whose removal reduces\npredictive strength. Thus, FSC offers an interpretable and robust framework for\nmodeling uncertainty in CHD risk prediction and supporting clinical\ndecision-making.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6a21\u7cca\u5b50\u56fe\u8fde\u901a\u6027\uff08FSC\uff09\u6784\u5efa\u51a0\u5fc3\u75c5\uff08CHD\uff09\u7684\u6a21\u7cca\u56fe\u6a21\u578b\uff0c\u901a\u8fc7\u91cf\u5316\u8282\u70b9\u4e0e\u5b50\u56fe\u95f4\u7684\u5173\u8054\u5f3a\u5ea6\uff0c\u8bc6\u522b\u5173\u952e\u8bca\u65ad\u8def\u5f84\u3001\u4e3b\u5bfc\u98ce\u9669\u56e0\u7d20\u548c\u5173\u952e\u8fde\u63a5\uff0c\u4e3aCHD\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u7a33\u5065\u7684\u6846\u67b6\u3002", "motivation": "\u51a0\u5fc3\u75c5\u7531\u4e0d\u53ef\u63a7\u56e0\u7d20\u3001\u53ef\u63a7\u751f\u6d3b\u65b9\u5f0f\u548c\u4e34\u5e8a\u6307\u6807\u590d\u6742\u4ea4\u4e92\u5f15\u8d77\uff0c\u5173\u7cfb\u5e38\u5177\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u7684\u7cfb\u7edf\u6027\u5de5\u5177\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e0d\u53ef\u63a7\u3001\u53ef\u63a7\u548c\u6307\u6807\u6210\u5206\u7684\u6a21\u7ccaCHD\u56fe\uff0c\u8fb9\u6743\u91cd\u7531\u6a21\u7cca\u96b6\u5c5e\u5ea6\u786e\u5b9a\uff0c\u5e76\u5229\u7528\u6a21\u7cca\u5b50\u56fe\u8fde\u901a\u6027\uff08FSC\uff09\u8bc4\u4f30\u56fe\u7684\u8fde\u901a\u6027\uff0c\u8bc6\u522b\u6700\u5f3a\u8bca\u65ad\u8def\u5f84\u3001\u4e3b\u5bfc\u98ce\u9669\u56e0\u7d20\u548c\u5173\u952e\u6865\u6881\u3002", "result": "FSC\u80fd\u591f\u7a81\u51fa\u91cd\u8981\u4f20\u64ad\u8def\u5f84\uff0c\u754c\u5b9a\u6700\u5f31\u4e0e\u6700\u5f3a\u76f8\u5173\u6027\u4e4b\u95f4\u7684\u8fde\u901a\u6027\uff0c\u5e76\u63ed\u793a\u79fb\u9664\u540e\u4f1a\u964d\u4f4e\u9884\u6d4b\u80fd\u529b\u7684\u5173\u952e\u8fb9\u3002", "conclusion": "FSC\u4e3a\u51a0\u5fc3\u75c5\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u7a33\u5065\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2509.16539", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16539", "abs": "https://arxiv.org/abs/2509.16539", "authors": ["Pushpa Devi", "Ayush Agrawal", "Ashutosh Dubey", "C. Ravindranath Chowdary"], "title": "Long document summarization using page specific target text alignment and distilling page importance", "comment": "8 pages, 2 figures", "summary": "The rapid growth of textual data across news, legal, medical, and scientific\ndomains is becoming a challenge for efficiently accessing and understanding\nlarge volumes of content. It is increasingly complex for users to consume and\nextract meaningful information efficiently. Thus, raising the need for\nsummarization. Unlike short document summarization, long document abstractive\nsummarization is resource-intensive, and very little literature is present in\nthis direction. BART is a widely used efficient sequence-to-sequence\n(seq-to-seq) model. However, when it comes to summarizing long documents, the\nlength of the context window limits its capabilities. We proposed a model\ncalled PTS (Page-specific Target-text alignment Summarization) that extends the\nseq-to-seq method for abstractive summarization by dividing the source document\ninto several pages. PTS aligns each page with the relevant part of the target\nsummary for better supervision. Partial summaries are generated for each page\nof the document. We proposed another model called PTSPI (Page-specific\nTarget-text alignment Summarization with Page Importance), an extension to PTS\nwhere an additional layer is placed before merging the partial summaries into\nthe final summary. This layer provides dynamic page weightage and explicit\nsupervision to focus on the most informative pages. We performed experiments on\nthe benchmark dataset and found that PTSPI outperformed the SOTA by 6.32\\% in\nROUGE-1 and 8.08\\% in ROUGE-2 scores.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u957f\u6587\u6863\u62bd\u8c61\u6458\u8981\u7684\u65b0\u6a21\u578bPTS\u548c\u5176\u6269\u5c55\u7248\u672cPTSPI\uff0c\u901a\u8fc7\u5206\u9875\u5904\u7406\u548c\u9875\u9762\u91cd\u8981\u6027\u52a0\u6743\u663e\u8457\u63d0\u5347\u4e86\u6458\u8981\u6027\u80fd\u3002", "motivation": "\u957f\u6587\u6863\u7684\u62bd\u8c61\u6458\u8981\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u8d44\u6e90\u6d88\u8017\u5927\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u957f\u6587\u672c\u3002", "method": "\u5c06\u6e90\u6587\u6863\u5206\u5272\u4e3a\u591a\u4e2a\u9875\u9762\uff0c\u4f7f\u7528PTS\u6a21\u578b\u5bf9\u6bcf\u4e00\u9875\u8fdb\u884c\u76ee\u6807\u6587\u672c\u5bf9\u9f50\u7684\u6458\u8981\u751f\u6210\uff0c\u5e76\u5728PTSPI\u4e2d\u5f15\u5165\u9875\u9762\u91cd\u8981\u6027\u52a0\u6743\u673a\u5236\uff0c\u52a8\u6001\u5206\u914d\u5404\u9875\u9762\u6743\u91cd\u4ee5\u751f\u6210\u6700\u7ec8\u6458\u8981\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPTSPI\u76f8\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u5728ROUGE-1\u4e0a\u63d0\u5347\u4e866.32%\uff0cROUGE-2\u4e0a\u63d0\u5347\u4e868.08%\u3002", "conclusion": "PTSPI\u901a\u8fc7\u5206\u9875\u5904\u7406\u548c\u663e\u5f0f\u7684\u9875\u9762\u91cd\u8981\u6027\u76d1\u7763\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u6587\u6863\u62bd\u8c61\u6458\u8981\u7684\u8d28\u91cf\uff0c\u662f\u5904\u7406\u957f\u6587\u672c\u6458\u8981\u7684\u4e00\u79cd\u9ad8\u6548\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.16736", "categories": ["cs.MA", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16736", "abs": "https://arxiv.org/abs/2509.16736", "authors": ["Minfeng Qi", "Tianqing Zhu", "Lefeng Zhang", "Ningran Li", "Wanlei Zhou"], "title": "Towards Transparent and Incentive-Compatible Collaboration in Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach", "comment": "17 pages, 7 figures", "summary": "Large Language Models (LLMs) have enabled the emergence of autonomous agents\ncapable of complex reasoning, planning, and interaction. However, coordinating\nsuch agents at scale remains a fundamental challenge, particularly in\ndecentralized environments where communication lacks transparency and agent\nbehavior cannot be shaped through centralized incentives. We propose a\nblockchain-based framework that enables transparent agent registration,\nverifiable task allocation, and dynamic reputation tracking through smart\ncontracts. The core of our design lies in two mechanisms: a matching\nscore-based task allocation protocol that evaluates agents by reputation,\ncapability match, and workload; and a behavior-shaping incentive mechanism that\nadjusts agent behavior via feedback on performance and reward. Our\nimplementation integrates GPT-4 agents with Solidity contracts and\ndemonstrates, through 50-round simulations, strong task success rates, stable\nutility distribution, and emergent agent specialization. The results underscore\nthe potential for trustworthy, incentive-compatible multi-agent coordination in\nopen environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u5408\u7ea6\u5b9e\u73b0\u900f\u660e\u4ee3\u7406\u6ce8\u518c\u3001\u53ef\u9a8c\u8bc1\u4efb\u52a1\u5206\u914d\u548c\u52a8\u6001\u58f0\u8a89\u8ddf\u8e2a\uff0c\u7ed3\u5408\u5339\u914d\u8bc4\u5206\u673a\u5236\u4e0e\u884c\u4e3a\u6fc0\u52b1\u673a\u5236\uff0c\u5728GPT-4\u4ee3\u7406\u4e0eSolidity\u5408\u7ea6\u96c6\u6210\u7684\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u4ee3\u7406\u4e13\u4e1a\u5316\u8d8b\u52bf\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\uff0c\u5927\u89c4\u6a21\u81ea\u4e3b\u4ee3\u7406\u7684\u534f\u8c03\u9762\u4e34\u901a\u4fe1\u4e0d\u900f\u660e\u548c\u7f3a\u4e4f\u96c6\u4e2d\u6fc0\u52b1\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5b9e\u73b0\u53ef\u4fe1\u534f\u4f5c\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6846\u67b6\uff0c\u91c7\u7528\u5339\u914d\u8bc4\u5206\u673a\u5236\uff08\u7efc\u5408\u58f0\u8a89\u3001\u80fd\u529b\u5339\u914d\u5ea6\u548c\u5de5\u4f5c\u8d1f\u8f7d\uff09\u8fdb\u884c\u4efb\u52a1\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u4e0e\u5956\u52b1\u8c03\u6574\u4ee3\u7406\u884c\u4e3a\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u4f7f\u7528GPT-4\u4ee3\u7406\u4e0eSolidity\u667a\u80fd\u5408\u7ea6\u96c6\u6210\u8fdb\u884c50\u8f6e\u4eff\u771f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3001\u7a33\u5b9a\u7684\u6548\u7528\u5206\u5e03\u4ee5\u53ca\u4ee3\u7406\u7684\u81ea\u53d1\u4e13\u4e1a\u5316\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u4fe1\u3001\u6fc0\u52b1\u517c\u5bb9\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e0b\u5927\u89c4\u6a21\u81ea\u4e3b\u4ee3\u7406\u7684\u53ef\u4fe1\u534f\u8c03\uff0c\u4e3a\u5f00\u653e\u3001\u65e0\u4e2d\u5fc3\u63a7\u5236\u573a\u666f\u4e0b\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16735", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.16735", "abs": "https://arxiv.org/abs/2509.16735", "authors": ["Dongdong Chen", "Linlin Yao", "Mengjun Liu", "Zhenrong Shen", "Yuqi Hu", "Zhiyun Song", "Shengyu Lu", "Qian Wang", "Dinggang Shen", "Lichi Zhang"], "title": "Brain Connectivity Network Structure Learning For Brain Disorder Diagnosis", "comment": null, "summary": "Recent studies in neuroscience highlight the significant potential of brain\nconnectivity networks, which are commonly constructed from functional magnetic\nresonance imaging (fMRI) data for brain disorder diagnosis. Traditional brain\nconnectivity networks are typically obtained using predefined methods that\nincorporate manually-set thresholds to estimate inter-regional relationships.\nHowever, such approaches often introduce redundant connections or overlook\nessential interactions, compromising the value of the constructed networks.\nBesides, the insufficiency of labeled data further increases the difficulty of\nlearning generalized representations of intrinsic brain characteristics. To\nmitigate those issues, we propose a self-supervised framework to learn an\noptimal structure and representation for brain connectivity networks, focusing\non individualized generation and optimization in an unsupervised manner. We\nfirstly employ two existing whole-brain connectomes to adaptively construct\ntheir complementary brain network structure learner, and then introduce a\nmulti-state graph-based encoder with a joint iterative learning strategy to\nsimultaneously optimize both the generated network structure and its\nrepresentation. By leveraging self-supervised pretraining on large-scale\nunlabeled brain connectivity data, our framework enables the brain connectivity\nnetwork learner to generalize e ffectively to unseen disorders, while requiring\nonly minimal finetuning of the encoder for adaptation to new diagnostic tasks.\nExtensive experiments on cross-dataset brain disorder diagnosis demonstrate\nthat our method consistently outperforms state-of-the-art approaches,\nvalidating its effectiveness and generalizability. The code is publicly\navailable at https://github.com/neochen1/BCNSL.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u4f53\u5316\u751f\u6210\u548c\u4f18\u5316\u8111\u8fde\u63a5\u7f51\u7edc\u7ed3\u6784\u4e0e\u8868\u793a\uff0c\u901a\u8fc7\u591a\u72b6\u6001\u56fe\u7f16\u7801\u5668\u548c\u8054\u5408\u8fed\u4ee3\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u8de8\u6570\u636e\u96c6\u8111\u75be\u75c5\u8bca\u65ad\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8111\u8fde\u63a5\u7f51\u7edc\u4f9d\u8d56\u9884\u5b9a\u4e49\u65b9\u6cd5\u548c\u4eba\u5de5\u8bbe\u5b9a\u9608\u503c\uff0c\u5bb9\u6613\u5f15\u5165\u5197\u4f59\u8fde\u63a5\u6216\u9057\u6f0f\u91cd\u8981\u4ea4\u4e92\uff0c\u4e14\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u9650\u5236\u4e86\u7279\u5f81\u8868\u793a\u7684\u5b66\u4e60\u3002", "method": "\u5229\u7528\u4e24\u4e2a\u73b0\u6709\u5168\u8111\u8fde\u63a5\u7ec4\u81ea\u9002\u5e94\u6784\u5efa\u4e92\u8865\u7684\u8111\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\u5668\uff0c\u7ed3\u5408\u591a\u72b6\u6001\u56fe\u7f16\u7801\u5668\u4e0e\u8054\u5408\u8fed\u4ee3\u5b66\u4e60\u7b56\u7565\uff0c\u540c\u6b65\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u53ca\u5176\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u8de8\u6570\u636e\u96c6\u8111\u75be\u75c5\u8bca\u65ad\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5 consistently \u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u6846\u67b6\u80fd\u6709\u6548\u5b66\u4e60\u8111\u8fde\u63a5\u7f51\u7edc\u7684\u6700\u4f18\u7ed3\u6784\u4e0e\u8868\u793a\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8111\u75be\u75c5\u7684\u4e2a\u4f53\u5316\u8bca\u65ad\u3002"}}
{"id": "2509.16343", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.16343", "abs": "https://arxiv.org/abs/2509.16343", "authors": ["Chung-En", "Yu", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute", "comment": null, "summary": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u63a8\u7406\u6846\u67b6VRA\uff0c\u901a\u8fc7Think--Critique--Act\u5faa\u73af\u663e\u8457\u63d0\u5347\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u9065\u611f\u548c\u533b\u5b66\u8bca\u65ad\uff09\u4e2d\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u5e7f\u6cdb\u9c81\u68d2\u6027\u4e14\u65e0\u9700\u6602\u8d35\u91cd\u8bad\u7ec3\u7684\u53ef\u4fe1\u667a\u80fd\u89c6\u89c9\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aVisual Reasoning Agent (VRA)\u7684\u8bad\u7ec3-free\u6846\u67b6\uff0c\u7ed3\u5408\u73b0\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7eaf\u89c6\u89c9\u7cfb\u7edf\uff0c\u91c7\u7528Think--Critique--Act\u5faa\u73af\u8fdb\u884c\u63a8\u7406\u3002", "result": "VRA\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u8fbe40%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4f46\u589e\u52a0\u4e86\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "VRA\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u63a8\u7406\u6027\u80fd\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u4f18\u5316\u67e5\u8be2\u8def\u7531\u548c\u65e9\u671f\u505c\u6b62\u673a\u5236\u4ee5\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u3002"}}
{"id": "2509.16353", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16353", "abs": "https://arxiv.org/abs/2509.16353", "authors": ["Shaoting Peng", "Dakarai Crowder", "Wenzhen Yuan", "Katherine Driggs-Campbell"], "title": "Tactile-Based Human Intent Recognition for Robot Assistive Navigation", "comment": null, "summary": "Robot assistive navigation (RAN) is critical for enhancing the mobility and\nindependence of the growing population of mobility-impaired individuals.\nHowever, existing systems often rely on interfaces that fail to replicate the\nintuitive and efficient physical communication observed between a person and a\nhuman caregiver, limiting their effectiveness. In this paper, we introduce\nTac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a\nStretch 3 mobile manipulator to provide a more natural and efficient interface\nfor human navigational intent recognition. To robustly classify the tactile\ndata, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an\nalgorithm that explicitly models the sensor's cylindrical geometry and is\nconsequently robust to the natural rotational shifts present in a user's grasp.\nComprehensive experiments were conducted to demonstrate the effectiveness of\nour classification algorithm and the overall system. Results show that CK-SVM\nachieved superior classification accuracy on both simulated (97.1%) and\nreal-world (90.8%) datasets compared to four baseline models. Furthermore, a\npilot study confirmed that users more preferred the Tac-Nav tactile interface\nover conventional joystick and voice-based controls.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5706\u67f1\u5f62\u89e6\u89c9\u76ae\u80a4\u7684\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edfTac-Nav\uff0c\u901a\u8fc7\u5f15\u5165\u8003\u8651\u4f20\u611f\u5668\u51e0\u4f55\u7ed3\u6784\u7684CK-SVM\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7528\u6237\u5bfc\u822a\u610f\u56fe\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u81ea\u7136\u6027\u3002", "motivation": "\u73b0\u6709\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u7684\u4ea4\u4e92\u65b9\u5f0f\u65e0\u6cd5\u590d\u73b0\u4eba\u4e0e\u770b\u62a4\u8005\u4e4b\u95f4\u76f4\u89c2\u9ad8\u6548\u7684\u7269\u7406\u6c9f\u901a\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "\u5f00\u53d1\u4e86Tac-Nav\u7cfb\u7edf\uff0c\u91c7\u7528\u5706\u67f1\u5f62\u89e6\u89c9\u76ae\u80a4\u611f\u77e5\u7528\u6237\u610f\u56fe\uff0c\u5e76\u63d0\u51faCK-SVM\u7b97\u6cd5\uff0c\u5229\u7528\u652f\u6301\u5411\u91cf\u673a\u663e\u5f0f\u5efa\u6a21\u4f20\u611f\u5668\u7684\u5706\u67f1\u51e0\u4f55\u7ed3\u6784\uff0c\u589e\u5f3a\u5bf9\u6293\u63e1\u65cb\u8f6c\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCK-SVM\u5728\u6a21\u62df\u6570\u636e\uff0897.1%\uff09\u548c\u771f\u5b9e\u6570\u636e\uff0890.8%\uff09\u4e0a\u5747\u4f18\u4e8e\u56db\u79cd\u57fa\u7ebf\u6a21\u578b\uff1b\u521d\u6b65\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u7528\u6237\u66f4\u504f\u597dTac-Nav\u7684\u89e6\u89c9\u63a5\u53e3\u800c\u975e\u4f20\u7edf\u6447\u6746\u6216\u8bed\u97f3\u63a7\u5236\u3002", "conclusion": "Tac-Nav\u7cfb\u7edf\u7ed3\u5408\u89e6\u89c9\u4ea4\u4e92\u4e0e\u51e0\u4f55\u611f\u77e5\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u81ea\u7136\u3001\u9ad8\u6548\u4e14\u7528\u6237\u504f\u597d\u7684\u8f85\u52a9\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16241", "categories": ["cs.CL", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.16241", "abs": "https://arxiv.org/abs/2509.16241", "authors": ["Eishkaran Singh", "Tanav Singh Bajaj", "Siddharth Nayak"], "title": "REAMS: Reasoning Enhanced Algorithm for Maths Solving", "comment": null, "summary": "The challenges of solving complex university-level mathematics problems,\nparticularly those from MIT, and Columbia University courses, and selected\ntasks from the MATH dataset, remain a significant obstacle in the field of\nartificial intelligence. Conventional methods have consistently fallen short in\nthis domain, highlighting the need for more advanced approaches. In this paper,\nwe introduce a language-based solution that leverages zero-shot learning and\nmathematical reasoning to effectively solve, explain, and generate solutions\nfor these advanced math problems. By integrating program synthesis, our method\nreduces reliance on large-scale training data while significantly improving\nproblem-solving accuracy. Our approach achieves an accuracy of 90.15%,\nrepresenting a substantial improvement over the previous benchmark of 81% and\nsetting a new standard in automated mathematical problem-solving. These\nfindings highlight the significant potential of advanced AI methodologies to\naddress and overcome the challenges presented by some of the most complex\nmathematical courses and datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7684\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u7a0b\u5e8f\u5408\u6210\u4e0e\u6570\u5b66\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u9ad8\u7b49\u6570\u5b66\u95ee\u9898\uff0c\u51c6\u786e\u7387\u8fbe\u523090.15%\uff0c\u663e\u8457\u8d85\u8d8a\u5148\u524d81%\u7684\u57fa\u51c6\u3002", "motivation": "\u4f20\u7edfAI\u65b9\u6cd5\u5728\u89e3\u51b3\u5927\u5b66\u9ad8\u9636\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u66f4\u5148\u8fdb\u7684\u6280\u672f\u6765\u5e94\u5bf9\u590d\u6742\u6570\u5b66\u63a8\u7406\u6311\u6218\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u4e0e\u7a0b\u5e8f\u5408\u6210\u6280\u672f\uff0c\u901a\u8fc7\u6570\u5b66\u63a8\u7406\u751f\u6210\u5e76\u9a8c\u8bc1\u89e3\u9898\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5728MIT\u3001\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66\u8bfe\u7a0b\u53caMATH\u6570\u636e\u96c6\u4e0a\u7684\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e2d\uff0c\u5b9e\u73b0\u4e8690.15%\u7684\u51c6\u786e\u7387\uff0c\u8f83\u4e4b\u524d\u768481%\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e\u7a0b\u5e8f\u5408\u6210\u7ed3\u5408\u5728\u81ea\u52a8\u6570\u5b66\u89e3\u9898\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u590d\u6742\u6570\u5b66\u95ee\u9898\u7684AI\u6c42\u89e3\u6811\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2509.16233", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16233", "abs": "https://arxiv.org/abs/2509.16233", "authors": ["Dipayan Sanpui", "Anirban Chandra", "Henry Chan", "Sukriti Manna", "Subramanian KRS Sankaranarayanan"], "title": "Comparison of Deterministic and Probabilistic Machine Learning Algorithms for Precise Dimensional Control and Uncertainty Quantification in Additive Manufacturing", "comment": null, "summary": "We present a probabilistic framework to accurately estimate dimensions of\nadditively manufactured components. Using a dataset of 405 parts from nine\nproduction runs involving two machines, three polymer materials, and two-part\nconfigurations, we examine five key design features. To capture both design\ninformation and manufacturing variability, we employ models integrating\ncontinuous and categorical factors. For predicting Difference from Target (DFT)\nvalues, we test deterministic and probabilistic machine learning methods.\nDeterministic models, trained on 80% of the dataset, provide precise point\nestimates, with Support Vector Regression (SVR) achieving accuracy close to\nprocess repeatability. To address systematic deviations, we adopt Gaussian\nProcess Regression (GPR) and Bayesian Neural Networks (BNNs). GPR delivers\nstrong predictive performance and interpretability, while BNNs capture both\naleatoric and epistemic uncertainties. We investigate two BNN approaches: one\nbalancing accuracy and uncertainty capture, and another offering richer\nuncertainty decomposition but with lower dimensional accuracy. Our results\nunderscore the importance of quantifying epistemic uncertainty for robust\ndecision-making, risk assessment, and model improvement. We discuss trade-offs\nbetween GPR and BNNs in terms of predictive power, interpretability, and\ncomputational efficiency, noting that model choice depends on analytical needs.\nBy combining deterministic precision with probabilistic uncertainty\nquantification, our study provides a rigorous foundation for uncertainty-aware\npredictive modeling in AM. This approach not only enhances dimensional accuracy\nbut also supports reliable, risk-informed design strategies, thereby advancing\ndata-driven manufacturing methodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u786e\u5b9a\u6027\u6a21\u578b\u548c\u6982\u7387\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u589e\u6750\u5236\u9020\u90e8\u4ef6\u7684\u5c3a\u5bf8\u504f\u5dee\uff0c\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u652f\u6301\u53ef\u9760\u7684\u8bbe\u8ba1\u51b3\u7b56\u3002", "motivation": "\u589e\u6750\u5236\u9020\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5236\u9020\u53d8\u5f02\u6027\u548c\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4f20\u7edf\u786e\u5b9a\u6027\u6a21\u578b\u96be\u4ee5\u5145\u5206\u91cf\u5316\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5f71\u54cd\u8bbe\u8ba1\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\u3001\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff08BNN\uff09\u5bf9405\u4e2a\u96f6\u4ef6\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5efa\u6a21\uff0c\u7ed3\u5408\u8fde\u7eed\u548c\u5206\u7c7b\u53d8\u91cf\uff0c\u9884\u6d4b\u76ee\u6807\u504f\u5dee\uff08DFT\uff09\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "SVR\u8fbe\u5230\u63a5\u8fd1\u5de5\u827a\u91cd\u590d\u6027\u7684\u7cbe\u5ea6\uff1bGPR\u5177\u6709\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff1bBNN\u80fd\u540c\u65f6\u6355\u6349\u968f\u673a\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u7cbe\u5ea6\u7565\u4f4e\uff1b\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6709\u52a9\u4e8e\u63d0\u5347\u51b3\u7b56\u9c81\u68d2\u6027\u548c\u98ce\u9669\u8bc4\u4f30\u3002", "conclusion": "\u7ed3\u5408\u786e\u5b9a\u6027\u7cbe\u5ea6\u4e0e\u6982\u7387\u6027\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6df7\u5408\u65b9\u6cd5\u4e3a\u589e\u6750\u5236\u9020\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u9884\u6d4b\u5efa\u6a21\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u6570\u636e\u9a71\u52a8\u5236\u9020\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.16298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16298", "abs": "https://arxiv.org/abs/2509.16298", "authors": ["Raquel Fernandez-Peralta", "Juan Vicente Riera"], "title": "A global view of diverse construction methods of fuzzy implication functions rooted on F-chains", "comment": null, "summary": "Fuzzy implication functions are one of the most important operators used in\nthe fuzzy logic framework. While their flexible definition allows for diverse\nfamilies with distinct properties, this variety needs a deeper theoretical\nunderstanding of their structural relationships. In this work, we focus on the\nstudy of construction methods, which employ different techniques to generate\nnew fuzzy implication functions from existing ones. Particularly, we generalize\nthe $F$-chain-based construction, recently introduced by Mesiar et al. to\nextend a method for constructing aggregation functions to the context of fuzzy\nimplication functions. Our generalization employs collections of fuzzy\nimplication functions rather than single ones, and uses two different\nincreasing functions instead of a unique $F$-chain. We analyze property\npreservation under this construction and establish sufficient conditions.\nFurthermore, we demonstrate that our generalized $F$-chain-based construction\nis a unifying framework for several existing methods. In particular, we show\nthat various construction techniques, such as contraposition, aggregation, and\ngeneralized vertical/horizontal threshold methods, can be reformulated within\nour approach. This reveals structural similarities between seemingly distinct\nconstruction strategies and provides a cohesive perspective on fuzzy\nimplication construction methods.", "AI": {"tldr": "\u672c\u6587\u63a8\u5e7f\u4e86\u57fa\u4e8e$F$-\u94fe\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591a\u4e2a\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u548c\u4e24\u4e2a\u589e\u51fd\u6570\u7684\u5e7f\u4e49\u6784\u9020\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u73b0\u6709\u6784\u9020\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6027\u8d28\u4fdd\u6301\u6761\u4ef6\u3002", "motivation": "\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u5728\u6a21\u7cca\u903b\u8f91\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u591a\u6837\u7684\u6784\u9020\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7406\u8bba\u5173\u8054\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u7ed3\u6784\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u51fd\u6570\u96c6\u5408\u4e0e\u4e24\u4e2a\u589e\u51fd\u6570\uff0c\u63a8\u5e7f\u4e86$F$-\u94fe\u6784\u9020\u6cd5\uff0c\u5e76\u5206\u6790\u8be5\u6784\u9020\u4e0b\u7684\u6027\u8d28\u4fdd\u6301\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u65b0\u6784\u9020\u65b9\u6cd5\u80fd\u7edf\u4e00\u591a\u79cd\u5df2\u6709\u65b9\u6cd5\uff08\u5982\u5bf9\u5076\u5316\u3001\u805a\u5408\u3001\u9608\u503c\u6cd5\uff09\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7b56\u7565\u95f4\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e7f\u4e49$F$-\u94fe\u6784\u9020\u6846\u67b6\u4e3a\u6a21\u7cca\u8574\u6db5\u51fd\u6570\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u589e\u5f3a\u4e86\u5bf9\u5404\u7c7b\u65b9\u6cd5\u95f4\u5173\u7cfb\u7684\u7406\u8bba\u8ba4\u8bc6\u3002"}}
{"id": "2509.16621", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16621", "abs": "https://arxiv.org/abs/2509.16621", "authors": ["Hiun Kim", "Tae Kwan Lee", "Taeryun Won"], "title": "The Role of Vocabularies in Learning Sparse Representations for Ranking", "comment": null, "summary": "Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for\neffective semantic 1st stage matching while enjoying the efficiency of inverted\nindices. A recent work on learning SPLADE models with expanded vocabularies\n(ESPLADE) was proposed to represent queries and documents into a sparse space\nof custom vocabulary which have different levels of vocabularic granularity.\nWithin this effort, however, there have not been many studies on the role of\nvocabulary in SPLADE models and their relationship to retrieval efficiency and\neffectiveness.\n  To study this, we construct BERT models with 100K-sized output vocabularies,\none initialized with the ESPLADE pretraining method and one initialized\nrandomly. After finetune on real-world search click logs, we applied logit\nscore-based queries and documents pruning to max size for further balancing\nefficiency. The experimental result in our evaluation set shows that, when\npruning is applied, the two models are effective compared to the 32K-sized\nnormal SPLADE model in the computational budget under the BM25. And the ESPLADE\nmodels are more effective than the random vocab model, while having a similar\nretrieval cost.\n  The result indicates that the size and pretrained weight of output\nvocabularies play the role of configuring the representational specification\nfor queries, documents, and their interactions in the retrieval engine, beyond\ntheir original meaning and purposes in NLP. These findings can provide a new\nroom for improvement for LSR by identifying the importance of representational\nspecification from vocabulary configuration for efficient and effective\nretrieval.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86SPLADE\u6a21\u578b\u4e2d\u8f93\u51fa\u8bcd\u6c47\u8868\u7684\u5927\u5c0f\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u5bf9\u7a00\u758f\u68c0\u7d22\u6548\u7387\u4e0e\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u66f4\u5927\u7684\u8bcd\u6c47\u8868\uff08\u598210\u4e07\uff09\u7ed3\u5408ESPLADE\u521d\u59cb\u5316\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u5728\u526a\u679d\u540e\u4ecd\u4fdd\u6301\u8f83\u4f4e\u68c0\u7d22\u6210\u672c\uff0c\u8868\u660e\u8bcd\u6c47\u914d\u7f6e\u5728\u8868\u793a\u89c4\u683c\u5316\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8SPLADE\u7c7b\u6a21\u578b\u4e2d\u8bcd\u6c47\u8868\u5728\u8bed\u4e49\u68c0\u7d22\u4e2d\u7684\u89d2\u8272\u53ca\u5176\u5bf9\u6548\u7387\u4e0e\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u8bcd\u6c47\u7c92\u5ea6\u4e0b\u7684\u8868\u73b0\uff0c\u586b\u8865\u8be5\u65b9\u5411\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e24\u4e2a10\u4e07\u8bcd\u8868\u7684BERT\u6a21\u578b\uff0c\u4e00\u4e2a\u91c7\u7528ESPLADE\u9884\u8bad\u7ec3\u521d\u59cb\u5316\uff0c\u53e6\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\uff0c\u5728\u771f\u5b9e\u641c\u7d22\u70b9\u51fb\u65e5\u5fd7\u4e0a\u5fae\u8c03\uff0c\u5e76\u5e94\u7528\u57fa\u4e8elogit\u5206\u6570\u7684\u67e5\u8be2\u4e0e\u6587\u6863\u526a\u679d\u4ee5\u63a7\u5236\u6700\u5927\u957f\u5ea6\uff0c\u8bc4\u4f30\u5176\u5728\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u526a\u679d\u6761\u4ef6\u4e0b\uff0c\u4e24\u4e2a\u5927\u8bcd\u8868\u6a21\u578b\u5747\u4f18\u4e8e\u6807\u51c632K SPLADE\u6a21\u578b\uff1b\u5176\u4e2dESPLADE\u521d\u59cb\u5316\u6a21\u578b\u6548\u679c\u66f4\u4f18\uff0c\u4e14\u68c0\u7d22\u6210\u672c\u4e0e\u968f\u673a\u8bcd\u8868\u6a21\u578b\u76f8\u8fd1\u3002", "conclusion": "\u8f93\u51fa\u8bcd\u8868\u7684\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u4e0d\u4ec5\u5f71\u54cd\u8bed\u8a00\u8868\u793a\uff0c\u66f4\u5728\u68c0\u7d22\u7cfb\u7edf\u4e2d\u8d77\u5230\u914d\u7f6e\u8868\u793a\u89c4\u683c\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u63d0\u5347\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17703", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2509.17703", "abs": "https://arxiv.org/abs/2509.17703", "authors": ["Zhou Ziheng", "Huacong Tang", "Mingjie Bi", "Yipeng Kang", "Wanying He", "Fang Sun", "Yizhou Sun", "Ying Nian Wu", "Demetri Terzopoulos", "Fangwei Zhong"], "title": "An LLM-based Agent Simulation Approach to Study Moral Evolution", "comment": null, "summary": "The evolution of morality presents a puzzle: natural selection should favor\nself-interest, yet humans developed moral systems promoting altruism. We\naddress this question by introducing a novel Large Language Model (LLM)-based\nagent simulation framework modeling prehistoric hunter-gatherer societies. This\nplatform is designed to probe diverse questions in social evolution, from\nsurvival advantages to inter-group dynamics. To investigate moral evolution, we\ndesigned agents with varying moral dispositions based on the Expanding Circle\nTheory \\citep{singer1981expanding}. We evaluated their evolutionary success\nacross a series of simulations and analyzed their decision-making in specially\ndesigned moral dilemmas. These experiments reveal how an agent's moral\nframework, in combination with its cognitive constraints, directly shapes its\nbehavior and determines its evolutionary outcome. Crucially, the emergent\npatterns echo seminal theories from related domains of social science,\nproviding external validation for the simulations. This work establishes\nLLM-based simulation as a powerful new paradigm to complement traditional\nresearch in evolutionary biology and anthropology, opening new avenues for\ninvestigating the complexities of moral and social evolution.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u53f2\u524d\u72e9\u730e\u91c7\u96c6\u793e\u4f1a\u4e2d\u7684\u9053\u5fb7\u6f14\u5316\uff0c\u53d1\u73b0\u9053\u5fb7\u89c2\u5ff5\u4e0e\u8ba4\u77e5\u9650\u5236\u5171\u540c\u5f71\u54cd\u884c\u4e3a\u548c\u8fdb\u5316\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u793e\u4f1a\u79d1\u5b66\u53d1\u5c55\u7406\u8bba\uff0c\u5e76\u4e3a\u9053\u5fb7\u4e0e\u793e\u4f1a\u6f14\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "motivation": "\u9053\u5fb7\u6f14\u5316\u5b58\u5728\u6096\u8bba\uff1a\u81ea\u7136\u9009\u62e9\u504f\u597d\u81ea\u6211\u5229\u76ca\uff0c\u4f46\u4eba\u7c7b\u5374\u53d1\u5c55\u51fa\u4fc3\u8fdb\u5229\u4ed6\u4e3b\u4e49\u7684\u9053\u5fb7\u7cfb\u7edf\u3002\u4e3a\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\uff0c\u9700\u8981\u63a2\u7d22\u9053\u5fb7\u5982\u4f55\u5728\u8fdb\u5316\u4e2d\u4ea7\u751f\u5e76\u5e26\u6765\u751f\u5b58\u4f18\u52bf\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6a21\u62df\u5e73\u53f0\uff0c\u6a21\u62df\u53f2\u524d\u72e9\u730e\u91c7\u96c6\u793e\u4f1a\uff1b\u8bbe\u8ba1\u5177\u6709\u4e0d\u540c\u9053\u5fb7\u503e\u5411\uff08\u57fa\u4e8e\u6269\u5c55\u5708\u7406\u8bba\uff09\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u8f6e\u6a21\u62df\u548c\u9053\u5fb7\u56f0\u5883\u4efb\u52a1\u8bc4\u4f30\u5176\u8fdb\u5316\u6210\u529f\u4e0e\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u667a\u80fd\u4f53\u7684\u9053\u5fb7\u6846\u67b6\u4e0e\u5176\u8ba4\u77e5\u9650\u5236\u5171\u540c\u51b3\u5b9a\u5176\u884c\u4e3a\u548c\u8fdb\u5316\u7ed3\u679c\uff1b\u6a21\u62df\u4e2d\u51fa\u73b0\u7684\u6a21\u5f0f\u4e0e\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u7ecf\u5178\u7406\u8bba\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u62df\u53ef\u4f5c\u4e3a\u7814\u7a76\u9053\u5fb7\u4e0e\u793e\u4f1a\u6f14\u5316\u7684\u6709\u529b\u5de5\u5177\uff0c\u8865\u5145\u4f20\u7edf\u8fdb\u5316\u751f\u7269\u5b66\u4e0e\u4eba\u7c7b\u5b66\u7814\u7a76\uff0c\u5f00\u8f9f\u65b0\u7684\u7814\u7a76\u8def\u5f84\u3002"}}
{"id": "2509.16869", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.MM", "eess.IV", "Artificial intelligence, Computer vision, Machine learning, Deep\n  learning", "I.3.3; I.4.5"], "pdf": "https://arxiv.org/pdf/2509.16869", "abs": "https://arxiv.org/abs/2509.16869", "authors": ["Hrishav Bakul Barua", "Kalin Stefanov", "Ganesh Krishnasamy", "KokSheik Wong", "Abhinav Dhall"], "title": "PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction", "comment": "Submitted to IEEE", "summary": "Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a\nfundamental task in many computational vision problems. Numerous data-driven\nmethods have been proposed to address this problem; however, they lack explicit\nmodeling of illumination, lighting, and scene geometry in images. This limits\nthe quality of the reconstructed HDR images. Since lighting and shadows\ninteract differently with different materials, (e.g., specular surfaces such as\nglass and metal, and lambertian or diffuse surfaces such as wood and stone),\nmodeling material-specific properties (e.g., specular and diffuse reflectance)\nhas the potential to improve the quality of HDR image reconstruction. This\npaper presents PhysHDR, a simple yet powerful latent diffusion-based generative\nmodel for HDR image reconstruction. The denoising process is conditioned on\nlighting and depth information and guided by a novel loss to incorporate\nmaterial properties of surfaces in the scene. The experimental results\nestablish the efficacy of PhysHDR in comparison to a number of recent\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u6f5c\u5728\u6269\u6563\u751f\u6210\u6a21\u578bPhysHDR\uff0c\u7528\u4e8e\u4ece\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u91cd\u5efa\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u56fe\u50cf\uff0c\u901a\u8fc7\u7ed3\u5408\u5149\u7167\u3001\u6df1\u5ea6\u548c\u6750\u8d28\u7279\u6027\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u56fe\u50cf\u4e2d\u5149\u7167\u3001\u7167\u660e\u548c\u573a\u666f\u51e0\u4f55\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u9650\u5236\u4e86HDR\u56fe\u50cf\u91cd\u5efa\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u4e0d\u540c\u6750\u8d28\u8868\u9762\u5bf9\u5149\u548c\u9634\u5f71\u7684\u4e0d\u540c\u54cd\u5e94\u672a\u88ab\u5145\u5206\u8003\u8651\u3002", "method": "\u63d0\u51faPhysHDR\u6a21\u578b\uff0c\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u673a\u5236\uff0c\u5c06\u53bb\u566a\u8fc7\u7a0b\u4e0e\u5149\u7167\u548c\u6df1\u5ea6\u4fe1\u606f\u6761\u4ef6\u5316\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\u4ee5\u878d\u5408\u573a\u666f\u4e2d\u8868\u9762\u7684\u6750\u8d28\u7279\u6027\uff08\u5982\u955c\u9762\u53cd\u5c04\u548c\u6f2b\u53cd\u5c04\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPhysHDR\u5728\u591a\u4e2a\u6700\u65b0\u5148\u8fdb\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684HDR\u56fe\u50cf\u91cd\u5efa\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5149\u7167\u3001\u6df1\u5ea6\u548c\u6750\u8d28\u7279\u6027\uff0cPhysHDR\u663e\u8457\u63d0\u5347\u4e86LDR\u5230HDR\u56fe\u50cf\u8f6c\u6362\u7684\u8d28\u91cf\uff0c\u4e3a\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\u7684\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16346", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16346", "abs": "https://arxiv.org/abs/2509.16346", "authors": ["Juan Castorena", "E. Louise Loudermilk", "Scott Pokswinski", "Rodman Linn"], "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR", "comment": null, "summary": "The 3D structure of living and non-living components in ecosystems plays a\ncritical role in determining ecological processes and feedbacks from both\nnatural and human-driven disturbances. Anticipating the effects of wildfire,\ndrought, disease, or atmospheric deposition depends on accurate\ncharacterization of 3D vegetation structure, yet widespread measurement remains\nprohibitively expensive and often infeasible. We introduce ForestGen3D, a novel\ngenerative modeling framework that synthesizes high-fidelity 3D forest\nstructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on\nconditional denoising diffusion probabilistic models (DDPMs) trained on\nco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate\nTLS-like 3D point clouds conditioned on sparse ALS observations, effectively\nreconstructing occluded sub-canopy detail at scale. To ensure ecological\nplausibility, we introduce a geometric containment prior based on the convex\nhull of ALS observations and provide theoretical and empirical guarantees that\ngenerated structures remain spatially consistent. We evaluate ForestGen3D at\ntree, plot, and landscape scales using real-world data from mixed conifer\necosystems, and show that it produces high-fidelity reconstructions that\nclosely match TLS references in terms of geometric similarity and biophysical\nmetrics, such as tree height, DBH, crown diameter and crown volume.\nAdditionally, we demonstrate that the containment property can serve as a\npractical proxy for generation quality in settings where TLS ground truth is\nunavailable. Our results position ForestGen3D as a scalable tool for ecological\nmodeling, wildfire simulation, and structural fuel characterization in ALS-only\nenvironments.", "AI": {"tldr": "ForestGen3D \u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u673a\u8f7d\u6fc0\u5149\u96f7\u8fbe\uff08ALS\uff09\u6570\u636e\u5408\u6210\u9ad8\u4fdd\u771f\u7684\u4e09\u7ef4\u68ee\u6797\u7ed3\u6784\uff0c\u80fd\u6709\u6548\u91cd\u5efa\u906e\u6321\u4e0b\u7684\u6797\u4e0b\u7ec6\u8282\uff0c\u5e76\u5728\u7f3a\u4e4f\u5730\u9762\u5b9e\u6d4b\u6570\u636e\u65f6\u63d0\u4f9b\u751f\u6001\u5408\u7406\u7684\u7ed3\u6784\u9884\u6d4b\u3002", "motivation": "\u51c6\u786e\u8868\u5f81\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4e09\u7ef4\u690d\u88ab\u7ed3\u6784\u5bf9\u4e8e\u9884\u6d4b\u706b\u707e\u3001\u5e72\u65f1\u3001\u75be\u75c5\u7b49\u5e72\u6270\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6d4b\u91cf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5927\u8303\u56f4\u5e94\u7528\u3002", "method": "\u63d0\u51fa ForestGen3D \u6846\u67b6\uff0c\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\uff0c\u5229\u7528\u5171\u914d\u51c6\u7684 ALS/TLS \u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u51f8\u5305\u7684\u51e0\u4f55\u5305\u542b\u5148\u9a8c\u786e\u4fdd\u751f\u6210\u7ed3\u6784\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u4e0e\u751f\u6001\u5408\u7406\u6027\u3002", "result": "\u5728\u771f\u5b9e\u6df7\u5408\u9488\u53f6\u6797\u751f\u6001\u7cfb\u7edf\u4e2d\uff0cForestGen3D \u5728\u6811\u4f53\u3001\u6837\u5730\u548c\u666f\u89c2\u5c3a\u5ea6\u4e0a\u5747\u80fd\u751f\u6210\u4e0eTLS\u53c2\u8003\u6570\u636e\u9ad8\u5ea6\u76f8\u4f3c\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u5728\u6811\u9ad8\u3001\u80f8\u5f84\u3001\u51a0\u5e45\u548c\u51a0\u5c42\u4f53\u79ef\u7b49\u751f\u7269\u7269\u7406\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5305\u542b\u5148\u9a8c\u53ef\u4f5c\u4e3a\u65e0TLS\u60c5\u51b5\u4e0b\u7684\u8d28\u91cf\u4ee3\u7406\u6307\u6807\u3002", "conclusion": "ForestGen3D \u4e3a\u4ec5\u6709\u673a\u8f7d\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u7684\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u751f\u6001\u5efa\u6a21\u3001\u91ce\u706b\u6a21\u62df\u548c\u71c3\u6599\u7ed3\u6784\u8868\u5f81\u5de5\u5177\u3002"}}
{"id": "2509.16398", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16398", "abs": "https://arxiv.org/abs/2509.16398", "authors": ["Francesco Argenziano", "Miguel Saavedra-Ruiz", "Sacha Morin", "Daniele Nardi", "Liam Paull"], "title": "Dynamic Objects Relocalization in Changing Environments with Flow Matching", "comment": null, "summary": "Task and motion planning are long-standing challenges in robotics, especially\nwhen robots have to deal with dynamic environments exhibiting long-term\ndynamics, such as households or warehouses. In these environments, long-term\ndynamics mostly stem from human activities, since previously detected objects\ncan be moved or removed from the scene. This adds the necessity to find such\nobjects again before completing the designed task, increasing the risk of\nfailure due to missed relocalizations. However, in these settings, the nature\nof such human-object interactions is often overlooked, despite being governed\nby common habits and repetitive patterns. Our conjecture is that these cues can\nbe exploited to recover the most likely objects' positions in the scene,\nhelping to address the problem of unknown relocalization in changing\nenvironments. To this end we propose FlowMaps, a model based on Flow Matching\nthat is able to infer multimodal object locations over space and time. Our\nresults present statistical evidence to support our hypotheses, opening the way\nto more complex applications of our approach. The code is publically available\nat https://github.com/Fra-Tsuna/flowmaps", "AI": {"tldr": "\u63d0\u51faFlowMaps\u6a21\u578b\uff0c\u5229\u7528\u4eba\u7c7b\u6d3b\u52a8\u7684\u91cd\u590d\u6a21\u5f0f\u9884\u6d4b\u52a8\u6001\u73af\u5883\u4e2d\u7269\u4f53\u7684\u4f4d\u7f6e\uff0c\u4ee5\u89e3\u51b3\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u672a\u77e5\u91cd\u5b9a\u4f4d\u95ee\u9898\u3002", "motivation": "\u5728\u5177\u6709\u957f\u671f\u52a8\u6001\u7684\u73af\u5883\u4e2d\uff08\u5982\u5bb6\u5ead\u6216\u4ed3\u5e93\uff09\uff0c\u673a\u5668\u4eba\u5e38\u56e0\u4eba\u7c7b\u6d3b\u52a8\u5bfc\u81f4\u7269\u4f53\u4f4d\u7f6e\u53d8\u5316\u800c\u96be\u4ee5\u5b8c\u6210\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u4e2d\u7684\u4e60\u60ef\u6027\u6a21\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u8fd9\u4e9b\u7ebf\u7d22\u6765\u63d0\u5347\u7269\u4f53\u91cd\u5b9a\u4f4d\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eFlow Matching\u6784\u5efaFlowMaps\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u7684\u591a\u6a21\u6001\u7269\u4f53\u4f4d\u7f6e\u5206\u5e03\uff0c\u63a8\u65ad\u7269\u4f53\u6700\u53ef\u80fd\u7684\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63d0\u4f9b\u4e86\u7edf\u8ba1\u8bc1\u636e\u652f\u6301\u6240\u63d0\u51fa\u7684\u5047\u8bbe\uff0c\u8868\u660e\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u6355\u6349\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u6a21\u5f0f\u5e76\u7528\u4e8e\u7269\u4f53\u4f4d\u7f6e\u9884\u6d4b\u3002", "conclusion": "FlowMaps\u4e3a\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2509.16256", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16256", "abs": "https://arxiv.org/abs/2509.16256", "authors": ["Asiya Ibrahim Zanga", "Salisu Mamman Abdulrahman", "Abubakar Ado", "Abdulkadir Abubakar Bichi", "Lukman Aliyu Jibril", "Abdulmajid Babangida Umar", "Alhassan Adamu", "Shamsuddeen Hassan Muhammad", "Bashir Salisu Abubakar"], "title": "HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language", "comment": "Masters Thesis, a Dataset Paper", "summary": "The development of Natural Language Processing (NLP) tools for low-resource\nlanguages is critically hindered by the scarcity of annotated datasets. This\npaper addresses this fundamental challenge by introducing HausaMovieReview, a\nnovel benchmark dataset comprising 5,000 YouTube comments in Hausa and\ncode-switched English. The dataset was meticulously annotated by three\nindependent annotators, demonstrating a robust agreement with a Fleiss' Kappa\nscore of 0.85 between annotators. We used this dataset to conduct a comparative\nanalysis of classical models (Logistic Regression, Decision Tree, K-Nearest\nNeighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results\nreveal a key finding: the Decision Tree classifier, with an accuracy and\nF1-score 89.72% and 89.60% respectively, significantly outperformed the deep\nlearning models. Our findings also provide a robust baseline, demonstrating\nthat effective feature engineering can enable classical models to achieve\nstate-of-the-art performance in low-resource contexts, thereby laying a solid\nfoundation for future research.\n  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8c6a\u8428\u8bed\u60c5\u611f\u5206\u6790\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6HausaMovieReview\uff0c\u5305\u542b5000\u6761\u8c6a\u8428\u8bed\u548c\u82f1\u8c6a\u6df7\u6742\u7684YouTube\u8bc4\u8bba\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u7ecf\u5178\u6a21\u578b\u4e0e\u5fae\u8c03\u540e\u7684Transformer\u6a21\u578b\uff0c\u53d1\u73b0\u51b3\u7b56\u6811\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53d1\u5c55\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5000\u6761\u8c6a\u8428\u8bed\u548c\u4ee3\u7801\u6df7\u5408\u82f1\u8bedYouTube\u8bc4\u8bba\u7684\u6570\u636e\u96c6\uff0c\u7531\u4e09\u540d\u72ec\u7acb\u6807\u6ce8\u8005\u6807\u6ce8\uff08Fleiss' Kappa=0.85\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u51b3\u7b56\u6811\u3001KNN\u7b49\u7ecf\u5178\u6a21\u578b\u4e0e\u5fae\u8c03\u7684BERT\u548cRoBERTa\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u51b3\u7b56\u6811\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f18\uff0c\u51c6\u786e\u7387\u8fbe89.72%\uff0cF1\u5206\u6570\u4e3a89.60%\uff0c\u663e\u8457\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u6709\u6548\u7684\u7279\u5f81\u5de5\u7a0b\uff0c\u7ecf\u5178\u6a21\u578b\u53ef\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\u3002"}}
{"id": "2509.16273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16273", "abs": "https://arxiv.org/abs/2509.16273", "authors": ["Jungseob Yi", "Seoyoung Choi", "Sun Kim", "Sangseon Lee"], "title": "SubDyve: Subgraph-Driven Dynamic Propagation for Virtual Screening Enhancement Controlling False Positive", "comment": "33 pages, 12 figures", "summary": "Virtual screening (VS) aims to identify bioactive compounds from vast\nchemical libraries, but remains difficult in low-label regimes where only a few\nactives are known. Existing methods largely rely on general-purpose molecular\nfingerprints and overlook class-discriminative substructures critical to\nbioactivity. Moreover, they consider molecules independently, limiting\neffectiveness in low-label regimes. We introduce SubDyve, a network-based VS\nframework that constructs a subgraph-aware similarity network and propagates\nactivity signals from a small known actives. When few active compounds are\navailable, SubDyve performs iterative seed refinement, incrementally promoting\nnew candidates based on local false discovery rate. This strategy expands the\nseed set with promising candidates while controlling false positives from\ntopological bias and overexpansion. We evaluate SubDyve on ten DUD-E targets\nunder zero-shot conditions and on the CDK7 target with a 10-million-compound\nZINC dataset. SubDyve consistently outperforms existing fingerprint or\nembedding-based approaches, achieving margins of up to +34.0 on the BEDROC and\n+24.6 on the EF1% metric.", "AI": {"tldr": "SubDyve\u662f\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u7684\u865a\u62df\u7b5b\u9009\u6846\u67b6\uff0c\u5229\u7528\u5b50\u7ed3\u6784\u611f\u77e5\u7684\u76f8\u4f3c\u6027\u7f51\u7edc\u548c\u6d3b\u6027\u4fe1\u53f7\u4f20\u64ad\uff0c\u5728\u4f4e\u6807\u7b7e\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u7b5b\u9009\u6027\u80fd\u3002", "motivation": "\u5728\u4ec5\u6709\u5c11\u91cf\u5df2\u77e5\u6d3b\u6027\u5316\u5408\u7269\u7684\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u865a\u62df\u7b5b\u9009\u65b9\u6cd5\u56e0\u4f9d\u8d56\u901a\u7528\u5206\u5b50\u6307\u7eb9\u4e14\u5ffd\u7565\u5173\u952e\u7684\u7c7b\u522b\u5224\u522b\u6027\u5b50\u7ed3\u6784\u800c\u8868\u73b0\u53d7\u9650\u3002", "method": "\u6784\u5efa\u5b50\u56fe\u611f\u77e5\u7684\u76f8\u4f3c\u6027\u7f51\u7edc\uff0c\u901a\u8fc7\u8fed\u4ee3\u79cd\u5b50\u4f18\u5316\u548c\u5c40\u90e8\u5047\u53d1\u73b0\u7387\u63a7\u5236\uff0c\u4ece\u5c0f\u91cf\u5df2\u77e5\u6d3b\u6027\u5206\u5b50\u51fa\u53d1\u4f20\u64ad\u6d3b\u6027\u4fe1\u53f7\u3002", "result": "\u5728DUD-E\u7684\u5341\u4e2a\u9776\u6807\u548cCDK7\u7684\u5343\u4e07\u7ea7ZINC\u6570\u636e\u96c6\u4e0a\uff0cSubDyve\u5728BEDROC\u548cEF1%\u6307\u6807\u4e0a\u5206\u522b\u6700\u9ad8\u63d0\u5347+34.0\u548c+24.6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SubDyve\u5728\u4f4e\u6807\u7b7e\u865a\u62df\u7b5b\u9009\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u6548\u8bc6\u522b\u751f\u7269\u6d3b\u6027\u5316\u5408\u7269\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16299", "abs": "https://arxiv.org/abs/2509.16299", "authors": ["Raquel Fernandez-Peralta", "Andrea Mesiarov\u00e1-Zem\u00e1nkov\u00e1"], "title": "On the Non-Uniqueness of Representation of $(U,N)$-Implications", "comment": null, "summary": "Fuzzy implication functions constitute fundamental operators in fuzzy logic\nsystems, extending classical conditionals to manage uncertainty in logical\ninference. Among the extensive families of these operators, generalizations of\nthe classical material implication have received considerable theoretical\nattention, particularly $(S,N)$-implications constructed from t-conorms and\nfuzzy negations, and their further generalizations to $(U,N)$-implications\nusing disjunctive uninorms. Prior work has established characterization\ntheorems for these families under the assumption that the fuzzy negation $N$ is\ncontinuous, ensuring uniqueness of representation. In this paper, we disprove\nthis last fact for $(U,N)$-implications and we show that they do not\nnecessarily possess a unique representation, even if the fuzzy negation is\ncontinuous. Further, we provide a comprehensive study of uniqueness conditions\nfor both uninorms with continuous and non-continuous underlying functions. Our\nresults offer important theoretical insights into the structural properties of\nthese operators.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8fde\u7eed\u548c\u975e\u8fde\u7eed\u5e95\u5c42\u51fd\u6570\u7684\u6790\u53d6\u6027uninorms\u4e0e\u6a21\u7cca\u5426\u5b9a\u6784\u6210\u7684(U,N)-\u8574\u6db5\u7684\u552f\u4e00\u8868\u793a\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u5373\u4f7f\u6a21\u7cca\u5426\u5b9a\u662f\u8fde\u7eed\u7684\uff0c(U,N)-\u8574\u6db5\u4e5f\u4e0d\u4e00\u5b9a\u5177\u6709\u552f\u4e00\u8868\u793a\uff0c\u5e76\u7ed9\u51fa\u4e86\u4fdd\u8bc1\u552f\u4e00\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u5047\u8bbe\u6a21\u7cca\u5426\u5b9aN\u662f\u8fde\u7eed\u7684\uff0c\u4ece\u800c\u4fdd\u8bc1(S,N)\u548c(U,N)\u8574\u6db5\u7684\u8868\u793a\u552f\u4e00\u6027\uff0c\u4f46\u672c\u6587\u6307\u51fa\u5728(U,N)-\u8574\u6db5\u60c5\u5f62\u4e0b\u8be5\u7ed3\u8bba\u4e0d\u6210\u7acb\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u5176\u8868\u793a\u7684\u552f\u4e00\u6027\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u6784\u9020\u53cd\u4f8b\u5426\u5b9a(U,N)-\u8574\u6db5\u5728\u8fde\u7eed\u6a21\u7cca\u5426\u5b9a\u4e0b\u5177\u6709\u552f\u4e00\u8868\u793a\u7684\u731c\u60f3\uff0c\u5e76\u7cfb\u7edf\u5206\u6790uninorms\u5728\u8fde\u7eed\u4e0e\u975e\u8fde\u7eed\u60c5\u51b5\u4e0b\u7684\u7ed3\u6784\u7279\u6027\uff0c\u63d0\u51fa\u786e\u4fdd\u8868\u793a\u552f\u4e00\u6027\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u5373\u4f7f\u6a21\u7cca\u5426\u5b9aN\u662f\u8fde\u7eed\u7684\uff0c(U,N)-\u8574\u6db5\u4e5f\u53ef\u80fd\u5b58\u5728\u591a\u79cd\u4e0d\u540c\u7684\u8868\u793a\u65b9\u5f0f\uff1b\u5e76\u5efa\u7acb\u4e86\u5173\u4e8euninorms\u548c\u6a21\u7cca\u5426\u5b9a\u7ec4\u5408\u4e0b\u8868\u793a\u552f\u4e00\u6027\u7684\u5b8c\u6574\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8fde\u7eed\u6a21\u7cca\u5426\u5b9a\u4e0d\u80fd\u4fdd\u8bc1(U,N)-\u8574\u6db5\u7684\u8868\u793a\u552f\u4e00\u6027\uff0c\u5fc5\u987b\u8fdb\u4e00\u6b65\u9650\u5236uninorm\u7684\u7ed3\u6784\u4ee5\u786e\u4fdd\u552f\u4e00\u6027\uff0c\u8fd9\u5bf9\u6a21\u7cca\u903b\u8f91\u4e2d\u8574\u6db5\u7b97\u5b50\u7684\u7406\u8bba\u6784\u5efa\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2509.16780", "categories": ["cs.IR", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16780", "abs": "https://arxiv.org/abs/2509.16780", "authors": ["Eason Chen", "Chuangji Li", "Shizhuo Li", "Conrad Borchers", "Zimo Xiao", "Chloe Qianhui Zhao", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook", "comment": null, "summary": "Technology-enhanced learning environments often help students retrieve\nrelevant learning content for questions arising during self-paced study. Large\nlanguage models (LLMs) have emerged as novel aids for information retrieval\nduring learning. While LLMs are effective for general-purpose\nquestion-answering, they typically lack alignment with the domain knowledge of\nspecific course materials such as textbooks and slides. We investigate\nRetrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced\nRAG approach, for page-level question answering in an undergraduate mathematics\ntextbook. While RAG has been effective for retrieving discrete, contextually\nrelevant passages, GraphRAG may excel in modeling interconnected concepts and\nhierarchical knowledge structures. We curate a dataset of 477 question-answer\npairs, each tied to a distinct textbook page. We then compare the standard\nembedding-based RAG methods to GraphRAG for evaluating both retrieval\naccuracy-whether the correct page is retrieved-and generated answer quality via\nF1 scores. Our findings show that embedding-based RAG achieves higher retrieval\naccuracy and better F1 scores compared to GraphRAG, which tends to retrieve\nexcessive and sometimes irrelevant content due to its entity-based structure.\nWe also explored re-ranking the retrieved pages with LLM and observed mixed\nresults, including performance drop and hallucinations when dealing with larger\ncontext windows. Overall, this study highlights both the promises and\nchallenges of page-level retrieval systems in educational contexts, emphasizing\nthe need for more refined retrieval methods to build reliable AI tutoring\nsolutions in providing reference page numbers.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684GraphRAG\u5728\u672c\u79d1\u6570\u5b66\u6559\u6750\u9875\u9762\u7ea7\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u57fa\u4e8e\u5d4c\u5165\u7684RAG\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u8d28\u91cf\u4e0a\u4f18\u4e8eGraphRAG\uff0c\u540e\u8005\u56e0\u5b9e\u4f53\u7ed3\u6784\u6613\u68c0\u7d22\u8fc7\u591a\u65e0\u5173\u5185\u5bb9\uff1b\u91cd\u65b0\u6392\u5e8f\u5c1d\u8bd5\u6548\u679c\u6709\u9650\u4e14\u53ef\u80fd\u5f15\u53d1\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u573a\u666f\u4e2d\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u8bfe\u7a0b\u6750\u6599\u7684\u77e5\u8bc6\u5bf9\u9f50\uff0c\u9700\u63d0\u5347\u5176\u5728\u6559\u6750\u9875\u9762\u7ea7\u95ee\u9898\u56de\u7b54\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b477\u4e2a\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u6807\u51c6RAG\u4e0eGraphRAG\u5728\u9875\u9762\u68c0\u7d22\u51c6\u786e\u7387\u548c\u751f\u6210\u7b54\u6848F1\u5206\u6570\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u4f7f\u7528LLM\u5bf9\u68c0\u7d22\u7ed3\u679c\u8fdb\u884c\u91cd\u6392\u5e8f\u7684\u5f71\u54cd\u3002", "result": "\u57fa\u4e8e\u5d4c\u5165\u7684RAG\u5728\u68c0\u7d22\u51c6\u786e\u7387\u548cF1\u5f97\u5206\u4e0a\u5747\u4f18\u4e8eGraphRAG\uff1bGraphRAG\u503e\u5411\u4e8e\u68c0\u7d22\u8fc7\u591a\u751a\u81f3\u65e0\u5173\u5185\u5bb9\uff1bLLM\u91cd\u6392\u5e8f\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "\u5c3d\u7ba1RAG\u5728\u6559\u80b2\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46GraphRAG\u5728\u5904\u7406\u5c42\u7ea7\u5316\u77e5\u8bc6\u65f6\u4ecd\u6709\u6f5c\u529b\u5f85\u6316\u6398\uff1b\u9700\u5f00\u53d1\u66f4\u7cbe\u7ec6\u7684\u68c0\u7d22\u65b9\u6cd5\u4ee5\u6784\u5efa\u53ef\u9760\u7684AI\u6559\u5b66\u8f85\u52a9\u7cfb\u7edf\u3002"}}
{"id": "2509.18088", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18088", "abs": "https://arxiv.org/abs/2509.18088", "authors": ["Chuhao Qin", "Evangelos Pournaras"], "title": "Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Decentralized combinatorial optimization in evolving multi-agent systems\nposes significant challenges, requiring agents to balance long-term\ndecision-making, short-term optimized collective outcomes, while preserving\nautonomy of interactive agents under unanticipated changes. Reinforcement\nlearning offers a way to model sequential decision-making through dynamic\nprogramming to anticipate future environmental changes. However, applying\nmulti-agent reinforcement learning (MARL) to decentralized combinatorial\noptimization problems remains an open challenge due to the exponential growth\nof the joint state-action space, high communication overhead, and privacy\nconcerns in centralized training. To address these limitations, this paper\nproposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel\napproach that leverages both MARL and decentralized collective learning based\non a hierarchical framework. Agents take high-level strategies using MARL to\ngroup possible plans for action space reduction and constrain the agent\nbehavior for Pareto optimality. Meanwhile, the low-level collective learning\nlayer ensures efficient and decentralized coordinated decisions among agents\nwith minimal communication. Extensive experiments in a synthetic scenario and\nreal-world smart city application models, including energy self-management and\ndrone swarm sensing, demonstrate that HRCL significantly improves performance,\nscalability, and adaptability compared to the standalone MARL and collective\nlearning approaches, achieving a win-win synthesis solution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHRCL\u7684\u5206\u5c42\u5f3a\u5316\u4e0e\u96c6\u4f53\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u5c42MARL\u7b56\u7565\u548c\u5e95\u5c42\u53bb\u4e2d\u5fc3\u5316\u534f\u540c\u5b66\u4e60\uff0c\u5728\u51cf\u5c11\u52a8\u4f5c\u7a7a\u95f4\u3001\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u7ec4\u5408\u4f18\u5316\u5728\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9762\u4e34\u8054\u5408\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u6307\u6570\u589e\u957f\u3001\u901a\u4fe1\u5f00\u9500\u9ad8\u548c\u9690\u79c1\u95ee\u9898\u7b49\u6311\u6218\uff0c\u73b0\u6709MARL\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u957f\u671f\u51b3\u7b56\u3001\u77ed\u671f\u4f18\u5316\u548c\u667a\u80fd\u4f53\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51faHRCL\u6846\u67b6\uff1a\u9ad8\u5c42\u91c7\u7528MARL\u751f\u6210\u7b56\u7565\u4ee5\u7f29\u5c0f\u52a8\u4f5c\u7a7a\u95f4\u5e76\u5f15\u5bfc\u5e15\u7d2f\u6258\u6700\u4f18\uff1b\u5e95\u5c42\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7684\u96c6\u4f53\u5b66\u4e60\u673a\u5236\u5b9e\u73b0\u4f4e\u901a\u4fe1\u5f00\u9500\u4e0b\u7684\u534f\u8c03\u51b3\u7b56\u3002", "result": "\u5728\u5408\u6210\u573a\u666f\u53ca\u667a\u6167\u57ce\u5e02\u5e94\u7528\uff08\u5982\u80fd\u6e90\u81ea\u7ba1\u7406\u3001\u65e0\u4eba\u673a\u7fa4\u611f\u77e5\uff09\u4e2d\uff0cHRCL\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528MARL\u6216\u96c6\u4f53\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "HRCL\u6709\u6548\u878d\u5408\u4e86MARL\u4e0e\u53bb\u4e2d\u5fc3\u5316\u96c6\u4f53\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u901a\u4fe1\u3001\u5f3a\u9002\u5e94\u6027\u7684\u591a\u667a\u80fd\u4f53\u534f\u540c\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16960", "categories": ["cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.16960", "abs": "https://arxiv.org/abs/2509.16960", "authors": ["Ruiyan Wang", "Zhengxue Cheng", "Zonghao Lin", "Jun Ling", "Yuzhou Liu", "Yanru An", "Rong Xie", "Li Song"], "title": "SemanticGarment: Semantic-Controlled Generation and Editing of 3D Gaussian Garments", "comment": null, "summary": "3D digital garment generation and editing play a pivotal role in fashion\ndesign, virtual try-on, and gaming. Traditional methods struggle to meet the\ngrowing demand due to technical complexity and high resource costs.\nLearning-based approaches offer faster, more diverse garment synthesis based on\nspecific requirements and reduce human efforts and time costs. However, they\nstill face challenges such as inconsistent multi-view geometry or textures and\nheavy reliance on detailed garment topology and manual rigging. We propose\nSemanticGarment, a 3D Gaussian-based method that realizes high-fidelity 3D\ngarment generation from text or image prompts and supports semantic-based\ninteractive editing for flexible user customization. To ensure multi-view\nconsistency and garment fitting, we propose to leverage structural human priors\nfor the generative model by introducing a 3D semantic clothing model, which\ninitializes the geometry structure and lays the groundwork for view-consistent\ngarment generation and editing. Without the need to regenerate or rely on\nexisting mesh templates, our approach allows for rapid and diverse\nmodifications to existing Gaussians, either globally or within a local region.\nTo address the artifacts caused by self-occlusion for garment reconstruction\nbased on single image, we develop a self-occlusion optimization strategy to\nmitigate holes and artifacts that arise when directly animating self-occluded\ngarments. Extensive experiments are conducted to demonstrate our superior\nperformance in 3D garment generation and editing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u7684\u8bed\u4e49\u670d\u88c5\u751f\u6210\u65b9\u6cd5SemanticGarment\uff0c\u80fd\u591f\u4ece\u6587\u672c\u6216\u56fe\u50cf\u63d0\u793a\u4e2d\u751f\u6210\u9ad8\u4fdd\u771f\u76843D\u670d\u88c5\uff0c\u5e76\u652f\u6301\u8bed\u4e49\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u7f16\u8f91\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u4eba\u4f53\u5148\u9a8c\u548c\u81ea\u906e\u6321\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0e\u9ad8\u8d28\u91cf\u7684\u670d\u88c5\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u57283D\u670d\u88c5\u751f\u6210\u4e2d\u9762\u4e34\u6280\u672f\u590d\u6742\u6027\u548c\u9ad8\u8d44\u6e90\u6d88\u8017\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u591a\u89c6\u89d2\u51e0\u4f55\u6216\u7eb9\u7406\u4e0d\u4e00\u81f4\u3001\u4f9d\u8d56\u624b\u52a8\u7ed1\u5b9a\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u4e0e\u7f16\u8f91\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSemanticGarment\uff0c\u57fa\u4e8e3D\u9ad8\u65af\u8868\u793a\uff0c\u5f15\u51653D\u8bed\u4e49\u670d\u88c5\u6a21\u578b\u4ee5\u5229\u7528\u4eba\u4f53\u7ed3\u6784\u5148\u9a8c\uff0c\u5b9e\u73b0\u89c6\u56fe\u4e00\u81f4\u7684\u751f\u6210\u4e0e\u7f16\u8f91\uff1b\u901a\u8fc7\u4fee\u6539\u9ad8\u65af\u5206\u5e03\u8fdb\u884c\u5feb\u901f\u5168\u5c40\u6216\u5c40\u90e8\u7f16\u8f91\uff0c\u5e76\u8bbe\u8ba1\u81ea\u906e\u6321\u4f18\u5316\u7b56\u7565\u7f13\u89e3\u5355\u56fe\u91cd\u5efa\u4e2d\u7684\u7a7a\u6d1e\u548c\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u57283D\u670d\u88c5\u751f\u6210\u4e0e\u7f16\u8f91\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u5907\u9ad8\u4fdd\u771f\u5ea6\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u826f\u597d\u7684\u53ef\u7f16\u8f91\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u751f\u6210\u6216\u4f9d\u8d56\u7f51\u683c\u6a21\u677f\u3002", "conclusion": "SemanticGarment\u4e3a3D\u670d\u88c5\u751f\u6210\u4e0e\u4ea4\u4e92\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u865a\u62df\u8bd5\u7a7f\u3001\u6e38\u620f\u548c\u65f6\u5c1a\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.16363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16363", "abs": "https://arxiv.org/abs/2509.16363", "authors": ["Hrishikesh Sharma"], "title": "Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution", "comment": null, "summary": "The problem of image data generation in computer vision has traditionally\nbeen a harder problem to solve, than discriminative problems. Such data\ngeneration entails placing relevant objects of appropriate sizes each, at\nmeaningful location in a scene canvas. There have been two classes of popular\napproaches to such generation: graphics based, and generative models-based.\nOptimization problems are known to lurk in the background for both these\nclasses of approaches. In this paper, we introduce a novel, practically useful\nmanifestation of the classical Bin Packing problem in the context of generation\nof synthetic image data. We conjecture that the newly introduced problem,\nResizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide\ndetailed arguments about our conjecture. As a first solution, we present a\nnovel heuristic algorithm that is generic enough and therefore scales and packs\narbitrary number of arbitrary-shaped regions at arbitrary locations, into an\nimage canvas. The algorithm follows greedy approach to iteratively pack region\npairs in a careful way, while obeying the optimization constraints. The\nalgorithm is validated by an implementation that was used to generate a\nlarge-scale synthetic anomaly detection dataset, with highly varying degree of\nbin packing parameters per image sample i.e. RARP instance. Visual inspection\nof such data and checking of the correctness of each solution proves the\neffectiveness of our algorithm. With generative modeling being on rise in deep\nlearning, and synthetic data generation poised to become mainstream, we expect\nthat the newly introduced problem will be valued in the imaging scientific\ncommunity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u4f38\u7f29\u951a\u5b9a\u533a\u57df\u6253\u5305\uff08RARP\uff09\u95ee\u9898\uff0c\u7528\u4e8e\u5408\u6210\u56fe\u50cf\u6570\u636e\u751f\u6210\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d2a\u5fc3\u7b56\u7565\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u6709\u6548\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6bd4\u5224\u522b\u4efb\u52a1\u66f4\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u653e\u7f6e\u5408\u9002\u5927\u5c0f\u548c\u4f4d\u7f6e\u7684\u5bf9\u8c61\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u96be\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5efa\u6a21\u65b9\u5f0f\u6765\u63d0\u5347\u5408\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "method": "\u5f15\u5165\u4e86Resizable Anchored Region Packing (RARP) \u95ee\u9898\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u4e00\u7c7b\u65b0\u578b\u7684\u88c5\u7bb1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u8d2a\u5fc3\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u9010\u6b65\u5728\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u4e0b\u6253\u5305\u4efb\u610f\u5f62\u72b6\u3001\u6570\u91cf\u548c\u4f4d\u7f6e\u7684\u533a\u57df\u3002", "result": "\u7b97\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u5408\u6210\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u751f\u6210\uff0c\u6bcf\u4e2a\u56fe\u50cf\u6837\u672c\u5177\u6709\u9ad8\u5ea6\u53d8\u5316\u7684RARP\u53c2\u6570\uff1b\u901a\u8fc7\u89c6\u89c9\u68c0\u67e5\u548c\u89e3\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "RARP\u95ee\u9898\u88ab\u8ba4\u4e3a\u662fNP\u96be\u7684\uff0c\u6240\u63d0\u7b97\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u590d\u6742\u573a\u666f\u4e0b\u7684\u5408\u6210\u56fe\u50cf\uff0c\u6709\u671b\u5728\u56fe\u50cf\u79d1\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u9886\u57df\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.16412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16412", "abs": "https://arxiv.org/abs/2509.16412", "authors": ["Zihao Deng", "Peng Gao", "Williard Joshua Jose", "Maggie Wigness", "John Rogers", "Brian Reily", "Christopher Reardon", "Hao Zhang"], "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation", "comment": null, "summary": "Coordinated multi-robot navigation is essential for robots to operate as a\nteam in diverse environments. During navigation, robot teams usually need to\nmaintain specific formations, such as circular formations to protect human\nteammates at the center. However, in complex scenarios such as narrow\ncorridors, rigidly preserving predefined formations can become infeasible.\nTherefore, robot teams must be capable of dynamically splitting into smaller\nsubteams and adaptively controlling the subteams to navigate through such\nscenarios while preserving formations. To enable this capability, we introduce\na novel method for SubTeaming and Adaptive Formation (STAF), which is built\nupon a unified hierarchical learning framework: (1) high-level deep graph cut\nfor team splitting, (2) intermediate-level graph learning for facilitating\ncoordinated navigation among subteams, and (3) low-level policy learning for\ncontrolling individual mobile robots to reach their goal positions while\navoiding collisions. To evaluate STAF, we conducted extensive experiments in\nboth indoor and outdoor environments using robotics simulations and physical\nrobot teams. Experimental results show that STAF enables the novel capability\nfor subteaming and adaptive formation control, and achieves promising\nperformance in coordinated multi-robot navigation through challenging\nscenarios. More details are available on the project website:\nhttps://hcrlab.gitlab.io/project/STAF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTAF\u7684\u5206\u961f\u4e0e\u81ea\u9002\u5e94\u7f16\u961f\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u540c\u5bfc\u822a\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u52a8\u6001\u5206\u961f\u5e76\u4fdd\u6301\u7f16\u961f\u63a7\u5236\u3002", "motivation": "\u5728\u72ed\u7a84\u6216\u590d\u6742\u73af\u5883\u4e2d\uff0c\u56fa\u5b9a\u7f16\u961f\u96be\u4ee5\u7ef4\u6301\uff0c\u9700\u8981\u673a\u5668\u4eba\u56e2\u961f\u80fd\u591f\u52a8\u6001\u62c6\u5206\u5e76\u81ea\u9002\u5e94\u63a7\u5236\u5b50\u56e2\u961f\u4ee5\u5b8c\u6210\u5bfc\u822a\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u7edf\u4e00\u7684\u5206\u5c42\u5b66\u4e60\u6846\u67b6\uff1a\u9ad8\u5c42\u4f7f\u7528\u6df1\u5ea6\u56fe\u5272\u8fdb\u884c\u56e2\u961f\u5206\u5272\uff0c\u4e2d\u5c42\u901a\u8fc7\u56fe\u5b66\u4e60\u5b9e\u73b0\u5b50\u56e2\u961f\u95f4\u7684\u534f\u540c\u5bfc\u822a\uff0c\u5e95\u5c42\u901a\u8fc7\u7b56\u7565\u5b66\u4e60\u63a7\u5236\u5355\u4e2a\u673a\u5668\u4eba\u907f\u969c\u5e76\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u3002", "result": "\u5728\u5ba4\u5185\u5916\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86STAF\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u52a8\u6001\u5206\u961f\u4e0e\u81ea\u9002\u5e94\u7f16\u961f\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u534f\u540c\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "STAF\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a8\u6001\u5206\u961f\u4e0e\u81ea\u9002\u5e94\u7f16\u961f\u63a7\u5236\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u540c\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2509.16264", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16264", "abs": "https://arxiv.org/abs/2509.16264", "authors": ["Wenjie Lin", "Hange Liu", "Xutao Mao", "Yingying Zhuang", "Jingwei Shi", "Xudong Han", "Tianyu Shi", "Jinrui Yang"], "title": "Gender and Political Bias in Large Language Models: A Demonstration Platform", "comment": "online demo: https://euro-parl-vote-demo.vercel.app/; Video:\n  https://www.youtube.com/@Jinrui-sf2jg", "summary": "We present ParlAI Vote, an interactive system for exploring European\nParliament debates and votes, and for testing LLMs on vote prediction and bias\nanalysis. This platform connects debate topics, speeches, and roll-call\noutcomes, and includes rich demographic data such as gender, age, country, and\npolitical group. Users can browse debates, inspect linked speeches, compare\nreal voting outcomes with predictions from frontier LLMs, and view error\nbreakdowns by demographic group. Visualizing the EuroParlVote benchmark and its\ncore tasks of gender classification and vote prediction, ParlAI Vote highlights\nsystematic performance bias in state-of-the-art LLMs. The system unifies data,\nmodels, and visual analytics in a single interface, lowering the barrier for\nreproducing findings, auditing behavior, and running counterfactual scenarios.\nIt supports research, education, and public engagement with legislative\ndecision-making, while making clear both the strengths and the limitations of\ncurrent LLMs in political analysis.", "AI": {"tldr": "ParlAI Vote \u662f\u4e00\u4e2a\u7528\u4e8e\u63a2\u7d22\u6b27\u6d32\u8bae\u4f1a\u8fa9\u8bba\u548c\u6295\u7968\u3001\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6295\u7968\u9884\u6d4b\u548c\u504f\u89c1\u5206\u6790\u4e2d\u8868\u73b0\u7684\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u7edf\u4e00\u6b27\u6d32\u8bae\u4f1a\u8fa9\u8bba\u3001\u6295\u7968\u7ed3\u679c\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\uff0c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u5ba1\u8ba1\u4e14\u652f\u6301\u53cd\u4e8b\u5b9e\u5206\u6790\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u4ee5\u8bc4\u4f30LLM\u5728\u653f\u6cbb\u51b3\u7b56\u5206\u6790\u4e2d\u7684\u80fd\u529b\u4e0e\u5c40\u9650\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u96c6\u6210\u6570\u636e\u3001\u6a21\u578b\u548c\u53ef\u89c6\u5316\u5206\u6790\u7684\u7cfb\u7edf\uff0c\u8fde\u63a5\u8fa9\u8bba\u4e3b\u9898\u3001\u6f14\u8bb2\u5185\u5bb9\u548c\u70b9\u540d\u6295\u7968\u7ed3\u679c\uff0c\u5e76\u7ed3\u5408\u6027\u522b\u3001\u5e74\u9f84\u3001\u56fd\u5bb6\u548c\u653f\u6cbb\u56e2\u4f53\u7b49\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u652f\u6301\u7528\u6237\u6d4f\u89c8\u3001\u6bd4\u8f83\u771f\u5b9e\u6295\u7968\u7ed3\u679c\u4e0eLLM\u9884\u6d4b\u7ed3\u679c\uff0c\u5e76\u6309\u4eba\u53e3\u7fa4\u4f53\u8fdb\u884c\u9519\u8bef\u5206\u6790\u3002", "result": "\u7cfb\u7edf\u63ed\u793a\u4e86\u524d\u6cbfLLM\u5728\u6027\u522b\u5206\u7c7b\u548c\u6295\u7968\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u6027\u80fd\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff1b\u53ef\u89c6\u5316EuroParlVote\u57fa\u51c6\u7684\u6838\u5fc3\u4efb\u52a1\u7a81\u663e\u4e86\u5f53\u524dLLM\u7684\u5c40\u9650\u6027\u3002", "conclusion": "ParlAI Vote \u4e3a\u7814\u7a76\u3001\u6559\u80b2\u548c\u516c\u4f17\u53c2\u4e0e\u7acb\u6cd5\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u540c\u65f6\u8868\u660e\u5f53\u524dLLM\u5728\u653f\u6cbb\u5206\u6790\u4e2d\u867d\u6709\u6f5c\u529b\u4f46\u4ecd\u6709\u663e\u8457\u504f\u89c1\uff0c\u9700\u8c28\u614e\u5e94\u7528\u3002"}}
{"id": "2509.16277", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16277", "abs": "https://arxiv.org/abs/2509.16277", "authors": ["Haobo Yang", "Shiyan Zhang", "Zhuoyi Yang", "Jilong Guo", "Jun Yang", "Xinyu Zhang"], "title": "Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception", "comment": null, "summary": "Deep perception networks in autonomous driving traditionally rely on\ndata-intensive training regimes and post-hoc anomaly detection, often\ndisregarding fundamental information-theoretic constraints governing stable\ninformation processing. We reconceptualize deep neural encoders as hierarchical\ncommunication chains that incrementally compress raw sensory inputs into\ntask-relevant latent features. Within this framework, we establish two\ntheoretically justified design principles for robust perception: (D1) smooth\nvariation of mutual information between consecutive layers, and (D2) monotonic\ndecay of latent entropy with network depth. Our analysis shows that, under\nrealistic architectural assumptions, particularly blocks comprising repeated\nlayers of similar capacity, enforcing smooth information flow (D1) naturally\nencourages entropy decay (D2), thus ensuring stable compression. Guided by\nthese insights, we propose Eloss, a novel entropy-based regularizer designed as\na lightweight, plug-and-play training objective. Rather than marginal accuracy\nimprovements, this approach represents a conceptual shift: it unifies\ninformation-theoretic stability with standard perception tasks, enabling\nexplicit, principled detection of anomalous sensor inputs through entropy\ndeviations. Experimental validation on large-scale 3D object detection\nbenchmarks (KITTI and nuScenes) demonstrates that incorporating Eloss\nconsistently achieves competitive or improved accuracy while dramatically\nenhancing sensitivity to anomalies, amplifying distribution-shift signals by up\nto two orders of magnitude. This stable information-compression perspective not\nonly improves interpretability but also establishes a solid theoretical\nfoundation for safer, more robust autonomous driving perception systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6df1\u5ea6\u611f\u77e5\u7f51\u7edc\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u5f15\u5165Eloss\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7f51\u7edc\u4f9d\u8d56\u6570\u636e\u5bc6\u96c6\u578b\u8bad\u7ec3\u548c\u4e8b\u540e\u5f02\u5e38\u68c0\u6d4b\uff0c\u5ffd\u89c6\u4e86\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u57fa\u672c\u4fe1\u606f\u8bba\u7ea6\u675f\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u65e8\u5728\u4ece\u4fe1\u606f\u538b\u7f29\u7a33\u5b9a\u6027\u89d2\u5ea6\u91cd\u65b0\u6784\u5efa\u611f\u77e5\u7f51\u7edc\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "method": "\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f16\u7801\u5668\u89c6\u4e3a\u5206\u5c42\u901a\u4fe1\u94fe\uff0c\u63d0\u51fa\u4e24\u4e2a\u7406\u8bba\u652f\u6301\u7684\u8bbe\u8ba1\u539f\u5219\uff1a(D1) \u8fde\u7eed\u5c42\u95f4\u4e92\u4fe1\u606f\u5e73\u6ed1\u53d8\u5316\uff1b(D2) \u6f5c\u5728\u7a7a\u95f4\u71b5\u968f\u7f51\u7edc\u6df1\u5ea6\u5355\u8c03\u8870\u51cf\u3002\u57fa\u4e8e\u6b64\uff0c\u8bbe\u8ba1\u4e86Eloss\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u9879\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5728KITTI\u548cnuScenes\u7b49\u5927\u89c4\u6a213D\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u8868\u660e\uff0cEloss\u5728\u4fdd\u6301\u6216\u63d0\u5347\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u589e\u5f3a\u5bf9\u5f02\u5e38\u8f93\u5165\u7684\u654f\u611f\u6027\uff0c\u5206\u5e03\u504f\u79fb\u4fe1\u53f7\u68c0\u6d4b\u80fd\u529b\u63d0\u9ad8\u8fbe\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u4fe1\u606f\u538b\u7f29\u7a33\u5b9a\u6027\u7684\u89c6\u89d2\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u5efa\u7acb\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2509.16330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16330", "abs": "https://arxiv.org/abs/2509.16330", "authors": ["Minxing Zhang", "Yi Yang", "Roy Xie", "Bhuwan Dhingra", "Shuyan Zhou", "Jian Pei"], "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey", "comment": null, "summary": "Large Language Model (LLM)-based agents have emerged as a new paradigm that\nextends LLMs' capabilities beyond text generation to dynamic interaction with\nexternal environments. By integrating reasoning with perception, memory, and\ntool use, agents are increasingly deployed in diverse domains like web\nnavigation and household robotics. A critical challenge, however, lies in\nensuring agent generalizability - the ability to maintain consistent\nperformance across varied instructions, tasks, environments, and domains,\nespecially those beyond agents' fine-tuning data. Despite growing interest, the\nconcept of generalizability in LLM-based agents remains underdefined, and\nsystematic approaches to measure and improve it are lacking. In this survey, we\nprovide the first comprehensive review of generalizability in LLM-based agents.\nWe begin by emphasizing agent generalizability's importance by appealing to\nstakeholders and clarifying the boundaries of agent generalizability by\nsituating it within a hierarchical domain-task ontology. We then review\ndatasets, evaluation dimensions, and metrics, highlighting their limitations.\nNext, we categorize methods for improving generalizability into three groups:\nmethods for the backbone LLM, for agent components, and for their interactions.\nMoreover, we introduce the distinction between generalizable frameworks and\ngeneralizable agents and outline how generalizable frameworks can be translated\ninto agent-level generalizability. Finally, we identify critical challenges and\nfuture directions, including developing standardized frameworks, variance- and\ncost-based metrics, and approaches that integrate methodological innovations\nwith architecture-level designs. By synthesizing progress and highlighting\nopportunities, this survey aims to establish a foundation for principled\nresearch on building LLM-based agents that generalize reliably across diverse\napplications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u5728\u8de8\u4efb\u52a1\u3001\u73af\u5883\u548c\u9886\u57df\u4e2d\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u65b9\u6cd5\u4e0e\u8bc4\u4f30\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u7f3a\u4e4f\u660e\u786e\u5b9a\u4e49\u548c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e9f\u9700\u7edf\u4e00\u6846\u67b6\u4e0e\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u7684\u9886\u57df-\u4efb\u52a1\u672c\u4f53\u6765\u754c\u5b9a\u6cdb\u5316\u6027\uff0c\u56de\u987e\u73b0\u6709\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u7ef4\u5ea6\u4e0e\u6307\u6807\uff0c\u5c06\u63d0\u5347\u6cdb\u5316\u7684\u65b9\u6cd5\u5206\u4e3a\u9aa8\u5e72LLM\u3001\u667a\u80fd\u4f53\u7ec4\u4ef6\u53ca\u5176\u4ea4\u4e92\u4e09\u7c7b\uff0c\u5e76\u533a\u5206\u53ef\u6cdb\u5316\u7684\u6846\u67b6\u4e0e\u53ef\u6cdb\u5316\u7684\u667a\u80fd\u4f53\u3002", "result": "\u68b3\u7406\u4e86\u5f53\u524d\u63d0\u5347LLM\u667a\u80fd\u4f53\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u4e0e\u5c40\u9650\uff0c\u660e\u786e\u4e86\u8bc4\u4f30\u4f53\u7cfb\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4ece\u6846\u67b6\u5230\u667a\u80fd\u4f53\u6cdb\u5316\u6027\u7684\u8f6c\u5316\u8def\u5f84\u3002", "conclusion": "\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3001\u8bbe\u8ba1\u57fa\u4e8e\u65b9\u5dee\u548c\u6210\u672c\u7684\u6307\u6807\u3001\u878d\u5408\u65b9\u6cd5\u521b\u65b0\u4e0e\u67b6\u6784\u8bbe\u8ba1\uff0c\u662f\u63a8\u52a8LLM\u667a\u80fd\u4f53\u5b9e\u73b0\u53ef\u9760\u6cdb\u5316\u7684\u5173\u952e\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2509.16895", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.16895", "abs": "https://arxiv.org/abs/2509.16895", "authors": ["Xinye Wanyan", "Danula Hettiachchi", "Chenglong Ma", "Ziqi Xu", "Jeffrey Chan"], "title": "Temporal-Aware User Behaviour Simulation with Large Language Models for Recommender Systems", "comment": null, "summary": "Large Language Models (LLMs) demonstrate human-like capabilities in language\nunderstanding, reasoning, and generation, driving interest in using LLM-based\nagents to simulate human feedback in recommender systems. However, most\nexisting approaches rely on static user profiling, neglecting the temporal and\ndynamic nature of user interests. This limitation stems from a disconnect\nbetween language modelling and behaviour modelling, which constrains the\ncapacity of agents to represent sequential patterns. To address this challenge,\nwe propose a Dynamic Temporal-aware Agent-based simulator for Recommender\nSystems, DyTA4Rec, which enables agents to model and utilise evolving user\nbehaviour based on historical interactions. DyTA4Rec features a dynamic updater\nfor real-time profile refinement, temporal-enhanced prompting for sequential\ncontext, and self-adaptive aggregation for coherent feedback. Experimental\nresults at group and individual levels show that DyTA4Rec significantly\nimproves the alignment between simulated and actual user behaviour by modelling\ndynamic characteristics and enhancing temporal awareness in LLM-based agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDyTA4Rec\u7684\u52a8\u6001\u65f6\u5e8f\u611f\u77e5\u4ee3\u7406\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7528\u6237\u53cd\u9988\u6a21\u62df\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u3001\u65f6\u5e8f\u589e\u5f3a\u63d0\u793a\u548c\u81ea\u9002\u5e94\u805a\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u884c\u4e3a\u4e0e\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5728\u6a21\u62df\u7528\u6237\u53cd\u9988\u65f6\u591a\u4f9d\u8d56\u9759\u6001\u7528\u6237\u753b\u50cf\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u5174\u8da3\u7684\u65f6\u5e8f\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u884c\u4e3a\u5efa\u6a21\u80fd\u529b\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86DyTA4Rec\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u66f4\u65b0\u6a21\u5757\u5b9e\u73b0\u5b9e\u65f6\u753b\u50cf\u4f18\u5316\uff0c\u65f6\u5e8f\u589e\u5f3a\u63d0\u793a\u5f15\u5165\u5e8f\u5217\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u805a\u5408\u673a\u5236\u751f\u6210\u8fde\u8d2f\u53cd\u9988\u3002", "result": "\u5728\u4e2a\u4f53\u548c\u7fa4\u4f53\u5c42\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDyTA4Rec\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u7528\u6237\u884c\u4e3a\u7684\u52a8\u6001\u7279\u5f81\uff0c\u63d0\u5347\u6a21\u62df\u884c\u4e3a\u4e0e\u771f\u5b9e\u884c\u4e3a\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "conclusion": "DyTA4Rec\u901a\u8fc7\u878d\u5408\u8bed\u8a00\u6a21\u578b\u4e0e\u52a8\u6001\u884c\u4e3a\u5efa\u6a21\uff0c\u589e\u5f3a\u4e86LLM\u4ee3\u7406\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5bf9\u65f6\u5e8f\u884c\u4e3a\u7684\u6a21\u62df\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u4eff\u771f\u6846\u67b6\u3002"}}
{"id": "2509.17168", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17168", "abs": "https://arxiv.org/abs/2509.17168", "authors": ["Chengwei Shi", "Chong Cao", "Xin Tong", "Xukun Shen"], "title": "Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics", "comment": "arXiv submission", "summary": "Head and gaze dynamics are crucial in expressive 3D facial animation for\nconveying emotion and intention. However, existing methods frequently address\nfacial components in isolation, overlooking the intricate coordination between\ngaze, head motion, and speech. The scarcity of high-quality gaze-annotated\ndatasets hinders the development of data-driven models capable of capturing\nrealistic, personalized gaze control. To address these challenges, we propose\nStyGazeTalk, an audio-driven method that generates synchronized gaze and head\nmotion styles. We extract speaker-specific motion traits from gaze-head\nsequences with a multi-layer LSTM structure incorporating a style encoder,\nenabling the generation of diverse animation styles. We also introduce a\nhigh-precision multimodal dataset comprising eye-tracked gaze, audio, head\npose, and 3D facial parameters, providing a valuable resource for training and\nevaluating head and gaze control models. Experimental results demonstrate that\nour method generates realistic, temporally coherent, and style-aware head-gaze\nmotions, significantly advancing the state-of-the-art in audio-driven facial\nanimation.", "AI": {"tldr": "\u63d0\u51faStyGazeTalk\uff0c\u4e00\u79cd\u97f3\u9891\u9a71\u52a8\u76843D\u9762\u90e8\u52a8\u753b\u4e2d\u540c\u6b65\u751f\u6210\u51dd\u89c6\u548c\u5934\u90e8\u8fd0\u52a8\u98ce\u683c\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u9ad8\u7cbe\u5ea6\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5e38\u5b64\u7acb\u5904\u7406\u9762\u90e8\u7ec4\u4ef6\uff0c\u5ffd\u89c6\u51dd\u89c6\u3001\u5934\u90e8\u8fd0\u52a8\u4e0e\u8bed\u97f3\u95f4\u7684\u534f\u8c03\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u5e26\u6807\u6ce8\u7684\u51dd\u89c6\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u591a\u5c42LSTM\u7ed3\u6784\u7ed3\u5408\u98ce\u683c\u7f16\u7801\u5668\uff0c\u4ece\u51dd\u89c6-\u5934\u90e8\u5e8f\u5217\u4e2d\u63d0\u53d6\u8bf4\u8bdd\u4eba\u7279\u5b9a\u7684\u8fd0\u52a8\u7279\u5f81\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u52a8\u753b\u98ce\u683c\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u903c\u771f\u3001\u65f6\u95f4\u8fde\u8d2f\u4e14\u5177\u98ce\u683c\u611f\u77e5\u7684\u5934-\u773c\u8fd0\u52a8\uff0c\u5728\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "StyGazeTalk\u6709\u6548\u89e3\u51b3\u4e86\u5934\u52a8\u4e0e\u51dd\u89c6\u534f\u540c\u751f\u6210\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u4e2a\u6027\u5316\u3001\u8868\u8fbe\u6027\u5f3a\u7684\u9762\u90e8\u52a8\u753b\u53d1\u5c55\u3002"}}
{"id": "2509.16382", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.2.1; I.5.2"], "pdf": "https://arxiv.org/pdf/2509.16382", "abs": "https://arxiv.org/abs/2509.16382", "authors": ["Saurabh Saini", "Kapil Ahuja", "Marc C. Steinbach", "Thomas Wick"], "title": "Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor", "comment": "15 Pages, 7 Figures, 5 Tables", "summary": "In this study, we develop a new CAD system for accurate thyroid cancer\nclassification with emphasis on feature extraction. Prior studies have shown\nthat thyroid texture is important for segregating the thyroid ultrasound images\ninto different classes. Based upon our experience with breast cancer\nclassification, we first conjuncture that the Discrete Cosine Transform (DCT)\nis the best descriptor for capturing textural features. Thyroid ultrasound\nimages are particularly challenging as the gland is surrounded by multiple\ncomplex anatomical structures leading to variations in tissue density. Hence,\nwe second conjuncture the importance of localization and propose that the Local\nDCT (LDCT) descriptor captures the textural features best in this context.\nAnother disadvantage of complex anatomy around the thyroid gland is scattering\nof ultrasound waves resulting in noisy and unclear textures. Hence, we third\nconjuncture that one image descriptor is not enough to fully capture the\ntextural features and propose the integration of another popular texture\ncapturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is\nknown to be noise resilient as well. We term our novel descriptor as Binary\nPattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification\nis carried out using a non-linear SVM. The proposed CAD system is evaluated on\nthe only two publicly available thyroid cancer datasets, namely TDID and AUITD.\nThe evaluation is conducted in two stages. In Stage I, thyroid nodules are\ncategorized as benign or malignant. In Stage II, the malignant cases are\nfurther sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I\nclassification, our proposed model demonstrates exceptional performance of\nnearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed\nmodel again attains excellent classification of close to 100% on TDID and 99%\non AUITD.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eBPD-LDCT\u7279\u5f81\u7684CAD\u7cfb\u7edf\uff0c\u7528\u4e8e\u7532\u72b6\u817a\u764c\u7684\u7cbe\u786e\u5206\u7c7b\uff0c\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1100%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u7532\u72b6\u817a\u8d85\u58f0\u56fe\u50cf\u56e0\u5468\u56f4\u590d\u6742\u89e3\u5256\u7ed3\u6784\u5bfc\u81f4\u7eb9\u7406\u566a\u58f0\u591a\u3001\u5bf9\u6bd4\u5ea6\u4f4e\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u63d0\u53d6\u7279\u5f81\uff0c\u9700\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u63cf\u8ff0\u5b50\u4ee5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51faBinary Pattern Driven Local Discrete Cosine Transform\uff08BPD-LDCT\uff09\u7279\u5f81\u63cf\u8ff0\u5b50\uff0c\u7ed3\u5408\u5c40\u90e8DCT\uff08LDCT\uff09\u548c\u6539\u8fdb\u7684\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f\uff08ILBP\uff09\uff0c\u5229\u7528LDCT\u6355\u6349\u5c40\u90e8\u9891\u57df\u7eb9\u7406\u7279\u5f81\uff0cILBP\u589e\u5f3a\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u91c7\u7528\u975e\u7ebf\u6027SVM\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728TDID\u548cAUITD\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cStage I\uff08\u826f\u6076\u6027\u5206\u7c7b\uff09\u51c6\u786e\u7387\u5206\u522b\u8fbe\u5230\u8fd1100%\u548c97%\uff1bStage II\uff08TI-RADS 4 vs 5\uff09\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a\u8fd1100%\u548c99%\u3002", "conclusion": "BPD-LDCT\u80fd\u6709\u6548\u6355\u6349\u7532\u72b6\u817a\u8d85\u58f0\u56fe\u50cf\u7684\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\u5e76\u6291\u5236\u566a\u58f0\u5f71\u54cd\uff0c\u6240\u63d0\u51fa\u7684CAD\u7cfb\u7edf\u5728\u7532\u72b6\u817a\u764c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5177\u6709\u4e34\u5e8a\u8f85\u52a9\u8bca\u65ad\u6f5c\u529b\u3002"}}
{"id": "2509.16434", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16434", "abs": "https://arxiv.org/abs/2509.16434", "authors": ["Ritvik Singh", "Karl Van Wyk", "Pieter Abbeel", "Jitendra Malik", "Nathan Ratliff", "Ankur Handa"], "title": "End-to-end RL Improves Dexterous Grasping Policies", "comment": "See our blog post: https://e2e4robotics.com/", "summary": "This work explores techniques to scale up image-based end-to-end learning for\ndexterous grasping with an arm + hand system. Unlike state-based RL,\nvision-based RL is much more memory inefficient, resulting in relatively low\nbatch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is\nstill an attractive method as unlike the more commonly used techniques which\ndistill state-based policies into vision networks, end-to-end RL can allow for\nemergent active vision behaviors. We identify a key bottleneck in training\nthese policies is the way most existing simulators scale to multiple GPUs using\ntraditional data parallelism techniques. We propose a new method where we\ndisaggregate the simulator and RL (both training and experience buffers) onto\nseparate GPUs. On a node with four GPUs, we have the simulator running on three\nof them, and PPO running on the fourth. We are able to show that with the same\nnumber of GPUs, we can double the number of existing environments compared to\nthe previous baseline of standard data parallelism. This allows us to train\nvision-based environments, end-to-end with depth, which were previously\nperforming far worse with the baseline. We train and distill both depth and\nstate-based policies into stereo RGB networks and show that depth distillation\nleads to better results, both in simulation and reality. This improvement is\nlikely due to the observability gap between state and vision policies which\ndoes not exist when distilling depth policies into stereo RGB. We further show\nthat the increased batch size brought about by disaggregated simulation also\nimproves real world performance. When deploying in the real world, we improve\nupon the previous state-of-the-art vision-based results using our end-to-end\npolicies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5206\u79bb\u5f0f\u6a21\u62df\u4e0e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u6a21\u62df\u5668\u548cRL\u7b97\u6cd5\u5206\u5e03\u5230\u4e0d\u540cGPU\u4e0a\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8f93\u5165\u7684\u7aef\u5230\u7aef\u7075\u5de7\u6293\u53d6\u8bad\u7ec3\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u73af\u5883\u5e76\u884c\u6570\u548c\u6279\u5927\u5c0f\uff0c\u4ece\u800c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8f93\u5165\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4f20\u7edf\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u53d7\u9650\u4e8e\u5185\u5b58\u6548\u7387\u4f4e\u3001\u6279\u5927\u5c0f\u5c0f\uff0c\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\uff0c\u4e14\u73b0\u6709\u6a21\u62df\u5668\u96be\u4ee5\u826f\u597d\u6269\u5c55\u5230\u591aGPU\u73af\u5883\u3002", "method": "\u63d0\u51fa\u5206\u79bb\u5f0f\u67b6\u6784\uff1a\u5c06\u6a21\u62df\u5668\u8fd0\u884c\u5728\u591a\u4e2aGPU\u4e0a\uff0c\u800c\u5c06PPO\u7b97\u6cd5\u548c\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u653e\u5728\u5355\u72ec\u7684GPU\u4e0a\uff1b\u5229\u7528\u6df1\u5ea6\u56fe\u50cf\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5c06\u5176\u84b8\u998f\u5230\u7acb\u4f53RGB\u7f51\u7edc\u4e2d\uff0c\u4ee5\u7f29\u5c0f\u72b6\u6001\u4e0e\u89c6\u89c9\u7b56\u7565\u4e4b\u95f4\u7684\u53ef\u89c2\u5bdf\u6027\u5dee\u8ddd\u3002", "result": "\u5728\u76f8\u540cGPU\u6570\u91cf\u4e0b\uff0c\u73af\u5883\u6570\u91cf\u8f83\u4f20\u7edf\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u7ffb\u500d\uff1b\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u5747\u4f18\u4e8e\u57fa\u7ebf\uff1b\u84b8\u998f\u81ea\u6df1\u5ea6\u7b56\u7565\u7684\u7acb\u4f53RGB\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff1b\u66f4\u5927\u7684\u6279\u5927\u5c0f\u63d0\u5347\u4e86\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u6548\u679c\u3002", "conclusion": "\u5206\u79bb\u5f0f\u6a21\u62df\u4e0e\u8bad\u7ec3\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6279\u5927\u5c0f\u74f6\u9888\uff0c\u652f\u6301\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u7aef\u5230\u7aef\u89c6\u89c9\u7075\u5de7\u64cd\u4f5c\u8bad\u7ec3\uff0c\u5e76\u5728\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u8fc1\u79fb\u6027\u80fd\u3002"}}
{"id": "2509.16278", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16278", "abs": "https://arxiv.org/abs/2509.16278", "authors": ["Alok N. Shah", "Khush Gupta", "Keshav Ramji", "Pratik Chaudhari"], "title": "Language Modeling with Learned Meta-Tokens", "comment": null, "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8emeta-token\u548cmeta-attention\u673a\u5236\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u7684meta-token\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u201c\u7f13\u5b58\u201d\uff0c\u6a21\u578b\u80fd\u591f\u5728\u63a8\u7406\u65f6\u5b9e\u73b0\u8d85\u8fc72\u500d\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3Transformer\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u6cdb\u5316\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u7684\u673a\u5236\u589e\u5f3a\u6a21\u578b\u5bf9\u957f\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u4e0e\u5229\u7528\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e00\u79cd\u5e26\u6709meta-attention\u673a\u5236\u7684GPT-2\u53d8\u4f53\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u6ce8\u5165\u7279\u6b8a\u7684meta-token\uff0c\u5e76\u901a\u8fc7\u4fee\u6539\u6ce8\u610f\u529b\u673a\u5236\u4f7f\u5176\u80fd\u591f\u5f15\u5bfc\u6a21\u578b\u6709\u6548\u5229\u7528\u8fd9\u4e9btoken\u6765\u538b\u7f29\u548c\u7d22\u5f15\u5386\u53f2\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u5c11\u4e8e100B token\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\uff0c\u8be5\u6a21\u578b\u5728\u4e00\u7cfb\u5217\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u548c\u957f\u5ea6\u5916\u63a8\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u4f7f\u7528YaRN\u6269\u5c55\u4e0a\u4e0b\u6587\u540e\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8fbe2\u500d\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u6cdb\u5316\uff1b\u53ef\u89c6\u5316\u548c\u4fe1\u606f\u8bba\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86meta-token\u5bf9\u4e0a\u4e0b\u6587\u538b\u7f29\u548c\u5b9a\u4f4d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f15\u5165meta-token\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e0e\u957f\u5ea6\u5916\u63a8\u80fd\u529b\uff0c\u540c\u65f6\u4e3a\u7406\u89e3\u6a21\u578b\u5185\u90e8\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.16287", "categories": ["cs.LG", "05C22, 05C90, 68R10"], "pdf": "https://arxiv.org/pdf/2509.16287", "abs": "https://arxiv.org/abs/2509.16287", "authors": ["Shanookha Ali", "Nitha Niralda", "Sunil Mathew"], "title": "Architectural change in neural networks using fuzzy vertex pooling", "comment": null, "summary": "The process of pooling vertices involves the creation of a new vertex, which\nbecomes adjacent to all the vertices that were originally adjacent to the\nendpoints of the vertices being pooled. After this, the endpoints of these\nvertices and all edges connected to them are removed. In this document, we\nintroduce a formal framework for the concept of fuzzy vertex pooling (FVP) and\nprovide an overview of its key properties with its applications to neural\nnetworks. The pooling model demonstrates remarkable efficiency in minimizing\nloss rapidly while maintaining competitive accuracy, even with fewer hidden\nlayer neurons. However, this advantage diminishes over extended training\nperiods or with larger datasets, where the model's performance tends to\ndegrade. This study highlights the limitations of pooling in later stages of\ndeep learning training, rendering it less effective for prolonged or\nlarge-scale applications. Consequently, pooling is recommended as a strategy\nfor early-stage training in advanced deep learning models to leverage its\ninitial efficiency.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u7cca\u9876\u70b9\u6c60\u5316\uff08FVP\uff09\u6846\u67b6\uff0c\u521d\u671f\u80fd\u9ad8\u6548\u51cf\u5c11\u635f\u5931\u5e76\u4fdd\u6301\u51c6\u786e\u7387\uff0c\u4f46\u957f\u671f\u6216\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u5efa\u8bae\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u3002", "motivation": "\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9876\u70b9\u6c60\u5316\u63d0\u4f9b\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5e76\u89e3\u51b3\u4f20\u7edf\u6c60\u5316\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u6548\u7387\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6a21\u7cca\u9876\u70b9\u6c60\u5316\uff08FVP\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u5e76\u9876\u70b9\u5e76\u91cd\u6784\u90bb\u63a5\u5173\u7cfb\uff0c\u5728\u4fdd\u6301\u7f51\u7edc\u7ed3\u6784\u7684\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u91cf\u3002", "result": "FVP\u5728\u8bad\u7ec3\u521d\u671f\u663e\u8457\u52a0\u5feb\u635f\u5931\u4e0b\u964d\u4e14\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5728\u957f\u65f6\u95f4\u8bad\u7ec3\u6216\u5927\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u9000\u5316\u3002", "conclusion": "\u6c60\u5316\u5728\u6df1\u5ea6\u5b66\u4e60\u540e\u671f\u6548\u679c\u6709\u9650\uff0c\u9002\u5408\u4f5c\u4e3a\u65e9\u671f\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u5347\u521d\u59cb\u6548\u7387\u3002"}}
{"id": "2509.16332", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16332", "abs": "https://arxiv.org/abs/2509.16332", "authors": ["Stephen Fitz", "Peter Romero", "Steven Basart", "Sipeng Chen", "Jose Hernandez-Orallo"], "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models", "comment": null, "summary": "Large Language Models increasingly mediate high-stakes interactions,\nintensifying research on their capabilities and safety. While recent work has\nshown that LLMs exhibit consistent and measurable synthetic personality traits,\nlittle is known about how modulating these traits affects model behavior. We\naddress this gap by investigating how psychometric personality control grounded\nin the Big Five framework influences AI behavior in the context of capability\nand safety benchmarks. Our experiments reveal striking effects: for example,\nreducing conscientiousness leads to significant drops in safety-relevant\nmetrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well\nas reduction in general capabilities as measured by MMLU. These findings\nhighlight personality shaping as a powerful and underexplored axis of model\ncontrol that interacts with both safety and general competence. We discuss the\nimplications for safety evaluation, alignment strategies, steering model\nbehavior after deployment, and risks associated with possible exploitation of\nthese findings. Our findings motivate a new line of research on\npersonality-sensitive safety evaluations and dynamic behavioral control in\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u6846\u67b6\u7684\u4e2a\u6027\u63a7\u5236\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u80fd\u529b\u548c\u5b89\u5168\u57fa\u51c6\u4e0a\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u964d\u4f4e\u5c3d\u8d23\u6027\u4f1a\u663e\u8457\u964d\u4f4e\u5b89\u5168\u6027\u548c\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u4e86\u89e3\u8c03\u8282LLMs\u7684\u5408\u6210\u4eba\u683c\u7279\u8d28\u5982\u4f55\u5f71\u54cd\u5176\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u4ea4\u4e92\u4e2d\u5bf9\u6a21\u578b\u5b89\u5168\u548c\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5927\u4e94\u4eba\u683c\u6846\u67b6\u8fdb\u884c\u5fc3\u7406\u6d4b\u91cf\u4eba\u683c\u63a7\u5236\uff0c\u5e76\u5728WMDP\u3001TruthfulQA\u3001ETHICS\u3001Sycophancy\u548cMMLU\u7b49\u57fa\u51c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u964d\u4f4e\u5c3d\u8d23\u6027\u5bfc\u81f4\u5b89\u5168\u6027\u6307\u6807\u548c\u901a\u7528\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff1b\u4eba\u683c\u5851\u9020\u88ab\u8bc1\u660e\u662f\u5f71\u54cd\u6a21\u578b\u5b89\u5168\u4e0e\u80fd\u529b\u7684\u91cd\u8981\u63a7\u5236\u7ef4\u5ea6\u3002", "conclusion": "\u4eba\u683c\u8c03\u63a7\u662f\u5f71\u54cdLLM\u884c\u4e3a\u7684\u4e00\u4e2a\u5f3a\u6709\u529b\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9014\u5f84\uff0c\u5efa\u8bae\u5f00\u5c55\u9488\u5bf9\u4eba\u683c\u654f\u611f\u7684\u5b89\u5168\u8bc4\u4f30\u548c\u52a8\u6001\u884c\u4e3a\u63a7\u5236\u7684\u65b0\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.16931", "categories": ["cs.IR", "cs.AI", "cs.LG", "I.2.0; I.5.0; I.7.0"], "pdf": "https://arxiv.org/pdf/2509.16931", "abs": "https://arxiv.org/abs/2509.16931", "authors": ["Yutong Li", "Yu Zhu", "Yichen Qiao", "Ziyu Guan", "Lv Shao", "Tong Liu", "Bo Zheng"], "title": "Equip Pre-ranking with Target Attention by Residual Quantization", "comment": "5 pages, 2 figures, submitted to WSDM 2026 Short Paper Track", "summary": "The pre-ranking stage in industrial recommendation systems faces a\nfundamental conflict between efficiency and effectiveness. While powerful\nmodels like Target Attention (TA) excel at capturing complex feature\ninteractions in the ranking stage, their high computational cost makes them\ninfeasible for pre-ranking, which often relies on simplistic vector-product\nmodels. This disparity creates a significant performance bottleneck for the\nentire system. To bridge this gap, we propose TARQ, a novel pre-ranking\nframework. Inspired by generative models, TARQ's key innovation is to equip\npre-ranking with an architecture approximate to TA by Residual Quantization.\nThis allows us to bring the modeling power of TA into the latency-critical\npre-ranking stage for the first time, establishing a new state-of-the-art\ntrade-off between accuracy and efficiency. Extensive offline experiments and\nlarge-scale online A/B tests at Taobao demonstrate TARQ's significant\nimprovements in ranking performance. Consequently, our model has been fully\ndeployed in production, serving tens of millions of daily active users and\nyielding substantial business improvements.", "AI": {"tldr": "\u63d0\u51faTARQ\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u91cf\u5316\u5c06\u76ee\u6807\u6ce8\u610f\u529b\u6a21\u578b\u5f15\u5165\u9884\u6392\u5e8f\u9636\u6bb5\uff0c\u5728\u4fdd\u8bc1\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6392\u5e8f\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u9884\u6392\u5e8f\u9636\u6bb5\u5728\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u5f3a\u5927\u7684\u6a21\u578b\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u96be\u4ee5\u5e94\u7528\u4e8e\u9884\u6392\u5e8f\u3002", "method": "\u53d7\u751f\u6210\u6a21\u578b\u542f\u53d1\uff0c\u91c7\u7528\u6b8b\u5dee\u91cf\u5316\u7684\u6280\u672f\uff0c\u5728\u9884\u6392\u5e8f\u9636\u6bb5\u6784\u5efa\u8fd1\u4f3c\u4e8e\u76ee\u6807\u6ce8\u610f\u529b\uff08TA\uff09\u7684\u7ed3\u6784\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u548c\u6dd8\u5b9d\u7684\u5927\u89c4\u6a21\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660e\uff0cTARQ\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f18\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6392\u5e8f\u6027\u80fd\u3002", "conclusion": "TARQ\u6210\u529f\u5c06\u590d\u6742\u7279\u5f81\u4ea4\u4e92\u5efa\u6a21\u5f15\u5165\u9884\u6392\u5e8f\u9636\u6bb5\uff0c\u5df2\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u670d\u52a1\u6570\u5343\u4e07\u65e5\u6d3b\u7528\u6237\u5e76\u5e26\u6765\u663e\u8457\u4e1a\u52a1\u63d0\u5347\u3002"}}
{"id": "2509.17240", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.17240", "abs": "https://arxiv.org/abs/2509.17240", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Ibrahim Ghaznavi", "Alaa Abd-alrazaq", "Aliya Tabassum", "Junaid Qadir"], "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System", "comment": null, "summary": "Systematic Literature Reviews (SLRs) are foundational to evidence-based\nresearch but remain labor-intensive and prone to inconsistency across\ndisciplines. We present an LLM-based SLR evaluation copilot built on a\nMulti-Agent System (MAS) architecture to assist researchers in assessing the\noverall quality of the systematic literature reviews. The system automates\nprotocol validation, methodological assessment, and topic relevance checks\nusing a scholarly database. Unlike conventional single-agent methods, our\ndesign integrates a specialized agentic approach aligned with PRISMA guidelines\nto support more structured and interpretable evaluations. We conducted an\ninitial study on five published SLRs from diverse domains, comparing system\noutputs to expert-annotated PRISMA scores, and observed 84% agreement. While\nearly results are promising, this work represents a first step toward scalable\nand accurate NLP-driven systems for interdisciplinary workflows and reveals\ntheir capacity for rigorous, domain-agnostic knowledge aggregation to\nstreamline the review process.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u67b6\u6784\u7684LLM\u9a71\u52a8\u7684\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u8bc4\u4f30\u534f\u4f5c\u8005\uff0c\u81ea\u52a8\u5316\u534f\u8bae\u9a8c\u8bc1\u3001\u65b9\u6cd5\u8bba\u8bc4\u4f30\u548c\u4e3b\u9898\u76f8\u5173\u6027\u68c0\u67e5\uff0c\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684PRISMA\u8bc4\u5206\u670984%\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u867d\u4e3a\u5faa\u8bc1\u7814\u7a76\u7684\u57fa\u7840\uff0c\u4f46\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u8de8\u5b66\u79d1\u4e00\u81f4\u6027\u5dee\uff0c\u4e9f\u9700\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\u4e0e\u6807\u51c6\u5316\u6c34\u5e73\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u9075\u5faaPRISMA\u6307\u5357\uff0c\u96c6\u6210\u534f\u8bae\u9a8c\u8bc1\u3001\u65b9\u6cd5\u8bba\u8bc4\u4f30\u548c\u4e3b\u9898\u76f8\u5173\u6027\u68c0\u67e5\uff0c\u5e76\u901a\u8fc7\u5b66\u672f\u6570\u636e\u5e93\u5b9e\u73b0\u81ea\u52a8\u5316\u5206\u6790\u3002", "result": "\u5728\u4e94\u4e2a\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u5df2\u53d1\u8868SLR\u4e0a\u8fdb\u884c\u521d\u6b65\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8f93\u51fa\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684PRISMA\u8bc4\u5206\u8fbe\u523084%\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53LLM\u534f\u4f5c\u8005\u662f\u8fc8\u5411\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u8de8\u5b66\u79d1NLP\u9a71\u52a8\u7cfb\u7edf\u7684\u521d\u6b65\u6210\u529f\uff0c\u5c55\u73b0\u51fa\u5728\u6807\u51c6\u5316\u548c\u7b80\u5316SLR\u6d41\u7a0b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17212", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17212", "abs": "https://arxiv.org/abs/2509.17212", "authors": ["Federico Stella", "Nicolas Talabot", "Hieu Le", "Pascal Fua"], "title": "High Resolution UDF Meshing via Iterative Networks", "comment": "Accepted at NeurIPS 2025", "summary": "Unsigned Distance Fields (UDFs) are a natural implicit representation for\nopen surfaces but, unlike Signed Distance Fields (SDFs), are challenging to\ntriangulate into explicit meshes. This is especially true at high resolutions\nwhere neural UDFs exhibit higher noise levels, which makes it hard to capture\nfine details. Most current techniques perform within single voxels without\nreference to their neighborhood, resulting in missing surface and holes where\nthe UDF is ambiguous or noisy. We show that this can be remedied by performing\nseveral passes and by reasoning on previously extracted surface elements to\nincorporate neighborhood information. Our key contribution is an iterative\nneural network that does this and progressively improves surface recovery\nwithin each voxel by spatially propagating information from increasingly\ndistant neighbors. Unlike single-pass methods, our approach integrates newly\ndetected surfaces, distance values, and gradients across multiple iterations,\neffectively correcting errors and stabilizing extraction in challenging\nregions. Experiments on diverse 3D models demonstrate that our method produces\nsignificantly more accurate and complete meshes than existing approaches,\nparticularly for complex geometries, enabling UDF surface extraction at higher\nresolutions where traditional methods fail.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fed\u4ee3\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u63a8\u7406\u548c\u90bb\u57df\u4fe1\u606f\u4f20\u64ad\u6765\u6539\u5584\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\uff08UDF\uff09\u7684\u8868\u9762\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u7f51\u683c\u751f\u6210\u7cbe\u5ea6\u4e0e\u5b8c\u6574\u6027\u3002", "motivation": "\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\uff08UDF\uff09\u867d\u9002\u5408\u8868\u793a\u5f00\u653e\u8868\u9762\uff0c\u4f46\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u566a\u58f0\u5927\uff0c\u4f20\u7edf\u5355\u6b21\u63d0\u53d6\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u90bb\u57df\u53c2\u8003\u6613\u5bfc\u81f4\u8868\u9762\u7f3a\u5931\u548c\u5b54\u6d1e\uff0c\u96be\u4ee5\u51c6\u786e\u4e09\u89d2\u5316\u4e3a\u7f51\u683c\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u8fed\u4ee3\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u8f6e\u5904\u7406\uff0c\u5728\u6bcf\u4e2a\u4f53\u7d20\u4e2d\u9010\u6b65\u878d\u5408\u65b0\u68c0\u6d4b\u5230\u7684\u8868\u9762\u3001\u8ddd\u79bb\u503c\u548c\u68af\u5ea6\u4fe1\u606f\uff0c\u5e76\u4ece\u8d8a\u6765\u8d8a\u8fdc\u7684\u90bb\u5c45\u4f20\u64ad\u7a7a\u95f4\u4fe1\u606f\uff0c\u4ee5\u4fee\u6b63\u8bef\u5dee\u5e76\u7a33\u5b9a\u63d0\u53d6\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u79cd3D\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u80fd\u751f\u6210\u66f4\u7cbe\u786e\u3001\u66f4\u5b8c\u6574\u7684\u7f51\u683c\uff0c\u5c24\u5176\u5728\u590d\u6742\u51e0\u4f55\u548c\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u5728\u4f20\u7edf\u65b9\u6cd5\u5931\u6548\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u63d0\u53d6\u8868\u9762\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8fed\u4ee3\u673a\u5236\u548c\u90bb\u57df\u4fe1\u606f\u6574\u5408\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86UDF\u5728\u9ad8\u566a\u58f0\u548c\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u8868\u9762\u63d0\u53d6\u96be\u9898\uff0c\u63a8\u52a8\u4e86UDF\u5728\u9ad8\u8d28\u91cf\u7f51\u683c\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.16415", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16415", "abs": "https://arxiv.org/abs/2509.16415", "authors": ["Zhengri Wu", "Yiran Wang", "Yu Wen", "Zeyu Zhang", "Biao Wu", "Hao Tang"], "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes", "comment": null, "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.", "AI": {"tldr": "\u63d0\u51faStereoAdapter\uff0c\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u9002\u914d\u7684\u5355\u76ee\u57fa\u7840\u7f16\u7801\u5668\u548c\u5faa\u73af\u7acb\u4f53\u7ec6\u5316\u6a21\u5757\uff0c\u7528\u4e8e\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6c34\u4e0b\u9886\u57df\u9002\u5e94\u5927\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\u6548\u7387\u4f4e\u4ee5\u53ca\u878d\u5408\u5355\u76ee\u5148\u9a8c\u4e0e\u7acb\u4f53\u5339\u914d\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5f15\u5165\u52a8\u6001LoRA\u9002\u5e94\u673a\u5236\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6UW-StereoDepth-40K\u4e0a\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u5faa\u73af\u7acb\u4f53\u7ec6\u5316\u6a21\u5757\u878d\u5408\u5355\u76ee\u5148\u9a8c\u4e0e\u7acb\u4f53\u5339\u914d\u3002", "result": "\u5728TartanAir\u548cSQUID\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u53476.11%\u548c5.12%\uff0c\u5e76\u5728BlueROV2\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "StereoAdapter\u5728\u6c34\u4e0b\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\uff0c\u517c\u987e\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002"}}
{"id": "2509.16445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16445", "abs": "https://arxiv.org/abs/2509.16445", "authors": ["Naoki Yokoyama", "Sehoon Ha"], "title": "FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning", "comment": null, "summary": "Enabling robotic assistants to navigate complex environments and locate\nobjects described in free-form language is a critical capability for real-world\ndeployment. While foundation models, particularly Vision-Language Models\n(VLMs), offer powerful semantic understanding, effectively adapting their\nweb-scale knowledge for embodied decision-making remains a key challenge. We\npresent FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that\ndirectly fine-tunes pre-trained VLM as the navigation policy. In contrast to\nmethods that use foundation models primarily in a zero-shot manner or for map\nannotation, FiLM-Nav learns to select the next best exploration frontier by\nconditioning directly on raw visual trajectory history and the navigation goal.\nLeveraging targeted simulated embodied experience allows the VLM to ground its\npowerful pre-trained representations in the specific dynamics and visual\npatterns relevant to goal-driven navigation. Critically, fine-tuning on a\ndiverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary\nspatial reasoning task proves essential for achieving robustness and broad\ngeneralization. FiLM-Nav sets a new state-of-the-art in both SPL and success\nrate on HM3D ObjectNav among open-vocabulary methods, and sets a\nstate-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating\nstrong generalization to unseen object categories. Our work validates that\ndirectly fine-tuning VLMs on diverse simulated embodied data is a highly\neffective pathway towards generalizable and efficient semantic navigation\ncapabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FiLM-Nav\uff0c\u4e00\u79cd\u76f4\u63a5\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f5c\u4e3a\u5bfc\u822a\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6a21\u62df\u4efb\u52a1\u6570\u636e\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u57fa\u4e8e\u81ea\u7531\u8bed\u8a00\u6307\u4ee4\u7684\u8bed\u4e49\u5bfc\u822a\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5982\u4f55\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u5177\u8eab\u667a\u80fd\u4f53\u7684\u8bed\u4e49\u5bfc\u822a\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u672a\u89c1\u7269\u4f53\u7c7b\u522b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFiLM-Nav\u65b9\u6cd5\uff0c\u76f4\u63a5\u5c06\u9884\u8bad\u7ec3VLM\u5fae\u8c03\u4e3a\u5bfc\u822a\u7b56\u7565\uff0c\u4ee5\u539f\u59cb\u89c6\u89c9\u8f68\u8ff9\u5386\u53f2\u548c\u5bfc\u822a\u76ee\u6807\u4e3a\u6761\u4ef6\uff0c\u9009\u62e9\u6700\u4f73\u63a2\u7d22\u524d\u6cbf\uff1b\u5e76\u5728\u5305\u542bObjectNav\u3001OVON\u3001ImageNav\u548c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u3002", "result": "FiLM-Nav\u5728HM3D ObjectNav\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7684SPL\u548c\u6210\u529f\u7387\uff0c\u5728HM3D-OVON\u57fa\u51c6\u4e0a\u4e5f\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684SPL\uff0c\u8868\u73b0\u51fa\u5bf9\u672a\u89c1\u7269\u4f53\u7c7b\u522b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u76f4\u63a5\u5728\u591a\u6837\u5316\u5177\u8eab\u6a21\u62df\u6570\u636e\u4e0a\u5fae\u8c03VLM\u662f\u4e00\u79cd\u5b9e\u73b0\u53ef\u6cdb\u5316\u3001\u9ad8\u6548\u8bed\u4e49\u5bfc\u822a\u7684\u6709\u6548\u9014\u5f84\uff0c\u9a8c\u8bc1\u4e86VLM\u4f5c\u4e3a\u7aef\u5230\u7aef\u5bfc\u822a\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16325", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16325", "abs": "https://arxiv.org/abs/2509.16325", "authors": ["Andrew Zhu", "Chris Callison-Burch"], "title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap", "comment": "8 pages, 1 figure", "summary": "Imagine AI assistants that enhance conversations without interrupting them:\nquietly providing relevant information during a medical consultation,\nseamlessly preparing materials as teachers discuss lesson plans, or\nunobtrusively scheduling meetings as colleagues debate calendars. While modern\nconversational LLM agents directly assist human users with tasks through a chat\ninterface, we study this alternative paradigm for interacting with LLM agents,\nwhich we call \"overhearing agents.\" Rather than demanding the user's attention,\noverhearing agents continuously monitor ambient activity and intervene only\nwhen they can provide contextual assistance. In this paper, we present the\nfirst analysis of overhearing LLM agents as a distinct paradigm in human-AI\ninteraction and establish a taxonomy of overhearing agent interactions and\ntasks grounded in a survey of works on prior LLM-powered agents and exploratory\nHCI studies. Based on this taxonomy, we create a list of best practices for\nresearchers and developers building overhearing agent systems. Finally, we\noutline the remaining research gaps and reveal opportunities for future\nresearch in the overhearing paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u65c1\u542c\u578bLLM\u4ee3\u7406\u201d\u8fd9\u4e00\u65b0\u7684\u4eba\u673a\u4ea4\u4e92\u8303\u5f0f\uff0c\u5373AI\u5728\u4e0d\u6253\u65ad\u4eba\u7c7b\u5bf9\u8bdd\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u76d1\u542c\u73af\u5883\u52a8\u6001\u63d0\u4f9b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f85\u52a9\uff0c\u5e76\u5efa\u7acb\u4e86\u8be5\u7c7b\u4ee3\u7406\u7684\u4ea4\u4e92\u4e0e\u4efb\u52a1\u5206\u7c7b\u4f53\u7cfb\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u6700\u4f73\u5b9e\u8df5\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u8bdd\u5f0fLLM\u4ee3\u7406\u9700\u8981\u7528\u6237\u4e3b\u52a8\u4ea4\u4e92\uff0c\u5bb9\u6613\u5206\u6563\u6ce8\u610f\u529b\uff1b\u800c\u73b0\u5b9e\u573a\u666f\u4e2d\u5b58\u5728\u5927\u91cf\u65e0\u9700\u4e3b\u52a8\u8bf7\u6c42\u4f46\u9700\u667a\u80fd\u8f85\u52a9\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u81ea\u7136\u3001\u4f4e\u5e72\u6270\u7684AI\u534f\u4f5c\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u5df2\u6709\u7684LLM\u4ee3\u7406\u5de5\u4f5c\u548c\u63a2\u7d22\u6027\u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u65c1\u542c\u578b\u4ee3\u7406\u7684\u4ea4\u4e92\u4e0e\u4efb\u52a1\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u8bbe\u8ba1\u51c6\u5219\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u9488\u5bf9\u65c1\u542c\u578bLLM\u4ee3\u7406\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u603b\u7ed3\u51fa\u9002\u7528\u4e8e\u6b64\u7c7b\u7cfb\u7edf\u7684\u8bbe\u8ba1\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u8bc6\u522b\u51fa\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u3002", "conclusion": "\u65c1\u542c\u578bLLM\u4ee3\u7406\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u4eba-AI\u4ea4\u4e92\u65b0\u8303\u5f0f\uff0c\u5177\u5907\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u5728\u9690\u79c1\u3001\u5e72\u9884\u65f6\u673a\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u7b49\u65b9\u9762\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2509.16293", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16293", "abs": "https://arxiv.org/abs/2509.16293", "authors": ["Borui Wan", "Gaohong Liu", "Zuquan Song", "Jun Wang", "Yun Zhang", "Guangming Sheng", "Shuguang Wang", "Houmin Wei", "Chenyuan Wang", "Weiqiang Lou", "Xi Yang", "Mofan Zhang", "Kaihua Jiang", "Cheng Ren", "Xiaoyun Zhi", "Menghan Yu", "Zhe Nan", "Zhuolin Zheng", "Baoquan Zhong", "Qinlong Wang", "Huan Yu", "Jinxin Chi", "Wang Zhang", "Yuhan Li", "Zixian Du", "Sida Zhao", "Yongqiang Zhang", "Jingzhe Tang", "Zherui Liu", "Chuan Wu", "Yanghua Peng", "Haibin Lin", "Wencong Xiao", "Xin Liu", "Liang Xiang"], "title": "Robust LLM Training Infrastructure at ByteDance", "comment": null, "summary": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs.", "AI": {"tldr": "ByteRobust\u662f\u4e00\u4e2a\u4e13\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21GPU\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u6548\u6545\u969c\u68c0\u6d4b\u3001\u5b9a\u4f4d\u4e0e\u6062\u590d\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u5bb9\u9519\u6027\u548c\u7a33\u5b9a\u8fde\u7eed\u7684\u8bad\u7ec3\uff0c\u5728\u8d8520\u4e07GPU\u7684\u751f\u4ea7\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e8697%\u7684\u4e09\u6708\u8bad\u7ec3\u4efb\u52a1ETTR\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u89c4\u6a21\u6269\u5927\u81f3\u6570\u4e07GPU\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2dCUDA\u9519\u8bef\u3001NaN\u503c\u3001\u4efb\u52a1\u6302\u8d77\u7b49\u6545\u969c\u9891\u53d1\uff0c\u4e25\u91cd\u5f71\u54cd\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4e9f\u9700\u4e00\u4e2a\u80fd\u6700\u5c0f\u5316\u4e2d\u65ad\u3001\u5feb\u901f\u8bca\u65ad\u548c\u5bb9\u9519\u7684\u7cfb\u7edf\u3002", "method": "ByteRobust\u9488\u5bf9LLM\u8bad\u7ec3\u7684\u72ec\u7279\u6027\uff0c\u4f18\u5148\u5b9e\u73b0\u5e38\u89c4\u5316\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\uff1b\u5229\u7528LLM\u8bad\u7ec3\u7684\u5e76\u884c\u6027\u4e0e\u7279\u5f81\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u5bb9\u91cf\u5bb9\u9519\u3001\u5feb\u901f\u6545\u969c\u5212\u5206\u4e0e\u5b9a\u4f4d\u3002", "result": "\u7cfb\u7edf\u5df2\u5728\u8d85\u8fc720\u4e07GPU\u7684\u751f\u4ea7\u5e73\u53f0\u90e8\u7f72\uff0c\u57289,600 GPU\u4e0a\u8fd0\u884c\u4e09\u4e2a\u6708\u7684\u8bad\u7ec3\u4efb\u52a1\u5b9e\u73b0\u4e8697%\u7684ETTR\uff08\u9884\u8ba1\u65f6\u95f4\u5230\u6062\u590d\uff09\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u8fde\u7eed\u6027\u4e0e\u6548\u7387\u3002", "conclusion": "ByteRobust\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u6570\u636e\u9a71\u52a8\u7b56\u7565\uff0c\u6709\u6548\u4fdd\u969c\u4e86\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u4e0e\u6548\u7387\uff0c\u5177\u5907\u5728\u8d85\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u4e2d\u5e7f\u6cdb\u90e8\u7f72\u7684\u80fd\u529b\u3002"}}
{"id": "2509.16348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16348", "abs": "https://arxiv.org/abs/2509.16348", "authors": ["Minxiao Wang", "Saurabh Kataria", "Juntong Ni", "Timothy G. Buchman", "Jocelyn Grunwell", "Mark Mai", "Wei Jin", "Matthew Clark", "Stephanie Brown", "Michael Fundora", "Puneet Sharma", "Tony Pan", "Sam Khan", "Timothy Ruchti", "Naveen Muthu", "Kevin Maher", "Sivasubramanium V Bhavani", "Xiao Hu"], "title": "A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)", "comment": null, "summary": "We present UNIPHY+, a unified physiological foundation model (physioFM)\nframework designed to enable continuous human health and diseases monitoring\nacross care settings using ubiquitously obtainable physiological data. We\npropose novel strategies for incorporating contextual information during\npretraining, fine-tuning, and lightweight model personalization via multi-modal\nlearning, feature fusion-tuning, and knowledge distillation. We advocate\ntesting UNIPHY+ with a broad set of use cases from intensive care to ambulatory\nmonitoring in order to demonstrate that UNIPHY+ can empower generalizable,\nscalable, and personalized physiological AI to support both clinical\ndecision-making and long-term health monitoring.", "AI": {"tldr": "UNIPHY+ \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u7406\u5b66\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u65e8\u5728\u5229\u7528\u5e7f\u6cdb\u53ef\u83b7\u53d6\u7684\u751f\u7406\u6570\u636e\u5b9e\u73b0\u8de8\u533b\u7597\u573a\u666f\u7684\u8fde\u7eed\u5065\u5eb7\u4e0e\u75be\u75c5\u76d1\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8de8\u4e0d\u540c\u533b\u7597\u73af\u5883\u7684\u901a\u7528\u3001\u53ef\u6269\u5c55\u548c\u4e2a\u6027\u5316\u7684\u751f\u7406AI\uff0c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u548c\u957f\u671f\u5065\u5eb7\u76d1\u6d4b\u3002", "method": "\u63d0\u51fa\u5728\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e2a\u6027\u5316\u4e2d\u878d\u5165\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u65b0\u7b56\u7565\uff0c\u91c7\u7528\u591a\u6a21\u6001\u5b66\u4e60\u3001\u7279\u5f81\u878d\u5408\u8c03\u4f18\u548c\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002", "result": "UNIPHY+ \u80fd\u591f\u901a\u8fc7\u591a\u79cd\u5e94\u7528\u573a\u666f\uff08\u5982\u91cd\u75c7\u76d1\u62a4\u548c\u95e8\u8bca\u76d1\u6d4b\uff09\u9a8c\u8bc1\u5176\u5728\u6cdb\u5316\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "UNIPHY+ \u6846\u67b6\u6709\u671b\u63a8\u52a8\u57fa\u4e8e\u751f\u7406\u6570\u636e\u7684\u667a\u80fd\u5065\u5eb7\u76d1\u6d4b\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u4e34\u5e8a\u548c\u65e5\u5e38\u4f7f\u7528\u573a\u666f\u3002"}}
{"id": "2509.17265", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17265", "abs": "https://arxiv.org/abs/2509.17265", "authors": ["David Liu", "Erik Weis", "Moritz Laber", "Tina Eliassi-Rad", "Brennan Klein"], "title": "Identifying and Upweighting Power-Niche Users to Mitigate Popularity Bias in Recommendations", "comment": null, "summary": "Recommender systems have been shown to exhibit popularity bias by\nover-recommending popular items and under-recommending relevant niche items. We\nseek to understand interactions with niche items in benchmark recommendation\ndatasets as a step toward mitigating popularity bias. We find that, compared to\nmainstream users, niche-preferring users exhibit a longer-tailed activity-level\ndistribution, indicating the existence of users who both prefer niche items and\nexhibit high activity levels. We partition users along two axes: (1) activity\nlevel (\"power\" vs. \"light\") and (2) item-popularity preference (\"mainstream\"\nvs. \"niche\"), and show that in several benchmark datasets, the number of\npower-niche users (high activity and niche preference) is statistically\nsignificantly larger than expected under a null configuration model. Motivated\nby this observation, we propose a framework for reweighting the Bayesian\nPersonalized Ranking (BPR) loss that simultaneously reweights based on user\nactivity level and item popularity. Our method introduces two interpretable\nparameters: one controlling the significance of user activity level, and the\nother of item popularity. Experiments on benchmark datasets show that\nupweighting power-niche users reduces popularity bias and can increase overall\nperformance. In contrast to previous work that only considers user activity\nlevel or item popularity in isolation, our results suggest that considering\ntheir interaction leads to Pareto-dominant performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u504f\u597d\u5c0f\u4f17\u7269\u54c1\u7684\u9ad8\u6d3b\u8dc3\u7528\u6237\u5728\u6570\u636e\u96c6\u4e2d\u663e\u8457\u5b58\u5728\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u6d3b\u8dc3\u5ea6\u548c\u7269\u54c1\u6d41\u884c\u5ea6\u4ea4\u4e92\u7684BPR\u635f\u5931\u91cd\u52a0\u6743\u6846\u67b6\uff0c\u6709\u6548\u964d\u4f4e\u6d41\u884c\u5ea6\u504f\u5dee\u5e76\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u7f13\u89e3\u63a8\u8350\u7cfb\u7edf\u4e2d\u8fc7\u5ea6\u63a8\u8350\u70ed\u95e8\u7269\u54c1\u800c\u5ffd\u89c6\u5c0f\u4f17\u76f8\u5173\u7269\u54c1\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u7406\u89e3\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u7528\u6237\u4e0e\u5c0f\u4f17\u7269\u54c1\u7684\u4ea4\u4e92\u884c\u4e3a\u3002", "method": "\u5c06\u7528\u6237\u6309\u6d3b\u8dc3\u7a0b\u5ea6\uff08\u9ad8/\u4f4e\uff09\u548c\u7269\u54c1\u504f\u597d\uff08\u4e3b\u6d41/\u5c0f\u4f17\uff09\u5212\u5206\uff0c\u6784\u5efa\u96f6\u6a21\u578b\u9a8c\u8bc1power-niche\u7528\u6237\uff08\u9ad8\u6d3b\u8dc3+\u5c0f\u4f17\u504f\u597d\uff09\u7684\u663e\u8457\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u53cc\u53c2\u6570\u91cd\u52a0\u6743BPR\u635f\u5931\u6846\u67b6\uff0c\u540c\u65f6\u8003\u8651\u7528\u6237\u6d3b\u8dc3\u5ea6\u548c\u7269\u54c1\u6d41\u884c\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cpower-niche\u7528\u6237\u6570\u91cf\u663e\u8457\u9ad8\u4e8e\u968f\u673a\u9884\u671f\uff1b\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u540c\u65f6\u63d0\u5347\u63a8\u8350\u6574\u4f53\u6027\u80fd\uff0c\u4e14\u4f18\u4e8e\u4ec5\u5355\u72ec\u8003\u8651\u7528\u6237\u6216\u7269\u54c1\u56e0\u7d20\u7684\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u540c\u65f6\u5efa\u6a21\u7528\u6237\u6d3b\u8dc3\u5ea6\u4e0e\u7269\u54c1\u6d41\u884c\u5ea6\u7684\u4ea4\u4e92\u5173\u7cfb\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u5730\u7f13\u89e3\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u6240\u63d0\u51fa\u7684\u91cd\u52a0\u6743\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u5360\u4f18\uff0c\u4e3a\u516c\u5e73\u3001\u591a\u6837\u5316\u7684\u63a8\u8350\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17728", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.17728", "abs": "https://arxiv.org/abs/2509.17728", "authors": ["Yara Zgheib", "Luca Calatroni", "Marc Antonini", "Roula Nassif"], "title": "A non-smooth regularization framework for learning over multitask graphs", "comment": null, "summary": "In this work, we consider learning over multitask graphs, where each agent\naims to estimate its own parameter vector. Although agents seek distinct\nobjectives, collaboration among them can be beneficial in scenarios where\nrelationships between tasks exist. Among the various approaches to promoting\nrelationships between tasks and, consequently, enhancing collaboration between\nagents, one notable method is regularization. While previous multitask learning\nstudies have focused on smooth regularization to enforce graph smoothness, this\nwork explores non-smooth regularization techniques that promote sparsity,\nmaking them particularly effective in encouraging piecewise constant\ntransitions on the graph. We begin by formulating a global regularized\noptimization problem, which involves minimizing the aggregate sum of individual\ncosts, regularized by a general non-smooth term designed to promote\npiecewise-constant relationships between the tasks of neighboring agents. Based\non the forward-backward splitting strategy, we propose a decentralized learning\napproach that enables efficient solutions to the regularized optimization\nproblem. Then, under convexity assumptions on the cost functions and\nco-regularization, we establish that the proposed approach converges in the\nmean-square-error sense within $O(\\mu)$ of the optimal solution of the globally\nregularized cost. For broader applicability and improved computational\nefficiency, we also derive closed-form expressions for commonly used non-smooth\n(and, possibly, non-convex) regularizers, such as the weighted sum of the\n$\\ell_0$-norm, $\\ell_1$-norm, and elastic net regularization. Finally, we\nillustrate both the theoretical findings and the effectiveness of the approach\nthrough simulations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u4efb\u52a1\u56fe\u4e0a\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u5149\u6ed1\u6b63\u5219\u5316\uff08\u4fc3\u8fdb\u7a00\u758f\u6027\u548c\u5206\u6bb5\u5e38\u6570\u5173\u7cfb\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u524d\u5411-\u540e\u5411\u5206\u88c2\u7b56\u7565\u6c42\u89e3\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\uff0c\u5728\u5747\u65b9\u8bef\u5dee\u610f\u4e49\u4e0b\u8bc1\u660e\u4e86\u7b97\u6cd5\u6536\u655b\u6027\uff0c\u540c\u65f6\u63a8\u5bfc\u4e86\u5e38\u7528\u975e\u5149\u6ed1\u6b63\u5219\u9879\u7684\u95ed\u5f0f\u89e3\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\uff0c\u5c3d\u7ba1\u6bcf\u4e2a\u667a\u80fd\u4f53\u6709\u5404\u81ea\u7684\u76ee\u6807\uff0c\u4f46\u4efb\u52a1\u95f4\u5b58\u5728\u5173\u8054\u65f6\u534f\u4f5c\u6709\u76ca\uff1b\u4f20\u7edf\u65b9\u6cd5\u591a\u91c7\u7528\u5e73\u6ed1\u6b63\u5219\u5316\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u975e\u5149\u6ed1\u6b63\u5219\u5316\u6280\u672f\uff0c\u4ee5\u66f4\u597d\u5730\u4fc3\u8fdb\u56fe\u4e0a\u4efb\u52a1\u95f4\u7684\u5206\u6bb5\u5e38\u6570\u5173\u7cfb\uff0c\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u975e\u5149\u6ed1\u6b63\u5219\u9879\u7684\u5168\u5c40\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u5404\u667a\u80fd\u4f53\u4ee3\u4ef7\u51fd\u6570\u4e4b\u548c\uff0c\u5e76\u57fa\u4e8e\u524d\u5411-\u540e\u5411\u5206\u88c2\u7b56\u7565\u8bbe\u8ba1\u53bb\u4e2d\u5fc3\u5316\u7684\u5b66\u4e60\u7b97\u6cd5\uff1b\u5728\u4ee3\u4ef7\u51fd\u6570\u548c\u534f\u540c\u6b63\u5219\u9879\u51f8\u6027\u7684\u5047\u8bbe\u4e0b\u5206\u6790\u6536\u655b\u6027\uff0c\u5e76\u63a8\u5bfc\u2113\u2080\u3001\u2113\u2081\u548c\u5f39\u6027\u7f51\u7edc\u7b49\u975e\u5149\u6ed1\u6b63\u5219\u9879\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u80fd\u5728O(\u03bc)\u7cbe\u5ea6\u5185\u6536\u655b\u5230\u5168\u5c40\u6b63\u5219\u5316\u4ee3\u4ef7\u7684\u6700\u4f18\u89e3\uff1b\u63a8\u5bfc\u51fa\u591a\u79cd\u5e38\u7528\u975e\u5149\u6ed1\uff08\u53ef\u80fd\u975e\u51f8\uff09\u6b63\u5219\u9879\u7684\u95ed\u5f0f\u66f4\u65b0\u8868\u8fbe\u5f0f\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff1b\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u53ca\u65b9\u6cd5\u5728\u4fc3\u8fdb\u5206\u6bb5\u5e38\u6570\u7ed3\u6784\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u975e\u5149\u6ed1\u6b63\u5219\u5316\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u4efb\u52a1\u95f4\u7684\u5206\u6bb5\u5e38\u6570\u5173\u7cfb\uff0c\u5177\u5907\u826f\u597d\u7684\u6536\u655b\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u7a00\u758f\u5efa\u6a21\u7684\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2509.17748", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17748", "abs": "https://arxiv.org/abs/2509.17748", "authors": ["Siyi Liu", "Kazi Injamamul Haque", "Zerrin Yumak"], "title": "\"I don't like my avatar\": Investigating Human Digital Doubles", "comment": "pre-print, 12 papges, accepted at ACM Siggraph Motion, Interaction\n  and Games 2025 (MIG 2025) conference", "summary": "Creating human digital doubles is becoming easier and much more accessible to\neveryone using consumer grade devices. In this work, we investigate how avatar\nstyle (realistic vs cartoon) and avatar familiarity (self, acquaintance,\nunknown person) affect self/other-identification, perceived realism, affinity\nand social presence with a controlled offline experiment. We created two styles\nof avatars (realistic-looking MetaHumans and cartoon-looking ReadyPlayerMe\navatars) and facial animations stimuli for them using performance capture.\nQuestionnaire responses demonstrate that higher appearance realism leads to a\nhigher level of identification, perceived realism and social presence. However,\navatars with familiar faces, especially those with high appearance realism,\nlead to a lower level of identification, perceived realism, and affinity.\nAlthough participants identified their digital doubles as their own, they\nconsistently did not like their avatars, especially of realistic appearance.\nBut they were less critical and more forgiving about their acquaintance's or an\nunknown person's digital double.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u98ce\u683c\uff08\u5199\u5b9e vs. \u5361\u901a\uff09\u548c\u719f\u6089\u5ea6\uff08\u81ea\u6211\u3001\u719f\u4eba\u3001\u964c\u751f\u4eba\uff09\u7684\u865a\u62df\u5f62\u8c61\u5bf9\u81ea\u6211/\u4ed6\u4eba\u8ba4\u540c\u3001\u611f\u77e5\u771f\u5b9e\u6027\u3001\u4eb2\u548c\u529b\u548c\u793e\u4f1a\u4e34\u573a\u611f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8\u5916\u89c2\u771f\u5b9e\u611f\u63d0\u5347\u8ba4\u540c\u4e0e\u4e34\u573a\u611f\uff0c\u4f46\u719f\u6089\u7684\u9762\u5b54\u53cd\u800c\u964d\u4f4e\u8fd9\u4e9b\u4f53\u9a8c\uff0c\u4e14\u4eba\u4eec\u5bf9\u81ea\u5df1\u7684\u5199\u5b9e\u6570\u5b57\u8eab\u5206\u666e\u904d\u4e0d\u559c\u6b22\u3002", "motivation": "\u968f\u7740\u6d88\u8d39\u7ea7\u8bbe\u5907\u666e\u53ca\uff0c\u521b\u5efa\u4eba\u7c7b\u6570\u5b57\u5206\u8eab\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u4f46\u4e0d\u540c\u98ce\u683c\u548c\u719f\u6089\u5ea6\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u865a\u62df\u5f62\u8c61\u7279\u5f81\u5bf9\u5fc3\u7406\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u79bb\u7ebf\u63a7\u5236\u5b9e\u9a8c\uff0c\u4f7f\u7528\u52a8\u4f5c\u6355\u6349\u6280\u672f\u751f\u6210\u4e24\u79cd\u98ce\u683c\uff08MetaHumans\u5199\u5b9e\u578b\u548cReadyPlayerMe\u5361\u901a\u578b\uff09\u7684\u865a\u62df\u5f62\u8c61\u53ca\u9762\u90e8\u52a8\u753b\uff0c\u7ed3\u5408\u95ee\u5377\u8bc4\u4f30\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u81ea\u6211/\u4ed6\u4eba\u8ba4\u540c\u3001\u611f\u77e5\u771f\u5b9e\u6027\u3001\u4eb2\u548c\u529b\u548c\u793e\u4f1a\u4e34\u573a\u611f\u3002", "result": "\u66f4\u9ad8\u7684\u5916\u89c2\u771f\u5b9e\u611f\u63d0\u5347\u4e86\u8ba4\u540c\u611f\u3001\u611f\u77e5\u771f\u5b9e\u6027\u548c\u793e\u4f1a\u4e34\u573a\u611f\uff1b\u7136\u800c\uff0c\u719f\u6089\u9762\u5b54\uff08\u5c24\u5176\u662f\u9ad8\u771f\u5b9e\u611f\uff09\u53cd\u800c\u964d\u4f4e\u4e86\u8ba4\u540c\u611f\u3001\u611f\u77e5\u771f\u5b9e\u6027\u548c\u4eb2\u548c\u529b\uff1b\u53c2\u4e0e\u8005\u867d\u80fd\u8bc6\u522b\u81ea\u5df1\u7684\u6570\u5b57\u5206\u8eab\uff0c\u4f46\u666e\u904d\u4e0d\u559c\u6b22\u81ea\u5df1\u7684\u5199\u5b9e\u5f62\u8c61\uff0c\u800c\u5bf9\u4ed6\u4eba\u6570\u5b57\u5206\u8eab\u66f4\u5bbd\u5bb9\u3002", "conclusion": "\u865a\u62df\u5f62\u8c61\u7684\u771f\u5b9e\u611f\u867d\u80fd\u589e\u5f3a\u6c89\u6d78\u4f53\u9a8c\uff0c\u4f46\u719f\u6089\u5ea6\u7279\u522b\u662f\u81ea\u6211\u5f62\u8c61\u7684\u771f\u5b9e\u5448\u73b0\u53ef\u80fd\u5f15\u53d1\u8d1f\u9762\u53cd\u5e94\uff0c\u63d0\u793a\u5728\u8bbe\u8ba1\u6570\u5b57\u5206\u8eab\u65f6\u9700\u6743\u8861\u771f\u5b9e\u611f\u4e0e\u7528\u6237\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2509.16421", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16421", "abs": "https://arxiv.org/abs/2509.16421", "authors": ["Aiden Chang", "Celso De Melo", "Stephanie M. Lukin"], "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead", "comment": "Accepted at NeurIPS 2025, 32 pages, 5 figures", "summary": "Real-time understanding of continuous video streams is essential for\nintelligent agents operating in high-stakes environments, including autonomous\nvehicles, surveillance drones, and disaster response robots. Yet, most existing\nvideo understanding and highlight detection methods assume access to the entire\nvideo during inference, making them unsuitable for online or streaming\nscenarios. In particular, current models optimize for offline summarization,\nfailing to support step-by-step reasoning needed for real-time decision-making.\nWe introduce Aha, an autoregressive highlight detection framework that predicts\nthe relevance of each video frame against a task described in natural language.\nWithout accessing future video frames, Aha utilizes a multimodal\nvision-language model and lightweight, decoupled heads trained on a large,\ncurated dataset of human-centric video labels. To enable scalability, we\nintroduce the Dynamic SinkCache mechanism that achieves constant memory usage\nacross infinite-length streams without degrading performance on standard\nbenchmarks. This encourages the hidden representation to capture high-level\ntask objectives, enabling effective frame-level rankings for informativeness,\nrelevance, and uncertainty with respect to the natural language task. Aha\nachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,\nsurpassing even prior offline, full-context approaches and video-language\nmodels by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).\nWe explore Aha's potential for real-world robotics applications given a\ntask-oriented natural language input and a continuous, robot-centric video.\nBoth experiments demonstrate Aha's potential effectiveness as a real-time\nreasoning module for downstream planning and long-horizon understanding.", "AI": {"tldr": "Aha\u662f\u4e00\u4e2a\u81ea\u56de\u5f52\u7684\u9ad8\u5149\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u8bbf\u95ee\u672a\u6765\u89c6\u9891\u5e27\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u65f6\u9884\u6d4b\u6bcf\u4e2a\u89c6\u9891\u5e27\u76f8\u5bf9\u4e8e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u76f8\u5173\u6027\uff0c\u5728TVSum\u548cMr.Hisum\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u591a\u4e3a\u79bb\u7ebf\u5904\u7406\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u51b3\u7b56\u6240\u9700\u7684\u9010\u6b65\u63a8\u7406\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u3001\u65e0\u4eba\u673a\u76d1\u63a7\u7b49\u5728\u7ebf\u573a\u666f\u3002", "method": "\u63d0\u51faAha\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0e\u8f7b\u91cf\u7ea7\u89e3\u8026\u5934\uff0c\u5f15\u5165Dynamic SinkCache\u673a\u5236\u5b9e\u73b0\u65e0\u9650\u957f\u5ea6\u89c6\u9891\u6d41\u7684\u6052\u5b9a\u5185\u5b58\u6d88\u8017\uff0c\u652f\u6301\u6d41\u5f0f\u5b9e\u65f6\u63a8\u7406\u3002", "result": "\u5728TVSum\u548cMr.Hisum\u6570\u636e\u96c6\u4e0a\u5206\u522b\u4ee5+5.9%\u548c+8.3%\u7684mAP\u8d85\u8d8a\u73b0\u6709\u79bb\u7ebf\u65b9\u6cd5\u548c\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u5149\u68c0\u6d4b\u7684SOTA\u6027\u80fd\u3002", "conclusion": "Aha\u80fd\u591f\u6709\u6548\u652f\u6301\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\uff0c\u5177\u5907\u5728\u673a\u5668\u4eba\u7b49\u957f\u65f6\u5e8f\u5e94\u7528\u4e2d\u4f5c\u4e3a\u5b9e\u65f6\u63a8\u7406\u6a21\u5757\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16469", "abs": "https://arxiv.org/abs/2509.16469", "authors": ["Guglielmo Cervettini", "Roberto Mauceri", "Alex Coppola", "Fabio Bergonti", "Luca Fiorio", "Marco Maggiali", "Daniele Pucci"], "title": "A Framework for Optimal Ankle Design of Humanoid Robots", "comment": "This paper has been accepted for publication at the 2025 IEEE-RAS\n  24th International Conference on Humanoid Robots (Humanoids), Seoul, 2025", "summary": "The design of the humanoid ankle is critical for safe and efficient ground\ninteraction. Key factors such as mechanical compliance and motor mass\ndistribution have driven the adoption of parallel mechanism architectures.\nHowever, selecting the optimal configuration depends on both actuator\navailability and task requirements. We propose a unified methodology for the\ndesign and evaluation of parallel ankle mechanisms. A multi-objective\noptimization synthesizes the mechanism geometry, the resulting solutions are\nevaluated using a scalar cost function that aggregates key performance metrics\nfor cross-architecture comparison. We focus on two representative\narchitectures: the Spherical-Prismatic-Universal (SPU) and the\nRevolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and\nfor the RSU, introduce a parameterization that ensures workspace feasibility\nand accelerates optimization. We validate our approach by redesigning the ankle\nof an existing humanoid robot. The optimized RSU consistently outperforms both\nthe original serial design and a conventionally engineered RSU, reducing the\ncost function by up to 41% and 14%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u8e1d\u5173\u8282\u5e76\u8054\u673a\u6784\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u548c\u6807\u91cf\u6210\u672c\u51fd\u6570\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\u6027\u80fd\uff0c\u91cd\u70b9\u7814\u7a76SPU\u548cRSU\u4e24\u79cd\u7ed3\u6784\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4f18\u5316\u540e\u7684RSU\u5728\u6210\u672c\u51fd\u6570\u4e0a\u8f83\u539f\u6709\u4e32\u884c\u8bbe\u8ba1\u548c\u5176\u4ed6\u5de5\u7a0b\u8bbe\u8ba1\u964d\u4f4e\u6700\u591a\u8fbe41%\u548c14%\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u8e1d\u5173\u8282\u7684\u8bbe\u8ba1\u5bf9\u5b89\u5168\u9ad8\u6548\u5730\u4e0e\u5730\u9762\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u673a\u68b0\u987a\u5e94\u6027\u548c\u7535\u673a\u8d28\u91cf\u5206\u5e03\u7b49\u56e0\u7d20\u4fc3\u4f7f\u91c7\u7528\u5e76\u8054\u673a\u6784\u67b6\u6784\uff0c\u4f46\u6700\u4f18\u6784\u578b\u7684\u9009\u62e9\u4f9d\u8d56\u4e8e\u6267\u884c\u5668\u53ef\u7528\u6027\u548c\u4efb\u52a1\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u5e76\u8054\u8e1d\u5173\u8282\u673a\u6784\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u7efc\u5408\u673a\u6784\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u805a\u5408\u5173\u952e\u6027\u80fd\u6307\u6807\u7684\u6807\u91cf\u6210\u672c\u51fd\u6570\u8fdb\u884c\u8de8\u67b6\u6784\u6bd4\u8f83\uff1b\u9488\u5bf9SPU\u548cRSU\u4e24\u79cd\u5178\u578b\u7ed3\u6784\u8fdb\u884c\u8fd0\u52a8\u5b66\u5206\u6790\uff0c\u5e76\u4e3aRSU\u5f15\u5165\u786e\u4fdd\u5de5\u4f5c\u7a7a\u95f4\u53ef\u884c\u6027\u548c\u52a0\u901f\u4f18\u5316\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u73b0\u6709\u673a\u5668\u4eba\u8e1d\u5173\u8282\u91cd\u8bbe\u8ba1\u4e2d\u9a8c\u8bc1\u6240\u63d0\u65b9\u6cd5\uff0c\u4f18\u5316\u540e\u7684RSU\u7ed3\u6784\u76f8\u6bd4\u539f\u59cb\u4e32\u884c\u8bbe\u8ba1\u548c\u4f20\u7edf\u5de5\u7a0b\u8bbe\u8ba1\u7684\u6210\u672c\u51fd\u6570\u5206\u522b\u6700\u591a\u964d\u4f4e41%\u548c14%\uff0c\u8868\u73b0\u51fa\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u8bbe\u8ba1\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u5e76\u8054\u8e1d\u5173\u8282\u7ed3\u6784\uff0cRSU\u67b6\u6784\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8bbe\u8ba1\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16326", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16326", "abs": "https://arxiv.org/abs/2509.16326", "authors": ["Yunsoo Kim", "Michal W. S. Ong", "Alex Shavick", "Honghan Wu", "Adam P. Levine"], "title": "HARE: an entity and relation centric evaluation framework for histopathology reports", "comment": "Accepted to EMNLP2025 Findings", "summary": "Medical domain automated text generation is an active area of research and\ndevelopment; however, evaluating the clinical quality of generated reports\nremains a challenge, especially in instances where domain-specific metrics are\nlacking, e.g. histopathology. We propose HARE (Histopathology Automated Report\nEvaluation), a novel entity and relation centric framework, composed of a\nbenchmark dataset, a named entity recognition (NER) model, a relation\nextraction (RE) model, and a novel metric, which prioritizes clinically\nrelevant content by aligning critical histopathology entities and relations\nbetween reference and generated reports. To develop the HARE benchmark, we\nannotated 813 de-identified clinical diagnostic histopathology reports and 652\nhistopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific\nentities and relations. We fine-tuned GatorTronS, a domain-adapted language\nmodel to develop HARE-NER and HARE-RE which achieved the highest overall\nF1-score (0.915) among the tested models. The proposed HARE metric outperformed\ntraditional metrics including ROUGE and Meteor, as well as radiology metrics\nsuch as RadGraph-XL, with the highest correlation and the best regression to\nexpert evaluations (higher than the second best method, GREEN, a large language\nmodel based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\\rho\n= 0.161$, Kendall $\\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release\nHARE, datasets, and the models at https://github.com/knowlab/HARE to foster\nadvancements in histopathology report generation, providing a robust framework\nfor improving the quality of reports.", "AI": {"tldr": "\u63d0\u51faHARE\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u75c5\u7406\u5b66\u62a5\u544a\u751f\u6210\u8d28\u91cf\uff0c\u901a\u8fc7\u5b9e\u4f53\u548c\u5173\u7cfb\u63d0\u53d6\u5b9e\u73b0\u5bf9\u4e34\u5e8a\u76f8\u5173\u5185\u5bb9\u7684\u7cbe\u51c6\u8bc4\u4f30\uff0c\u5e76\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u3002", "motivation": "\u7f3a\u4e4f\u9488\u5bf9\u75c5\u7406\u5b66\u62a5\u544a\u751f\u6210\u8d28\u91cf\u7684\u9886\u57df\u7279\u5b9a\u8bc4\u4f30\u6307\u6807\uff0c\u96be\u4ee5\u51c6\u786e\u8861\u91cf\u751f\u6210\u62a5\u544a\u7684\u4e34\u5e8a\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u5305\u542b\u6807\u6ce8\u6570\u636e\u96c6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6a21\u578b\u3001\u5173\u7cfb\u62bd\u53d6\uff08RE\uff09\u6a21\u578b\u53ca\u65b0\u8bc4\u4f30\u6307\u6807\u7684HARE\u6846\u67b6\uff0c\u57fa\u4e8eGatorTronS\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u5b9e\u4f53\u4e0e\u5173\u7cfb\u63d0\u53d6\u3002", "result": "HARE-NER\u548cHARE-RE\u5728F1\u5206\u6570\u4e0a\u8fbe\u52300.915\uff1bHARE\u6307\u6807\u5728\u4e0e\u4e13\u5bb6\u8bc4\u5206\u7684\u76f8\u5173\u6027\u4e0a\u663e\u8457\u4f18\u4e8eROUGE\u3001METEOR\u3001RadGraph-XL\u548cGREEN\u7b49\u65b9\u6cd5\u3002", "conclusion": "HARE\u4e3a\u75c5\u7406\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.16300", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16300", "abs": "https://arxiv.org/abs/2509.16300", "authors": ["Manh Cuong Dao", "The Hung Tran", "Phi Le Nguyen", "Thao Nguyen Truong", "Trong Nghia Hoang"], "title": "ROOT: Rethinking Offline Optimization as Distributional Translation via Probabilistic Bridge", "comment": "The first two authors contributed equally", "summary": "This paper studies the black-box optimization task which aims to find the\nmaxima of a black-box function using a static set of its observed input-output\npairs. This is often achieved via learning and optimizing a surrogate function\nwith that offline data. Alternatively, it can also be framed as an inverse\nmodeling task that maps a desired performance to potential input candidates\nthat achieve it. Both approaches are constrained by the limited amount of\noffline data. To mitigate this limitation, we introduce a new perspective that\ncasts offline optimization as a distributional translation task. This is\nformulated as learning a probabilistic bridge transforming an implicit\ndistribution of low-value inputs (i.e., offline data) into another distribution\nof high-value inputs (i.e., solution candidates). Such probabilistic bridge can\nbe learned using low- and high-value inputs sampled from synthetic functions\nthat resemble the target function. These synthetic functions are constructed as\nthe mean posterior of multiple Gaussian processes fitted with different\nparameterizations on the offline data, alleviating the data bottleneck. The\nproposed approach is evaluated on an extensive benchmark comprising most recent\nmethods, demonstrating significant improvement and establishing a new\nstate-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u79bb\u7ebf\u4f18\u5316\u89c6\u4e3a\u5206\u5e03\u8f6c\u6362\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4ece\u4f4e\u4ef7\u503c\u8f93\u5165\u5206\u5e03\u5230\u9ad8\u4ef7\u503c\u8f93\u5165\u5206\u5e03\u7684\u6982\u7387\u6865\u6881\uff0c\u5728\u6709\u9650\u6570\u636e\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u9ed1\u76d2\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u79bb\u7ebf\u6570\u636e\u91cf\u5c11\uff0c\u96be\u4ee5\u6709\u6548\u5b66\u4e60 surrogate \u51fd\u6570\u6216\u9006\u6a21\u578b\u3002", "method": "\u5c06\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u5206\u5e03\u8f6c\u6362\u4efb\u52a1\uff0c\u5229\u7528\u591a\u4e2a\u9ad8\u65af\u8fc7\u7a0b\u7684\u540e\u9a8c\u5747\u503c\u6784\u5efa\u7c7b\u4f3c\u76ee\u6807\u51fd\u6570\u7684\u5408\u6210\u51fd\u6570\uff0c\u4ece\u4e2d\u91c7\u6837\u9ad8\u4f4e\u503c\u8f93\u5165\u4ee5\u8bad\u7ec3\u6982\u7387\u6865\u6881\u6a21\u578b\u3002", "result": "\u5728\u5305\u542b\u6700\u65b0\u65b9\u6cd5\u7684\u5e7f\u6cdb\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u8f6c\u6362\u89c6\u89d2\u6709\u6548\u7f13\u89e3\u4e86\u79bb\u7ebf\u6570\u636e\u74f6\u9888\uff0c\u4e3a\u9ed1\u76d2\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u65b0\u9896\u7684\u6846\u67b6\u3002"}}
{"id": "2509.16372", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16372", "abs": "https://arxiv.org/abs/2509.16372", "authors": ["Balu Bhasuran", "Mattia Prosperi", "Karim Hanna", "John Petrilli", "Caretia JeLayne Washington", "Zhe He"], "title": "Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation", "comment": null, "summary": "This study evaluates causal reasoning in large language models (LLMs) using\n99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of\nCausation: association, intervention, and counterfactual reasoning. We examined\ncommon laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and\npaired them with relevant causal factors including age, gender, obesity, and\nsmoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with\nresponses evaluated by four medically trained human experts. GPT-o1\ndemonstrated stronger discriminative performance (AUROC overall = 0.80 +/-\n0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores\nacross association (0.75 vs 0.72), intervention (0.84 vs 0.70), and\ncounterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and\nspecificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings\nshowing similar trends. Both models performed best on intervention questions\nand worst on counterfactuals, particularly in altered outcome scenarios. These\nfindings suggest GPT-o1 provides more consistent causal reasoning, but\nrefinement is required before adoption in high-stakes clinical applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u752899\u4e2a\u4e34\u5e8a\u76f8\u5173\u7684\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\u573a\u666f\uff0c\u8bc4\u4f30\u4e86GPT-o1\u548cLlama-3.2-8b-instruct\u4e24\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\uff08\u57fa\u4e8ePearl\u56e0\u679c\u9636\u68af\uff1a\u5173\u8054\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793aGPT-o1\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8eLlama-3.2-8b-instruct\uff0c\u4f46\u4e24\u7c7b\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u5c1a\u9700\u6539\u8fdb\u624d\u80fd\u7528\u4e8e\u9ad8\u98ce\u9669\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728Pearl\u56e0\u679c\u9636\u68af\u7684\u4e09\u4e2a\u5c42\u7ea7\uff08\u5173\u8054\u3001\u5e72\u9884\u3001\u53cd\u4e8b\u5b9e\uff09\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4e3a\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u6784\u5efa99\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\uff08\u5982\u7cd6\u5316\u8840\u7ea2\u86cb\u767d\u3001\u808c\u9150\u3001\u7ef4\u751f\u7d20D\uff09\u7684\u56e0\u679c\u63a8\u7406\u573a\u666f\uff0c\u7ed3\u5408\u5e74\u9f84\u3001\u6027\u522b\u3001\u80a5\u80d6\u3001\u5438\u70df\u7b49\u56e0\u679c\u56e0\u7d20\uff0c\u6d4b\u8bd5GPT-o1\u548cLlama-3.2-8b-instruct\u4e24\u4e2a\u6a21\u578b\uff0c\u7531\u56db\u4f4d\u533b\u5b66\u4e13\u5bb6\u5bf9\u56de\u7b54\u8fdb\u884c\u8bc4\u5206\u548c\u5206\u6790\u3002", "result": "GPT-o1\u5728AUROC\uff080.80 vs 0.73\uff09\u3001\u654f\u611f\u6027\uff080.90 vs 0.84\uff09\u3001\u7279\u5f02\u6027\uff080.93 vs 0.80\uff09\u53ca\u5404\u5c42\u7ea7\u56e0\u679c\u63a8\u7406\u5f97\u5206\u4e0a\u5747\u4f18\u4e8eLlama-3.2-8b-instruct\uff1b\u4e24\u6a21\u578b\u5728\u5e72\u9884\u7c7b\u95ee\u9898\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u53cd\u4e8b\u5b9e\u7c7b\u6700\u5dee\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u679c\u6539\u53d8\u7684\u60c5\u5883\u4e0b\u3002", "conclusion": "GPT-o1\u5c55\u73b0\u51fa\u66f4\u4e00\u81f4\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e0a\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u624d\u53ef\u7528\u4e8e\u9ad8\u98ce\u9669\u4e34\u5e8a\u73af\u5883\u3002"}}
{"id": "2509.17359", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17359", "abs": "https://arxiv.org/abs/2509.17359", "authors": ["Tianyuan Li", "Lei Wang", "Ahtamjan Ahmat", "Yating Yang", "Bo Ma", "Rui Dong", "Bangju Han"], "title": "MLLM-Driven Semantic Identifier Generation for Generative Cross-Modal Retrieval", "comment": null, "summary": "Generative cross-modal retrieval, which treats retrieval as a generation\ntask, has emerged as a promising direction with the rise of Multimodal Large\nLanguage Models (MLLMs). In this setting, the model responds to a text query by\ngenerating an identifier corresponding to the target image. However, existing\nmethods typically rely on manually crafted string IDs, clustering-based labels,\nor atomic identifiers requiring vocabulary expansion, all of which face\nchallenges in semantic alignment or scalability.To address these limitations,\nwe propose a vocabulary-efficient identifier generation framework that prompts\nMLLMs to generate Structured Semantic Identifiers from image-caption pairs.\nThese identifiers are composed of concept-level tokens such as objects and\nactions, naturally aligning with the model's generation space without modifying\nthe tokenizer. Additionally, we introduce a Rationale-Guided Supervision\nStrategy, prompting the model to produce a one-sentence explanation alongside\neach identifier serves as an auxiliary supervision signal that improves\nsemantic grounding and reduces hallucinations during training.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u8bed\u4e49\u6807\u8bc6\u7b26\u7684\u751f\u6210\u5f0f\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7531\u6982\u5ff5\u7ea7\u8bcd\u6c47\u7ec4\u6210\u7684\u56fe\u50cf\u6807\u8bc6\uff0c\u5e76\u5f15\u5165\u7406\u7531\u5f15\u5bfc\u76d1\u7763\u7b56\u7565\u4ee5\u63d0\u5347\u8bed\u4e49\u5bf9\u9f50\u548c\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u6216\u539f\u5b50\u6807\u8bc6\u7b26\uff0c\u5b58\u5728\u8bed\u4e49\u5bf9\u9f50\u56f0\u96be\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u7ed3\u6784\u5316\u8bed\u4e49\u6807\u8bc6\u7b26\uff0c\u7531\u5bf9\u8c61\u548c\u52a8\u4f5c\u7b49\u6982\u5ff5\u7ea7token\u7ec4\u6210\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\uff1b\u5f15\u5165\u7406\u7531\u5f15\u5bfc\u76d1\u7763\u7b56\u7565\uff0c\u8981\u6c42\u6a21\u578b\u5728\u751f\u6210\u6807\u8bc6\u7b26\u7684\u540c\u65f6\u751f\u6210\u4e00\u53e5\u8bdd\u89e3\u91ca\u4f5c\u4e3a\u8f85\u52a9\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u4e0d\u4fee\u6539\u5206\u8bcd\u5668\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u751f\u6210\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u8bcd\u6c47\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u5f0f\u8de8\u6a21\u6001\u68c0\u7d22\u4e2d\u6807\u8bc6\u7b26\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u57fa\u4e8eMLLM\u7684\u68c0\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17757", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.17757", "abs": "https://arxiv.org/abs/2509.17757", "authors": ["Hongxing Fan", "Lipeng Wang", "Haohua Chen", "Zehuan Huang", "Jiangtao Wu", "Lu Sheng"], "title": "Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance", "comment": null, "summary": "Amodal completion, generating invisible parts of occluded objects, is vital\nfor applications like image editing and AR. Prior methods face challenges with\ndata needs, generalization, or error accumulation in progressive pipelines. We\npropose a Collaborative Multi-Agent Reasoning Framework based on upfront\ncollaborative reasoning to overcome these issues. Our framework uses multiple\nagents to collaboratively analyze occlusion relationships and determine\nnecessary boundary expansion, yielding a precise mask for inpainting.\nConcurrently, an agent generates fine-grained textual descriptions, enabling\nFine-Grained Semantic Guidance. This ensures accurate object synthesis and\nprevents the regeneration of occluders or other unwanted elements, especially\nwithin large inpainting areas. Furthermore, our method directly produces\nlayered RGBA outputs guided by visible masks and attention maps from a\nDiffusion Transformer, eliminating extra segmentation. Extensive evaluations\ndemonstrate our framework achieves state-of-the-art visual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u540c\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u906e\u6321\u7269\u4f53\u90e8\u5206\u751f\u6210\u4e2d\u7684\u6570\u636e\u9700\u6c42\u3001\u6cdb\u5316\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u9700\u6c42\u3001\u6cdb\u5316\u80fd\u529b\u6216\u6e10\u8fdb\u5f0f\u6d41\u6c34\u7ebf\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4e14\u96be\u4ee5\u51c6\u786e\u751f\u6210\u88ab\u906e\u6321\u7269\u4f53\u7684\u4e0d\u53ef\u89c1\u90e8\u5206\u3002", "method": "\u91c7\u7528\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u540c\u5206\u6790\u906e\u6321\u5173\u7cfb\u5e76\u786e\u5b9a\u8fb9\u754c\u6269\u5c55\uff0c\u751f\u6210\u7cbe\u786e\u7684\u4fee\u590d\u63a9\u7801\uff1b\u540c\u65f6\u4e00\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u4ee5\u63d0\u4f9b\u8bed\u4e49\u6307\u5bfc\uff0c\u5e76\u7ed3\u5408\u53ef\u89c1\u63a9\u7801\u548cDiffusion Transformer\u7684\u6ce8\u610f\u529b\u56fe\u76f4\u63a5\u751f\u6210\u5206\u5c42RGBA\u8f93\u51fa\u3002", "result": "\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u80fd\u51c6\u786e\u5408\u6210\u7269\u4f53\u5e76\u907f\u514d\u906e\u6321\u7269\u8bef\u751f\u6210\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u533a\u57df\u4fee\u590d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u540c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u906e\u6321\u8865\u5168\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u65e0\u9700\u989d\u5916\u5206\u5272\u6b65\u9aa4\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u5206\u5c42\u56fe\u50cf\u8f93\u51fa\u3002"}}
{"id": "2509.17803", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17803", "abs": "https://arxiv.org/abs/2509.17803", "authors": ["Nabila Amadou", "Kazi Injamamul Haque", "Zerrin Yumak"], "title": "Effect of Appearance and Animation Realism on the Perception of Emotionally Expressive Virtual Humans", "comment": "pre-print, 8 pages, accepted at ACM International Conference on\n  Intelligent Virtual Agents 2023 (IVA 2023)", "summary": "3D Virtual Human technology is growing with several potential applications in\nhealth, education, business and telecommunications. Investigating the\nperception of these virtual humans can help guide to develop better and more\neffective applications. Recent developments show that the appearance of the\nvirtual humans reached to a very realistic level. However, there is not yet\nadequate analysis on the perception of appearance and animation realism for\nemotionally expressive virtual humans. In this paper, we designed a user\nexperiment and analyzed the effect of a realistic virtual human's appearance\nrealism and animation realism in varying emotion conditions. We found that\nhigher appearance realism and higher animation realism leads to higher social\npresence and higher attractiveness ratings. We also found significant effects\nof animation realism on perceived realism and emotion intensity levels. Our\nstudy sheds light into how appearance and animation realism effects the\nperception of highly realistic virtual humans in emotionally expressive\nscenarios and points out to future directions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5916\u89c2\u771f\u5b9e\u611f\u548c\u52a8\u753b\u771f\u5b9e\u611f\u5bf9\u60c5\u611f\u8868\u8fbe\u578b\u865a\u62df\u4eba\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e24\u8005\u5747\u663e\u8457\u5f71\u54cd\u793e\u4f1a\u5b58\u5728\u611f\u3001\u5438\u5f15\u529b\u3001\u611f\u77e5\u771f\u5b9e\u611f\u548c\u60c5\u7eea\u5f3a\u5ea6\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5916\u89c2\u548c\u52a8\u753b\u771f\u5b9e\u611f\u5728\u60c5\u611f\u8868\u8fbe\u578b\u865a\u62df\u4eba\u4e2d\u7684\u611f\u77e5\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7a76\u5176\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u9879\u7528\u6237\u5b9e\u9a8c\uff0c\u5206\u6790\u5728\u4e0d\u540c\u60c5\u7eea\u6761\u4ef6\u4e0b\uff0c\u9ad8\u771f\u5b9e\u611f\u865a\u62df\u4eba\u7684\u5916\u89c2\u771f\u5b9e\u611f\u548c\u52a8\u753b\u771f\u5b9e\u611f\u5bf9\u5176\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u66f4\u9ad8\u7684\u5916\u89c2\u548c\u52a8\u753b\u771f\u5b9e\u611f\u5e26\u6765\u66f4\u9ad8\u7684\u793e\u4f1a\u5b58\u5728\u611f\u548c\u5438\u5f15\u529b\uff1b\u52a8\u753b\u771f\u5b9e\u611f\u5bf9\u611f\u77e5\u771f\u5b9e\u611f\u548c\u60c5\u7eea\u5f3a\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5916\u89c2\u548c\u52a8\u753b\u771f\u5b9e\u611f\u663e\u8457\u5f71\u54cd\u9ad8\u5ea6\u771f\u5b9e\u865a\u62df\u4eba\u5728\u60c5\u611f\u8868\u8fbe\u573a\u666f\u4e2d\u7684\u611f\u77e5\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u865a\u62df\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.16423", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16423", "abs": "https://arxiv.org/abs/2509.16423", "authors": ["Maria Taktasheva", "Lily Goli", "Alessandro Fiorini", "Zhen", "Li", "Daniel Rebain", "Andrea Tagliasacchi"], "title": "3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction", "comment": null, "summary": "Recent advances in radiance fields and novel view synthesis enable creation\nof realistic digital twins from photographs. However, current methods struggle\nwith flat, texture-less surfaces, creating uneven and semi-transparent\nreconstructions, due to an ill-conditioned photometric reconstruction\nobjective. Surface reconstruction methods solve this issue but sacrifice visual\nquality. We propose a novel hybrid 2D/3D representation that jointly optimizes\nconstrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)\nGaussians for the rest of the scene. Our end-to-end approach dynamically\ndetects and refines planar regions, improving both visual fidelity and\ngeometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++\nand ScanNetv2, and excels at mesh extraction without overfitting to a specific\ncamera model, showing its effectiveness in producing high-quality\nreconstruction of indoor scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54082D\u548c3D\u9ad8\u65af\u7684\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u65e0\u7eb9\u7406\u5e73\u9762\u533a\u57df\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5ba4\u5185\u573a\u666f\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u8f90\u5c04\u573a\u65b9\u6cd5\u5728\u5904\u7406\u65e0\u7eb9\u7406\u3001\u5e73\u5766\u8868\u9762\u65f6\u56e0\u5149\u5ea6\u91cd\u5efa\u76ee\u6807\u4e0d\u9002\u5b9a\u800c\u4ea7\u751f\u4e0d\u5747\u5300\u3001\u534a\u900f\u660e\u91cd\u5efa\u7ed3\u679c\uff0c\u800c\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u867d\u89e3\u51b3\u6b64\u95ee\u9898\u5374\u727a\u7272\u4e86\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8054\u5408\u4f18\u5316\u7684\u6df7\u54082D/3D\u8868\u793a\u65b9\u6cd5\uff1a\u4f7f\u7528\u53d7\u7ea6\u675f\u7684\u5e73\u9762\uff082D\uff09\u9ad8\u65af\u5efa\u6a21\u5e73\u5766\u533a\u57df\uff0c\u81ea\u7531\u5f62\u6001\uff083D\uff09\u9ad8\u65af\u5efa\u6a21\u5176\u4f59\u573a\u666f\uff0c\u5e76\u5728\u7aef\u5230\u7aef\u6846\u67b6\u4e2d\u52a8\u6001\u68c0\u6d4b\u548c\u4f18\u5316\u5e73\u9762\u533a\u57df\u3002", "result": "\u5728ScanNet++\u548cScanNetv2\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u5728\u65e0\u9700\u8fc7\u62df\u5408\u7279\u5b9a\u76f8\u673a\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u7684\u7f51\u683c\u63d0\u53d6\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u7eb9\u7406\u5e73\u9762\u91cd\u5efa\u96be\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u8d28\u91cf\u5ba4\u5185\u573a\u666f\u6570\u5b57\u5b6a\u751f\u6784\u5efa\u3002"}}
{"id": "2509.16482", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.DS", "physics.app-ph", "49", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.16482", "abs": "https://arxiv.org/abs/2509.16482", "authors": ["Pranav Tiwari", "Soumyodipta Nath"], "title": "Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems", "comment": "6 Pages, 8 Figures. First two authors have contributed equally", "summary": "Coordinated path following in multi-agent systems is a key challenge in\nrobotics, with applications in automated logistics, surveillance, and\ncollaborative exploration. Traditional formation control techniques often rely\non time-parameterized trajectories and path integrals, which can result in\nsynchronization issues and rigid behavior. In this work, we address the problem\nof sequential path following, where agents maintain fixed spatial separation\nalong a common trajectory, guided by a leader under centralized control. We\nintroduce Robot Conga, a leader-follower control strategy that updates each\nagent's desired state based on the leader's spatial displacement rather than\ntime, assuming access to a global position reference, an assumption valid in\nindoor environments equipped with motion capture, vision-based tracking, or UWB\nlocalization systems. The algorithm was validated in simulation using both\nTurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate\ntrajectory tracking, stable inter-agent spacing, and fast convergence, with all\nagents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped\ncase, and almost instantaneously in the TurtleBot3 implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRobot Conga\u7684\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u987a\u5e8f\u8def\u5f84\u8ddf\u8e2a\uff0c\u901a\u8fc7\u57fa\u4e8e\u9886\u5bfc\u8005\u7684\u7a7a\u95f4\u4f4d\u79fb\u800c\u975e\u65f6\u95f4\u6765\u66f4\u65b0\u6bcf\u4e2a\u4ee3\u7406\u7684\u671f\u671b\u72b6\u6001\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u7a33\u5b9a\u7684\u4ee3\u7406\u95f4\u95f4\u8ddd\u3002", "motivation": "\u4f20\u7edf\u7f16\u961f\u63a7\u5236\u6280\u672f\u4f9d\u8d56\u4e8e\u65f6\u95f4\u53c2\u6570\u5316\u8f68\u8ff9\u548c\u8def\u5f84\u79ef\u5206\uff0c\u53ef\u80fd\u5bfc\u81f4\u540c\u6b65\u95ee\u9898\u548c\u50f5\u786c\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u8def\u5f84\u8ddf\u968f\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Robot Conga\u63a7\u5236\u7b56\u7565\uff0c\u5229\u7528\u5168\u5c40\u4f4d\u7f6e\u53c2\u8003\uff0c\u6839\u636e\u9886\u5bfc\u8005\u7684\u7a7a\u95f4\u4f4d\u79fb\u66f4\u65b0\u5404\u4ee3\u7406\u7684\u671f\u671b\u72b6\u6001\uff0c\u5b9e\u73b0\u4ee3\u7406\u95f4\u7684\u534f\u8c03\u8fd0\u52a8\u3002", "result": "\u5728TurtleBot3\u548c\u56db\u8db3\u673a\u5668\u4eba\uff08Laikago\uff09\u4e0a\u7684\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u51c6\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3001\u7a33\u5b9a\u7684\u4ee3\u7406\u95f4\u8ddd\u79bb\uff0c\u5e76\u5728250\u4e2a\u65f6\u95f4\u6b65\u5185\u5feb\u901f\u6536\u655b\u3002", "conclusion": "Robot Conga\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u987a\u5e8f\u8def\u5f84\u8ddf\u968f\u95ee\u9898\uff0c\u5177\u6709\u5feb\u901f\u6536\u655b\u6027\u548c\u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u652f\u6301\u7684\u73af\u5883\u3002"}}
{"id": "2509.16360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16360", "abs": "https://arxiv.org/abs/2509.16360", "authors": ["Weikang Qiu", "Tinglin Huang", "Ryan Rullo", "Yucheng Kuang", "Ali Maatouk", "S. Raquel Ramos", "Rex Ying"], "title": "RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering", "comment": "ACM KDD Health Track 2025 Blue Sky Best Paper", "summary": "Large Language Models (LLMs) hold promise in addressing complex medical\nproblems. However, while most prior studies focus on improving accuracy and\nreasoning abilities, a significant bottleneck in developing effective\nhealthcare agents lies in the readability of LLM-generated responses,\nspecifically, their ability to answer public health problems clearly and simply\nto people without medical backgrounds. In this work, we introduce RephQA, a\nbenchmark for evaluating the readability of LLMs in public health question\nanswering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across\n13 topics, and includes a proxy multiple-choice task to assess informativeness,\nalong with two readability metrics: Flesch-Kincaid grade level and professional\nscore. Evaluation of 25 LLMs reveals that most fail to meet readability\nstandards, highlighting a gap between reasoning and effective communication. To\naddress this, we explore four readability-enhancing strategies-standard\nprompting, chain-of-thought prompting, Group Relative Policy Optimization\n(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best\nresults, advancing the development of more practical and user-friendly public\nhealth agents. These results represent a step toward building more practical\nagents for public health.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RephQA\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u516c\u5171\u536b\u751f\u95ee\u7b54\u4e2d\u53ef\u8bfb\u6027\u7684\u57fa\u51c6\uff0c\u5e76\u63a2\u7d22\u4e86\u63d0\u5347\u6a21\u578b\u53ef\u8bfb\u6027\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u57fa\u4e8e\u4ee4\u724c\u9002\u914d\u7684GRPO\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u63d0\u5347LLM\u5728\u533b\u7597\u95ee\u9898\u4e2d\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5ffd\u89c6\u4e86\u5176\u56de\u7b54\u5bf9\u975e\u533b\u5b66\u80cc\u666f\u516c\u4f17\u7684\u53ef\u8bfb\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u516c\u5171\u536b\u751f\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b533\u4e2a\u4e13\u5bb6\u5ba1\u6838\u95ee\u7b54\u5bf9\u7684RephQA\u57fa\u51c6\uff0c\u6db5\u76d613\u4e2a\u4e3b\u9898\uff0c\u5e76\u91c7\u7528Flesch-Kincaid\u5e74\u7ea7\u6c34\u5e73\u548c\u4e13\u4e1a\u8bc4\u5206\u4e24\u4e2a\u6307\u6807\u8bc4\u4f3025\u4e2aLLM\u7684\u53ef\u8bfb\u6027\uff1b\u63d0\u51fa\u56db\u79cd\u63d0\u5347\u53ef\u8bfb\u6027\u7684\u7b56\u7565\uff1a\u6807\u51c6\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u63d0\u793a\u3001\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u53ca\u4ee4\u724c\u9002\u914dGRPO\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5927\u591a\u6570LLM\u672a\u80fd\u8fbe\u5230\u53ef\u8bfb\u6027\u6807\u51c6\uff1b\u4ee4\u724c\u9002\u914d\u7684GRPO\u5728\u63d0\u5347\u53ef\u8bfb\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u4fe1\u606f\u91cf\u3002", "conclusion": "\u63d0\u5347LLM\u8f93\u51fa\u7684\u53ef\u8bfb\u6027\u5bf9\u4e8e\u53d1\u5c55\u5b9e\u7528\u7684\u516c\u5171\u536b\u751f\u667a\u80fd\u4f53\u81f3\u5173\u91cd\u8981\uff0c\u4ee4\u724c\u9002\u914dGRPO\u4e3a\u5b9e\u73b0\u66f4\u7528\u6237\u53cb\u597d\u7684\u5065\u5eb7\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2509.16324", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.16324", "abs": "https://arxiv.org/abs/2509.16324", "authors": ["Jiale Han", "Chun Gan", "Chengcheng Zhang", "Jie He", "Zhangang Lin", "Ching Law", "Xiaowu Dai"], "title": "Auto-bidding under Return-on-Spend Constraints with Uncertainty Quantification", "comment": null, "summary": "Auto-bidding systems are widely used in advertising to automatically\ndetermine bid values under constraints such as total budget and Return-on-Spend\n(RoS) targets. Existing works often assume that the value of an ad impression,\nsuch as the conversion rate, is known. This paper considers the more realistic\nscenario where the true value is unknown. We propose a novel method that uses\nconformal prediction to quantify the uncertainty of these values based on\nmachine learning methods trained on historical bidding data with contextual\nfeatures, without assuming the data are i.i.d. This approach is compatible with\ncurrent industry systems that use machine learning to predict values. Building\non prediction intervals, we introduce an adjusted value estimator derived from\nmachine learning predictions, and show that it provides performance guarantees\nwithout requiring knowledge of the true value. We apply this method to enhance\nexisting auto-bidding algorithms with budget and RoS constraints, and establish\ntheoretical guarantees for achieving high reward while keeping RoS violations\nlow. Empirical results on both simulated and real-world industrial datasets\ndemonstrate that our approach improves performance while maintaining\ncomputational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u65b0\u578b\u81ea\u52a8\u51fa\u4ef7\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u9884\u7b97\u548c\u6295\u8d44\u56de\u62a5\u7387\uff08RoS\uff09\u7ea6\u675f\u4e0b\u91cf\u5316\u5e7f\u544a\u5c55\u793a\u4ef7\u503c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u5047\u8bbe\u6570\u636e\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u4e14\u517c\u5bb9\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u51fa\u4ef7\u7cfb\u7edf\u901a\u5e38\u5047\u8bbe\u5e7f\u544a\u5c55\u793a\u7684\u4ef7\u503c\uff08\u5982\u8f6c\u5316\u7387\uff09\u5df2\u77e5\uff0c\u4f46\u73b0\u5b9e\u4e2d\u8fd9\u4e00\u503c\u5f80\u5f80\u662f\u672a\u77e5\u7684\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u8fdb\u884c\u6709\u6548\u51fa\u4ef7\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5171\u5f62\u9884\u6d4b\u6280\u672f\uff0c\u7ed3\u5408\u5386\u53f2\u51fa\u4ef7\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u6784\u5efa\u4e0d\u4f9d\u8d56i.i.d.\u5047\u8bbe\u7684\u4ef7\u503c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff1b\u57fa\u4e8e\u9884\u6d4b\u533a\u95f4\u63d0\u51fa\u8c03\u6574\u540e\u7684\u4ef7\u503c\u4f30\u8ba1\u91cf\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u73b0\u6709\u81ea\u52a8\u51fa\u4ef7\u7b97\u6cd5\u4e2d\u4ee5\u5904\u7406\u9884\u7b97\u548cRoS\u7ea6\u675f\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5728\u4fdd\u8bc1\u4f4eRoS\u8fdd\u89c4\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6536\u76ca\uff1b\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u9ad8\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u4ef7\u503c\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u51fa\u4ef7\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6548\u679c\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.16399", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16399", "abs": "https://arxiv.org/abs/2509.16399", "authors": ["Guojun Xiong", "Milind Tambe"], "title": "VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping", "comment": "28pages, 19figures", "summary": "In social impact optimization, AI decision systems often rely on solvers that\noptimize well-calibrated mathematical objectives. However, these solvers cannot\ndirectly accommodate evolving human preferences, typically expressed in natural\nlanguage rather than formal constraints. Recent approaches address this by\nusing large language models (LLMs) to generate new reward functions from\npreference descriptions. While flexible, they risk sacrificing the system's\ncore utility guarantees. In this paper, we propose \\texttt{VORTEX}, a\nlanguage-guided reward shaping framework that preserves established\noptimization goals while adaptively incorporating human feedback. By\nformalizing the problem as multi-objective optimization, we use LLMs to\niteratively generate shaping rewards based on verbal reinforcement and\ntext-gradient prompt updates. This allows stakeholders to steer decision\nbehavior via natural language without modifying solvers or specifying trade-off\nweights. We provide theoretical guarantees that \\texttt{VORTEX} converges to\nPareto-optimal trade-offs between utility and preference satisfaction.\nEmpirical results in real-world allocation tasks demonstrate that\n\\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage\ngoals while maintaining high task performance. This work introduces a practical\nand theoretically grounded paradigm for human-AI collaborative optimization\nguided by natural language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VORTEX\uff0c\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u5956\u52b1\u5851\u5f62\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u539f\u6709\u4f18\u5316\u76ee\u6807\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u7cfb\u7edf\uff0c\u5b9e\u73b0\u6548\u7528\u4e0e\u4eba\u7c7b\u504f\u597d\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684AI\u51b3\u7b56\u7cfb\u7edf\u96be\u4ee5\u76f4\u63a5\u878d\u5165\u4ee5\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u52a8\u6001\u4eba\u7c7b\u504f\u597d\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u4fdd\u969c\u7cfb\u7edf\u6838\u5fc3\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6839\u636e\u8bed\u8a00\u53cd\u9988\u548c\u6587\u672c\u68af\u5ea6\u63d0\u793a\u66f4\u65b0\u8fed\u4ee3\u751f\u6210\u5851\u9020\u5956\u52b1\uff0c\u4ece\u800c\u5728\u4e0d\u4fee\u6539\u6c42\u89e3\u5668\u6216\u6307\u5b9a\u6743\u8861\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u5f15\u5bfc\u51b3\u7b56\u884c\u4e3a\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u8d44\u6e90\u5206\u914d\u4efb\u52a1\u4e2d\uff0cVORTEX\u5728\u6ee1\u8db3\u4eba\u7c7b\u5bf9\u8986\u76d6\u76ee\u6807\u7684\u504f\u597d\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u5230\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "VORTEX\u4e3a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u4eba\u673a\u534f\u540c\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6709\u7406\u8bba\u57fa\u7840\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.17361", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17361", "abs": "https://arxiv.org/abs/2509.17361", "authors": ["Ruihan Luo", "Xuanjing Chen", "Ziyang Ding"], "title": "SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing", "comment": null, "summary": "Personalized content marketing has become a crucial strategy for digital\nplatforms, aiming to deliver tailored advertisements and recommendations that\nmatch user preferences. Traditional recommendation systems often suffer from\ntwo limitations: (1) reliance on limited supervised signals derived from\nexplicit user feedback, and (2) vulnerability to noisy or unintentional\ninteractions. To address these challenges, we propose SeqUDA-Rec, a novel deep\nlearning framework that integrates user behavior sequences with global\nunsupervised data augmentation to enhance recommendation accuracy and\nrobustness. Our approach first constructs a Global User-Item Interaction Graph\n(GUIG) from all user behavior sequences, capturing both local and global item\nassociations. Then, a graph contrastive learning module is applied to generate\nrobust embeddings, while a sequential Transformer-based encoder models users'\nevolving preferences. To further enhance diversity and counteract sparse\nsupervised labels, we employ a GAN-based augmentation strategy, generating\nplausible interaction patterns and supplementing training data. Extensive\nexperiments on two real-world marketing datasets (Amazon Ads and TikTok Ad\nClicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art\nbaselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%\nimprovement in NDCG@10 and 11.3% improvement in HR@10, proving its\neffectiveness in personalized advertising and intelligent content\nrecommendation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeqUDA-Rec\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u548c\u5168\u5c40\u65e0\u76d1\u7763\u6570\u636e\u589e\u5f3a\u6765\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u771f\u5b9e\u5e7f\u544a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u6709\u9650\u7684\u663e\u5f0f\u53cd\u9988\u4fe1\u53f7\uff0c\u4e14\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u4ea4\u4e92\u7684\u5f71\u54cd\uff0c\u96be\u4ee5\u5145\u5206\u6355\u6349\u7528\u6237\u504f\u597d\u5e76\u4fdd\u8bc1\u63a8\u8350\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u6784\u5efa\u5168\u5c40\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u56fe\uff08GUIG\uff09\uff0c\u5229\u7528\u56fe\u5bf9\u6bd4\u5b66\u4e60\u751f\u6210\u9c81\u68d2\u5d4c\u5165\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u5e8f\u5217\u7f16\u7801\u5668\u5efa\u6a21\u7528\u6237\u52a8\u6001\u504f\u597d\uff1b\u540c\u65f6\u5f15\u5165GAN-based\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u751f\u6210\u5408\u7406\u7684\u4ea4\u4e92\u6a21\u5f0f\u4ee5\u7f13\u89e3\u6807\u7b7e\u7a00\u758f\u95ee\u9898\u3002", "result": "\u5728Amazon Ads\u548cTikTok Ad Clicks\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSeqUDA-Rec\u5728NDCG@10\u4e0a\u63d0\u53476.7%\uff0cHR@10\u4e0a\u63d0\u534711.3%\uff0c\u663e\u8457\u4f18\u4e8eSASRec\u3001BERT4Rec\u548cGCL4SR\u7b49\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SeqUDA-Rec\u901a\u8fc7\u878d\u5408\u5168\u5c40\u56fe\u7ed3\u6784\u4fe1\u606f\u4e0e\u65e0\u76d1\u7763\u6570\u636e\u589e\u5f3a\uff0c\u5728\u4e2a\u6027\u5316\u5185\u5bb9\u63a8\u8350\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5e7f\u544a\u63a8\u8350\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.17974", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.17974", "abs": "https://arxiv.org/abs/2509.17974", "authors": ["Son Le Thanh", "Tino Weinkauf"], "title": "A Comparative Study of Different Edit Distance-Based Methods for Feature Tracking using Merge Trees on Time-Varying Scalar Fields", "comment": null, "summary": "Feature tracking in time-varying scalar fields is a fundamental task in\nscientific computing. Topological descriptors, which summarize important\nfeatures of data, have proved to be viable tools to facilitate this task. The\nmerge tree is a topological descriptor that captures the connectivity behaviors\nof the sub- or superlevel sets of a scalar field. Edit distances between merge\ntrees play a vital role in effective temporal data tracking. Existing methods\nto compute them fall into two main classes, namely whether they are dependent\nor independent of the branch decomposition. These two classes represent the\nmost prominent approaches for producing tracking results. In this paper, we\ncompare four different merge tree edit distance-based methods for feature\ntracking. We demonstrate that these methods yield distinct results with both\nanalytical and real-world data sets. Furthermore, we investigate how these\nresults vary and identify the factors that influence them. Our experiments\nreveal significant differences in tracked features over time, even among those\nproduced by techniques within the same category.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u57fa\u4e8e\u5408\u5e76\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u7279\u5f81\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5728\u5206\u6790\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u4e0d\u540c\u7ed3\u679c\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f71\u54cd\u8fd9\u4e9b\u7ed3\u679c\u7684\u56e0\u7d20\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u540c\u4e00\u7c7b\u6280\u672f\u4e2d\uff0c\u968f\u65f6\u95f4\u8ffd\u8e2a\u5230\u7684\u7279\u5f81\u4e5f\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u8fdb\u884c\u65f6\u53d8\u6807\u91cf\u573a\u4e2d\u7684\u7279\u5f81\u8ddf\u8e2a\uff0c\u9700\u8981\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540c\u5408\u5e76\u6811\u7f16\u8f91\u8ddd\u79bb\u65b9\u6cd5\u7684\u8868\u73b0\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u672c\u6587\u5bf9\u6bd4\u4e86\u56db\u79cd\u57fa\u4e8e\u5408\u5e76\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5206\u4e3a\u4f9d\u8d56\u6216\u72ec\u7acb\u4e8e\u5206\u652f\u5206\u89e3\u7684\u4e24\u7c7b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u65b9\u6cd5\u4ea7\u751f\u7684\u8ddf\u8e2a\u7ed3\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5373\u4f7f\u5728\u540c\u4e00\u7c7b\u522b\u5185\u7684\u65b9\u6cd5\u4e4b\u95f4\u4e5f\u662f\u5982\u6b64\uff1b\u5f71\u54cd\u56e0\u7d20\u5305\u62ec\u6570\u636e\u7279\u6027\u53ca\u7f16\u8f91\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\u7684\u4e0d\u540c\u3002", "conclusion": "\u73b0\u6709\u7684\u5408\u5e76\u6811\u7f16\u8f91\u8ddd\u79bb\u65b9\u6cd5\u5728\u7279\u5f81\u8ddf\u8e2a\u4e2d\u8868\u73b0\u5404\u5f02\uff0c\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u9700\u8003\u8651\u5177\u4f53\u5e94\u7528\u573a\u666f\u548c\u6570\u636e\u7279\u70b9\u3002"}}
{"id": "2509.16429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16429", "abs": "https://arxiv.org/abs/2509.16429", "authors": ["Itzik Waizman", "Yakov Gusakov", "Itay Benou", "Tammy Riklin Raviv"], "title": "TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks", "comment": null, "summary": "White matter tractography is an advanced neuroimaging technique that\nreconstructs the 3D white matter pathways of the brain from diffusion MRI data.\nIt can be framed as a pathfinding problem aiming to infer neural fiber\ntrajectories from noisy and ambiguous measurements, facing challenges such as\ncrossing, merging, and fanning white-matter configurations. In this paper, we\npropose a novel tractography method that leverages Transformers to model the\nsequential nature of white matter streamlines, enabling the prediction of fiber\ndirections by integrating both the trajectory context and current diffusion MRI\nmeasurements. To incorporate spatial information, we utilize CNNs that extract\nmicrostructural features from local neighborhoods around each voxel. By\ncombining these complementary sources of information, our approach improves the\nprecision and completeness of neural pathway mapping compared to traditional\ntractography models. We evaluate our method with the Tractometer toolkit,\nachieving competitive performance against state-of-the-art approaches, and\npresent qualitative results on the TractoInferno dataset, demonstrating strong\ngeneralization to real-world data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Transformer\u548cCNN\u7684\u65b0\u578b\u767d\u8d28\u7ea4\u7ef4\u675f\u6210\u50cf\u65b9\u6cd5\uff0c\u5229\u7528\u8f68\u8ff9\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u5fae\u7ed3\u6784\u7279\u5f81\u63d0\u9ad8\u795e\u7ecf\u901a\u8def\u6620\u5c04\u7684\u7cbe\u5ea6\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u4f20\u7edf\u767d\u8d28\u7ea4\u7ef4\u675f\u6210\u50cf\u65b9\u6cd5\u5728\u5904\u7406\u4ea4\u53c9\u3001\u878d\u5408\u548c\u53d1\u6563\u7b49\u590d\u6742\u767d\u8d28\u7ed3\u6784\u65f6\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u63a8\u65ad\u795e\u7ecf\u7ea4\u7ef4\u8d70\u5411\u3002", "method": "\u91c7\u7528Transformer\u5efa\u6a21\u767d\u8d28\u7ea4\u7ef4\u8f68\u8ff9\u7684\u5e8f\u5217\u7279\u6027\uff0c\u7ed3\u5408CNN\u63d0\u53d6\u4f53\u7d20\u5c40\u90e8\u90bb\u57df\u7684\u5fae\u7ed3\u6784\u7279\u5f81\uff0c\u878d\u5408\u4e24\u8005\u4fe1\u606f\u9884\u6d4b\u7ea4\u7ef4\u65b9\u5411\u3002", "result": "\u5728Tractometer\u5de5\u5177\u5305\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u5f53\u524d\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff1b\u5728TractoInferno\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u6027\u7ed3\u679c\u8868\u660e\u5176\u5bf9\u771f\u5b9e\u6570\u636e\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u767d\u8d28\u7ea4\u7ef4\u675f\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u4e3a\u8111\u8fde\u63a5\u7ec4\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u6709\u5229\u7684\u5de5\u5177\u3002"}}
{"id": "2509.16492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16492", "abs": "https://arxiv.org/abs/2509.16492", "authors": ["Tinapat Limsila", "Mehul Sharma", "Paulo Garcia"], "title": "Substrate-Timing-Independence for Meta-State Stability of Distributed Robotic Swarms", "comment": null, "summary": "Emergent properties in distributed systems arise due to timing\nunpredictability; asynchronous state evolution within each sub-system may lead\nthe macro-system to faulty meta-states. Empirical validation of correctness is\noften prohibitively expensive, as the size of the state-space is too large to\nbe tractable. In robotic swarms this problem is exacerbated, when compared to\nsoftware systems, by the variability of the implementation substrate across the\ndesign, or even the deployment, process. We present an approach for formally\nreasoning about the correctness of robotic swarm design in a\nsubstrate-timing-independent way. By leveraging concurrent process calculi\n(namely, Communicating Sequential Processes), we introduce a methodology that\ncan automatically identify possible causes of faulty meta-states and correct\nsuch designs such that meta-states are consistently stable, even in the\npresence of timing variability due to substrate changes. We evaluate this\napproach on a robotic swarm with a clearly identified fault, realized in both\nsimulation and reality. Results support the research hypothesis, showing that\nthe swarm reaches an illegal meta-state before the correction is applied, but\nbehaves consistently correctly after the correction. Our techniques are\ntransferable across different design methodologies, contributing to the toolbox\nof formal methods for roboticists.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e76\u53d1\u8fdb\u7a0b\u6f14\u7b97\u7684\u673a\u5668\u4eba\u96c6\u7fa4\u8bbe\u8ba1\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5e95\u5c42\u65f6\u5e8f\u7684\u60c5\u51b5\u4e0b\u81ea\u52a8\u8bc6\u522b\u5e76\u7ea0\u6b63\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u5143\u72b6\u6001\u7684\u8bbe\u8ba1\u7f3a\u9677\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u6d8c\u73b0\u7279\u6027\u7531\u4e8e\u65f6\u5e8f\u4e0d\u53ef\u9884\u6d4b\u6027\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u8fdb\u5165\u9519\u8bef\u7684\u5b8f\u89c2\u72b6\u6001\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u96c6\u7fa4\u4e2d\uff0c\u56e0\u5b9e\u73b0\u5e73\u53f0\u7684\u53d8\u5f02\u6027\u4f7f\u5f97\u6b63\u786e\u6027\u9a8c\u8bc1\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u91c7\u7528\u901a\u4fe1\u987a\u5e8f\u8fdb\u7a0b\uff08CSP\uff09\u7b49\u5e76\u53d1\u8fdb\u7a0b\u6f14\u7b97\uff0c\u5efa\u7acb\u4e0e\u5e95\u5c42\u65f6\u5e8f\u65e0\u5173\u7684\u5f62\u5f0f\u5316\u63a8\u7406\u65b9\u6cd5\uff0c\u81ea\u52a8\u68c0\u6d4b\u548c\u4fee\u6b63\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u5143\u72b6\u6001\u7684\u8bbe\u8ba1\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u96c6\u7fa4\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u4fee\u6b63\u524d\u7cfb\u7edf\u4f1a\u8fdb\u5165\u975e\u6cd5\u5143\u72b6\u6001\uff0c\u4fee\u6b63\u540e\u884c\u4e3a\u4e00\u81f4\u4e14\u6b63\u786e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u96c6\u7fa4\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u8fc1\u79fb\u7684\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2509.16375", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16375", "abs": "https://arxiv.org/abs/2509.16375", "authors": ["Cihan Xiao", "Matthew Wiesner", "Debashish Chakraborty", "Reno Kriz", "Keith Cunningham", "Kenton Murray", "Kevin Duh", "Luis Tavarez-Arce", "Paul McNamee", "Sanjeev Khudanpur"], "title": "Whisper-UT: A Unified Translation Framework for Speech and Text", "comment": "EMNLP 2025 Main Conference", "summary": "Encoder-decoder models have achieved remarkable success in speech and text\ntasks, yet efficiently adapting these models to diverse uni/multi-modal\nscenarios remains an open challenge. In this paper, we propose Whisper-UT, a\nunified and efficient framework that leverages lightweight adapters to enable\nseamless adaptation across tasks, including a multi-modal machine translation\n(MMT) task that explicitly conditions translation on both speech and source\nlanguage text inputs. By incorporating ASR hypotheses or ground-truth\ntranscripts as prompts, this approach not only enables the system to process\nboth modalities simultaneously but also enhances speech translation (ST)\nperformance through a 2-stage decoding strategy. We demonstrate our methods\nusing the Whisper model, though in principle they are general and could be\napplied to similar multitask models. We highlight the effectiveness of\ncross-modal and cross-task fine-tuning, which improves performance without\nrequiring 3-way parallel data. Our results underscore the flexibility,\nefficiency, and general applicability of the proposed framework for multi-modal\ntranslation.", "AI": {"tldr": "\u63d0\u51faWhisper-UT\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8f7b\u91cf\u9002\u914d\u5668\u7684\u7edf\u4e00\u9ad8\u6548\u6846\u67b6\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u548c\u591a\u6a21\u6001\uff08\u8bed\u97f3+\u6587\u672c\uff09\u7684\u65e0\u7f1d\u9002\u5e94\uff0c\u5c24\u5176\u5728\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u548c\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u9002\u5e94\u4ecd\u5177\u6311\u6218\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u8bed\u97f3\u4e0e\u6587\u672c\u53cc\u8f93\u5165\u8054\u5408\u5efa\u6a21\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u5728Whisper\u7b49\u6a21\u578b\u4e2d\u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u901a\u8fc7ASR\u5047\u8bbe\u6216\u771f\u5b9e\u8f6c\u5f55\u4f5c\u4e3a\u63d0\u793a\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u89e3\u7801\u7b56\u7565\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u3001\u8de8\u4efb\u52a1\u7684\u5fae\u8c03\u3002", "result": "\u5728\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\uff0c\u65e0\u9700\u4e09\u5411\u5e73\u884c\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u4e0e\u9ad8\u6548\u6027\u3002", "conclusion": "Whisper-UT\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u4e0e\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16339", "abs": "https://arxiv.org/abs/2509.16339", "authors": ["Josias K. Moukpe", "Philip K. Chan", "Ming Zhang"], "title": "Highly Imbalanced Regression with Tabular Data in SEP and Other Applications", "comment": "ICMLA 2025", "summary": "We investigate imbalanced regression with tabular data that have an imbalance\nratio larger than 1,000 (\"highly imbalanced\"). Accurately estimating the target\nvalues of rare instances is important in applications such as forecasting the\nintensity of rare harmful Solar Energetic Particle (SEP) events. For\nregression, the MSE loss does not consider the correlation between predicted\nand actual values. Typical inverse importance functions allow only convex\nfunctions. Uniform sampling might yield mini-batches that do not have rare\ninstances. We propose CISIR that incorporates correlation, Monotonically\nDecreasing Involution (MDI) importance, and stratified sampling. Based on five\ndatasets, our experimental results indicate that CISIR can achieve lower error\nand higher correlation than some recent methods. Also, adding our correlation\ncomponent to other recent methods can improve their performance. Lastly, MDI\nimportance can outperform other importance functions. Our code can be found in\nhttps://github.com/Machine-Earning/CISIR.", "AI": {"tldr": "\u63d0\u51faCISIR\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u56de\u5f52\u95ee\u9898\uff0c\u7ed3\u5408\u76f8\u5173\u6027\u3001\u5355\u8c03\u9012\u51cf\u5bf9\u5408\u91cd\u8981\u6027\uff08MDI\uff09\u548c\u5206\u5c42\u91c7\u6837\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u56de\u5f52\u4efb\u52a1\u4e2d\uff08\u5982\u7f55\u89c1\u592a\u9633\u9ad8\u80fd\u7c92\u5b50\u4e8b\u4ef6\u9884\u6d4b\uff09\uff0c\u4f20\u7edfMSE\u635f\u5931\u4e0d\u8003\u8651\u9884\u6d4b\u503c\u4e0e\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u4e14\u5e38\u7528\u91cd\u8981\u6027\u52a0\u6743\u65b9\u6cd5\u53d7\u9650\u4e8e\u51f8\u51fd\u6570\u5f62\u5f0f\uff0c\u5747\u5300\u91c7\u6837\u96be\u4ee5\u8986\u76d6\u7a00\u6709\u6837\u672c\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCISIR\u65b9\u6cd5\uff0c\u5f15\u5165\u76f8\u5173\u6027\u635f\u5931\u9879\u3001\u5355\u8c03\u9012\u51cf\u5bf9\u5408\uff08MDI\uff09\u91cd\u8981\u6027\u52a0\u6743\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u66f4\u597d\u5904\u7406\u7a00\u6709\u6837\u672c\u7684\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCISIR\u76f8\u6bd4\u4e00\u4e9b\u6700\u65b0\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u8bef\u5dee\u548c\u66f4\u9ad8\u7684\u76f8\u5173\u6027\uff1b\u5c06\u76f8\u5173\u6027\u7ec4\u4ef6\u52a0\u5165\u5176\u4ed6\u65b9\u6cd5\u4e5f\u80fd\u63d0\u5347\u5176\u6027\u80fd\uff1bMDI\u91cd\u8981\u6027\u4f18\u4e8e\u5176\u4ed6\u91cd\u8981\u6027\u51fd\u6570\u3002", "conclusion": "CISIR\u901a\u8fc7\u6574\u5408\u76f8\u5173\u6027\u5efa\u6a21\u3001MDI\u91cd\u8981\u6027\u548c\u5206\u5c42\u91c7\u6837\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u5ea6\u4e0d\u5e73\u8861\u56de\u5f52\u4efb\u52a1\u4e2d\u5bf9\u7a00\u6709\u5b9e\u4f8b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5982\u7a7a\u95f4\u5929\u6c14\u9884\u6d4b\u7b49\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2509.16431", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16431", "abs": "https://arxiv.org/abs/2509.16431", "authors": ["Mohammad Iqbal Rasul Seeam", "Victor S. Sheng"], "title": "Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing", "comment": "7 pages, 3 figures, no .bbl file needed because bibliography already\n  in main.tex file", "summary": "In the manufacturing industry, it is very important to keep machines and\nprocesses running smoothly and without unexpected problems. One of the most\ncommon tools used to check if everything is working properly is called\nStatistical Process Control (SPC). Traditional SPC methods work by checking\nwhether recent measurements are within acceptable limits. However, they only\nreact after a problem has already occurred. This can lead to wasted materials,\nmachine downtime, and increased costs. In this paper, we present a smarter way\nto use SPC. Instead of just reacting to issues after they happen, our system\ncan predict future problems before they occur. We use a machine learning tool\ncalled Facebook Prophet, which is designed to work with time-series data (data\nthat changes over time). Prophet looks at past data and forecasts what the next\nvalue will be. Then, we use SPC rules to decide if the predicted value is in a\nSafe zone (no problem), a Warning zone (needs attention), or a Critical zone\n(may require shutting down the process). We applied this system to real data\nfrom a semiconductor manufacturing company. One of the challenges with this\ndata is that the measurements are not taken at regular time intervals. This\nmakes it harder to predict future values accurately. Despite this, our model\nwas able to make strong predictions and correctly classify the risk level of\nfuture measurements. The main benefit of our system is that it gives engineers\nand technicians a chance to act early - before something goes wrong. This helps\nreduce unexpected failures and improves the overall stability and reliability\nof the production process. By combining machine learning with traditional SPC,\nwe make quality control more proactive, accurate, and useful for modern\nindustry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u4e0e\u4f20\u7edf\u7edf\u8ba1\u8fc7\u7a0b\u63a7\u5236\uff08SPC\uff09\u7684\u9884\u6d4b\u6027\u8d28\u91cf\u76d1\u63a7\u65b9\u6cd5\uff0c\u4f7f\u7528Facebook Prophet\u6a21\u578b\u5bf9\u975e\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7SPC\u89c4\u5219\u5212\u5206\u5b89\u5168\u3001\u8b66\u544a\u548c\u5173\u952e\u533a\u57df\uff0c\u5b9e\u73b0\u5bf9\u534a\u5bfc\u4f53\u5236\u9020\u8fc7\u7a0b\u4e2d\u6f5c\u5728\u95ee\u9898\u7684\u65e9\u671f\u9884\u8b66\u3002", "motivation": "\u4f20\u7edfSPC\u4ec5\u5728\u95ee\u9898\u53d1\u751f\u540e\u54cd\u5e94\uff0c\u5bfc\u81f4\u6750\u6599\u6d6a\u8d39\u3001\u505c\u673a\u548c\u6210\u672c\u589e\u52a0\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3\u5236\u9020\u4e1a\u5bf9\u4e3b\u52a8\u8d28\u91cf\u63a7\u5236\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528Facebook Prophet\u6a21\u578b\u5bf9\u5386\u53f2\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u7ed3\u5408SPC\u63a7\u5236\u89c4\u5219\uff0c\u5224\u65ad\u672a\u6765\u6d4b\u91cf\u503c\u6240\u5904\u7684\u98ce\u9669\u533a\u57df\uff08\u5b89\u5168\u3001\u8b66\u544a\u3001\u5173\u952e\uff09\uff0c\u5e76\u5728\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u5b9e\u9645\u534a\u5bfc\u4f53\u5236\u9020\u6570\u636e\u4e0a\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u672a\u6765\u8d8b\u52bf\u5e76\u6b63\u786e\u5206\u7c7b\u98ce\u9669\u7b49\u7ea7\uff0c\u5373\u4f7f\u5728\u6570\u636e\u91c7\u6837\u4e0d\u89c4\u5f8b\u7684\u60c5\u51b5\u4e0b\u4ecd\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u5de5\u7a0b\u5e08\u80fd\u5728\u6545\u969c\u53d1\u751f\u524d\u91c7\u53d6\u63aa\u65bd\uff0c\u663e\u8457\u51cf\u5c11\u610f\u5916\u505c\u673a\uff0c\u63d0\u5347\u751f\u4ea7\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e3b\u52a8\u3001\u7cbe\u51c6\u7684\u8d28\u91cf\u63a7\u5236\u3002"}}
{"id": "2509.17440", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17440", "abs": "https://arxiv.org/abs/2509.17440", "authors": ["J\u00fcri Keller", "Maik Fr\u00f6be", "Gijs Hendriksen", "Daria Alexander", "Martin Potthast", "Philipp Schaer"], "title": "Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion and Document Boosting", "comment": "Best of labs paper for LongEval at CLEF 2024", "summary": "The longitudinal evaluation of retrieval systems aims to capture how\ninformation needs and documents evolve over time. However, classical\nCranfield-style retrieval evaluations only consist of a static set of queries\nand documents and thereby miss time as an evaluation dimension. Therefore,\nlongitudinal evaluations need to complement retrieval toolkits with custom\nlogic. This custom logic increases the complexity of research software, which\nmight reduce the reproducibility and extensibility of experiments. Based on our\nsubmissions to the 2024 edition of LongEval, we propose a custom extension of\nir_datasets for longitudinal retrieval experiments. This extension allows for\ndeclaratively, instead of imperatively, describing important aspects of\nlongitudinal retrieval experiments, e.g., which queries, documents, and/or\nrelevance feedback are available at which point in time. We reimplement our\nsubmissions to LongEval 2024 against our new ir_datasets extension, and find\nthat the declarative access can reduce the complexity of the code.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eir_datasets\u7684\u7eb5\u5411\u68c0\u7d22\u5b9e\u9a8c\u6269\u5c55\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u65b9\u6cd5\u964d\u4f4e\u5b9e\u9a8c\u4ee3\u7801\u590d\u6742\u6027\u3002", "motivation": "\u4f20\u7edfCranfield\u98ce\u683c\u7684\u68c0\u7d22\u8bc4\u4f30\u7f3a\u4e4f\u65f6\u95f4\u7ef4\u5ea6\uff0c\u96be\u4ee5\u652f\u6301\u7eb5\u5411\u8bc4\u4f30\uff0c\u4e14\u81ea\u5b9a\u4e49\u903b\u8f91\u589e\u52a0\u4e86\u7814\u7a76\u8f6f\u4ef6\u7684\u590d\u6742\u6027\u548c\u4e0d\u53ef\u590d\u73b0\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2air_datasets\u7684\u6269\u5c55\uff0c\u652f\u6301\u58f0\u660e\u5f0f\u63cf\u8ff0\u7eb5\u5411\u68c0\u7d22\u5b9e\u9a8c\u4e2d\u7684\u67e5\u8be2\u3001\u6587\u6863\u548c\u76f8\u5173\u53cd\u9988\u968f\u65f6\u95f4\u7684\u53d8\u5316\u60c5\u51b5\uff0c\u5e76\u91cd\u65b0\u5b9e\u73b0\u4e86LongEval 2024\u7684\u63d0\u4ea4\u65b9\u6848\u4ee5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u4f7f\u7528\u65b0\u6269\u5c55\u540e\uff0c\u4ee3\u7801\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u5b9e\u9a8c\u66f4\u6613\u4e8e\u590d\u73b0\u548c\u6269\u5c55\u3002", "conclusion": "\u8be5\u6269\u5c55\u6709\u6548\u7b80\u5316\u4e86\u7eb5\u5411\u68c0\u7d22\u5b9e\u9a8c\u7684\u5b9e\u73b0\uff0c\u63d0\u5347\u4e86\u7814\u7a76\u5de5\u5177\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2509.17979", "categories": ["cs.GR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17979", "abs": "https://arxiv.org/abs/2509.17979", "authors": ["Yiwen Song", "Hongyang Li", "Kuang Yuan", "Ran Bi", "Swarun Kumar"], "title": "Towards Seeing Bones at Radio Frequency", "comment": null, "summary": "Wireless sensing literature has long aspired to achieve X-ray-like vision at\nradio frequencies. Yet, state-of-the-art wireless sensing literature has yet to\ngenerate the archetypal X-ray image: one of the bones beneath flesh. In this\npaper, we explore MCT, a penetration-based RF-imaging system for imaging bones\nat mm-resolution, one that significantly exceeds prior penetration-based RF\nimaging literature. Indeed the long wavelength, significant attenuation and\ncomplex diffraction that occur as RF propagates through flesh, have long\nlimited imaging resolution (to several centimeters at best). We address these\nconcerns through a novel penetration-based synthetic aperture algorithm,\ncoupled with a learning-based pipeline to correct for diffraction-induced\nartifacts. A detailed evaluation of meat models demonstrates a resolution\nimprovement from sub-decimeter to sub-centimeter over prior art in RF\npenetrative imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7f\u900f\u7684\u5c04\u9891\u6210\u50cf\u7cfb\u7edfMCT\uff0c\u80fd\u591f\u4ee5\u6beb\u7c73\u7ea7\u5206\u8fa8\u7387\u6210\u50cf\u9aa8\u9abc\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u7684\u5c04\u9891\u7a7f\u900f\u6210\u50cf\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u611f\u77e5\u6280\u672f\u5728\u5c04\u9891\u7a7f\u900f\u6210\u50cf\u4e2d\u53d7\u9650\u4e8e\u957f\u6ce2\u957f\u3001\u5f3a\u8870\u51cf\u548c\u590d\u6742\u884d\u5c04\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\uff08\u5c24\u5176\u662f\u9aa8\u9abc\u6210\u50cf\uff09\uff0c\u65e0\u6cd5\u8fbe\u5230\u7c7b\u4f3cX\u5149\u7684\u89c6\u89c9\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7a7f\u900f\u7684\u5408\u6210\u5b54\u5f84\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5b66\u4e60\u7684\u6d41\u7a0b\u6765\u6821\u6b63\u884d\u5c04\u5f15\u8d77\u7684\u4f2a\u5f71\uff0c\u4ece\u800c\u63d0\u5347\u6210\u50cf\u5206\u8fa8\u7387\u3002", "result": "\u5728\u8089\u7c7b\u6a21\u578b\u4e0a\u7684\u8be6\u7ec6\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u5c04\u9891\u7a7f\u900f\u6210\u50cf\u7684\u5206\u8fa8\u7387\u4ece\u4e9a\u5206\u7c73\u7ea7\u63d0\u5347\u81f3\u4e9a\u5398\u7c73\u7ea7\uff0c\u663e\u8457\u8d85\u8d8a\u5148\u524d\u6280\u672f\u3002", "conclusion": "MCT\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u7684\u5c04\u9891\u9aa8\u9abc\u6210\u50cf\uff0c\u63a8\u52a8\u4e86\u65e0\u7ebf\u611f\u77e5\u5411X\u5149\u822c\u89c6\u89c9\u6548\u679c\u7684\u8fc8\u8fdb\u3002"}}
{"id": "2509.16436", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16436", "abs": "https://arxiv.org/abs/2509.16436", "authors": ["Zhejia Zhang", "Junjie Wang", "Le Zhang"], "title": "Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation", "comment": null, "summary": "In real-world clinical settings, magnetic resonance imaging (MRI) frequently\nsuffers from missing modalities due to equipment variability or patient\ncooperation issues, which can significantly affect model performance. To\naddress this issue, we propose a multimodal MRI classification model based on\nthe mmFormer architecture with an adaptive module for handling arbitrary\ncombinations of missing modalities. Specifically, this model retains the hybrid\nmodality-specific encoders and the modality-correlated encoder from mmFormer to\nextract consistent lesion features across available modalities. In addition, we\nintegrate a missing-modality compensation module which leverages zero-padding,\nmodality availability masks, and a Delta Function with learnable statistical\nparameters to dynamically synthesize proxy features for recovering missing\ninformation. To further improve prediction performance, we adopt a\ncross-validation ensemble strategy by training multiple models on different\nfolds and applying soft voting during inference. This method is evaluated on\nthe test set of Comprehensive Analysis & Computing of REal-world medical images\n(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based\non non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),\nT2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis\nDetection and Substantial Fibrosis Detection on in-distribution vendors, our\nmodel obtains accuracies of 66.67%, and 74.17%, and corresponding area under\nthe curve (AUC) scores of 71.73% and 68.48%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8emmFormer\u7684\u591a\u6a21\u6001MRI\u5206\u7c7b\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6a21\u5757\u5904\u7406\u4efb\u610f\u7f3a\u5931\u6a21\u6001\uff0c\u5728CARE 2025\u6311\u6218\u8d5b\u7684\u809d\u7ea4\u7ef4\u5316\u5206\u671f\u4efb\u52a1\u4e2d\u5c55\u73b0\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8aMRI\u5e38\u56e0\u8bbe\u5907\u6216\u60a3\u8005\u539f\u56e0\u5bfc\u81f4\u6a21\u6001\u7f3a\u5931\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u9700\u63d0\u5347\u6a21\u578b\u5bf9\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8emmFormer\u67b6\u6784\uff0c\u91c7\u7528\u6df7\u5408\u6a21\u6001\u7279\u5f02\u6027\u7f16\u7801\u5668\u548c\u6a21\u6001\u76f8\u5173\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5f15\u5165\u7f3a\u5931\u6a21\u6001\u8865\u507f\u6a21\u5757\uff08\u7ed3\u5408\u96f6\u586b\u5145\u3001\u6a21\u6001\u63a9\u7801\u548c\u53ef\u5b66\u4e60Delta\u51fd\u6570\uff09\u751f\u6210\u4ee3\u7406\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u96c6\u6210\u4e0e\u8f6f\u6295\u7968\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728CARE 2025 LiFS\u4efb\u52a1\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5bf9\u5206\u5e03\u5185\u5382\u5546\u6570\u636e\uff0c\u809d\u786c\u5316\u68c0\u6d4b\u51c6\u786e\u738766.67%\uff0cAUC 71.73%\uff1b\u663e\u8457\u7ea4\u7ef4\u5316\u68c0\u6d4b\u51c6\u786e\u738774.17%\uff0cAUC 68.48%\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u591a\u6a21\u6001MRI\u4e2d\u4efb\u610f\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u7279\u5f81\u8865\u507f\u4e0e\u6a21\u578b\u96c6\u6210\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5177\u6709\u8f83\u5f3a\u4e34\u5e8a\u9002\u7528\u6027\u3002"}}
{"id": "2509.16532", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16532", "abs": "https://arxiv.org/abs/2509.16532", "authors": ["Run Yu", "Yangdi Liu", "Wen-Da Wei", "Chen Li"], "title": "No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning", "comment": null, "summary": "Recently,vision-based robotic manipulation has garnered significant attention\nand witnessed substantial advancements. 2D image-based and 3D point cloud-based\npolicy learning represent two predominant paradigms in the field, with recent\nstudies showing that the latter consistently outperforms the former in terms of\nboth policy performance and generalization, thereby underscoring the value and\nsignificance of 3D information. However, 3D point cloud-based approaches face\nthe significant challenge of high data acquisition costs, limiting their\nscalability and real-world deployment. To address this issue, we propose a\nnovel framework NoReal3D: which introduces the 3DStructureFormer, a learnable\n3D perception module capable of transforming monocular images into\ngeometrically meaningful pseudo-point cloud features, effectively fused with\nthe 2D encoder output features. Specially, the generated pseudo-point clouds\nretain geometric and topological structures so we design a pseudo-point cloud\nencoder to preserve these properties, making it well-suited for our framework.\nWe also investigate the effectiveness of different feature fusion\nstrategies.Our framework enhances the robot's understanding of 3D spatial\nstructures while completely eliminating the substantial costs associated with\n3D point cloud acquisition.Extensive experiments across various tasks validate\nthat our framework can achieve performance comparable to 3D point cloud-based\nmethods, without the actual point cloud data.", "AI": {"tldr": "\u63d0\u51faNoReal3D\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u76843D\u611f\u77e5\u6a21\u5757\u5c06\u5355\u76ee\u56fe\u50cf\u8f6c\u6362\u4e3a\u5177\u6709\u51e0\u4f55\u610f\u4e49\u7684\u4f2a\u70b9\u4e91\u7279\u5f81\uff0c\u878d\u54082D\u7f16\u7801\u7279\u5f81\uff0c\u5728\u65e0\u9700\u771f\u5b9e\u70b9\u4e91\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e0e3D\u70b9\u4e91\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "3D\u70b9\u4e91\u65b9\u6cd5\u867d\u6027\u80fd\u4f18\u8d8a\u4f46\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u80fd\u4fdd\u75593D\u7ed3\u6784\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba13DStructureFormer\u6a21\u5757\uff0c\u5c06\u5355\u76ee\u56fe\u50cf\u8f6c\u5316\u4e3a\u4f2a\u70b9\u4e91\u7279\u5f81\uff0c\u5e76\u7ed3\u54082D\u7f16\u7801\u5668\u8f93\u51fa\uff1b\u91c7\u7528\u4f2a\u70b9\u4e91\u7f16\u7801\u5668\u4fdd\u6301\u51e0\u4f55\u4e0e\u62d3\u6251\u7ed3\u6784\uff0c\u63a2\u7d22\u591a\u79cd\u7279\u5f81\u878d\u5408\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u771f\u5b9e\u70b9\u4e91\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e3D\u70b9\u4e91\u65b9\u6cd5\u76f8\u5f53\u7684\u7b56\u7565\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "NoReal3D\u6709\u6548\u964d\u4f4e\u4e86\u5bf9\u771f\u5b9e3D\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.16394", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16394", "abs": "https://arxiv.org/abs/2509.16394", "authors": ["Deuksin Kwon", "Kaleen Shrestha", "Bin Han", "Elena Hayoung Lee", "Gale Lucas"], "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans", "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Large Language Models (LLMs) are increasingly deployed in socially complex,\ninteraction-driven tasks, yet their ability to mirror human behavior in\nemotionally and strategically complex contexts remains underexplored. This\nstudy assesses the behavioral alignment of personality-prompted LLMs in\nadversarial dispute resolution by simulating multi-turn conflict dialogues that\nincorporate negotiation. Each LLM is guided by a matched Five-Factor\npersonality profile to control for individual variation and enhance realism. We\nevaluate alignment across three dimensions: linguistic style, emotional\nexpression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the\nclosest alignment with humans in linguistic style and emotional dynamics, while\nClaude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial\nalignment gaps persist. Our findings establish a benchmark for alignment\nbetween LLMs and humans in socially complex interactions, underscoring both the\npromise and the limitations of personality conditioning in dialogue modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u5305\u542b\u8c08\u5224\u7684\u591a\u8f6e\u51b2\u7a81\u5bf9\u8bdd\uff0c\u8bc4\u4f30\u4e86\u5177\u6709\u4eba\u683c\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5bf9\u6297\u6027\u7ea0\u7eb7\u89e3\u51b3\u4e2d\u7684\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u53d1\u73b0GPT-4.1\u5728\u8bed\u8a00\u98ce\u683c\u548c\u60c5\u7eea\u52a8\u6001\u4e0a\u6700\u63a5\u8fd1\u4eba\u7c7b\uff0c\u800cClaude-3.7-Sonnet\u5728\u7b56\u7565\u884c\u4e3a\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u7684\u4e00\u81f4\u6027\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u60c5\u611f\u548c\u6218\u7565\u590d\u6742\u60c5\u5883\u4e2d\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u793e\u4ea4\u590d\u6742\u7684\u4e92\u52a8\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528\u5339\u914d\u7684\u4e94\u5927\u4eba\u683c\u7279\u5f81\u6863\u6848\u5f15\u5bfcLLM\uff0c\u6a21\u62df\u591a\u8f6e\u51b2\u7a81\u5bf9\u8bdd\uff0c\u5e76\u4ece\u8bed\u8a00\u98ce\u683c\u3001\u60c5\u7eea\u8868\u8fbe\u548c\u7b56\u7565\u884c\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u5176\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "result": "GPT-4.1\u5728\u8bed\u8a00\u98ce\u683c\u548c\u60c5\u7eea\u52a8\u6001\u65b9\u9762\u4e0e\u4eba\u7c7b\u6700\u4e00\u81f4\uff0cClaude-3.7-Sonnet\u5728\u7b56\u7565\u884c\u4e3a\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u4f46\u6240\u6709\u6a21\u578b\u4ecd\u5b58\u5728\u660e\u663e\u7684\u884c\u4e3a\u5bf9\u9f50\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86LLM\u4e0e\u4eba\u7c7b\u5728\u793e\u4ea4\u590d\u6742\u4e92\u52a8\u4e2d\u884c\u4e3a\u5bf9\u9f50\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4eba\u683c\u63d0\u793a\u5728\u5bf9\u8bdd\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002"}}
{"id": "2509.16345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16345", "abs": "https://arxiv.org/abs/2509.16345", "authors": ["Minxiao Wang", "Runze Yan", "Carol Li", "Saurabh Kataria", "Xiao Hu", "Matthew Clark", "Timothy Ruchti", "Timothy G. Buchman", "Sivasubramanium V Bhavani", "Randall J. Lee"], "title": "Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model -- a UNIPHY+ Approach", "comment": null, "summary": "Clinical laboratory tests provide essential biochemical measurements for\ndiagnosis and treatment, but are limited by intermittent and invasive sampling.\nIn contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded\nsignal in intensive care units (ICUs) that reflects cardiovascular dynamics and\ncan serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a\nframework that combines a large-scale PPG foundation model for local waveform\nencoding with a patient-aware Mamba model for long-range temporal modeling. Our\narchitecture addresses three challenges: (1) capturing extended temporal trends\nin laboratory values, (2) accounting for patient-specific baseline variation\nvia FiLM-modulated initial states, and (3) performing multi-task estimation for\ninterrelated biomarkers. We evaluate our method on the two ICU datasets for\npredicting the five key laboratory tests. The results show substantial\nimprovements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$\namong most of the estimation targets. This work demonstrates the feasibility of\ncontinuous, personalized lab value estimation from routine PPG monitoring,\noffering a pathway toward non-invasive biochemical surveillance in critical\ncare.", "AI": {"tldr": "\u63d0\u51faUNIPHY+Lab\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21PPG\u57fa\u7840\u6a21\u578b\u548c\u60a3\u8005\u611f\u77e5\u7684Mamba\u6a21\u578b\uff0c\u5b9e\u73b0ICU\u4e2d\u8fde\u7eed\u3001\u4e2a\u6027\u5316\u7684\u5b9e\u9a8c\u5ba4\u503c\u4f30\u8ba1\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\u53d7\u9650\u4e8e\u95f4\u6b47\u6027\u548c\u4fb5\u5165\u6027\u91c7\u6837\uff0c\u800cPPG\u4fe1\u53f7\u53ef\u8fde\u7eed\u975e\u4fb5\u5165\u8bb0\u5f55\uff0c\u53cd\u6620\u751f\u7406\u52a8\u6001\u53d8\u5316\uff0c\u56e0\u6b64\u5e0c\u671b\u5229\u7528PPG\u5b9e\u73b0\u8fde\u7eed\u3001\u4e2a\u6027\u5316\u7684\u751f\u5316\u76d1\u6d4b\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21PPG\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5c40\u90e8\u6ce2\u5f62\u7f16\u7801\uff0c\u4f7f\u7528\u60a3\u8005\u611f\u77e5\u7684Mamba\u6a21\u578b\u8fdb\u884c\u957f\u7a0b\u65f6\u5e8f\u5efa\u6a21\uff0c\u5f15\u5165FiLM\u8c03\u5236\u521d\u59cb\u72b6\u6001\u4ee5\u9002\u5e94\u4e2a\u4f53\u57fa\u7ebf\u5dee\u5f02\uff0c\u5e76\u8fdb\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u9884\u6d4b\u591a\u4e2a\u76f8\u5173\u751f\u7269\u6807\u5fd7\u7269\u3002", "result": "\u5728\u4e24\u4e2aICU\u6570\u636e\u96c6\u4e0a\u9884\u6d4b\u4e94\u9879\u5173\u952e\u5b9e\u9a8c\u5ba4\u6307\u6807\uff0cMAE\u3001RMSE\u548cR\u00b2\u5747\u663e\u8457\u4f18\u4e8eLSTM\u548c\u524d\u5411\u586b\u5145\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UNIPHY+Lab\u5b9e\u73b0\u4e86\u4ece\u5e38\u89c4PPG\u76d1\u6d4b\u4e2d\u8fdb\u884c\u8fde\u7eed\u3001\u4e2a\u6027\u5316\u5b9e\u9a8c\u5ba4\u503c\u4f30\u8ba1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u91cd\u75c7\u76d1\u62a4\u4e2d\u7684\u975e\u4fb5\u5165\u6027\u751f\u5316\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.16444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16444", "abs": "https://arxiv.org/abs/2509.16444", "authors": ["Chenhan Lyu", "Yutong Song", "Pengfei Zhang", "Amir M. Rahmani"], "title": "Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots", "comment": null, "summary": "Mental health applications have emerged as a critical area in computational\nhealth, driven by rising global rates of mental illness, the integration of AI\nin psychological care, and the need for scalable solutions in underserved\ncommunities. These include therapy chatbots, crisis detection, and wellness\nplatforms handling sensitive data, requiring specialized AI safety beyond\ngeneral safeguards due to emotional vulnerability, risks like misdiagnosis or\nsymptom exacerbation, and precise management of vulnerable states to avoid\nsevere outcomes such as self-harm or loss of trust. Despite AI safety advances,\ngeneral safeguards inadequately address mental health-specific challenges,\nincluding crisis intervention accuracy to avert escalations, therapeutic\nguideline adherence to prevent misinformation, scale limitations in\nresource-constrained settings, and adaptation to nuanced dialogues where\ngenerics may introduce biases or miss distress signals. We introduce an\napproach to apply Constitutional AI training with domain-specific mental health\nprinciples for safe, domain-adapted CAI systems in computational mental health\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5baa\u6cd5AI\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u7279\u5b9a\u539f\u5219\uff0c\u4ee5\u6784\u5efa\u5b89\u5168\u3001\u9002\u5e94\u6027\u5f3a\u7684AI\u7cfb\u7edf\uff0c\u5e94\u5bf9\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7684\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u6d89\u53ca\u654f\u611f\u6570\u636e\u548c\u60c5\u611f\u8106\u5f31\u6027\uff0c\u901a\u7528AI\u5b89\u5168\u63aa\u65bd\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u8bef\u8bca\u3001\u75c7\u72b6\u6076\u5316\u548c\u5371\u673a\u5e72\u9884\u4e0d\u51c6\u786e\u7b49\u7279\u5b9a\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5baa\u6cd5AI\uff08Constitutional AI\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u878d\u5165\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u4e13\u4e1a\u539f\u5219\uff0c\u4ee5\u63d0\u5347AI\u5728\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u6027\u3001\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u7ba1\u7406\u7528\u6237\u7684\u60c5\u7eea\u72b6\u6001\uff0c\u63d0\u9ad8\u5371\u673a\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u9075\u5faa\u6cbb\u7597\u6307\u5357\uff0c\u51cf\u5c11\u504f\u89c1\uff0c\u5e76\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u90e8\u7f72\u3002", "conclusion": "\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u539f\u5219\u7684\u5baa\u6cd5AI\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u5fc3\u7406\u5065\u5eb7AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u4e3a\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u63d0\u4f9b\u66f4\u5177\u9002\u5e94\u6027\u548c\u4f26\u7406\u5408\u89c4\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17442", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17442", "abs": "https://arxiv.org/abs/2509.17442", "authors": ["Hideaki Joko", "Shakiba Amirshahi", "Charles L. A. Clarke", "Faegheh Hasibi"], "title": "WildClaims: Information Access Conversations in the Wild(Chat)", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has transformed\nconversational systems into practical tools used by millions. However, the\nnature and necessity of information retrieval in real-world conversations\nremain largely unexplored, as research has focused predominantly on\ntraditional, explicit information access conversations. The central question\nis: What do real-world information access conversations look like? To this end,\nwe first conduct an observational study on the WildChat dataset, large-scale\nuser-ChatGPT conversations, finding that users' access to information occurs\nimplicitly as check-worthy factual assertions made by the system, even when the\nconversation's primary intent is non-informational, such as creative writing.\nTo enable the systematic study of this phenomenon, we release the WildClaims\ndataset, a novel resource consisting of 121,905 extracted factual claims from\n7,587 utterances in 3,000 WildChat conversations, each annotated for\ncheck-worthiness. Our preliminary analysis of this resource reveals that\nconservatively 18% to 51% of conversations contain check-worthy assertions,\ndepending on the methods employed, and less conservatively, as many as 76% may\ncontain such assertions. This high prevalence underscores the importance of\nmoving beyond the traditional understanding of explicit information access, to\naddress the implicit information access that arises in real-world user-system\nconversations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790WildChat\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u7528\u6237\u4e0eChatGPT\u7684\u5bf9\u8bdd\u4e2d\u666e\u904d\u5b58\u5728\u9690\u5f0f\u4fe1\u606f\u83b7\u53d6\u73b0\u8c61\uff0c\u5373\u4f7f\u5728\u975e\u4fe1\u606f\u6027\u5bf9\u8bdd\uff08\u5982\u521b\u610f\u5199\u4f5c\uff09\u4e2d\uff0c\u7cfb\u7edf\u4e5f\u4f1a\u505a\u51fa\u53ef\u9a8c\u8bc1\u7684\u4e8b\u5b9e\u9648\u8ff0\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u53d1\u5e03\u4e86WildClaims\u6570\u636e\u96c6\uff0c\u5305\u542b12\u4e07\u591a\u4e2a\u4ece3000\u4e2a\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u7684\u6807\u6ce8\u4e8b\u5b9e\u58f0\u660e\uff0c\u5e76\u53d1\u73b018%\u81f376%\u7684\u5bf9\u8bdd\u5305\u542b\u53ef\u9a8c\u8bc1\u5185\u5bb9\uff0c\u5f3a\u8c03\u9700\u8d85\u8d8a\u4f20\u7edf\u663e\u5f0f\u4fe1\u606f\u8bbf\u95ee\u7684\u7814\u7a76\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f20\u7edf\u7684\u663e\u5f0f\u4fe1\u606f\u83b7\u53d6\u5bf9\u8bdd\uff0c\u800c\u771f\u5b9e\u573a\u666f\u4e2d\u7528\u6237\u4e0eLLM\u7684\u4ea4\u4e92\u5e38\u6d89\u53ca\u9690\u542b\u7684\u4e8b\u5b9e\u9648\u8ff0\uff0c\u5176\u6027\u8d28\u548c\u5fc5\u8981\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u63a2\u7a76\u73b0\u5b9e\u4e16\u754c\u4e2d\u4fe1\u606f\u8bbf\u95ee\u5bf9\u8bdd\u7684\u771f\u5b9e\u5f62\u6001\u3002", "method": "\u57fa\u4e8e\u5927\u89c4\u6a21\u7528\u6237-ChatGPT\u5bf9\u8bdd\u6570\u636e\u96c6WildChat\u8fdb\u884c\u89c2\u5bdf\u6027\u7814\u7a76\uff0c\u63d0\u53d6\u5176\u4e2d\u7684\u4e8b\u5b9e\u4e3b\u5f20\u5e76\u6784\u5efaWildClaims\u6570\u636e\u96c6\uff0c\u5bf9\u6bcf\u6761\u4e3b\u5f20\u6807\u6ce8\u5176\u53ef\u9a8c\u8bc1\u6027\uff0c\u8fdb\u800c\u7edf\u8ba1\u5206\u6790\u9690\u5f0f\u4fe1\u606f\u8bbf\u95ee\u7684\u53d1\u751f\u9891\u7387\u4e0e\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u4fdd\u5b88\u4f30\u8ba118%\u523051%\u7684\u5bf9\u8bdd\u5305\u542b\u53ef\u9a8c\u8bc1\u7684\u4e8b\u5b9e\u4e3b\u5f20\uff0c\u975e\u4fdd\u5b88\u4f30\u8ba1\u53ef\u8fbe76%\uff0c\u8868\u660e\u9690\u5f0f\u4fe1\u606f\u83b7\u53d6\u5728\u5b9e\u9645\u5bf9\u8bdd\u4e2d\u6781\u4e3a\u666e\u904d\u3002", "conclusion": "\u5e94\u6269\u5c55\u4fe1\u606f\u8bbf\u95ee\u7684\u7814\u7a76\u8303\u7574\uff0c\u4ece\u4f20\u7edf\u7684\u663e\u5f0f\u67e5\u8be2\u8f6c\u5411\u66f4\u5e7f\u6cdb\u7684\u9690\u5f0f\u4fe1\u606f\u63d0\u4f9b\u573a\u666f\uff0c\u4ee5\u66f4\u597d\u652f\u6301\u548c\u8bc4\u4f30\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002"}}
{"id": "2509.17985", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2509.17985", "abs": "https://arxiv.org/abs/2509.17985", "authors": ["Geonung Kim", "Janghyeok Han", "Sunghyun Cho"], "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models", "comment": "Project page: https://kimgeonung.github.io/VideoFrom3D/", "summary": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVideoFrom3D\uff0c\u4e00\u79cd\u4ece\u7c97\u7565\u51e0\u4f55\u3001\u76f8\u673a\u8f68\u8ff9\u548c\u53c2\u8003\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u50cf\u4e0e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u65f6\u5e8f\u4e00\u81f4\u7684\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u96be\u4ee5\u5728\u590d\u6742\u573a\u666f\u4e2d\u540c\u65f6\u4fdd\u8bc1\u89c6\u89c9\u8d28\u91cf\u3001\u8fd0\u52a8\u81ea\u7136\u6027\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u4e14\u7f3a\u4e4f\u914d\u5bf9\u76843D\u6a21\u578b\u4e0e\u771f\u5b9e\u56fe\u50cf\u6570\u636e\uff0c\u9650\u5236\u4e86\u9ad8\u8d28\u91cf3D\u573a\u666f\u89c6\u9891\u7684\u751f\u6210\u3002", "method": "\u63d0\u51fa\u5305\u542b\u7a00\u758f\u951a\u70b9\u89c6\u56fe\u751f\u6210\uff08SAG\uff09\u548c\u51e0\u4f55\u5f15\u5bfc\u751f\u6210\u63d2\u503c\uff08GGI\uff09\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1aSAG\u5229\u7528\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u8de8\u89c6\u89d2\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u951a\u70b9\u89c6\u56fe\uff1bGGI\u57fa\u4e8e\u8fd9\u4e9b\u89c6\u56fe\uff0c\u7ed3\u5408\u6d41\u5f15\u5bfc\u76f8\u673a\u63a7\u5236\u548c\u7ed3\u6784\u5f15\u5bfc\uff0c\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d2\u503c\u4e2d\u95f4\u5e27\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\u5747\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u4e00\u81f4\u76843D\u573a\u666f\u89c6\u9891\uff0c\u4f18\u4e8e\u7b80\u5355\u53ca\u6269\u5c55\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u914d\u5bf9\u76843D-\u56fe\u50cf\u6570\u636e\u96c6\u3002", "conclusion": "VideoFrom3D\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u4e0e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u67423D\u573a\u666f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u4e0e\u4e00\u81f4\u6027\u96be\u9898\uff0c\u4e3a3D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u65b0\u65b9\u6848\u3002"}}
{"id": "2509.16438", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16438", "abs": "https://arxiv.org/abs/2509.16438", "authors": ["Mohamed Eltahir", "Osamah Sarraj", "Abdulrahman Alfrihidi", "Taha Alshatiri", "Mohammed Khurd", "Mohammed Bremoo", "Tanveer Hussain"], "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks", "comment": "Accepted at ArabicNLP 2025 (EMNLP 2025 workshop)", "summary": "Video-to-text and text-to-video retrieval are dominated by English benchmarks\n(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet\nArabic remains underserved, lacking localized evaluation metrics. We introduce\na three-stage framework, AutoArabic, utilizing state-of-the-art large language\nmodels (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,\nreducing the manual revision required by nearly fourfold. The framework\nincorporates an error detection module that automatically flags potential\ntranslation errors with 97% accuracy. Applying the framework to DiDeMo, a video\nretrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent\nArabic descriptions. An analysis of the translation errors is provided and\norganized into an insightful taxonomy to guide future Arabic localization\nefforts. We train a CLIP-style baseline with identical hyperparameters on the\nArabic and English variants of the benchmark, finding a moderate performance\ngap (about 3 percentage points at Recall@1), indicating that Arabic\nlocalization preserves benchmark difficulty. We evaluate three post-editing\nbudgets (zero/ flagged-only/ full) and find that performance improves\nmonotonically with more post-editing, while the raw LLM output (zero-budget)\nremains usable. To ensure reproducibility to other languages, we made the code\navailable at https://github.com/Tahaalshatiri/AutoArabic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAutoArabic\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u975e\u963f\u62c9\u4f2f\u8bed\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u57fa\u51c6\u7ffb\u8bd1\u4e3a\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\uff0c\u5e76\u5f15\u5165\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\u4ee5\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\uff0c\u6700\u7ec8\u751f\u6210\u4e86DiDeMo-AR\u6570\u636e\u96c6\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u5728\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u9886\u57df\u7f3a\u4e4f\u672c\u5730\u5316\u8bc4\u4f30\u8d44\u6e90\uff0c\u73b0\u6709\u57fa\u51c6\u591a\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u9650\u5236\u4e86\u963f\u62c9\u4f2f\u8bed\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6AutoArabic\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ffb\u8bd1\uff1b2\uff09\u901a\u8fc7\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\u81ea\u52a8\u6807\u8bb0\u6f5c\u5728\u9519\u8bef\uff08\u51c6\u786e\u7387\u8fbe97%\uff09\uff1b3\uff09\u751f\u6210\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7248\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6DiDeMo-AR\uff08\u542b40,144\u6761\u63cf\u8ff0\uff09\u3002\u540c\u65f6\u8bad\u7ec3CLIP\u98ce\u683c\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u6570\u636e\u4e0a\u6bd4\u8f83\u6027\u80fd\u3002", "result": "\u6210\u529f\u6784\u5efaDiDeMo-AR\u6570\u636e\u96c6\uff0c\u7ffb\u8bd1\u9519\u8bef\u51cf\u5c11\u8fd1\u56db\u500d\uff1b\u9519\u8bef\u68c0\u6d4b\u6a21\u5757\u51c6\u786e\u7387\u4e3a97%\uff1b\u963f\u62c9\u4f2f\u8bed\u4e0e\u82f1\u8bed\u57fa\u51c6\u95f4\u6027\u80fd\u5dee\u8ddd\u7ea6\u4e3aRecall@1\u4e0b\u964d3\u4e2a\u767e\u5206\u70b9\uff1b\u540e\u7f16\u8f91\u6295\u5165\u8d8a\u591a\uff0c\u6027\u80fd\u8d8a\u9ad8\uff0c\u4f46\u96f6\u9884\u7b97\u4e0b\u7684\u539f\u59cbLLM\u8f93\u51fa\u4ecd\u53ef\u7528\u3002", "conclusion": "AutoArabic\u6709\u6548\u652f\u6301\u963f\u62c9\u4f2f\u8bed\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u7684\u672c\u5730\u5316\uff0c\u4fdd\u6301\u57fa\u51c6\u96be\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6210\u672c\uff0c\u4e14\u6846\u67b6\u53ef\u590d\u7528\u4e8e\u5176\u4ed6\u8bed\u8a00\u3002"}}
{"id": "2509.16550", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16550", "abs": "https://arxiv.org/abs/2509.16550", "authors": ["Yinghao Wu", "Shuhong Hou", "Haowen Zheng", "Yichen Li", "Weiyi Lu", "Xun Zhou", "Yitian Shao"], "title": "TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation", "comment": "8 pages, 7 figures", "summary": "Robotic manipulation tasks such as inserting a key into a lock or plugging a\nUSB device into a port can fail when visual perception is insufficient to\ndetect misalignment. In these situations, touch sensing is crucial for the\nrobot to monitor the task's states and make precise, timely adjustments.\nCurrent touch sensing solutions are either insensitive to detect subtle changes\nor demand excessive sensor data. Here, we introduce TranTac, a data-efficient\nand low-cost tactile sensing and control framework that integrates a single\ncontact-sensitive 6-axis inertial measurement unit within the elastomeric tips\nof a robotic gripper for completing fine insertion tasks. Our customized\nsensing system can detect dynamic translational and torsional deformations at\nthe micrometer scale, enabling the tracking of visually imperceptible pose\nchanges of the grasped object. By leveraging transformer-based encoders and\ndiffusion policy, TranTac can imitate human insertion behaviors using transient\ntactile cues detected at the gripper's tip during insertion processes. These\ncues enable the robot to dynamically control and correct the 6-DoF pose of the\ngrasped object. When combined with vision, TranTac achieves an average success\nrate of 79% on object grasping and insertion tasks, outperforming both\nvision-only policy and the one augmented with end-effector 6D force/torque\nsensing. Contact localization performance is also validated through\ntactile-only misaligned insertion tasks, achieving an average success rate of\n88%. We assess the generalizability by training TranTac on a single prism-slot\npair and testing it on unseen data, including a USB plug and a metal key, and\nfind that the insertion tasks can still be completed with an average success\nrate of nearly 70%. The proposed framework may inspire new robotic tactile\nsensing systems for delicate manipulation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTranTac\u7684\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u89e6\u89c9\u611f\u77e5\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u5355\u4e2a\u5d4c\u5165\u5f0f6\u8f74\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u5b9e\u73b0\u673a\u5668\u4eba\u5939\u6301\u5668\u6307\u5c16\u7684\u5fae\u7c73\u7ea7\u52a8\u6001\u5e73\u79fb\u548c\u626d\u8f6c\u5f62\u53d8\u68c0\u6d4b\uff0c\u7ed3\u5408\u89c6\u89c9\u4e0e\u89e6\u89c9\u4fe1\u606f\uff0c\u5728\u7cbe\u7ec6\u63d2\u5165\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u5728\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u4ee5\u68c0\u6d4b\u5bf9\u9f50\u8bef\u5dee\u7684\u60c5\u51b5\u4e0b\uff08\u5982\u63d2\u94a5\u5319\u6216USB\uff09\uff0c\u673a\u5668\u4eba\u64cd\u4f5c\u5bb9\u6613\u5931\u8d25\uff0c\u56e0\u6b64\u9700\u8981\u7075\u654f\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e6\u89c9\u611f\u77e5\u6765\u5b9e\u65f6\u76d1\u6d4b\u5e76\u8c03\u6574\u7269\u4f53\u59ff\u6001\u3002", "method": "\u5c06\u4e00\u4e2a\u9ad8\u7075\u654f\u5ea6\u76846\u8f74\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u96c6\u6210\u5230\u5f39\u6027\u5939\u6301\u5668\u6307\u5c16\u4e2d\uff0c\u68c0\u6d4b\u77ac\u6001\u89e6\u89c9\u4fe1\u53f7\uff1b\u91c7\u7528\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u548c\u6269\u6563\u7b56\u7565\u7684\u6a21\u578b\uff0c\u6a21\u4eff\u4eba\u7c7b\u63d2\u5165\u884c\u4e3a\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u88ab\u6293\u53d6\u7269\u4f53\u76846\u81ea\u7531\u5ea6\u59ff\u6001\u3002", "result": "\u7ed3\u5408\u89c6\u89c9\u65f6\uff0c\u5e73\u5747\u6210\u529f\u7387\u8fbe79%\uff0c\u4f18\u4e8e\u4ec5\u89c6\u89c9\u6216\u589e\u5f3a\u529b/\u626d\u77e9\u611f\u77e5\u7684\u65b9\u6cd5\uff1b\u7eaf\u89e6\u89c9\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8fbe88%\uff1b\u5728\u672a\u89c1\u5bf9\u8c61\uff08\u5982USB\u548c\u91d1\u5c5e\u94a5\u5319\uff09\u4e0a\u6d4b\u8bd5\u4ecd\u8fbe\u5230\u8fd170%\u7684\u6210\u529f\u7387\u3002", "conclusion": "TranTac\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e6\u89c9\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u7cbe\u7ec6\u63d2\u5165\u4efb\u52a1\u4e2d\u6709\u6548\u5229\u7528\u77ac\u6001\u89e6\u89c9\u4fe1\u53f7\u8fdb\u884c\u59ff\u6001\u6821\u6b63\uff0c\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u89e6\u89c9\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16400", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16400", "abs": "https://arxiv.org/abs/2509.16400", "authors": ["Huy Nghiem", "Phuong-Anh Nguyen-Le", "John Prindle", "Rachel Rudinger", "Hal Daum\u00e9 III"], "title": "'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?", "comment": "EMNLP 2025, ver 1, 35 pages", "summary": "Large Language Models (LLMs) are increasingly involved in high-stakes\ndomains, yet how they reason about socially sensitive decisions remains\nunderexplored. We present a large-scale audit of LLMs' treatment of\nsocioeconomic status (SES) in college admissions decisions using a novel\ndual-process framework inspired by cognitive science. Leveraging a synthetic\ndataset of 30,000 applicant profiles grounded in real-world correlations, we\nprompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2\nmodes: a fast, decision-only setup (System 1) and a slower, explanation-based\nsetup (System 2). Results from 5 million prompts reveal that LLMs consistently\nfavor low-SES applicants -- even when controlling for academic performance --\nand that System 2 amplifies this tendency by explicitly invoking SES as\ncompensatory justification, highlighting both their potential and volatility as\ndecision-makers. We then propose DPAF, a dual-process audit framework to probe\nLLMs' reasoning behaviors in sensitive applications.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u7684\u53cc\u8fc7\u7a0b\u6846\u67b6\uff0c\u5927\u89c4\u6a21\u5ba1\u8ba1\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5927\u5b66\u5f55\u53d6\u51b3\u7b56\u4e2d\u5bf9\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\uff08SES\uff09\u7684\u5904\u7406\u65b9\u5f0f\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u504f\u597d\u4f4eSES\u7533\u8bf7\u8005\uff0c\u4e14\u5728\u9700\u8981\u89e3\u91ca\u7684\u6162\u901f\u51b3\u7b56\u6a21\u5f0f\u4e0b\u8fd9\u79cd\u503e\u5411\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d89\u53ca\u793e\u4f1a\u654f\u611f\u8bae\u9898\uff08\u5982\u5927\u5b66\u5f55\u53d6\uff09\u4e2d\u7684\u51b3\u7b56\u63a8\u7406\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5bf9\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u516c\u5e73\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b3\u4e07\u4e2a\u4eba\u5de5\u751f\u6210\u7533\u8bf7\u4eba\u8d44\u6599\u7684\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u76f8\u5173\u6027\uff0c\u5e76\u5728\u4e24\u79cd\u6a21\u5f0f\u4e0b\uff08\u5feb\u901f\u76f4\u63a5\u51b3\u7b56\u7684System 1\u548c\u6162\u901f\u9700\u89e3\u91ca\u7684System 2\uff09\u6d4b\u8bd5\u56db\u4e2a\u5f00\u6e90\u5927\u6a21\u578b\uff08Qwen 2\u3001Mistral v0.3\u3001Gemma 2\u3001Llama 3.1\uff09\uff0c\u5206\u6790\u5176\u51b3\u7b56\u4e0e\u89e3\u91ca\u4e2d\u7684SES\u504f\u597d\u3002", "result": "\u5728500\u4e07\u6b21\u63d0\u793a\u4e0b\uff0c\u6240\u6709\u6a21\u578b\u5747\u4e00\u81f4\u504f\u597d\u4f4eSES\u7533\u8bf7\u4eba\uff0c\u5373\u4f7f\u63a7\u5236\u5b66\u672f\u8868\u73b0\u540e\u4ecd\u5b58\u5728\uff1bSystem 2\u6a21\u5f0f\u8fdb\u4e00\u6b65\u5f3a\u5316\u8be5\u504f\u597d\uff0c\u56e0\u6a21\u578b\u5e38\u5c06SES\u4f5c\u4e3a\u8865\u507f\u6027\u7406\u7531\u663e\u5f0f\u5f15\u7528\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u867d\u53ef\u80fd\u4f53\u73b0\u516c\u5e73\u610f\u56fe\uff0c\u4f46\u4e5f\u663e\u793a\u5176\u51b3\u7b56\u6613\u53d7\u793e\u4f1a\u56e0\u7d20\u5f71\u54cd\uff0c\u5177\u6709\u6f5c\u5728\u4e0d\u7a33\u5b9a\u6027\uff1b\u63d0\u51fa\u7684DPAF\u53cc\u8fc7\u7a0b\u5ba1\u8ba1\u6846\u67b6\u6709\u52a9\u4e8e\u6df1\u5165\u63a2\u67e5\u6b64\u7c7b\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u3002"}}
{"id": "2509.16354", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16354", "abs": "https://arxiv.org/abs/2509.16354", "authors": ["Sivan Sarafian", "Yehudit Aperstein"], "title": "Improving Deep Tabular Learning", "comment": "18 pages, 4 figures", "summary": "Tabular data remain a dominant form of real-world information but pose\npersistent challenges for deep learning due to heterogeneous feature types,\nlack of natural structure, and limited label-preserving augmentations. As a\nresult, ensemble models based on decision trees continue to dominate benchmark\nleaderboards. In this work, we introduce RuleNet, a transformer-based\narchitecture specifically designed for deep tabular learning. RuleNet\nincorporates learnable rule embeddings in a decoder, a piecewise linear\nquantile projection for numerical features, and feature masking ensembles for\nrobustness and uncertainty estimation. Evaluated on eight benchmark datasets,\nRuleNet matches or surpasses state-of-the-art tree-based methods in most cases,\nwhile remaining computationally efficient, offering a practical neural\nalternative for tabular prediction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RuleNet\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7528\u4e8e\u8868\u683c\u6570\u636e\u5b66\u4e60\u7684\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u7684\u6811\u6a21\u578b\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8868\u683c\u6570\u636e\u7531\u4e8e\u7279\u5f81\u7c7b\u578b\u5f02\u6784\u3001\u7f3a\u4e4f\u81ea\u7136\u7ed3\u6784\u4ee5\u53ca\u6807\u7b7e\u4fdd\u6301\u589e\u5f3a\u624b\u6bb5\u6709\u9650\uff0c\u7ed9\u6df1\u5ea6\u5b66\u4e60\u5e26\u6765\u4e86\u6301\u7eed\u6311\u6218\uff0c\u76ee\u524d\u4ecd\u4ee5\u51b3\u7b56\u6811\u96c6\u6210\u6a21\u578b\u4e3a\u4e3b\u5bfc\u3002", "method": "\u5f15\u5165RuleNet\uff0c\u91c7\u7528\u53ef\u5b66\u4e60\u89c4\u5219\u5d4c\u5165\u7684\u89e3\u7801\u5668\u3001\u5206\u6bb5\u7ebf\u6027\u5206\u4f4d\u6570\u6295\u5f71\u5904\u7406\u6570\u503c\u7279\u5f81\uff0c\u4ee5\u53ca\u7279\u5f81\u63a9\u7801\u96c6\u6210\u6765\u63d0\u5347\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRuleNet\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u6811\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "RuleNet\u4e3a\u8868\u683c\u6570\u636e\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.16456", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16456", "abs": "https://arxiv.org/abs/2509.16456", "authors": ["Jiahao Yu", "Zelei Cheng", "Xian Wu", "Xinyu Xing"], "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Large language models (LLMs) are increasingly used in various domains,\nshowing impressive potential on different tasks. Recently, reasoning LLMs have\nbeen proposed to improve the \\textit{reasoning} or \\textit{thinking}\ncapabilities of LLMs to solve complex problems. Despite the promising results\nof reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs\nstill remains a significant challenge. While existing optimization methods have\nadvanced the LLM reasoning capabilities, they often treat reasoning\ntrajectories as a whole, without considering the underlying critical steps\nwithin the trajectory. In this paper, we introduce \\textbf{G}uided\n\\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that\ndives into the reasoning process to enable more effective improvements. GPO\nfirst identifies the `critical step' within a reasoning trajectory - a point\nthat the model must carefully proceed to succeed at the problem. We locate the\ncritical step by estimating the advantage function. GPO then resets the policy\nto the critical step, samples the new rollout and prioritizes the learning\nprocess on those rollouts. This focus allows the model to learn more\neffectively from pivotal moments within the reasoning process to improve the\nreasoning performance. We demonstrate that GPO is a general strategy that can\nbe integrated with various optimization methods to improve reasoning\nperformance. Besides theoretical analysis, our experiments across challenging\nreasoning benchmarks show that GPO can consistently and significantly enhance\nthe performance of existing optimization methods, showcasing its effectiveness\nand generalizability in improving LLM reasoning by concentrating on pivotal\nmoments within the generation process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GPO\uff08Guided Pivotal Optimization\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\u5e76\u96c6\u4e2d\u4f18\u5316\u8fd9\u4e9b\u5173\u952e\u70b9\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u5c06\u6574\u4e2a\u63a8\u7406\u8def\u5f84\u89c6\u4e3a\u6574\u4f53\uff0c\u5ffd\u89c6\u4e86\u5176\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u5bfc\u81f4\u591a\u6b65\u63a8\u7406\u80fd\u529b\u63d0\u5347\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u805a\u7126\u4e8e\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5173\u952e\u8282\u70b9\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "GPO\u9996\u5148\u901a\u8fc7\u4f30\u8ba1\u4f18\u52bf\u51fd\u6570\u8bc6\u522b\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u201c\u5173\u952e\u6b65\u9aa4\u201d\uff0c\u7136\u540e\u5c06\u7b56\u7565\u91cd\u7f6e\u5230\u8be5\u6b65\u9aa4\uff0c\u751f\u6210\u65b0\u7684 rollout \u5e76\u4f18\u5148\u5b66\u4e60\u8fd9\u4e9b rollout\uff0c\u4ece\u800c\u96c6\u4e2d\u4f18\u5316\u5173\u952e\u51b3\u7b56\u70b9\u3002", "result": "\u5728\u591a\u4e2a\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPO\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "GPO\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u5173\u6ce8\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u65f6\u523b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2509.17469", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17469", "abs": "https://arxiv.org/abs/2509.17469", "authors": ["Matteo Cancellieri", "Alaa El-Ebshihy", "Tobias Fink", "Maik Fr\u00f6be", "Petra Galu\u0161\u010d\u00e1kov\u00e1", "Gabriela Gonzalez-Saez", "Lorraine Goeuriot", "David Iommi", "J\u00fcri Keller", "Petr Knoth", "Philippe Mulhem", "Florina Piroi", "David Pride", "Philipp Schaer"], "title": "LongEval at CLEF 2025: Longitudinal Evaluation of IR Systems on Web and Scientific Data", "comment": null, "summary": "The LongEval lab focuses on the evaluation of information retrieval systems\nover time. Two datasets are provided that capture evolving search scenarios\nwith changing documents, queries, and relevance assessments. Systems are\nassessed from a temporal perspective-that is, evaluating retrieval\neffectiveness as the data they operate on changes. In its third edition,\nLongEval featured two retrieval tasks: one in the area of ad-hoc web retrieval,\nand another focusing on scientific article retrieval. We present an overview of\nthis year's tasks and datasets, as well as the participating systems. A total\nof 19 teams submitted their approaches, which we evaluated using nDCG and a\nvariety of measures that quantify changes in retrieval effectiveness over time.", "AI": {"tldr": "LongEval\u5b9e\u9a8c\u5ba4\u4e13\u6ce8\u4e8e\u968f\u65f6\u95f4\u63a8\u79fb\u5bf9\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u4e24\u4e2a\u53cd\u6620\u6587\u6863\u3001\u67e5\u8be2\u548c\u76f8\u5173\u6027\u5224\u65ad\u52a8\u6001\u53d8\u5316\u7684\u6570\u636e\u96c6\u3002\u4eca\u5e74\u7684\u7b2c\u4e09\u7248\u5305\u542b\u4e24\u4e2a\u68c0\u7d22\u4efb\u52a1\uff1a\u5373\u5e2d\u7f51\u9875\u68c0\u7d22\u548c\u79d1\u5b66\u8bba\u6587\u68c0\u7d22\uff0c\u5171\u670919\u652f\u56e2\u961f\u63d0\u4ea4\u4e86\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528nDCG\u53ca\u591a\u79cd\u8861\u91cf\u68c0\u7d22\u6548\u679c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6307\u6807\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5728\u6570\u636e\u52a8\u6001\u6f14\u53d8\u73af\u5883\u4e0b\u7684\u957f\u671f\u8868\u73b0\uff0c\u9700\u8981\u4ece\u65f6\u95f4\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u7684\u68c0\u7d22\u6548\u679c\u3002", "method": "\u63d0\u4f9b\u4e24\u4e2a\u6db5\u76d6\u52a8\u6001\u6587\u6863\u3001\u67e5\u8be2\u548c\u76f8\u5173\u6027\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u7f6e\u4e24\u4e2a\u68c0\u7d22\u4efb\u52a1\uff08\u5373\u5e2d\u7f51\u9875\u68c0\u7d22\u548c\u79d1\u5b66\u6587\u732e\u68c0\u7d22\uff09\uff0c\u5e76\u91c7\u7528nDCG\u4ee5\u53ca\u591a\u79cd\u8861\u91cf\u6548\u679c\u6ce2\u52a8\u7684\u6307\u6807\u5bf9\u53c2\u4e0e\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5171\u6536\u5230\u6765\u81ea19\u4e2a\u56e2\u961f\u7684\u63d0\u4ea4\uff0c\u7cfb\u7edf\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u63a5\u53d7\u4e86\u57fa\u4e8e\u65f6\u95f4\u53d8\u5316\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "LongEval\u4e3a\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5728\u65f6\u95f4\u6f14\u5316\u573a\u666f\u4e0b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u5e73\u53f0\uff0c\u7a81\u663e\u4e86\u6301\u7eed\u8bc4\u4f30\u5728\u73b0\u5b9e\u68c0\u7d22\u73af\u5883\u4e2d\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.16773", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.16773", "abs": "https://arxiv.org/abs/2509.16773", "authors": ["Mohamad Mofeed Chaar", "Jamal Raiyn", "Galia Weidl"], "title": "Improve bounding box in Carla Simulator", "comment": "9 pages, 12 figures,VEHITS Conference 2024", "summary": "The CARLA simulator (Car Learning to Act) serves as a robust platform for\ntesting algorithms and generating datasets in the field of Autonomous Driving\n(AD). It provides control over various environmental parameters, enabling\nthorough evaluation. Development bounding boxes are commonly utilized tools in\ndeep learning and play a crucial role in AD applications. The predominant\nmethod for data generation in the CARLA Simulator involves identifying and\ndelineating objects of interest, such as vehicles, using bounding boxes. The\noperation in CARLA entails capturing the coordinates of all objects on the map,\nwhich are subsequently aligned with the sensor's coordinate system at the ego\nvehicle and then enclosed within bounding boxes relative to the ego vehicle's\nperspective. However, this primary approach encounters challenges associated\nwith object detection and bounding box annotation, such as ghost boxes.\nAlthough these procedures are generally effective at detecting vehicles and\nother objects within their direct line of sight, they may also produce false\npositives by identifying objects that are obscured by obstructions. We have\nenhanced the primary approach with the objective of filtering out unwanted\nboxes. Performance analysis indicates that the improved approach has achieved\nhigh accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684CARLA\u6a21\u62df\u5668\u4e2d\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u751f\u6210\u7684\u8fb9\u754c\u6846\u65b9\u6cd5\uff0c\u6709\u6548\u8fc7\u6ee4\u4e86\u88ab\u906e\u6321\u7269\u4f53\u5bfc\u81f4\u7684\u865a\u5047\u68c0\u6d4b\uff08\u5982\u5e7d\u7075\u6846\uff09\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\uff0c\u4f20\u7edf\u8fb9\u754c\u6846\u751f\u6210\u65b9\u6cd5\u5bb9\u6613\u56e0\u906e\u6321\u4ea7\u751f\u9519\u8bef\u6807\u6ce8\uff08\u5982\u5e7d\u7075\u6846\uff09\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u8bad\u7ec3\u4e0e\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5bf9\u8c61\u68c0\u6d4b\u4e0e\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u5728\u539f\u6709\u57fa\u4e8e\u5750\u6807\u6620\u5c04\u7684\u8fb9\u754c\u6846\u751f\u6210\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u906e\u6321\u5224\u65ad\u673a\u5236\uff0c\u901a\u8fc7\u5206\u6790\u7269\u4f53\u4e0e\u4f20\u611f\u5668\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u8fc7\u6ee4\u6389\u88ab\u969c\u788d\u7269\u906e\u6321\u7684\u65e0\u6548\u8fb9\u754c\u6846\uff0c\u4ece\u800c\u63d0\u5347\u6807\u6ce8\u51c6\u786e\u6027\u3002", "result": "\u6539\u8fdb\u540e\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u865a\u5047\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6027\u80fd\u5206\u6790\u663e\u793a\u5176\u5177\u6709\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86CARLA\u4e2d\u7531\u906e\u6321\u5f15\u8d77\u7684\u8fb9\u754c\u6846\u8bef\u68c0\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u7b97\u6cd5\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.16452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16452", "abs": "https://arxiv.org/abs/2509.16452", "authors": ["Son Hai Nguyen", "Diwei Wang", "Jinhyeok Jang", "Hyewon Seo"], "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models", "comment": null, "summary": "Accurate vision-based action recognition is crucial for developing autonomous\nrobots that can operate safely and reliably in complex, real-world\nenvironments. In this work, we advance video-based recognition of indoor daily\nactions for robotic perception by leveraging vision-language models (VLMs)\nenriched with domain-specific knowledge. We adapt a prompt-learning framework\nin which class-level textual descriptions of each action are embedded as\nlearnable prompts into a frozen pre-trained VLM backbone. Several strategies\nfor structuring and encoding these textual descriptions are designed and\nevaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our\nmethod, using only RGB video inputs at test time, achieves over 95\\% accuracy\nand outperforms state-of-the-art approaches. These results highlight the\neffectiveness of knowledge-augmented prompts in enabling robust action\nrecognition with minimal supervision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6587\u672c\u63d0\u793a\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u5ba4\u5185\u65e5\u5e38\u52a8\u4f5c\u7684\u7406\u89e3\uff0c\u5728\u4ec5\u4f7f\u7528RGB\u89c6\u9891\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e2d\u5b89\u5168\u53ef\u9760\u5730\u6267\u884c\u52a8\u4f5c\u8bc6\u522b\u7684\u80fd\u529b\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u52a8\u4f5c\u7684\u7c7b\u522b\u7ea7\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u5d4c\u5165\u5230\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3VLM\u4e3b\u5e72\u7f51\u7edc\u4e2d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u79cd\u6587\u672c\u63cf\u8ff0\u7684\u7ed3\u6784\u5316\u548c\u7f16\u7801\u7b56\u7565\u3002", "result": "\u5728ETRI-Activity3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528RGB\u89c6\u9891\u8f93\u5165\u5373\u8fbe\u5230\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u77e5\u8bc6\u589e\u5f3a\u7684\u63d0\u793a\u80fd\u6709\u6548\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0c\u4e14\u5728\u6700\u5c11\u76d1\u7763\u4e0b\u5b9e\u73b0\u9c81\u68d2\u8bc6\u522b\u3002"}}
{"id": "2509.16611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16611", "abs": "https://arxiv.org/abs/2509.16611", "authors": ["Xiwei Zhao", "Yiwei Wang", "Yansong Wu", "Fan Wu", "Teng Sun", "Zhonghua Miao", "Sami Haddadin", "Alois Knoll"], "title": "Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly", "comment": null, "summary": "Modern manufacturing demands robotic assembly systems with enhanced\nflexibility and reliability. However, traditional approaches often rely on\nprogramming tailored to each product by experts for fixed settings, which are\ninherently inflexible to product changes and lack the robustness to handle\nvariations. As Behavior Trees (BTs) are increasingly used in robotics for their\nmodularity and reactivity, we propose a novel hierarchical framework,\nVideo-to-BT, that seamlessly integrates high-level cognitive planning with\nlow-level reactive control, with BTs serving both as the structured output of\nplanning and as the governing structure for execution. Our approach leverages a\nVision-Language Model (VLM) to decompose human demonstration videos into\nsubtasks, from which Behavior Trees are generated. During the execution, the\nplanned BTs combined with real-time scene interpretation enable the system to\noperate reactively in the dynamic environment, while VLM-driven replanning is\ntriggered upon execution failure. This closed-loop architecture ensures\nstability and adaptivity. We validate our framework on real-world assembly\ntasks through a series of experiments, demonstrating high planning reliability,\nrobust performance in long-horizon assembly tasks, and strong generalization\nacross diverse and perturbed conditions. Project website:\nhttps://video2bt.github.io/video2bt_page/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVideo-to-BT\u7684\u5206\u5c42\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u4eba\u7c7b\u793a\u8303\u89c6\u9891\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u751f\u6210\u884c\u4e3a\u6811\uff08BT\uff09\uff0c\u5b9e\u73b0\u4ece\u9ad8\u5c42\u8ba4\u77e5\u89c4\u5212\u5230\u4f4e\u5c42\u53cd\u5e94\u63a7\u5236\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5177\u5907\u9ad8\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u7cfb\u7edf\u4f9d\u8d56\u4e13\u5bb6\u7f16\u7a0b\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u5e94\u5bf9\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u4ee3\u5236\u9020\u5bf9\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u89e3\u6790\u4eba\u7c7b\u793a\u8303\u89c6\u9891\uff0c\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u751f\u6210\u884c\u4e3a\u6811\uff08BT\uff09\uff1b\u6267\u884c\u65f6\u7ed3\u5408\u5b9e\u65f6\u573a\u666f\u7406\u89e3\u548cVLM\u9a71\u52a8\u7684\u5931\u8d25\u91cd\u89c4\u5212\uff0c\u5f62\u6210\u95ed\u73af\u67b6\u6784\u3002", "result": "\u5728\u771f\u5b9e\u88c5\u914d\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8868\u73b0\u51fa\u9ad8\u89c4\u5212\u53ef\u9760\u6027\u3001\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u4ee5\u53ca\u5728\u591a\u6837\u5316\u548c\u6270\u52a8\u73af\u5883\u4e0b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Video-to-BT\u6846\u67b6\u6210\u529f\u878d\u5408\u4e86\u8ba4\u77e5\u89c4\u5212\u4e0e\u53cd\u5e94\u63a7\u5236\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u88c5\u914d\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3001\u7a33\u5b9a\u6027\u548c\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.16413", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16413", "abs": "https://arxiv.org/abs/2509.16413", "authors": ["Richard Diehl Martinez", "David Demitri Africa", "Yuval Weiss", "Suchir Salhan", "Ryan Daniels", "Paula Buttery"], "title": "Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research", "comment": null, "summary": "Building language models (LMs), especially small and medium ones, remains\nmore art than science. While large LMs often improve by sheer scale, it is\nstill unclear why many design choices work. For small LMs, this uncertainty is\nmore limiting: tight parameter budgets make each decision critical, yet\nresearchers still lack systematic, scientific ways to test and refine new\nideas.\n  We introduce Pico, a lightweight, modular framework that enables systematic,\nhypothesis-driven research for small and medium-scale language model\ndevelopment. Pico consists of two libraries that together provide a practical\nsandbox where researchers can make targeted changes to a model's architecture\nor training procedures and directly observe their effects on the model's\nbehavior. To support reproducible experimentation, we also release a suite of\nbaseline models, pico-decoder, trained under standardized conditions and\nopen-sourced for the community. Case studies highlight how Pico can support\niterative small LM design and analysis.", "AI": {"tldr": "Pico\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u65e8\u5728\u652f\u6301\u5c0f\u89c4\u6a21\u548c\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u3001\u5047\u8bbe\u9a71\u52a8\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u73af\u5883\u548c\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u8bbe\u8ba1\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u79d1\u5b66\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u9009\u62e9\u7684\u5f71\u54cd\u4e0d\u660e\u786e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u4f18\u5316\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3aPico\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5e93\uff0c\u652f\u6301\u5bf9\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u4fee\u6539\uff0c\u5e76\u89c2\u5bdf\u5176\u5f71\u54cd\uff1b\u540c\u65f6\u53d1\u5e03\u6807\u51c6\u5316\u8bad\u7ec3\u7684\u5f00\u6e90\u57fa\u7ebf\u6a21\u578bpico-decoder\u3002", "result": "Pico\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5b9e\u9a8c\u6c99\u76d2\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7684\u5c0f\u6a21\u578b\u7814\u7a76\uff0c\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u8fed\u4ee3\u8bbe\u8ba1\u548c\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Pico\u4e3a\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u3001\u79d1\u5b66\u5316\u7684\u5de5\u5177\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53ef\u590d\u73b0\u5b9e\u9a8c\u4e0e\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2509.16357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16357", "abs": "https://arxiv.org/abs/2509.16357", "authors": ["Aniruddh Raghu", "Sebastian Ober", "Maxwell Kazman", "Hunter Elliott"], "title": "Guided Sequence-Structure Generative Modeling for Iterative Antibody Optimization", "comment": "GEM Workshop, ICLR 2025", "summary": "Therapeutic antibody candidates often require extensive engineering to\nimprove key functional and developability properties before clinical\ndevelopment. This can be achieved through iterative design, where starting\nmolecules are optimized over several rounds of in vitro experiments. While\nprotein structure can provide a strong inductive bias, it is rarely used in\niterative design due to the lack of structural data for continually evolving\nlead molecules over the course of optimization. In this work, we propose a\nstrategy for iterative antibody optimization that leverages both sequence and\nstructure as well as accumulating lab measurements of binding and\ndevelopability. Building on prior work, we first train a sequence-structure\ndiffusion generative model that operates on antibody-antigen complexes. We then\noutline an approach to use this model, together with carefully predicted\nantibody-antigen complexes, to optimize lead candidates throughout the\niterative design process. Further, we describe a guided sampling approach that\nbiases generation toward desirable properties by integrating models trained on\nexperimental data from iterative design. We evaluate our approach in multiple\nin silico and in vitro experiments, demonstrating that it produces\nhigh-affinity binders at multiple stages of an active antibody optimization\ncampaign.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5e8f\u5217\u3001\u7ed3\u6784\u548c\u5b9e\u9a8c\u6570\u636e\u7684\u8fed\u4ee3\u6297\u4f53\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u6269\u6563\u751f\u6210\u6a21\u578b\u548c\u5f15\u5bfc\u91c7\u6837\u65b9\u6cd5\uff0c\u5728\u591a\u9636\u6bb5\u4f18\u5316\u4e2d\u751f\u6210\u9ad8\u4eb2\u548c\u529b\u6297\u4f53\u3002", "motivation": "\u4f20\u7edf\u6297\u4f53\u4f18\u5316\u7f3a\u4e4f\u5bf9\u6301\u7eed\u6f14\u5316\u5206\u5b50\u7684\u7ed3\u6784\u4fe1\u606f\u5229\u7528\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6574\u5408\u7ed3\u6784\u4e0e\u5b9e\u9a8c\u6570\u636e\u7684\u65b0\u578b\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u57fa\u4e8e\u6297\u4f53-\u6297\u539f\u590d\u5408\u7269\u7684\u5e8f\u5217-\u7ed3\u6784\u6269\u6563\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u6d4b\u7684\u590d\u5408\u7269\u7ed3\u6784\u548c\u5b9e\u9a8c\u6570\u636e\u9a71\u52a8\u7684\u5f15\u5bfc\u91c7\u6837\uff0c\u7528\u4e8e\u8fed\u4ee3\u4f18\u5316\u6297\u4f53\u5019\u9009\u5206\u5b50\u3002", "result": "\u5728\u591a\u79cd\u8ba1\u7b97\u673a\u548c\u4f53\u5916\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6297\u4f53\u4f18\u5316\u7684\u4e0d\u540c\u9636\u6bb5\u751f\u6210\u9ad8\u4eb2\u548c\u529b\u7ed3\u5408\u4f53\u3002", "conclusion": "\u8be5\u7b56\u7565\u6709\u6548\u6574\u5408\u4e86\u7ed3\u6784\u4fe1\u606f\u4e0e\u7d2f\u79ef\u5b9e\u9a8c\u6570\u636e\uff0c\u63d0\u5347\u4e86\u6297\u4f53\u8fed\u4ee3\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2509.16547", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16547", "abs": "https://arxiv.org/abs/2509.16547", "authors": ["Adrian Wurm"], "title": "Checking extracted rules in Neural Networks", "comment": "7 pages, one figure", "summary": "In this paper we investigate formal verification of extracted rules for\nNeural Networks under a complexity theoretic point of view. A rule is a global\nproperty or a pattern concerning a large portion of the input space of a\nnetwork. These rules are algorithmically extracted from networks in an effort\nto better understand their inner way of working. Here, three problems will be\nin the focus: Does a given set of rules apply to a given network? Is a given\nset of rules consistent or do the rules contradict themselves? Is a given set\nof rules exhaustive in the sense that for every input the output is determined?\nFinding algorithms that extract such rules out of networks has been\ninvestigated over the last 30 years, however, to the author's current\nknowledge, no attempt in verification was made until now. A lot of attempts of\nextracting rules use heuristics involving randomness and over-approximation, so\nit might be beneficial to know whether knowledge obtained in that way can\nactually be trusted.\n  We investigate the above questions for neural networks with ReLU-activation\nas well as for Boolean networks, each for several types of rules. We\ndemonstrate how these problems can be reduced to each other and show that most\nof them are co-NP-complete.", "AI": {"tldr": "\u672c\u6587\u4ece\u590d\u6742\u6027\u7406\u8bba\u7684\u89d2\u5ea6\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u89c4\u5219\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u95ee\u9898\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u89c4\u5219\u5728\u7ed9\u5b9a\u7f51\u7edc\u4e2d\u7684\u9002\u7528\u6027\u3001\u4e00\u81f4\u6027\u4e0e\u5b8c\u5907\u6027\uff0c\u5e76\u8bc1\u660e\u5927\u591a\u6570\u76f8\u5173\u95ee\u9898\u5c5e\u4e8eco-NP\u5b8c\u5168\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u901a\u8fc7\u542f\u53d1\u5f0f\u6216\u8fd1\u4f3c\u65b9\u6cd5\u63d0\u53d6\u51fa\u7684\u89c4\u5219\u662f\u53ef\u4fe1\u7684\uff0c\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "method": "\u5c06\u89c4\u5219\u9a8c\u8bc1\u95ee\u9898\uff08\u5982\u9002\u7528\u6027\u3001\u4e00\u81f4\u6027\u548c\u5b8c\u5907\u6027\uff09\u5efa\u6a21\u4e3a\u8ba1\u7b97\u95ee\u9898\uff0c\u5e76\u5728ReLU\u6fc0\u6d3b\u7f51\u7edc\u548c\u5e03\u5c14\u7f51\u7edc\u4e0a\u5206\u6790\u5176\u590d\u6742\u6027\uff0c\u901a\u8fc7\u95ee\u9898\u95f4\u7684\u5f52\u7ea6\u8fdb\u884c\u8bba\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u6570\u89c4\u5219\u9a8c\u8bc1\u95ee\u9898\u5728ReLU\u7f51\u7edc\u548c\u5e03\u5c14\u7f51\u7edc\u4e2d\u5747\u4e3aco-NP\u5b8c\u5168\u95ee\u9898\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u89c4\u5219\u7684\u9a8c\u8bc1\u5177\u6709\u9ad8\u5ea6\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u662f\u5fc5\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\uff0c\u672a\u6765\u9700\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\u6216\u8fd1\u4f3c\u65b9\u6cd5\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2509.17619", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17619", "abs": "https://arxiv.org/abs/2509.17619", "authors": ["Zhefan Wang", "Ning Geng", "Zhiqiang Guo", "Weizhi Ma", "Min Zhang"], "title": "Human vs. Agent in Task-Oriented Conversations", "comment": "SIGIR-AP 2025", "summary": "Task-oriented conversational systems are essential for efficiently addressing\ndiverse user needs, yet their development requires substantial amounts of\nhigh-quality conversational data that is challenging and costly to obtain.\nWhile large language models (LLMs) have demonstrated potential in generating\nsynthetic conversations, the extent to which these agent-generated interactions\ncan effectively substitute real human conversations remains unclear. This work\npresents the first systematic comparison between LLM-simulated users and human\nusers in personalized task-oriented conversations. We propose a comprehensive\nanalytical framework encompassing three key aspects (conversation strategy,\ninteraction style, and conversation evaluation) and ten distinct dimensions for\nevaluating user behaviors, and collect parallel conversational datasets from\nboth human users and LLM agent users across four representative scenarios under\nidentical conditions. Our analysis reveals significant behavioral differences\nbetween the two user types in problem-solving approaches, question broadness,\nuser engagement, context dependency, feedback polarity and promise, language\nstyle, and hallucination awareness. We found consistency in the agent users and\nhuman users across the depth-first or breadth-first dimensions, as well as the\nusefulness dimensions. These findings provide critical insights for advancing\nLLM-based user simulation. Our multi-dimensional taxonomy constructed a\ngeneralizable framework for analyzing user behavior patterns, offering insights\nfrom LLM agent users and human users. By this work, we provide perspectives on\nrethinking how to use user simulation in conversational systems in the future.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6a21\u62df\u7528\u6237\u4e0e\u771f\u5b9e\u4eba\u7c7b\u7528\u6237\u5728\u4e2a\u6027\u5316\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u5173\u952e\u65b9\u9762\u548c\u5341\u4e2a\u7ef4\u5ea6\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e76\u5728\u56db\u4e2a\u5178\u578b\u573a\u666f\u4e0b\u6536\u96c6\u4e86\u4eba\u7c7b\u4e0eLLM\u4ee3\u7406\u751f\u6210\u7684\u5e73\u884c\u5bf9\u8bdd\u6570\u636e\u96c6\u3002\u7814\u7a76\u53d1\u73b0\u4e24\u8005\u5728\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\u3001\u63d0\u95ee\u5e7f\u5ea6\u3001\u7528\u6237\u53c2\u4e0e\u5ea6\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u5728\u641c\u7d22\u7b56\u7565\u548c\u6709\u7528\u6027\u65b9\u9762\u5177\u6709\u4e00\u81f4\u6027\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u7528\u6237\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u7684\u6d1e\u89c1\u4e0e\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u9ad8\u8d28\u91cf\u771f\u5b9e\u5bf9\u8bdd\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u7814\u7a76\u8005\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u5bf9\u8bdd\uff0c\u4f46LLM\u6a21\u62df\u7528\u6237\u662f\u5426\u80fd\u6709\u6548\u66ff\u4ee3\u771f\u5b9e\u4eba\u7c7b\u7528\u6237\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e8c\u8005\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u4e0e\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6db5\u76d6\u5bf9\u8bdd\u7b56\u7565\u3001\u4ea4\u4e92\u98ce\u683c\u548c\u5bf9\u8bdd\u8bc4\u4f30\u4e09\u65b9\u9762\u7684\u5206\u6790\u6846\u67b6\uff0c\u5b9a\u4e49\u5341\u4e2a\u884c\u4e3a\u7ef4\u5ea6\uff0c\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u573a\u666f\u4e2d\uff0c\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u5206\u522b\u6536\u96c6\u4eba\u7c7b\u7528\u6237\u548cLLM\u4ee3\u7406\u7528\u6237\u7684\u5e73\u884c\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u591a\u7ef4\u5ea6\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53d1\u73b0LLM\u6a21\u62df\u7528\u6237\u4e0e\u4eba\u7c7b\u7528\u6237\u5728\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\u3001\u63d0\u95ee\u5e7f\u5ea6\u3001\u7528\u6237\u53c2\u4e0e\u5ea6\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u3001\u53cd\u9988\u60c5\u611f\u4e0e\u627f\u8bfa\u3001\u8bed\u8a00\u98ce\u683c\u548c\u5e7b\u89c9\u610f\u8bc6\u7b49\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u4f46\u5728\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u6216\u5e7f\u5ea6\u4f18\u5148\u7684\u63a2\u7d22\u7b56\u7565\u4ee5\u53ca\u5bf9\u8bdd\u6709\u7528\u6027\u65b9\u9762\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "LLM\u4ee3\u7406\u867d\u5728\u90e8\u5206\u884c\u4e3a\u6a21\u5f0f\u4e0a\u63a5\u8fd1\u4eba\u7c7b\uff0c\u4f46\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u771f\u5b9e\u7528\u6237\uff1b\u672c\u6587\u63d0\u51fa\u7684\u591a\u7ef4\u5206\u6790\u6846\u67b6\u4e3a\u672a\u6765\u6539\u8fdbLLM\u7528\u6237\u6a21\u62df\u548c\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u8bad\u7ec3\u6548\u679c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u4e0e\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.17755", "categories": ["cs.LG", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.17755", "abs": "https://arxiv.org/abs/2509.17755", "authors": ["Fizza Rubab", "Ntumba Elie Nsampi", "Martin Balint", "Felix Mujkanovic", "Hans-Peter Seidel", "Tobias Ritschel", "Thomas Leimk\u00fchler"], "title": "Learning Neural Antiderivatives", "comment": null, "summary": "Neural fields offer continuous, learnable representations that extend beyond\ntraditional discrete formats in visual computing. We study the problem of\nlearning neural representations of repeated antiderivatives directly from a\nfunction, a continuous analogue of summed-area tables. Although widely used in\ndiscrete domains, such cumulative schemes rely on grids, which prevents their\napplicability in continuous neural contexts. We introduce and analyze a range\nof neural methods for repeated integration, including both adaptations of prior\nwork and novel designs. Our evaluation spans multiple input dimensionalities\nand integration orders, assessing both reconstruction quality and performance\nin downstream tasks such as filtering and rendering. These results enable\nintegrating classical cumulative operators into modern neural systems and offer\ninsights into learning tasks involving differential and integral operators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8fde\u7eed\u795e\u7ecf\u573a\u4e2d\u5b66\u4e60\u51fd\u6570\u91cd\u590d\u53cd\u5bfc\u6570\u7684\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u79bb\u6563\u7d2f\u79ef\u8868\u7684\u8fde\u7eed\u7c7b\u6bd4\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u7d2f\u79ef\u7b97\u5b50\u5728\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u7d2f\u79ef\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u79bb\u6563\u7f51\u683c\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u8fde\u7eed\u795e\u7ecf\u573a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u5728\u8fde\u7eed\u57df\u4e2d\u8fdb\u884c\u91cd\u590d\u79ef\u5206\u7684\u795e\u7ecf\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u5e76\u5206\u6790\u4e86\u4e00\u7cfb\u5217\u7528\u4e8e\u91cd\u590d\u79ef\u5206\u7684\u795e\u7ecf\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9\u5148\u524d\u5de5\u4f5c\u7684\u6539\u8fdb\u548c\u65b0\u63d0\u51fa\u7684\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8f93\u5165\u7ef4\u5ea6\u548c\u79ef\u5206\u9636\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u91cd\u6784\u8d28\u91cf\u4ee5\u53ca\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6ee4\u6ce2\u548c\u6e32\u67d3\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5c06\u7ecf\u5178\u7d2f\u79ef\u7b97\u5b50\u96c6\u6210\u5230\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\u4e2d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u795e\u7ecf\u8868\u793a\u4e2d\u5efa\u6a21\u5fae\u5206\u4e0e\u79ef\u5206\u7b97\u5b50\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u63a8\u52a8\u4e86\u8fde\u7eed\u57df\u4e0a\u7d2f\u79ef\u64cd\u4f5c\u7684\u5b66\u4e60\u4e0e\u5e94\u7528\u3002"}}
{"id": "2509.16472", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16472", "abs": "https://arxiv.org/abs/2509.16472", "authors": ["Parth Agarwal", "Sangaa Chatterjee", "Md Faisal Kabir", "Suman Saha"], "title": "Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models", "comment": "The paper got accepted in ICMLA-2025. It is a camera-ready version", "summary": "Gait is a key indicator in diagnosing movement disorders, but most models\nlack interpretability and rely on single datasets. We propose a dual-branch\nCNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D\nbranch on silhouettes from OU-MVLP. Interpretability is provided by SHAP\n(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,\nthe system achieves 98.6% accuracy with strong recall and F1. This approach\nadvances explainable gait analysis across both clinical and biometric domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652fCNN-LSTM\u6846\u67b6\uff0c\u7ed3\u54081D\u5173\u8282\u7279\u5f81\u548c3D\u8f6e\u5ed3\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u6b65\u6001\u5206\u6790\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u6b65\u6001\u5206\u6790\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u548c\u751f\u7269\u8bc6\u522b\u9886\u57df\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\uff1a1D\u5206\u652f\u5904\u7406GAVD\u7684\u5173\u8282\u70b9\u7279\u5f81\uff0c3D\u5206\u652f\u5904\u7406OU-MVLP\u7684\u8f6e\u5ed3\u56fe\u50cf\uff1b\u7ed3\u5408SHAP\u8fdb\u884c\u65f6\u5e8f\u5f52\u56e0\u5206\u6790\uff0cGrad-CAM\u5b9e\u73b0\u7a7a\u95f4\u5b9a\u4f4d\u3002", "result": "\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523098.6%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u8de8\u9886\u57df\u7684\u53ef\u89e3\u91ca\u6b65\u6001\u5206\u6790\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u8bca\u65ad\u4e0e\u751f\u7269\u7279\u5f81\u8bc6\u522b\u3002"}}
{"id": "2509.16614", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16614", "abs": "https://arxiv.org/abs/2509.16614", "authors": ["Bojan Deraji\u0107", "Sebastian Bernhard", "Wolfgang H\u00f6nig"], "title": "ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks", "comment": null, "summary": "Control barrier functions (CBFs) have been demonstrated as an effective\nmethod for safety-critical control of autonomous systems. Although CBFs are\nsimple to deploy, their design remains challenging, motivating the development\nof learning-based approaches. Yet, issues such as suboptimal safe sets,\napplicability in partially observable environments, and lack of rigorous safety\nguarantees persist. In this work, we propose observation-conditioned neural\nCBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately\nrecover the maximal safe sets. We exploit certain mathematical properties of\nthe HJ value function, ensuring that the predicted safe set never intersects\nwith the observed failure set. Moreover, we leverage a hypernetwork-based\narchitecture that is particularly suitable for the design of\nobservation-conditioned safety filters. The proposed method is examined both in\nsimulation and hardware experiments for a ground robot and a quadcopter. The\nresults show improved success rates and generalization to out-of-domain\nenvironments compared to the baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eHamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u7684\u89c2\u6d4b\u6761\u4ef6\u795e\u7ecfCBF\uff0c\u53ef\u8fd1\u4f3c\u6062\u590d\u6700\u5927\u5b89\u5168\u96c6\uff0c\u5e76\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u5730\u9762\u673a\u5668\u4eba\u548c\u56db\u65cb\u7ffc\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\u8bbe\u8ba1\u56f0\u96be\uff0c\u5b58\u5728\u5b89\u5168\u96c6\u6b21\u4f18\u3001\u90e8\u5206\u53ef\u89c2\u73af\u5883\u9002\u7528\u6027\u5dee\u53ca\u7f3a\u4e4f\u4e25\u683c\u5b89\u5168\u6027\u4fdd\u8bc1\u7b49\u95ee\u9898\u3002", "method": "\u7ed3\u5408Hamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u5229\u7528HJ\u503c\u51fd\u6570\u7684\u6570\u5b66\u7279\u6027\u6784\u5efa\u89c2\u6d4b\u6761\u4ef6\u795e\u7ecfCBF\uff0c\u5e76\u91c7\u7528\u8d85\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u89c2\u6d4b\u6761\u4ef6\u5b89\u5168\u6ee4\u6ce2\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u786c\u4ef6\uff08\u5730\u9762\u673a\u5668\u4eba\u548c\u56db\u65cb\u7ffc\uff09\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u5bf9\u57df\u5916\u73af\u5883\u7684\u66f4\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u903c\u8fd1\u6700\u5927\u5b89\u5168\u96c6\uff0c\u786e\u4fdd\u9884\u6d4b\u5b89\u5168\u96c6\u4e0d\u4e0e\u89c2\u6d4b\u6545\u969c\u96c6\u76f8\u4ea4\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u578bCBF\u7684\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.16422", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16422", "abs": "https://arxiv.org/abs/2509.16422", "authors": ["Tom Mackintosh", "Harish Tayyar Madabushi", "Claire Bonial"], "title": "Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning", "comment": null, "summary": "We probe large language models' ability to learn deep form-meaning mappings\nas defined by construction grammars. We introduce the ConTest-NLI benchmark of\n80k sentences covering eight English constructions from highly lexicalized to\nhighly schematic. Our pipeline generates diverse synthetic NLI triples via\ntemplating and the application of a model-in-the-loop filter. This provides\naspects of human validation to ensure challenge and label reliability.\nZero-shot tests on leading LLMs reveal a 24% drop in accuracy between\nnaturalistic (88%) and adversarial data (64%), with schematic patterns proving\nhardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,\nyet our results highlight persistent abstraction gaps in current LLMs and offer\na scalable framework for evaluating construction-informed learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ConTest-NLI\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6784\u5f0f\u8bed\u6cd5\u4e0b\u7684\u5f62\u5f0f-\u610f\u4e49\u6620\u5c04\u80fd\u529b\uff0c\u901a\u8fc7\u5408\u6210NLI\u4e09\u5143\u7ec4\u6d4b\u8bd5\u53d1\u73b0\u6a21\u578b\u5728\u62bd\u8c61\u6784\u5f0f\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u62bd\u8c61\u6cdb\u5316\u7f3a\u9677\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u771f\u6b63\u7406\u89e3\u4ece\u5177\u4f53\u5230\u62bd\u8c61\u7684\u8bed\u8a00\u6784\u5f0f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4f9d\u8d56\u8868\u5c42\u7edf\u8ba1\u6a21\u5f0f\u3002", "method": "\u6784\u5efa\u5305\u542b80k\u53e5\u5b50\u7684ConTest-NLI\u57fa\u51c6\uff0c\u8986\u76d6\u516b\u79cd\u82f1\u8bed\u6784\u5f0f\uff0c\u91c7\u7528\u6a21\u677f\u751f\u6210\u52a0\u6a21\u578b\u8fc7\u6ee4\u7684pipeline\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210NLI\u6570\u636e\u3002", "result": "\u96f6\u6837\u672c\u6d4b\u8bd5\u663e\u793a\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6570\u636e\u4e0a\u51c6\u786e\u7387\u4e0b\u964d24%\uff08\u4ece88%\u964d\u81f364%\uff09\uff0c\u62bd\u8c61\u6784\u5f0f\u6700\u96be\uff1b\u5fae\u8c03\u53ef\u5e26\u6765\u6700\u591a9%\u7684\u63d0\u5347\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5c42\u5f62\u5f0f-\u610f\u4e49\u6620\u5c04\u5c24\u5176\u662f\u62bd\u8c61\u6784\u5f0f\u5b66\u4e60\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0cConTest-NLI\u4e3a\u8bc4\u4f30\u6b64\u7c7b\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2509.16379", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16379", "abs": "https://arxiv.org/abs/2509.16379", "authors": ["Xinran Liu", "Shansita D. Sharma", "Soheil Kolouri"], "title": "EMPEROR: Efficient Moment-Preserving Representation of Distributions", "comment": null, "summary": "We introduce EMPEROR (Efficient Moment-Preserving Representation of\nDistributions), a mathematically rigorous and computationally efficient\nframework for representing high-dimensional probability measures arising in\nneural network representations. Unlike heuristic global pooling operations,\nEMPEROR encodes a feature distribution through its statistical moments. Our\napproach leverages the theory of sliced moments: features are projected onto\nmultiple directions, lightweight univariate Gaussian mixture models (GMMs) are\nfit to each projection, and the resulting slice parameters are aggregated into\na compact descriptor. We establish determinacy guarantees via Carleman's\ncondition and the Cram\\'er-Wold theorem, ensuring that the GMM is uniquely\ndetermined by its sliced moments, and we derive finite-sample error bounds that\nscale optimally with the number of slices and samples. Empirically, EMPEROR\ncaptures richer distributional information than common pooling schemes across\nvarious data modalities, while remaining computationally efficient and broadly\napplicable.", "AI": {"tldr": "EMPEROR \u662f\u4e00\u79cd\u57fa\u4e8e\u5207\u7247\u77e9\u548c\u4e00\u7ef4\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u9ad8\u6548\u3001\u6570\u5b66\u4e25\u8c28\u7684\u9ad8\u7ef4\u6982\u7387\u5206\u5e03\u8868\u793a\u6846\u67b6\uff0c\u4f18\u4e8e\u4f20\u7edf\u6c60\u5316\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5168\u5c40\u6c60\u5316\u64cd\u4f5c\u7f3a\u4e4f\u5bf9\u7279\u5f81\u5206\u5e03\u7684\u7cbe\u7ec6\u5efa\u6a21\u80fd\u529b\uff0c\u96be\u4ee5\u6355\u6349\u9ad8\u7ef4\u795e\u7ecf\u7f51\u7edc\u8868\u5f81\u4e2d\u7684\u4e30\u5bcc\u7edf\u8ba1\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u7406\u8bba\u53ef\u9760\u7684\u5206\u5e03\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5207\u7247\u77e9\u7406\u8bba\uff0c\u5c06\u7279\u5f81\u6295\u5f71\u5230\u591a\u4e2a\u65b9\u5411\uff0c\u5bf9\u6bcf\u4e2a\u6295\u5f71\u62df\u5408\u8f7b\u91cf\u7ea7\u4e00\u7ef4\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\uff0c\u5e76\u805a\u5408\u8fd9\u4e9b\u5207\u7247\u53c2\u6570\u5f62\u6210\u7d27\u51d1\u63cf\u8ff0\u7b26\uff1b\u901a\u8fc7Carleman\u6761\u4ef6\u548cCram\u00e9r-Wold\u5b9a\u7406\u4fdd\u8bc1\u5206\u5e03\u7684\u552f\u4e00\u786e\u5b9a\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u6700\u4f18\u7684\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002", "result": "EMPEROR \u80fd\u6bd4\u5e38\u89c1\u7684\u6c60\u5316\u65b9\u6848\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u5206\u5e03\u4fe1\u606f\uff0c\u5728\u591a\u79cd\u6570\u636e\u6a21\u6001\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u9ad8\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "EMPEROR \u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u4e25\u8c28\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u5206\u5e03\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u4fdd\u7559\u9ad8\u7ef4\u7279\u5f81\u5206\u5e03\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5404\u79cd\u8868\u793a\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2509.16561", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16561", "abs": "https://arxiv.org/abs/2509.16561", "authors": ["Yue Xin", "Chen Shen", "Shaotian Yan", "Xiaosong Yuan", "Yaoming Wang", "Xiaofeng Zhang", "Chenxi Huang", "Jieping Ye"], "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning", "comment": "accpeted by EMNLP 2025", "summary": "Chain-of-Thought (CoT) prompting enhances the math reasoning capability of\nlarge language models (LLMs) to a large margin. However, the mechanism\nunderlying such improvements remains unexplored. In this paper, we present\n\\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed\n\\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd}\nM\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a\nmathematically rigorous evaluation metric for quantifying component-level\ncontributions in few-shot CoT reasoning. Concretely, we leverage the Shapley\nvalue for mathematical expression attribution and develop an efficient\nstratified sampling algorithm that significantly reduces the computational\ncomplexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality\n\\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance\nanalysis. Comprehensive validation across popular LLM models and diverse\nmathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder\nframework exhibits a robust monotonic correlation with model performance, not\nonly providing theoretical explanations for the empirical success of existing\nfew-shot CoT but also establishing mathematically rigorous principles for\nprompt construction optimization. Furthermore, we verify the reliability of the\nexplanation, based on which we unify the insights of previous work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SalaMAnder\u6846\u67b6\uff0c\u5229\u7528Shapley\u503c\u8fdb\u884c\u6570\u5b66\u8868\u8fbe\u5f0f\u5f52\u56e0\uff0c\u5e76\u8bbe\u8ba1\u4e86CoSP\u6307\u6807\u6765\u91cf\u5316\u5c11\u6837\u672c\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u5404\u7ec4\u4ef6\u7684\u8d21\u732e\uff0c\u63ed\u793a\u4e86\u5176\u5bf9\u5927\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u7406\u8bba\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u601d\u7ef4\u94fe\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u80cc\u540e\u7684\u4f5c\u7528\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e00\u79cd\u7406\u8bba\u4e25\u8c28\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u548c\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eShapley\u503c\u63d0\u51fa\u6570\u5b66\u8868\u8fbe\u5f0f\u5f52\u56e0\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u5206\u5c42\u91c7\u6837\u7b97\u6cd5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u534f\u65b9\u5dee\u5206\u6790\u6784\u5efaCoSP\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u548c\u6570\u5b66\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoSP\u6307\u6807\u4e0e\u6a21\u578b\u6027\u80fd\u5177\u6709\u5f3a\u5355\u8c03\u76f8\u5173\u6027\uff0c\u80fd\u6709\u6548\u89e3\u91ca\u73b0\u6709\u601d\u7ef4\u94fe\u65b9\u6cd5\u7684\u6210\u529f\u5e76\u6307\u5bfc\u63d0\u793a\u4f18\u5316\u3002", "conclusion": "SalaMAnder\u4e3a\u5c11\u6837\u672c\u601d\u7ef4\u94fe\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u548c\u6570\u5b66\u4e25\u8c28\u7684\u8bc4\u4f30\u51c6\u5219\uff0c\u7edf\u4e00\u4e86\u5148\u524d\u7814\u7a76\u7684\u89c1\u89e3\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5f52\u56e0\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.17749", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17749", "abs": "https://arxiv.org/abs/2509.17749", "authors": ["Changjiang Zhou", "Ruqing Zhang", "Jiafeng Guo", "Yu-An Liu", "Fan Zhang", "Ganyuan Luo", "Xueqi Cheng"], "title": "A Generative Framework for Personalized Sticker Retrieval", "comment": "Findings of EMNLP2025", "summary": "Formulating information retrieval as a variant of generative modeling,\nspecifically using autoregressive models to generate relevant identifiers for a\ngiven query, has recently attracted considerable attention. However, its\napplication to personalized sticker retrieval remains largely unexplored and\npresents unique challenges: existing relevance-based generative retrieval\nmethods typically lack personalization, leading to a mismatch between diverse\nuser expectations and the retrieved results. To address this gap, we propose\nPEARL, a novel generative framework for personalized sticker retrieval, and\nmake two key contributions: (i) To encode user-specific sticker preferences, we\ndesign a representation learning model to learn discriminative user\nrepresentations. It is trained on three prediction tasks that leverage personal\ninformation and click history; and (ii) To generate stickers aligned with a\nuser's query intent, we propose a novel intent-aware learning objective that\nprioritizes stickers associated with higher-ranked intents. Empirical results\nfrom both offline evaluations and online tests demonstrate that PEARL\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faPEARL\uff0c\u4e00\u79cd\u7528\u4e8e\u4e2a\u6027\u5316\u8d34\u7eb8\u68c0\u7d22\u7684\u65b0\u578b\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u8868\u793a\u5b66\u4e60\u548c\u610f\u56fe\u611f\u77e5\u5b66\u4e60\u76ee\u6807\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u76f8\u5173\u6027\u751f\u6210\u68c0\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u4e0e\u7528\u6237\u591a\u6837\u5316\u671f\u671b\u4e0d\u5339\u914d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8868\u793a\u5b66\u4e60\u6a21\u578b\u6765\u5b66\u4e60\u533a\u5206\u6027\u7528\u6237\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u610f\u56fe\u611f\u77e5\u5b66\u4e60\u76ee\u6807\uff0c\u4f18\u5148\u751f\u6210\u4e0e\u9ad8\u6392\u540d\u610f\u56fe\u76f8\u5173\u7684\u8d34\u7eb8\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u548c\u5728\u7ebf\u6d4b\u8bd5\u8868\u660e\uff0cPEARL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PEARL\u6709\u6548\u89e3\u51b3\u4e86\u4e2a\u6027\u5316\u8d34\u7eb8\u68c0\u7d22\u4e2d\u7684\u7528\u6237\u504f\u597d\u5efa\u6a21\u548c\u67e5\u8be2\u610f\u56fe\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u68c0\u7d22\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18097", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.18097", "abs": "https://arxiv.org/abs/2509.18097", "authors": ["Julian Kaltheuner", "Alexander Oebel", "Hannah Droege", "Patrick Stotko", "Reinhard Klein"], "title": "Preconditioned Deformation Grids", "comment": "GitHub: https://github.com/vc-bonn/preconditioned-deformation-grids", "summary": "Dynamic surface reconstruction of objects from point cloud sequences is a\nchallenging field in computer graphics. Existing approaches either require\nmultiple regularization terms or extensive training data which, however, lead\nto compromises in reconstruction accuracy as well as over-smoothing or poor\ngeneralization to unseen objects and motions. To address these lim- itations,\nwe introduce Preconditioned Deformation Grids, a novel technique for estimating\ncoherent deformation fields directly from unstructured point cloud sequences\nwithout requiring or forming explicit correspondences. Key to our approach is\nthe use of multi-resolution voxel grids that capture the overall motion at\nvarying spatial scales, enabling a more flexible deformation representation. In\nconjunction with incorporating grid-based Sobolev preconditioning into\ngradient-based optimization, we show that applying a Chamfer loss between the\ninput point clouds as well as to an evolving template mesh is sufficient to\nobtain accurate deformations. To ensure temporal consistency along the object\nsurface, we include a weak isometry loss on mesh edges which complements the\nmain objective without constraining deformation fidelity. Extensive evaluations\ndemonstrate that our method achieves superior results, particularly for long\nsequences, compared to state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u9884\u6761\u4ef6\u53d8\u5f62\u7f51\u683c\uff08Preconditioned Deformation Grids\uff09\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u65e0\u7ed3\u6784\u70b9\u4e91\u5e8f\u5217\u4f30\u8ba1\u8fde\u8d2f\u7684\u53d8\u5f62\u573a\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u5e94\u5173\u7cfb\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u4f53\u7d20\u7f51\u683c\u548cSobolev\u9884\u6761\u4ef6\u4f18\u5316\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u52a8\u6001\u8868\u9762\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u591a\u4e2a\u6b63\u5219\u5316\u9879\u6216\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u91cd\u5efa\u7cbe\u5ea6\u4e0b\u964d\u3001\u8fc7\u5ea6\u5e73\u6ed1\u6216\u5bf9\u672a\u89c1\u7269\u4f53\u548c\u8fd0\u52a8\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u91c7\u7528\u591a\u5206\u8fa8\u7387\u4f53\u7d20\u7f51\u683c\u6355\u6349\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u7684\u6574\u4f53\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u7f51\u683c\u7684Sobolev\u9884\u6761\u4ef6\u68af\u5ea6\u4f18\u5316\uff1b\u4ec5\u4f7f\u7528Chamfer\u635f\u5931\u548c\u5f31\u7b49\u8ddd\u635f\u5931\u8fdb\u884c\u4f18\u5316\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u5e94\u3002", "result": "\u5728\u957f\u5e8f\u5217\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u52a8\u6001\u8868\u9762\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u591a\u6b63\u5219\u5316\u9879\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u6709\u6548\u63d0\u5347\u52a8\u6001\u8868\u9762\u91cd\u5efa\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16474", "abs": "https://arxiv.org/abs/2509.16474", "authors": ["Gabrielle Chavez", "Laureano Moro-Velazquez", "Ankur Butala", "Najim Dehak", "Thomas Thebaud"], "title": "Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion", "comment": "5 pages, 2 figures, submitted to International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP)", "summary": "Handwriting is significantly affected by neurological disorders (ND) such as\nParkinson's disease (PD) and Alzheimer's disease (AD). Prior works have\nanalyzed handwriting tasks using feature-based approaches or computer-vision\ntechniques, but these methods have struggled to generalize across multiple\ndatasets, particularly between temporal features represented as time-series and\nimages. We propose a framework that leverages both time-series and images of\nhandwriting through a joint classifier, based on a ResNet50 pretrained on\nImageNet-1k. Binary classification experiments demonstrate state-of-the-art\nperformances on existing time-series and image datasets, with significant\nimprovement on specific drawing and writing tasks from the NeuroLogical Signals\n(NLS) dataset. In particular, the proposed model demonstrates improved\nperformance on Draw Clock and Spiral tasks. Additionally, cross-dataset and\nmulti-dataset experiments were consistently able to achieve high F1 scores, up\nto 98 for PD detection, highlighting the potential of the proposed model to\ngeneralize over different forms of handwriting signals, and enhance the\ndetection of motor deficits in ND.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u7684\u8054\u5408\u5206\u7c7b\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u624b\u5199\u5206\u6790\u68c0\u6d4b\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8F1\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\uff08\u5c24\u5176\u662f\u65f6\u95f4\u5e8f\u5217\u4e0e\u56fe\u50cf\uff09\u7684\u624b\u5199\u5206\u6790\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u5f15\u8d77\u7684\u624b\u5199\u53d8\u5316\u3002", "method": "\u5229\u7528\u57fa\u4e8eImageNet-1k\u9884\u8bad\u7ec3\u7684ResNet50\u6784\u5efa\u8054\u5408\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u5904\u7406\u624b\u5199\u7684\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\u3002", "result": "\u5728NLS\u6570\u636e\u96c6\u7684Draw Clock\u548cSpiral\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4bF1\u5206\u6570\u9ad8\u8fbe98%\uff0c\u5e76\u5728\u8de8\u6570\u636e\u96c6\u548c\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u878d\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u624b\u5199\u4fe1\u53f7\uff0c\u63d0\u5347\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u76f8\u5173\u8fd0\u52a8\u7f3a\u9677\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2509.16615", "categories": ["cs.RO", "68T40", "I.2.9; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.16615", "abs": "https://arxiv.org/abs/2509.16615", "authors": ["Jelle Luijkx", "Runyu Ma", "Zlatan Ajanovi\u0107", "Jens Kober"], "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning", "comment": "8 pages, 7 figures", "summary": "Reinforcement learning (RL) is a promising approach for robotic manipulation,\nbut it can suffer from low sample efficiency and requires extensive exploration\nof large state-action spaces. Recent methods leverage the commonsense knowledge\nand reasoning abilities of large language models (LLMs) to guide exploration\ntoward more meaningful states. However, LLMs can produce plans that are\nsemantically plausible yet physically infeasible, yielding unreliable behavior.\nWe introduce LLM-TALE, a framework that uses LLMs' planning to directly steer\nRL exploration. LLM-TALE integrates planning at both the task level and the\naffordance level, improving learning efficiency by directing agents toward\nsemantically meaningful actions. Unlike prior approaches that assume optimal\nLLM-generated plans or rewards, LLM-TALE corrects suboptimality online and\nexplores multimodal affordance-level plans without human supervision. We\nevaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing\nimprovements in both sample efficiency and success rates over strong baselines.\nReal-robot experiments indicate promising zero-shot sim-to-real transfer. Code\nand supplementary material are available at https://llm-tale.github.io.", "AI": {"tldr": "\u63d0\u51faLLM-TALE\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u548c\u53ef\u64cd\u4f5c\u6027\u5c42\u9762\u7684\u89c4\u5212\u6765\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u63a2\u7d22\u7a7a\u95f4\u5927\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u63a2\u7d22\u7684\u65b9\u6cd5\u5e38\u751f\u6210\u8bed\u4e49\u5408\u7406\u4f46\u7269\u7406\u4e0d\u53ef\u884c\u7684\u8ba1\u5212\uff0c\u5bfc\u81f4\u884c\u4e3a\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faLLM-TALE\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\u540c\u65f6\u5e94\u7528\u4e8e\u4efb\u52a1\u5c42\u7ea7\u548c\u53ef\u64cd\u4f5c\u6027\u5c42\u7ea7\uff0c\u901a\u8fc7\u5728\u7ebf\u4fee\u6b63\u6b21\u4f18\u8ba1\u5212\u5e76\u63a2\u7d22\u591a\u6a21\u6001\u7684\u53ef\u64cd\u4f5c\u6027\u7ea7\u8ba1\u5212\uff0c\u76f4\u63a5\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u7684\u6293\u53d6\u653e\u7f6e\u4efb\u52a1\u4e2d\uff0cLLM-TALE\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\uff1b\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u663e\u793a\u51fa\u826f\u597d\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "LLM-TALE\u80fd\u6709\u6548\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u89c4\u5212\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u5373\u53ef\u5904\u7406\u4e0d\u5b8c\u7f8e\u7684\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u3002"}}
{"id": "2509.16449", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16449", "abs": "https://arxiv.org/abs/2509.16449", "authors": ["Tsz Fung Pang", "Maryam Berijanian", "Thomas Orth", "Breanna Shi", "Charlotte S. Alexander"], "title": "PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization", "comment": null, "summary": "Legal documents are often long, dense, and difficult to comprehend, not only\nfor laypeople but also for legal experts. While automated document\nsummarization has great potential to improve access to legal knowledge,\nprevailing task-based evaluators overlook divergent user and stakeholder needs.\nTool development is needed to encompass the technicality of a case summary for\na litigator yet be accessible for a self-help public researching for their\nlawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation\nframework that scores summaries through the lens of six personas, including\nlegal and non-legal users. We also introduce a controlled dimension-shifted\npilot dataset of U.S. civil rights case summaries that varies along depth,\naccessibility, and procedural detail as well as Diversity-Coverage Index (DCI)\nto expose divergent optima of legal summary between persona-aware and\npersona-agnostic judges. This work enables refinement of legal AI summarization\nsystems for both expert and non-expert users, with the potential to increase\naccess to legal knowledge. The code base and data are publicly available in\nGitHub.", "AI": {"tldr": "\u63d0\u51faPersonaMatrix\u6846\u67b6\u548c\u591a\u6837\u6027\u8986\u76d6\u6307\u6570\uff08DCI\uff09\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u89c6\u89d2\u8bc4\u4f30\u6cd5\u5f8b\u6587\u6863\u6458\u8981\u8d28\u91cf\uff0c\u63d0\u5347\u6cd5\u5f8bAI\u6458\u8981\u7cfb\u7edf\u5bf9\u4e13\u5bb6\u4e0e\u975e\u4e13\u5bb6\u7528\u6237\u7684\u9002\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u6587\u6863\u81ea\u52a8\u6458\u8981\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e0d\u540c\u7528\u6237\u548c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u5dee\u5f02\uff0c\u96be\u4ee5\u517c\u987e\u6cd5\u5f8b\u4e13\u4e1a\u4eba\u58eb\u7684\u6280\u672f\u6027\u9700\u6c42\u548c\u666e\u901a\u516c\u4f17\u7684\u53ef\u8bfb\u6027\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1PersonaMatrix\u8fd9\u4e00\u57fa\u4e8e\u89d2\u8272\u548c\u6807\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u516d\u4e2a\u4e0d\u540c\u7528\u6237\u89d2\u8272\uff1b\u6784\u5efa\u63a7\u5236\u53d8\u91cf\u7684\u7f8e\u56fd\u6c11\u6743\u6848\u4f8b\u6458\u8981\u6570\u636e\u96c6\uff0c\u8c03\u8282\u6df1\u5ea6\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u7a0b\u5e8f\u7ec6\u8282\uff1b\u63d0\u51fa\u591a\u6837\u6027\u8986\u76d6\u6307\u6570\uff08DCI\uff09\u6765\u63ed\u793a\u4e0d\u540c\u8bc4\u4f30\u8005\u7684\u6700\u4f18\u6458\u8981\u5dee\u5f02\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u6cd5\u5f8b\u6458\u8981\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u53d1\u73b0\u89d2\u8272\u611f\u77e5\u4e0e\u89d2\u8272\u65e0\u5173\u7684\u8bc4\u4f30\u65b9\u5f0f\u5b58\u5728\u663e\u8457\u4e0d\u540c\u7684\u4f18\u5316\u65b9\u5411\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u63d0\u5347\u6458\u8981\u666e\u9002\u6027\u548c\u9488\u5bf9\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u6cd5\u5f8bAI\u6458\u8981\u7cfb\u7edf\u7684\u7cbe\u7ec6\u5316\u53d1\u5c55\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u6cd5\u5f8b\u77e5\u8bc6\u7684\u53ef\u53ca\u6027\uff0c\u652f\u6301\u4e13\u5bb6\u4e0e\u975e\u4e13\u5bb6\u7528\u6237\u7684\u4e0d\u540c\u9700\u6c42\u3002"}}
{"id": "2509.16391", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16391", "abs": "https://arxiv.org/abs/2509.16391", "authors": ["Yasser H. Khalil", "Mehdi Setayesh", "Hongliang Li"], "title": "CoUn: Empowering Machine Unlearning via Contrastive Learning", "comment": null, "summary": "Machine unlearning (MU) aims to remove the influence of specific \"forget\"\ndata from a trained model while preserving its knowledge of the remaining\n\"retain\" data. Existing MU methods based on label manipulation or model weight\nperturbations often achieve limited unlearning effectiveness. To address this,\nwe introduce CoUn, a novel MU framework inspired by the observation that a\nmodel retrained from scratch using only retain data classifies forget data\nbased on their semantic similarity to the retain data. CoUn emulates this\nbehavior by adjusting learned data representations through contrastive learning\n(CL) and supervised learning, applied exclusively to retain data. Specifically,\nCoUn (1) leverages semantic similarity between data samples to indirectly\nadjust forget representations using CL, and (2) maintains retain\nrepresentations within their respective clusters through supervised learning.\nExtensive experiments across various datasets and model architectures show that\nCoUn consistently outperforms state-of-the-art MU baselines in unlearning\neffectiveness. Additionally, integrating our CL module into existing baselines\nempowers their unlearning effectiveness.", "AI": {"tldr": "\u63d0\u51faCoUn\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u8c03\u6574\u6570\u636e\u8868\u793a\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5728\u9057\u5fd8\u6548\u679c\u4e0a\u6709\u9650\uff0c\u9700\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6d88\u9664\u7279\u5b9a\u6570\u636e\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u5728\u4ec5\u4f7f\u7528\u4fdd\u7559\u6570\u636e\u91cd\u8bad\u7ec3\u540e\u5bf9\u9057\u5fd8\u6570\u636e\u7684\u5206\u7c7b\u884c\u4e3a\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u8c03\u6574\u6570\u636e\u8868\u793a\uff0c\u4ec5\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCoUn\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u4e14\u5176\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u53ef\u63d0\u5347\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u7684\u6548\u679c\u3002", "conclusion": "CoUn\u901a\u8fc7\u6a21\u62df\u91cd\u8bad\u7ec3\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u9057\u5fd8\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u53ef\u96c6\u6210\u6027\u3002"}}
{"id": "2509.16578", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.16578", "abs": "https://arxiv.org/abs/2509.16578", "authors": ["Wenyao Li", "Ran Zhang", "Pengyang Wang", "Yuanchun Zhou", "Pengfei Wang"], "title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning", "comment": null, "summary": "Human mobility forecasting is important for applications such as\ntransportation planning, urban management, and personalized recommendations.\nHowever, existing methods often fail to generalize to unseen users or locations\nand struggle to capture dynamic intent due to limited labeled data and the\ncomplexity of mobility patterns. We propose ZHMF, a framework for zero-shot\nhuman mobility forecasting that combines a semantic enhanced retrieval and\nreflection mechanism with a hierarchical language model based reasoning system.\nThe task is reformulated as a natural language question answering paradigm.\nLeveraging LLMs semantic understanding of user histories and context, our\napproach handles previously unseen prediction scenarios. We further introduce a\nhierarchical reflection mechanism for iterative reasoning and refinement by\ndecomposing forecasting into an activity level planner and a location level\nselector, enabling collaborative modeling of long term user intentions and\nshort term contextual preferences. Experiments on standard human mobility\ndatasets show that our approach outperforms existing models. Ablation studies\nreveal the contribution of each module, and case studies illustrate how the\nmethod captures user intentions and adapts to diverse contextual scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZHMF\u7684\u96f6\u6837\u672c\u4eba\u7c7b\u79fb\u52a8\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u589e\u5f3a\u68c0\u7d22\u4e0e\u5206\u5c42\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u5c06\u9884\u6d4b\u4efb\u52a1\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u5f62\u5f0f\uff0c\u6709\u6548\u5e94\u5bf9\u672a\u89c1\u7528\u6237\u548c\u573a\u666f\u7684\u9884\u6d4b\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u7528\u6237\u6216\u4f4d\u7f6e\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u52a8\u6001\u610f\u56fe\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u548c\u79fb\u52a8\u6a21\u5f0f\u7684\u590d\u6742\u6027\u3002", "method": "\u5c06\u79fb\u52a8\u6027\u9884\u6d4b\u4efb\u52a1\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u95ee\u9898\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7528\u6237\u5386\u53f2\u548c\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u7ed3\u5408\u8bed\u4e49\u589e\u5f3a\u68c0\u7d22\u4e0e\u53cd\u5c04\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u7cfb\u7edf\uff08\u6d3b\u52a8\u5c42\u7ea7\u89c4\u5212\u5668\u548c\u4f4d\u7f6e\u5c42\u7ea7\u9009\u62e9\u5668\uff09\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6\u4eba\u7c7b\u79fb\u52a8\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u8d21\u732e\uff0c\u6848\u4f8b\u5206\u6790\u5c55\u793a\u4e86\u5176\u5bf9\u7528\u6237\u610f\u56fe\u548c\u591a\u6837\u5316\u573a\u666f\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "ZHMF\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u6a21\u578b\u548c\u5206\u5c42\u63a8\u7406\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u7528\u6237\u548c\u573a\u666f\u7684\u6709\u6548\u96f6\u6837\u672c\u79fb\u52a8\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.17918", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17918", "abs": "https://arxiv.org/abs/2509.17918", "authors": ["Yuanrong Wang", "Yingpeng Du"], "title": "Shilling Recommender Systems by Generating Side-feature-aware Fake User Profiles", "comment": null, "summary": "Recommender systems (RS) greatly influence users' consumption decisions,\nmaking them attractive targets for malicious shilling attacks that inject fake\nuser profiles to manipulate recommendations. Existing shilling methods can\ngenerate effective and stealthy fake profiles when training data only contain\nrating matrix, but they lack comprehensive solutions for scenarios where side\nfeatures are present and utilized by the recommender. To address this gap, we\nextend the Leg-UP framework by enhancing the generator architecture to\nincorporate side features, enabling the generation of side-feature-aware fake\nuser profiles. Experiments on benchmarks show that our method achieves strong\nattack performance while maintaining stealthiness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Leg-UP\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u751f\u6210\u5668\u67b6\u6784\u4ee5\u7ed3\u5408\u4fa7\u8fb9\u7279\u5f81\uff0c\u751f\u6210\u66f4\u9690\u853d\u4e14\u6709\u6548\u7684\u865a\u5047\u7528\u6237\u914d\u7f6e\u6587\u4ef6\uff0c\u4ece\u800c\u5728\u5b58\u5728\u4fa7\u8fb9\u4fe1\u606f\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u5f3a\u7684\u653b\u51fb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u865a\u5047\u914d\u7f6e\u6587\u4ef6\u751f\u6210\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u8bc4\u5206\u77e9\u9635\u65f6\u6709\u6548\uff0c\u4f46\u5728\u63a8\u8350\u7cfb\u7edf\u5229\u7528\u4fa7\u8fb9\u7279\u5f81\u65f6\u7f3a\u4e4f\u5168\u9762\u89e3\u51b3\u65b9\u6848\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u5347\u5bf9\u5177\u5907\u4fa7\u8fb9\u7279\u5f81\u7684\u63a8\u8350\u7cfb\u7edf\u7684\u653b\u51fb\u80fd\u529b\u3002", "method": "\u6269\u5c55Leg-UP\u6846\u67b6\uff0c\u6539\u8fdb\u751f\u6210\u5668\u67b6\u6784\u4ee5\u878d\u5408\u7528\u6237\u548c\u9879\u76ee\u7684\u4fa7\u8fb9\u7279\u5f81\uff0c\u4ece\u800c\u751f\u6210\u4e0e\u4fa7\u8fb9\u7279\u5f81\u4e00\u81f4\u4e14\u5177\u6709\u6b3a\u9a97\u6027\u7684\u865a\u5047\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u63a8\u8350\u6a21\u578b\u4e0a\u5747\u5b9e\u73b0\u4e86\u8f83\u5f3a\u7684\u653b\u51fb\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u9690\u853d\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9762\u5411\u5e26\u4fa7\u8fb9\u7279\u5f81\u7684\u63a8\u8350\u7cfb\u7edf\u7684\u865a\u5047\u7528\u6237\u914d\u7f6e\u6587\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u6b64\u7c7b\u7cfb\u7edf\u5728\u9762\u5bf9\u7279\u5f81\u611f\u77e5\u578b\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2509.16476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16476", "abs": "https://arxiv.org/abs/2509.16476", "authors": ["Qinyu Chen", "Jiawen Qi"], "title": "Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs", "comment": "11 pages", "summary": "Vision-Language Models (VLMs) deliver impressive performance in understanding\nvisual content with language instructions. However, redundancy in vision tokens\nresults in the degenerated inference efficiency of VLMs, which hinders\nreal-time use on edge consumer devices such as AR/VR devices. Existing\nefficiency methods commonly prune visual tokens using learned saliency, sparse\nattention schedules, or controller policies, but they often require\narchitectural modification or access to intermediate activations. These\npipelines add inference-time modules that increase compute and memory and often\nlead to an accuracy trade-off. Moreover, they also suffer from misalignment\nbetween the prompts and the region of interest in the images. Without human\nguidance, the model may focus on the wrong regions and miss small,\nhigh-frequency details when prompts or scenes change. In this paper, we propose\nGazeVLM, a training-free framework that uses the human eye gaze as a natural\nsupervisory signal to allocate computation where it matters. By extracting\ngaze-driven regions of interest (ROIs) and optionally combining them with a\nlow-resolution global view, GazeVLM mimics fovea-periphery perception to cut\nredundant visual tokens while preserving task-relevant details. We evaluate the\nvisual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark\nwith human gaze. Quality of the answer is assessed by GPT-4o pairwise judging\nand a weighted score over coverage, accuracy, details, and fluency. Efficiency\nis measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to\n93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better\nanswer quality relative to full-resolution baselines. Our results show that\naligning model computation with human gaze offers a simple, plug-and-play path\ntoward efficient VLM inference on consumer devices.", "AI": {"tldr": "GazeVLM \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6548\u7387\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u4eba\u7c7b\u773c\u52a8\u6570\u636e\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u805a\u7126\u5173\u952e\u56fe\u50cf\u533a\u57df\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u89c6\u89c9token\u5197\u4f59\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\uff0c\u4e14\u6ce8\u610f\u529b\u673a\u5236\u5e38\u4e0e\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u5b9e\u65f6\u8fd0\u884c\uff1b\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u9700\u4fee\u6539\u67b6\u6784\u6216\u5f15\u5165\u989d\u5916\u6a21\u5757\uff0c\u5e26\u6765\u7cbe\u5ea6\u635f\u5931\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa GazeVLM\uff0c\u5229\u7528\u4eba\u7c7b\u773c\u52a8\u8f68\u8ff9\u63d0\u53d6\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\uff0c\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u5168\u5c40\u89c6\u56fe\u6a21\u62df\u4eba\u773c\u4e2d\u592e-\u5916\u56f4\u611f\u77e5\u673a\u5236\uff0c\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u7684\u524d\u63d0\u4e0b\u51cf\u5c11\u5197\u4f59\u89c6\u89c9token\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u5728 Qwen2.5-VL-3B/7B \u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u6700\u591a\u51cf\u5c11 93.1% \u89c6\u89c9token\u300159.6% \u603btoken \u548c 50% FLOPs\uff0c\u540c\u65f6\u901a\u8fc7 GPT-4o \u8bc4\u4f30\u663e\u793a\u56de\u7b54\u8d28\u91cf\u4f18\u4e8e\u5168\u5206\u8fa8\u7387\u57fa\u7ebf\u3002", "conclusion": "\u5c06\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4e0e\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u5bf9\u9f50\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2509.16638", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16638", "abs": "https://arxiv.org/abs/2509.16638", "authors": ["Jinrui Han", "Weiji Xie", "Jiakun Zheng", "Jiyuan Shi", "Weinan Zhang", "Ting Xiao", "Chenjia Bai"], "title": "KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control", "comment": null, "summary": "Learning versatile whole-body skills by tracking various human motions is a\nfundamental step toward general-purpose humanoid robots. This task is\nparticularly challenging because a single policy must master a broad repertoire\nof motion skills while ensuring stability over long-horizon sequences. To this\nend, we present VMS, a unified whole-body controller that enables humanoid\nrobots to learn diverse and dynamic behaviors within a single policy. Our\nframework integrates a hybrid tracking objective that balances local motion\nfidelity with global trajectory consistency, and an Orthogonal\nMixture-of-Experts (OMoE) architecture that encourages skill specialization\nwhile enhancing generalization across motions. A segment-level tracking reward\nis further introduced to relax rigid step-wise matching, enhancing robustness\nwhen handling global displacements and transient inaccuracies. We validate VMS\nextensively in both simulation and real-world experiments, demonstrating\naccurate imitation of dynamic skills, stable performance over minute-long\nsequences, and strong generalization to unseen motions. These results highlight\nthe potential of VMS as a scalable foundation for versatile humanoid whole-body\ncontrol. The project page is available at\nhttps://kungfubot2-humanoid.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5168\u8eab\u63a7\u5236\u5668VMS\uff0c\u4f7f\u7c7b\u4eba\u673a\u5668\u4eba\u80fd\u591f\u5728\u5355\u4e00\u7b56\u7565\u4e0b\u5b66\u4e60\u591a\u6837\u5316\u548c\u52a8\u6001\u7684\u884c\u4e3a\uff0c\u5177\u5907\u826f\u597d\u7684\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u901a\u7528\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u76ee\u6807\uff0c\u9700\u8981\u8ba9\u673a\u5668\u4eba\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\u7528\u4e8e\u5b66\u4e60\u591a\u79cd\u5168\u8eab\u6280\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u957f\u65f6\u5e8f\u4e2d\u4fdd\u6301\u7a33\u5b9a\u5e76\u6db5\u76d6\u5e7f\u6cdb\u7684\u8fd0\u52a8\u6280\u80fd\u3002", "method": "\u63d0\u51fa\u4e86VMS\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u8ddf\u8e2a\u76ee\u6807\uff08\u5e73\u8861\u5c40\u90e8\u52a8\u4f5c\u4fdd\u771f\u5ea6\u4e0e\u5168\u5c40\u8f68\u8ff9\u4e00\u81f4\u6027\uff09\u3001\u6b63\u4ea4\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff08OMoE\uff09\u4ee5\u4fc3\u8fdb\u6280\u80fd\u4e13\u4e1a\u5316\u4e0e\u6cdb\u5316\uff0c\u5e76\u5f15\u5165\u5206\u6bb5\u7ea7\u8ddf\u8e2a\u5956\u52b1\u6765\u7f13\u89e3\u4e25\u683c\u7684\u9010\u5e27\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86VMS\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u7cbe\u786e\u6a21\u4eff\u52a8\u6001\u6280\u80fd\uff0c\u5728\u957f\u8fbe\u4e00\u5206\u949f\u7684\u52a8\u4f5c\u5e8f\u5217\u4e2d\u4fdd\u6301\u7a33\u5b9a\uff0c\u5e76\u5bf9\u672a\u89c1\u8fc7\u7684\u52a8\u4f5c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VMS\u4e3a\u591a\u529f\u80fd\u7c7b\u4eba\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2509.16457", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16457", "abs": "https://arxiv.org/abs/2509.16457", "authors": ["Yunzhe Wang", "Gale M. Lucas", "Burcin Becerik-Gerber", "Volkan Ustun"], "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations", "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025), Main Conference", "summary": "Language-driven generative agents have enabled large-scale social simulations\nwith transformative uses, from interpersonal training to aiding global\npolicy-making. However, recent studies indicate that generative agent behaviors\noften deviate from expert expectations and real-world data--a phenomenon we\nterm the Behavior-Realism Gap. To address this, we introduce a theoretical\nframework called Persona-Environment Behavioral Alignment (PEBA), formulated as\na distribution matching problem grounded in Lewin's behavior equation stating\nthat behavior is a function of the person and their environment. Leveraging\nPEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that\niteratively refines agent personas, implicitly aligning their collective\nbehaviors with realistic expert benchmarks within a specified environmental\ncontext. We validate PEvo in an active shooter incident simulation we\ndeveloped, achieving an 84% average reduction in distributional divergence\ncompared to no steering and a 34% improvement over explicit instruction\nbaselines. Results also show PEvo-refined personas generalize to novel, related\nsimulation scenarios. Our method greatly enhances behavioral realism and\nreliability in high-stakes social simulations. More broadly, the PEBA-PEvo\nframework provides a principled approach to developing trustworthy LLM-driven\nsocial simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPEBA\u7684\u7406\u8bba\u6846\u67b6\u548c\u57fa\u4e8eLLM\u7684\u4f18\u5316\u7b97\u6cd5PEvo\uff0c\u7528\u4e8e\u63d0\u5347\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u884c\u4e3a\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u667a\u80fd\u4f53\u4eba\u683c\u4ee5\u5339\u914d\u73b0\u5b9e\u4e13\u5bb6\u884c\u4e3a\u5206\u5e03\uff0c\u5728\u4e3b\u52a8\u5c04\u51fb\u4e8b\u4ef6\u6a21\u62df\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u884c\u4e3a\u5206\u5e03\u504f\u5dee\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u5e38\u504f\u79bb\u4e13\u5bb6\u9884\u671f\u548c\u73b0\u5b9e\u6570\u636e\uff0c\u5b58\u5728\u2018\u884c\u4e3a-\u73b0\u5b9e\u5dee\u8ddd\u2019\uff08Behavior-Realism Gap\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u53ef\u4fe1\u5ea6\u4e0e\u5e94\u7528\u6548\u679c\u3002", "method": "\u57fa\u4e8eLewin\u884c\u4e3a\u65b9\u7a0b\u63d0\u51faPersona-Environment Behavioral Alignment\uff08PEBA\uff09\u6846\u67b6\uff0c\u5c06\u884c\u4e3a\u5bf9\u9f50\u5efa\u6a21\u4e3a\u5206\u5e03\u5339\u914d\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1PersonaEvolve\uff08PEvo\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u73af\u5883\u4e0a\u4e0b\u6587\u9690\u5f0f\u4f18\u5316\u667a\u80fd\u4f53\u4eba\u683c\uff0c\u4f7f\u5176\u96c6\u4f53\u884c\u4e3a\u903c\u8fd1\u771f\u5b9e\u4e13\u5bb6\u57fa\u51c6\u3002", "result": "\u5728\u81ea\u4e3b\u6784\u5efa\u7684\u4e3b\u52a8\u67aa\u624b\u4e8b\u4ef6\u6a21\u62df\u4e2d\uff0cPEvo\u76f8\u6bd4\u65e0\u5f15\u5bfc\u57fa\u7ebf\u5e73\u5747\u51cf\u5c1184%\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u8f83\u663e\u5f0f\u6307\u4ee4\u57fa\u7ebf\u63d0\u534734%\uff0c\u4e14\u4f18\u5316\u540e\u7684\u4eba\u683c\u53ef\u6cdb\u5316\u81f3\u65b0\u7684\u76f8\u5173\u573a\u666f\u3002", "conclusion": "PEBA-PEvo\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u9ad8\u98ce\u9669\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u884c\u4e3a\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u7684LLM\u9a71\u52a8\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002"}}
{"id": "2509.16393", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.16393", "abs": "https://arxiv.org/abs/2509.16393", "authors": ["Manuel Noseda", "Alberto De Luca", "Lukas Von Briel", "Nathan Lacour"], "title": "Federated Learning for Financial Forecasting", "comment": null, "summary": "This paper studies Federated Learning (FL) for binary classification of\nvolatile financial market trends. Using a shared Long Short-Term Memory (LSTM)\nclassifier, we compare three scenarios: (i) a centralized model trained on the\nunion of all data, (ii) a single-agent model trained on an individual data\nsubset, and (iii) a privacy-preserving FL collaboration in which agents\nexchange only model updates, never raw data. We then extend the study with\nadditional market features, deliberately introducing not independent and\nidentically distributed data (non-IID) across agents, personalized FL and\nemploying differential privacy. Our numerical experiments show that FL achieves\naccuracy and generalization on par with the centralized baseline, while\nsignificantly outperforming the single-agent model. The results show that\ncollaborative, privacy-preserving learning provides collective tangible value\nin finance, even under realistic data heterogeneity and personalization\nrequirements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u4e8c\u5143\u5206\u7c7b\u91d1\u878d\u5e02\u573a\u4ef7\u683c\u6ce2\u52a8\u8d8b\u52bf\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u96c6\u4e2d\u5f0f\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u5355\u4e2a\u4ee3\u7406\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u5728\u91d1\u878d\u9886\u57df\u4e2d\u5982\u4f55\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u4e2a\u6027\u5316\u9700\u6c42\u4e0b\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u7684LSTM\u5206\u7c7b\u5668\uff0c\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u6a21\u578b\u3001\u5355\u4ee3\u7406\u6a21\u578b\u548c\u8054\u90a6\u5b66\u4e60\u4e09\u79cd\u573a\u666f\uff0c\u5e76\u5f15\u5165\u975eIID\u6570\u636e\u3001\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u9690\u79c1\u8fdb\u884c\u6269\u5c55\u5b9e\u9a8c\u3002", "result": "\u8054\u90a6\u5b66\u4e60\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4e0e\u96c6\u4e2d\u5f0f\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5355\u4ee3\u7406\u6a21\u578b\uff0c\u8bc1\u660e\u5176\u5728\u91d1\u878d\u9886\u57df\u7684\u534f\u4f5c\u4ef7\u503c\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5728\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u5728\u91d1\u878d\u5e02\u573a\u7684\u6ce2\u52a8\u9884\u6d4b\u4e2d\u63d0\u4f9b\u6709\u6548\u7684\u96c6\u4f53\u4ef7\u503c\uff0c\u5373\u4f7f\u5728\u6570\u636e\u5f02\u6784\u548c\u4e2a\u6027\u5316\u8981\u6c42\u4e0b\u4e5f\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2509.16590", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.16590", "abs": "https://arxiv.org/abs/2509.16590", "authors": ["Manuel Borroto", "Katie Gallagher", "Antonio Ielo", "Irfan Kareem", "Francesco Ricca", "Alessandra Russo"], "title": "Question Answering with LLMs and Learning from Answer Sets", "comment": "Under consideration for TPLP journal", "summary": "Large Language Models (LLMs) excel at understanding natural language but\nstruggle with explicit commonsense reasoning. A recent trend of research\nsuggests that the combination of LLM with robust symbolic reasoning systems can\novercome this problem on story-based question answering tasks. In this setting,\nexisting approaches typically depend on human expertise to manually craft the\nsymbolic component. We argue, however, that this component can also be\nautomatically learned from examples. In this work, we introduce LLM2LAS, a\nhybrid system that effectively combines the natural language understanding\ncapabilities of LLMs, the rule induction power of the Learning from Answer Sets\n(LAS) system ILASP, and the formal reasoning strengths of Answer Set\nProgramming (ASP). LLMs are used to extract semantic structures from text,\nwhich ILASP then transforms into interpretable logic rules. These rules allow\nan ASP solver to perform precise and consistent reasoning, enabling correct\nanswers to previously unseen questions. Empirical results outline the strengths\nand weaknesses of our automatic approach for learning and reasoning in a\nstory-based question answering benchmark.", "AI": {"tldr": "\u63d0\u51faLLM2LAS\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u5b66\u4e60\u7cfb\u7edfILASP\u53caASP\u6c42\u89e3\u5668\uff0c\u81ea\u52a8\u4ece\u6587\u672c\u4e2d\u5b66\u4e60\u903b\u8f91\u89c4\u5219\u5e76\u8fdb\u884c\u7cbe\u786e\u63a8\u7406\uff0c\u7528\u4e8e\u6545\u4e8b\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7b26\u53f7\u7cfb\u7edf\uff0c\u4e0d\u591f\u81ea\u52a8\u5316\u3002", "method": "\u5229\u7528LLM\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\uff0c\u901a\u8fc7ILASP\u4ece\u7b54\u6848\u96c6\u5b66\u4e60\u751f\u6210\u53ef\u89e3\u91ca\u903b\u8f91\u89c4\u5219\uff0c\u5e76\u7528ASP\u6c42\u89e3\u5668\u8fdb\u884c\u5f62\u5f0f\u5316\u63a8\u7406\u3002", "result": "\u5728\u6545\u4e8b\u95ee\u7b54\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u81ea\u52a8\u5b66\u4e60\u89c4\u5219\u5e76\u51c6\u786e\u56de\u7b54\u65b0\u95ee\u9898\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5176\u4f18\u7f3a\u70b9\u3002", "conclusion": "LLM2LAS\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u7b26\u53f7\u89c4\u5219\u5b66\u4e60\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u795e\u7ecf\u4e0e\u7b26\u53f7\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5e38\u8bc6\u63a8\u7406\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.18054", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18054", "abs": "https://arxiv.org/abs/2509.18054", "authors": ["Nikhil N S", "Amol Dilip Joshi", "Bilal Muhammed", "Soban Babu"], "title": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem", "comment": "10 pages, 5 figures", "summary": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08KG-RAG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u65bd\u5e03\u5c40\u95ee\u9898\uff08FLP\uff09\u7684\u7b97\u6cd5\u63a8\u8350\uff0c\u901a\u8fc7\u591a\u89d2\u5ea6\u68c0\u7d22\u673a\u5236\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6570\u636e\u9a71\u52a8\u7684\u63a8\u8350\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u4f18\u4e8e\u5546\u4e1aLLM\u3002", "motivation": "\u8bbe\u65bd\u5e03\u5c40\u95ee\u9898\uff08FLP\uff09\u7b97\u6cd5\u9009\u62e9\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u4e14\u53d7\u95ee\u9898\u7279\u5f81\u5f71\u54cd\u5927\uff0c\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u63a8\u8350\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u9886\u57df\u7279\u5b9a\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u91c7\u7528\u56fe\u641c\u7d22\u3001\u5411\u91cf\u641c\u7d22\u548c\u805a\u7c7b\u641c\u7d22\u4e09\u79cd\u65b9\u5f0f\u68c0\u7d22\u8bc1\u636e\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b97\u6cd5\u63a8\u8350\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9eFLP\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u63a5\u5165\u77e5\u8bc6\u5e93\u7684\u5546\u4e1aLLM\u804a\u5929\u673a\u5668\u4eba\uff0c\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "KG-RAG\u6846\u67b6\u80fd\u6709\u6548\u652f\u6301FLP\u4e2d\u7684\u7b97\u6cd5\u9009\u62e9\uff0c\u63d0\u5347\u81ea\u52a8\u5316\u8bbe\u8ba1\u7cfb\u7edf\u7684\u667a\u80fd\u51b3\u7b56\u6c34\u5e73\u3002"}}
{"id": "2509.16479", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16479", "abs": "https://arxiv.org/abs/2509.16479", "authors": ["Christopher Silver", "Thangarajah Akilan"], "title": "Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture", "comment": null, "summary": "Falls among seniors are a major public health issue. Existing solutions using\nwearable sensors, ambient sensors, and RGB-based vision systems face challenges\nin reliability, user compliance, and practicality. Studies indicate that\nstakeholders, such as older adults and eldercare facilities, prefer\nnon-wearable, passive, privacy-preserving, and real-time fall detection systems\nthat require no user interaction. This study proposes an advanced thermal fall\ndetection method using a Bidirectional Convolutional Long Short-Term Memory\n(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general\nattention mechanisms. Through systematic experimentation across hundreds of\nmodel variations exploring the integration of attention mechanisms, recurrent\nmodules, and motion flow, we identified top-performing architectures. Among\nthem, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of\n$99.7\\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly\nemerged, diverse, and privacy-preserving benchmark. These results highlight the\ngeneralizability and practicality of the proposed model, setting new standards\nfor thermal fall detection and paving the way toward deployable,\nhigh-performance solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eBiConvLSTM\u5e76\u878d\u5408\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\u7684\u70ed\u6210\u50cf\u8dcc\u5012\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728TSF\u548cTF-66\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u8dcc\u5012\u68c0\u6d4b\u65b9\u6848\u5728\u53ef\u9760\u6027\u3001\u7528\u6237\u4f9d\u4ece\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8001\u5e74\u4eba\u548c\u517b\u8001\u673a\u6784\u66f4\u503e\u5411\u4e8e\u975e\u7a7f\u6234\u3001\u88ab\u52a8\u5f0f\u3001\u4fdd\u62a4\u9690\u79c1\u4e14\u65e0\u9700\u4ea4\u4e92\u7684\u5b9e\u65f6\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u53cc\u5411\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08BiConvLSTM\uff09\uff0c\u7ed3\u5408\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u7279\u5f81\u3001\u81ea\u6ce8\u610f\u529b\u548c\u901a\u7528\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528\u70ed\u6210\u50cf\u548c\u8fd0\u52a8\u6d41\u4fe1\u606f\u8fdb\u884c\u8dcc\u5012\u68c0\u6d4b\u3002", "result": "\u5728TSF\u6570\u636e\u96c6\u4e0aROC-AUC\u8fbe\u523099.7%\uff0c\u5728\u65b0\u63d0\u51fa\u7684TF-66\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u70ed\u6210\u50cf\u8dcc\u5012\u68c0\u6d4b\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u4e3a\u9ad8\u6027\u80fd\u3001\u53ef\u843d\u5730\u7684\u975e\u7a7f\u6234\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.16757", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16757", "abs": "https://arxiv.org/abs/2509.16757", "authors": ["Haoyang Weng", "Yitang Li", "Nikhil Sobanbabu", "Zihan Wang", "Zhengyi Luo", "Tairan He", "Deva Ramanan", "Guanya Shi"], "title": "HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos", "comment": "website: hdmi-humanoid.github.io", "summary": "Enabling robust whole-body humanoid-object interaction (HOI) remains\nchallenging due to motion data scarcity and the contact-rich nature. We present\nHDMI (HumanoiD iMitation for Interaction), a simple and general framework that\nlearns whole-body humanoid-object interaction skills directly from monocular\nRGB videos. Our pipeline (i) extracts and retargets human and object\ntrajectories from unconstrained videos to build structured motion datasets,\n(ii) trains a reinforcement learning (RL) policy to co-track robot and object\nstates with three key designs: a unified object representation, a residual\naction space, and a general interaction reward, and (iii) zero-shot deploys the\nRL policies on real humanoid robots. Extensive sim-to-real experiments on a\nUnitree G1 humanoid demonstrate the robustness and generality of our approach:\nHDMI achieves 67 consecutive door traversals and successfully performs 6\ndistinct loco-manipulation tasks in the real world and 14 tasks in simulation.\nOur results establish HDMI as a simple and general framework for acquiring\ninteractive humanoid skills from human videos.", "AI": {"tldr": "\u63d0\u51faHDMI\u6846\u67b6\uff0c\u4ece\u5355\u76eeRGB\u89c6\u9891\u4e2d\u5b66\u4e60\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u7269\u4f53\u7684\u5168\u8eab\u4ea4\u4e92\u6280\u80fd\uff0c\u901a\u8fc7\u63d0\u53d6\u8f68\u8ff9\u3001\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u548c\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5176\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u7531\u4e8e\u8fd0\u52a8\u6570\u636e\u7a00\u7f3a\u4e14\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u63a5\u89e6\u9891\u7e41\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u4eba\u5f62\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\uff081\uff09\u4ece\u65e0\u7ea6\u675f\u89c6\u9891\u4e2d\u63d0\u53d6\u5e76\u91cd\u5b9a\u5411\u4eba\u548c\u7269\u4f53\u7684\u8f68\u8ff9\u4ee5\u6784\u5efa\u7ed3\u6784\u5316\u8fd0\u52a8\u6570\u636e\u96c6\uff1b\uff082\uff09\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u7edf\u4e00\u7269\u4f53\u8868\u793a\u3001\u6b8b\u5dee\u52a8\u4f5c\u7a7a\u95f4\u548c\u901a\u7528\u4ea4\u4e92\u5956\u52b1\uff1b\uff083\uff09\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5b9e\u9a8c\uff0cHDMI\u5b9e\u73b0\u4e8667\u6b21\u8fde\u7eed\u8fc7\u95e8\u64cd\u4f5c\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6210\u529f\u5b8c\u62106\u9879\u4e0d\u540c\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5728\u4eff\u771f\u4e2d\u5b8c\u621014\u9879\u4efb\u52a1\u3002", "conclusion": "HDMI\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u901a\u7528\u7684\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u83b7\u53d6\u4ea4\u4e92\u5f0f\u4eba\u5f62\u673a\u5668\u4eba\u6280\u80fd\u7684\u6846\u67b6\uff0c\u5177\u5907\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16462", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16462", "abs": "https://arxiv.org/abs/2509.16462", "authors": ["'Mina Arzaghi'", "'Alireza Dehghanpour Farashah'", "'Florian Carichon'", "' Golnoosh Farnadi'"], "title": "Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) exhibit socio-economic biases that can propagate\ninto downstream tasks. While prior studies have questioned whether intrinsic\nbias in LLMs affects fairness at the downstream task level, this work\nempirically investigates the connection. We present a unified evaluation\nframework to compare intrinsic bias mitigation via concept unlearning with\nextrinsic bias mitigation via counterfactual data augmentation (CDA). We\nexamine this relationship through real-world financial classification tasks,\nincluding salary prediction, employment status, and creditworthiness\nassessment. Using three open-source LLMs, we evaluate models both as frozen\nembedding extractors and as fine-tuned classifiers. Our results show that\nintrinsic bias mitigation through unlearning reduces intrinsic gender bias by\nup to 94.9%, while also improving downstream task fairness metrics, such as\ndemographic parity by up to 82%, without compromising accuracy. Our framework\noffers practical guidance on where mitigation efforts can be most effective and\nhighlights the importance of applying early-stage mitigation before downstream\ndeployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u793e\u4f1a\u7ecf\u6d4e\u504f\u89c1\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5e73\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u901a\u8fc7\u6982\u5ff5\u9057\u5fd8\uff08\u5185\u5728\u53bb\u504f\uff09\u548c\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08\u5916\u5728\u53bb\u504f\uff09\u4e24\u79cd\u53bb\u504f\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u91d1\u878d\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u4e09\u4e2a\u5f00\u6e90LLM\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5185\u5728\u53bb\u504f\u80fd\u663e\u8457\u964d\u4f4e\u6027\u522b\u504f\u89c1\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5e73\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u793e\u4f1a\u7ecf\u6d4e\u504f\u89c1\u53ef\u80fd\u4f20\u9012\u5230\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5f71\u54cd\u51b3\u7b56\u516c\u5e73\u6027\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8d28\u7591\u6a21\u578b\u5185\u5728\u504f\u89c1\u662f\u5426\u771f\u6b63\u5f71\u54cd\u4e0b\u6e38\u516c\u5e73\u6027\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u8bc1\u9a8c\u8bc1\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5efa\u7acb\u5185\u5728\u504f\u89c1\u4e0e\u4e0b\u6e38\u4efb\u52a1\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u901a\u8fc7\u6982\u5ff5\u9057\u5fd8\uff08\u5185\u5728\u53bb\u504f\uff09\u548c\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08CDA\uff0c\u5916\u5728\u53bb\u504f\uff09\u4e24\u79cd\u65b9\u6cd5\u5728\u91d1\u878d\u5206\u7c7b\u4efb\u52a1\uff08\u5982\u85aa\u8d44\u9884\u6d4b\u3001\u5c31\u4e1a\u72b6\u6001\u3001\u4fe1\u7528\u8bc4\u4f30\uff09\u4e2d\u7684\u6548\u679c\u3002\u4f7f\u7528\u4e09\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u522b\u4f5c\u4e3a\u51bb\u7ed3\u7684\u5d4c\u5165\u63d0\u53d6\u5668\u548c\u5fae\u8c03\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9057\u5fd8\u8fdb\u884c\u5185\u5728\u53bb\u504f\uff0c\u6700\u591a\u53ef\u51cf\u5c1194.9%\u7684\u5185\u5728\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4f7f\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5e73\u6027\u6307\u6807\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u6027\uff09\u63d0\u5347\u6700\u591a82%\uff0c\u4e14\u4e0d\u727a\u7272\u6a21\u578b\u51c6\u786e\u6027\u3002", "conclusion": "\u5185\u5728\u504f\u89c1\u7f13\u89e3\u4e0d\u4ec5\u6709\u6548\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u516c\u5e73\u6027\uff0c\u5efa\u8bae\u5728\u6a21\u578b\u90e8\u7f72\u524d\u5c3d\u65e9\u5b9e\u65bd\u65e9\u671f\u53bb\u504f\u7b56\u7565\u3002\u672c\u7814\u7a76\u4e3a\u53bb\u504f\u65b9\u6cd5\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u578b\u5f00\u53d1\u65e9\u671f\u9636\u6bb5\u8fdb\u884c\u5e72\u9884\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.16397", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16397", "abs": "https://arxiv.org/abs/2509.16397", "authors": ["Taqiya Ehsan", "Shuren Xia", "Jorge Ortiz"], "title": "GRID: Graph-based Reasoning for Intervention and Discovery in Built Environments", "comment": null, "summary": "Manual HVAC fault diagnosis in commercial buildings takes 8-12 hours per\nincident and achieves only 60 percent diagnostic accuracy, reflecting analytics\nthat stop at correlation instead of causation. To close this gap, we present\nGRID (Graph-based Reasoning for Intervention and Discovery), a three-stage\ncausal discovery pipeline that combines constraint-based search, neural\nstructural equation modeling, and language model priors to recover directed\nacyclic graphs from building sensor data. Across six benchmarks: synthetic\nrooms, EnergyPlus simulation, the ASHRAE Great Energy Predictor III dataset,\nand a live office testbed, GRID achieves F1 scores ranging from 0.65 to 1.00,\nwith exact recovery (F1 = 1.00) in three controlled environments (Base, Hidden,\nPhysical) and strong performance on real-world data (F1 = 0.89 on ASHRAE, 0.86\nin noisy conditions). The method outperforms ten baseline approaches across all\nevaluation scenarios. Intervention scheduling achieves low operational impact\nin most scenarios (cost <= 0.026) while reducing risk metrics compared to\nbaseline approaches. The framework integrates constraint-based methods, neural\narchitectures, and domain-specific language model prompts to address the\nobservational-causal gap in building analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRID\u7684\u4e09\u9636\u6bb5\u56e0\u679c\u53d1\u73b0\u7ba1\u9053\uff0c\u7528\u4e8e\u63d0\u9ad8\u5546\u4e1a\u5efa\u7b51\u4e2dHVAC\u6545\u969c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684HVAC\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u4ec5\u505c\u7559\u5728\u76f8\u5173\u6027\u5206\u6790\uff0c\u5bfc\u81f4\u8bca\u65ad\u8017\u65f6\u957f\u4e14\u51c6\u786e\u7387\u4f4e\uff0860%\uff09\uff0c\u9700\u8981\u5411\u56e0\u679c\u63a8\u7406\u8f6c\u53d8\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u7ea6\u675f\u7684\u641c\u7d22\u3001\u795e\u7ecf\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\uff0c\u4ece\u5efa\u7b51\u4f20\u611f\u5668\u6570\u636e\u4e2d\u6062\u590d\u6709\u5411\u65e0\u73af\u56fe\uff0c\u6784\u5efa\u56e0\u679c\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u5e72\u9884\u8c03\u5ea6\u7b56\u7565\u4ee5\u964d\u4f4e\u64cd\u4f5c\u5f71\u54cd\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRID\u7684F1\u5f97\u5206\u4ecb\u4e8e0.65\u52301.00\u4e4b\u95f4\uff0c\u5728\u4e09\u4e2a\u53d7\u63a7\u73af\u5883\u4e2d\u5b9e\u73b0\u5b8c\u5168\u51c6\u786e\u6062\u590d\uff08F1=1.00\uff09\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff08ASHRAE\u4e0aF1=0.89\uff0c\u566a\u58f0\u6761\u4ef6\u4e0bF1=0.86\uff09\uff0c\u4f18\u4e8e\u5341\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5e72\u9884\u6210\u672c\u4f4e\u3001\u98ce\u9669\u66f4\u5c0f\u3002", "conclusion": "GRID\u6709\u6548\u5f25\u5408\u4e86\u5efa\u7b51\u5206\u6790\u4e2d\u7684\u89c2\u6d4b\u4e0e\u56e0\u679c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86HVAC\u6545\u969c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.16648", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16648", "abs": "https://arxiv.org/abs/2509.16648", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs", "comment": "Accepted in the Findings of EMNLP, 2025", "summary": "The accurate trust assessment of multimodal large language models (MLLMs)\ngenerated predictions, which can enable selective prediction and improve user\nconfidence, is challenging due to the diverse multi-modal input paradigms. We\npropose Functionally Equivalent Sampling for Trust Assessment (FESTA), a\nmultimodal input sampling technique for MLLMs, that generates an uncertainty\nmeasure based on the equivalent and complementary input samplings. The proposed\ntask-preserving sampling approach for uncertainty quantification expands the\ninput space to probe the consistency (through equivalent samples) and\nsensitivity (through complementary samples) of the model. FESTA uses only\ninput-output access of the model (black-box), and does not require ground truth\n(unsupervised). The experiments are conducted with various off-the-shelf\nmulti-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA\nuncertainty estimate achieves significant improvement (33.3% relative\nimprovement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in\nselective prediction performance, based on\narea-under-receiver-operating-characteristic curve (AUROC) metric in detecting\nmispredictions. The code implementation is open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFESTA\u7684\u591a\u6a21\u6001\u8f93\u5165\u91c7\u6837\u6280\u672f\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7684\u4fe1\u4efb\u5ea6\uff0c\u901a\u8fc7\u7b49\u6548\u548c\u4e92\u8865\u8f93\u5165\u91c7\u6837\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u5728\u89c6\u89c9\u548c\u97f3\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8bef\u9884\u6d4b\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9884\u6d4b\u7684\u4fe1\u4efb\u5ea6\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u591a\u6a21\u6001\u8f93\u5165\u8303\u5f0f\u591a\u6837\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faFESTA\u65b9\u6cd5\uff0c\u901a\u8fc7\u529f\u80fd\u7b49\u6548\u91c7\u6837\u751f\u6210\u7b49\u6548\u548c\u4e92\u8865\u8f93\u5165\u6837\u672c\uff0c\u5728\u4e0d\u4f9d\u8d56\u771f\u5b9e\u6807\u7b7e\u548c\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u63a2\u6d4b\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u654f\u611f\u6027\u4ee5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u79cd\u73b0\u6210\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cFESTA\u5728AUROC\u6307\u6807\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u89c6\u89c9-LLM\u548c\u97f3\u9891-LLM\u4e2d\u5206\u522b\u53d6\u5f9733.3%\u548c29.6%\u7684\u76f8\u5bf9\u63d0\u5347\u3002", "conclusion": "FESTA\u662f\u4e00\u79cd\u6709\u6548\u7684\u9ed1\u76d2\u3001\u65e0\u76d1\u7763\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9009\u62e9\u6027\u9884\u6d4b\u4e2d\u7684\u4fe1\u4efb\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2509.18091", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18091", "abs": "https://arxiv.org/abs/2509.18091", "authors": ["Sunhao Dai", "Jiakai Tang", "Jiahua Wu", "Kun Wang", "Yuxuan Zhu", "Bingjun Chen", "Bangyang Hong", "Yu Zhao", "Cong Fu", "Kangle Wu", "Yabo Ni", "Anxiang Zeng", "Wenjie Wang", "Xu Chen", "Jun Xu", "See-Kiong Ng"], "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System", "comment": "OnePiece Technical Report; Applied in Shopee", "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OnePiece\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u4e0e\u591a\u6b65\u63a8\u7406\u673a\u5236\u5f15\u5165\u5de5\u4e1a\u7ea7\u641c\u7d22\u4e0e\u63a8\u8350\u7cfb\u7edf\uff0c\u5728Shopee\u7684\u5b9e\u9645\u90e8\u7f72\u4e2d\u663e\u8457\u63d0\u5347\u4e86GMV\u548c\u5e7f\u544a\u6536\u5165\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u6cbf\u7528Transformer\u67b6\u6784\uff0c\u6539\u8fdb\u6709\u9650\uff1b\u800c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u6e90\u4e8e\u4e0a\u4e0b\u6587\u5de5\u7a0b\u548c\u591a\u6b65\u63a8\u7406\uff0c\u8fd9\u4e9b\u5728\u5de5\u4e1a\u6392\u5e8f\u7cfb\u7edf\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faOnePiece\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3001\u5757\u72b6\u9690\u5f0f\u63a8\u7406\u3001\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u8bad\u7ec3\uff0c\u5e76\u96c6\u6210\u4e8e\u68c0\u7d22\u4e0e\u6392\u5e8f\u6a21\u578b\u4e2d\u3002", "result": "\u5728Shopee\u4e3b\u641c\u573a\u666f\u4e2d\u4e0a\u7ebf\u540e\uff0c\u6bcf\u7528\u6237GMV\u63d0\u5347\u8d852%\uff0c\u5e7f\u544a\u6536\u5165\u589e\u52a02.90%\u3002", "conclusion": "\u5c06\u4e0a\u4e0b\u6587\u5de5\u7a0b\u4e0e\u591a\u6b65\u63a8\u7406\u7cfb\u7edf\u5316\u5f15\u5165\u63a8\u8350\u7cfb\u7edf\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cOnePiece\u4e3a\u5de5\u4e1a\u7ea7\u6392\u5e8f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.16483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16483", "abs": "https://arxiv.org/abs/2509.16483", "authors": ["Xujia Zhang", "Brendan Crowe", "Christoffer Heckman"], "title": "Octree Latent Diffusion for Semantic 3D Scene Generation and Completion", "comment": null, "summary": "The completion, extension, and generation of 3D semantic scenes are an\ninterrelated set of capabilities that are useful for robotic navigation and\nexploration. Existing approaches seek to decouple these problems and solve them\noneoff. Additionally, these approaches are often domain-specific, requiring\nseparate models for different data distributions, e.g. indoor vs. outdoor\nscenes. To unify these techniques and provide cross-domain compatibility, we\ndevelop a single framework that can perform scene completion, extension, and\ngeneration in both indoor and outdoor scenes, which we term Octree Latent\nSemantic Diffusion. Our approach operates directly on an efficient dual octree\ngraph latent representation: a hierarchical, sparse, and memory-efficient\noccupancy structure. This technique disentangles synthesis into two stages: (i)\nstructure diffusion, which predicts binary split signals to construct a coarse\noccupancy octree, and (ii) latent semantic diffusion, which generates semantic\nembeddings decoded by a graph VAE into voxellevel semantic labels. To perform\nsemantic scene completion or extension, our model leverages inference-time\nlatent inpainting, or outpainting respectively. These inference-time methods\nuse partial LiDAR scans or maps to condition generation, without the need for\nretraining or finetuning. We demonstrate highquality structure, coherent\nsemantics, and robust completion from single LiDAR scans, as well as zero-shot\ngeneralization to out-of-distribution LiDAR data. These results indicate that\ncompletion-through-generation in a dual octree graph latent space is a\npractical and scalable alternative to regression-based pipelines for real-world\nrobotic perception tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOctree Latent Semantic Diffusion\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u53ef\u5728\u5ba4\u5185\u5916\u573a\u666f\u4e2d\u5b8c\u62103D\u8bed\u4e49\u573a\u666f\u7684\u8865\u5168\u3001\u6269\u5c55\u548c\u751f\u6210\uff0c\u91c7\u7528\u53cc\u516b\u53c9\u56fe\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u7ed3\u6784\u6269\u6563\u548c\u6f5c\u5728\u8bed\u4e49\u6269\u6563\u4e24\u9636\u6bb5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u8fde\u8d2f\u4e14\u9c81\u68d2\u7684\u8bed\u4e49\u573a\u666f\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c063D\u8bed\u4e49\u573a\u666f\u7684\u8865\u5168\u3001\u6269\u5c55\u548c\u751f\u6210\u95ee\u9898\u89e3\u8026\u5904\u7406\uff0c\u4e14\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\uff08\u5982\u5ba4\u5185\u6216\u5ba4\u5916\uff09\uff0c\u7f3a\u4e4f\u8de8\u57df\u517c\u5bb9\u6027\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u3001\u901a\u7528\u7684\u6846\u67b6\u6765\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faOctree Latent Semantic Diffusion\u6846\u67b6\uff0c\u57fa\u4e8e\u9ad8\u6548\u7684\u53cc\u516b\u53c9\u56fe\u6f5c\u5728\u8868\u793a\uff0c\u5206\u4e24\u9636\u6bb5\u8fdb\u884c\uff1a(i) \u7ed3\u6784\u6269\u6563\uff0c\u9884\u6d4b\u4e8c\u503c\u5206\u5272\u4fe1\u53f7\u6784\u5efa\u7c97\u7565\u5360\u636e\u516b\u53c9\u6811\uff1b(ii) \u6f5c\u5728\u8bed\u4e49\u6269\u6563\uff0c\u751f\u6210\u7531\u56feVAE\u89e3\u7801\u4e3a\u4f53\u7d20\u7ea7\u8bed\u4e49\u6807\u7b7e\u7684\u8bed\u4e49\u5d4c\u5165\u3002\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7684inpainting\u6216outpainting\u5b9e\u73b0\u573a\u666f\u8865\u5168\u6216\u6269\u5c55\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u5355\u4e2aLiDAR\u626b\u63cf\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7ed3\u6784\u91cd\u5efa\u548c\u8fde\u8d2f\u8bed\u4e49\u751f\u6210\uff0c\u5177\u5907\u5bf9\u5206\u5e03\u5916LiDAR\u6570\u636e\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u5728\u53cc\u516b\u53c9\u56fe\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u5b9e\u73b0\u8bed\u4e49\u573a\u666f\u8865\u5168\u662f\u56de\u5f52\u5f0f\u6d41\u6c34\u7ebf\u7684\u4e00\u79cd\u66f4\u4f18\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8de8\u57df\u3001\u591a\u4efb\u52a1\u76843D\u8bed\u4e49\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2509.16464", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16464", "abs": "https://arxiv.org/abs/2509.16464", "authors": ["Margaret Hughes", "Brandon Roy", "Elinor Poole-Dayan", "Deb Roy", "Jad Kabbara"], "title": "Computational Analysis of Conversation Dynamics through Participant Responsivity", "comment": null, "summary": "Growing literature explores toxicity and polarization in discourse, with\ncomparatively less work on characterizing what makes dialogue prosocial and\nconstructive. We explore conversational discourse and investigate a method for\ncharacterizing its quality built upon the notion of ``responsivity'' -- whether\none person's conversational turn is responding to a preceding turn. We develop\nand evaluate methods for quantifying responsivity -- first through semantic\nsimilarity of speaker turns, and second by leveraging state-of-the-art large\nlanguage models (LLMs) to identify the relation between two speaker turns. We\nevaluate both methods against a ground truth set of human-annotated\nconversations. Furthermore, selecting the better performing LLM-based approach,\nwe characterize the nature of the response -- whether it responded to that\npreceding turn in a substantive way or not.\n  We view these responsivity links as a fundamental aspect of dialogue but note\nthat conversations can exhibit significantly different responsivity structures.\nAccordingly, we then develop conversation-level derived metrics to address\nvarious aspects of conversational discourse. We use these derived metrics to\nexplore other conversations and show that they support meaningful\ncharacterizations and differentiations across a diverse collection of\nconversations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u54cd\u5e94\u6027\u201d\u7684\u5bf9\u8bdd\u8d28\u91cf\u523b\u753b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u5927\u8bed\u8a00\u6a21\u578b\u6765\u91cf\u5316\u5bf9\u8bdd\u4e2d\u7684\u54cd\u5e94\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86\u4f1a\u8bdd\u7ea7\u522b\u7684\u6307\u6807\u4ee5\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u5bf9\u8bdd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8f83\u591a\u5173\u6ce8\u5bf9\u8bdd\u4e2d\u7684\u6bd2\u6027\u548c\u4e24\u6781\u5316\uff0c\u8f83\u5c11\u5173\u6ce8\u5982\u4f55\u523b\u753b\u5efa\u8bbe\u6027\u548c\u4eb2\u793e\u4f1a\u7684\u5bf9\u8bdd\u7279\u5f81\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u6027\u5ea6\u91cf\u8bf4\u8bdd\u8005\u8bdd\u8bed\u95f4\u7684\u54cd\u5e94\u6027\uff0c\u7136\u540e\u4f7f\u7528\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u4e24\u8f6e\u8bdd\u8bed\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff1b\u9009\u62e9\u8868\u73b0\u66f4\u597d\u7684LLM\u65b9\u6cd5\u8fdb\u4e00\u6b65\u5206\u6790\u56de\u5e94\u662f\u5426\u5177\u6709\u5b9e\u8d28\u6027\u3002", "result": "LLM-based\u65b9\u6cd5\u5728\u8bc6\u522b\u54cd\u5e94\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u53ef\u7528\u4e8e\u533a\u5206\u5b9e\u8d28\u6027\u4e0e\u975e\u5b9e\u8d28\u6027\u56de\u5e94\uff1b\u57fa\u4e8e\u6b64\u6784\u5efa\u7684\u4f1a\u8bdd\u7ea7\u6307\u6807\u80fd\u6709\u6548\u523b\u753b\u548c\u533a\u5206\u591a\u79cd\u5bf9\u8bdd\u7c7b\u578b\u3002", "conclusion": "\u54cd\u5e94\u6027\u662f\u5bf9\u8bdd\u7684\u57fa\u672c\u7279\u5f81\u4e4b\u4e00\uff0c\u6240\u63d0\u51fa\u7684\u6307\u6807\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u5bf9\u8bdd\u7ed3\u6784\u548c\u8d28\u91cf\uff0c\u4e3a\u4fc3\u8fdb\u5efa\u8bbe\u6027\u5bf9\u8bdd\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2509.16447", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16447", "abs": "https://arxiv.org/abs/2509.16447", "authors": ["Arwen Bradley"], "title": "Local Mechanisms of Compositional Generalization in Conditional Diffusion", "comment": "10 pages, 7 figures", "summary": "Conditional diffusion models appear capable of compositional generalization,\ni.e., generating convincing samples for out-of-distribution combinations of\nconditioners, but the mechanisms underlying this ability remain unclear. To\nmake this concrete, we study length generalization, the ability to generate\nimages with more objects than seen during training. In a controlled CLEVR\nsetting (Johnson et al., 2017), we find that length generalization is\nachievable in some cases but not others, suggesting that models only sometimes\nlearn the underlying compositional structure. We then investigate locality as a\nstructural mechanism for compositional generalization. Prior works proposed\nscore locality as a mechanism for creativity in unconditional diffusion models\n(Kamb & Ganguli, 2024; Niedoba et al., 2024), but did not address flexible\nconditioning or compositional generalization. In this paper, we prove an exact\nequivalence between a specific compositional structure (\"conditional projective\ncomposition\") (Bradley et al., 2025) and scores with sparse dependencies on\nboth pixels and conditioners (\"local conditional scores\"). This theory also\nextends to feature-space compositionality. We validate our theory empirically:\nCLEVR models that succeed at length generalization exhibit local conditional\nscores, while those that fail do not. Furthermore, we show that a causal\nintervention explicitly enforcing local conditional scores restores length\ngeneralization in a previously failing model. Finally, we investigate\nfeature-space compositionality in color-conditioned CLEVR, and find preliminary\nevidence of compositional structure in SDXL.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u7ec4\u5408\u6cdb\u5316\uff08\u5982\u957f\u5ea6\u6cdb\u5316\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u5e76\u8bc1\u660e\u4e86\u201c\u5c40\u90e8\u6761\u4ef6\u5206\u6570\u201d\u4e0e\u7279\u5b9a\u7ec4\u5408\u7ed3\u6784\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u673a\u5236\u5728CLEVR\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e14\u53ef\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u6062\u590d\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u5728SDXL\u4e2d\u53d1\u73b0\u4e86\u7279\u5f81\u7a7a\u95f4\u7ec4\u5408\u6027\u7684\u521d\u6b65\u8bc1\u636e\u3002", "motivation": "\u7406\u89e3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e3a\u4f55\u80fd\u5728\u672a\u89c1\u7684\u6761\u4ef6\u7ec4\u5408\u4e0b\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\uff0c\u7279\u522b\u662f\u957f\u5ea6\u6cdb\u5316\uff0c\u5e76\u63ed\u793a\u5176\u80cc\u540e\u7684\u7ed3\u6784\u6027\u673a\u5236\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u5efa\u7acb\u5c40\u90e8\u6761\u4ef6\u5206\u6570\u4e0e\u6761\u4ef6\u6295\u5f71\u7ec4\u5408\u7ed3\u6784\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u5e76\u5728\u63a7\u5236\u53d8\u91cf\u7684CLEVR\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u540c\u65f6\u5f15\u5165\u56e0\u679c\u5e72\u9884\u6765\u6d4b\u8bd5\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5c40\u90e8\u6761\u4ef6\u5206\u6570\u662f\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u7684\u5173\u952e\u673a\u5236\uff1b\u5728CLEVR\u4e2d\uff0c\u6210\u529f\u6cdb\u5316\u7684\u6a21\u578b\u8868\u73b0\u51fa\u5c40\u90e8\u6761\u4ef6\u5206\u6570\uff0c\u5931\u8d25\u7684\u5219\u6ca1\u6709\uff1b\u901a\u8fc7\u5f3a\u5236\u5c40\u90e8\u5206\u6570\u53ef\u6062\u590d\u6cdb\u5316\u80fd\u529b\uff1b\u5e76\u5728SDXL\u4e2d\u53d1\u73b0\u7279\u5f81\u7a7a\u95f4\u7ec4\u5408\u6027\u7684\u521d\u6b65\u8bc1\u636e\u3002", "conclusion": "\u5c40\u90e8\u6761\u4ef6\u5206\u6570\u662f\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u7684\u6838\u5fc3\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u6a21\u578b\u5728\u5206\u5e03\u5916\u6761\u4ef6\u4e0b\u7684\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5e72\u9884\u624b\u6bb5\u3002"}}
{"id": "2509.16656", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16656", "abs": "https://arxiv.org/abs/2509.16656", "authors": ["Changyu Zeng", "Yifan Wang", "Zimu Wang", "Wei Wang", "Zhengni Yang", "Muyi Bao", "Jiming Xiao", "Ahn Nguyen", "Yutao Yue"], "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities", "comment": null, "summary": "Recent advancements in 2D multimodal large language models (MLLMs) have\nsignificantly improved performance in vision-language tasks. However, extending\nthese capabilities to 3D environments remains a distinct challenge due to the\ncomplexity of spatial reasoning. Nevertheless, existing 3D benchmarks often\nlack fine-grained numerical reasoning task annotations, limiting MLLMs' ability\nto perform precise spatial measurements and complex numerical reasoning. To\naddress this gap, we introduce NUMINA, the first Natural Understanding\nbenchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities\nto enhance multimodal indoor perceptual understanding. NUMINA features\nmulti-scale annotations and various question-answer pairs, generated using\nNUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and\nrule-based self-verification. We evaluate the performance of various\nstate-of-the-art LLMs on NUMINA following the Chat-Scene framework,\ndemonstrating that current LLMs struggle with multimodal numerical reasoning,\nparticularly in performing precise computations such as distance and volume\nestimation, highlighting the need for further advancements in 3D models. The\ndataset and source codes can be obtained from\nhttps://github.com/fengshun124/NUMINA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NUMINA\uff0c\u9996\u4e2a\u7528\u4e8e\u591a\u7ef4\u667a\u80fd\u548c\u6570\u503c\u63a8\u7406\u80fd\u529b\u7684\u81ea\u7136\u7406\u89e3\u57fa\u51c6\uff0c\u4ee5\u589e\u5f3a\u591a\u6a21\u6001\u5ba4\u5185\u611f\u77e5\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u76843D\u57fa\u51c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u6570\u503c\u63a8\u7406\u4efb\u52a1\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u786e\u7a7a\u95f4\u6d4b\u91cf\u548c\u590d\u6742\u6570\u503c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faNUMINA\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u5199\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u81ea\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0bNUMINA-Flow\u751f\u6210\u591a\u5c3a\u5ea6\u6807\u6ce8\u548c\u591a\u79cd\u95ee\u7b54\u5bf9\u3002", "result": "\u5728Chat-Scene\u6846\u67b6\u4e0b\u8bc4\u4f30\u4e86\u591a\u79cd\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u8ddd\u79bb\u548c\u4f53\u79ef\u4f30\u8ba1\u7b49\u7cbe\u786e\u8ba1\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u57283D\u591a\u6a21\u6001\u6570\u503c\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb3D\u6a21\u578b\u3002"}}
{"id": "2509.18095", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18095", "abs": "https://arxiv.org/abs/2509.18095", "authors": ["Zilin Xiao", "Qi Ma", "Mengting Gu", "Chun-cheng Jason Chen", "Xintao Chen", "Vicente Ordonez", "Vijai Mohan"], "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction", "comment": null, "summary": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MetaEmbed\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u68c0\u7d22\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684Meta Token\u751f\u6210\u7d27\u51d1\u4e14\u5bcc\u6709\u8868\u8fbe\u529b\u7684\u591a\u5411\u91cf\u5d4c\u5165\uff0c\u5e76\u652f\u6301\u6d4b\u8bd5\u65f6\u6839\u636e\u9700\u6c42\u7075\u6d3b\u8c03\u6574\u68c0\u7d22\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5d4c\u5165\u4e2d\u8981\u4e48\u4f7f\u7528\u5355\u5411\u91cf\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u8868\u8fbe\uff0c\u8981\u4e48\u4ea7\u751f\u8fc7\u591a\u5411\u91cf\u5bfc\u81f4\u68c0\u7d22\u6210\u672c\u8fc7\u9ad8\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8868\u8fbe\u529b\u53c8\u80fd\u517c\u987e\u6548\u7387\u7684\u5e73\u8861\u65b9\u6848\u3002", "method": "\u5728\u8bad\u7ec3\u65f6\u5c06\u56fa\u5b9a\u6570\u91cf\u7684\u53ef\u5b66\u4e60Meta Token\u6dfb\u52a0\u5230\u8f93\u5165\u5e8f\u5217\u4e2d\uff1b\u5728\u6d4b\u8bd5\u65f6\uff0c\u4f7f\u7528\u8fd9\u4e9bToken\u5728\u6700\u540e\u4e00\u5c42\u7684\u4e0a\u4e0b\u6587\u5316\u8868\u793a\u4f5c\u4e3a\u591a\u5411\u91cf\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7Matryoshka Multi-Vector Retrieval\u8bad\u7ec3\u7b56\u7565\u6309\u7c92\u5ea6\u7ec4\u7ec7\u4fe1\u606f\u3002", "result": "\u5728MMEB\u548cViDoRe\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86MetaEmbed\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u6269\u5c55\u81f3320\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u652f\u6301\u68c0\u7d22\u8d28\u91cf\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u7075\u6d3b\u6743\u8861\u3002", "conclusion": "MetaEmbed\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u5411\u91cf\u8868\u793a\u548c\u6d4b\u8bd5\u65f6\u53ef\u6269\u5c55\u673a\u5236\uff0c\u5728\u5927\u89c4\u6a21\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.16500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16500", "abs": "https://arxiv.org/abs/2509.16500", "authors": ["Tianyi Yan", "Wencheng Han", "Xia Zhou", "Xueyang Zhang", "Kun Zhan", "Cheng-zhong Xu", "Jianbing Shen"], "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation", "comment": "NeurIPS 2025", "summary": "Synthetic data is crucial for advancing autonomous driving (AD) systems, yet\ncurrent state-of-the-art video generation models, despite their visual realism,\nsuffer from subtle geometric distortions that limit their utility for\ndownstream perception tasks. We identify and quantify this critical issue,\ndemonstrating a significant performance gap in 3D object detection when using\nsynthetic versus real data. To address this, we introduce Reinforcement\nLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion\nmodels by incorporating rewards from specialized latent-space AD perception\nmodels. Its core components include an efficient Latent-Space Windowing\nOptimization technique for targeted feedback during diffusion, and a\nHierarchical Geometric Reward (HGR) system providing multi-level rewards for\npoint-line-plane alignment, and scene occupancy coherence. To quantify these\ndistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,\nRLGF substantially reduces geometric errors (e.g., VP error by 21\\%, Depth\nerror by 57\\%) and dramatically improves 3D object detection mAP by 12.7\\%,\nnarrowing the gap to real-data performance. RLGF offers a plug-and-play\nsolution for generating geometrically sound and reliable synthetic videos for\nAD development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08RLGF\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u51e0\u4f55\u51c6\u786e\u6027\uff0c\u663e\u8457\u6539\u55843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u89c6\u89c9\u4e0a\u867d\u903c\u771f\uff0c\u4f46\u5b58\u5728\u51e0\u4f55\u5931\u771f\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u6846\u67b6RLGF\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u7a97\u53e3\u4f18\u5316\u548c\u5206\u5c42\u51e0\u4f55\u5956\u52b1\uff08HGR\uff09\u7cfb\u7edf\uff0c\u5229\u7528AD\u611f\u77e5\u6a21\u578b\u63d0\u4f9b\u53cd\u9988\uff0c\u4f18\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5e94\u7528RLGF\u540e\uff0cVP\u8bef\u5dee\u964d\u4f4e21%\uff0c\u6df1\u5ea6\u8bef\u5dee\u51cf\u5c1157%\uff0c3D\u68c0\u6d4bmAP\u63d0\u534712.7%\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "RLGF\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u751f\u6210\u51e0\u4f55\u51c6\u786e\u3001\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u7528\u5408\u6210\u89c6\u9891\u3002"}}
{"id": "2509.16812", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16812", "abs": "https://arxiv.org/abs/2509.16812", "authors": ["Priyanshu Agrawal", "Shalabh Gupta", "Zongyuan Shen"], "title": "SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree", "comment": null, "summary": "This paper presents SMART-3D, an extension of the SMART algorithm to 3D\nenvironments. SMART-3D is a tree-based adaptive replanning algorithm for\ndynamic environments with fast moving obstacles. SMART-3D morphs the underlying\ntree to find a new path in real-time whenever the current path is blocked by\nobstacles. SMART-3D removed the grid decomposition requirement of the SMART\nalgorithm by replacing the concept of hot-spots with that of hot-nodes, thus\nmaking it computationally efficient and scalable to 3D environments. The\nhot-nodes are nodes which allow for efficient reconnections to morph the\nexisting tree to find a new safe and reliable path. The performance of SMART-3D\nis evaluated by extensive simulations in 2D and 3D environments populated with\nrandomly moving dynamic obstacles. The results show that SMART-3D achieves high\nsuccess rates and low replanning times, thus highlighting its suitability for\nreal-time onboard applications.", "AI": {"tldr": "SMART-3D\u662f\u5c06SMART\u7b97\u6cd5\u6269\u5c55\u5230\u4e09\u7ef4\u73af\u5883\u7684\u6811\u5f62\u81ea\u9002\u5e94\u91cd\u89c4\u5212\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5b58\u5728\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u7684\u52a8\u6001\u73af\u5883\uff0c\u5177\u6709\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u5b9e\u65f6\u6027\u7279\u70b9\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u539fSMART\u7b97\u6cd5\u5728\u4e09\u7ef4\u73af\u5883\u4e2d\u56e0\u4f9d\u8d56\u7f51\u683c\u5212\u5206\u800c\u5bfc\u81f4\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8def\u5f84\u91cd\u89c4\u5212\u65b9\u6cd5\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5feb\u901f\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u201c\u70ed\u8282\u70b9\u201d\uff08hot-nodes\uff09\u66ff\u4ee3\u539f\u6709\u7684\u201c\u70ed\u70b9\u201d\u6982\u5ff5\uff0c\u53bb\u9664\u5bf9\u7f51\u683c\u5206\u89e3\u7684\u4f9d\u8d56\uff1b\u5229\u7528\u6811\u7ed3\u6784\u7684\u5f62\u6001\u8c03\u6574\uff0c\u5728\u8def\u5f84\u88ab\u963b\u65ad\u65f6\u5b9e\u65f6\u751f\u6210\u65b0\u8def\u5f84\uff0c\u5b9e\u73b0\u5bf9\u52a8\u6001\u969c\u788d\u7269\u7684\u5feb\u901f\u54cd\u5e94\u3002", "result": "\u57282D\u548c3D\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u91cf\u4eff\u771f\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660eSMART-3D\u5177\u6709\u9ad8\u6210\u529f\u7387\u548c\u4f4e\u91cd\u89c4\u5212\u65f6\u95f4\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u968f\u673a\u79fb\u52a8\u7684\u52a8\u6001\u969c\u788d\u7269\u3002", "conclusion": "SMART-3D\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u4e09\u7ef4\u52a8\u6001\u73af\u5883\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u8def\u5f84\u91cd\u89c4\u5212\u7b97\u6cd5\uff0c\u9002\u5408\u673a\u8f7d\u7b49\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2509.16487", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16487", "abs": "https://arxiv.org/abs/2509.16487", "authors": ["Zixun Chen", "Petr Babkin", "Akshat Gupta", "Gopala Anumanchipalli", "Xiaomo Liu"], "title": "The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia", "comment": null, "summary": "Dialogue is one of the landmark abilities of large language models (LLMs).\nDespite its ubiquity, few studies actually distinguish specific ingredients\nunderpinning dialogue behavior emerging during post-training. We employ a\ncomprehensive suite of model-based metrics, each targeting a distinct\nfine-grained aspect of dialogue, motivated by linguistic theory. We evaluate\nhow the performance of pre-trained Pythia models changes with respect to each\nof those dimensions, depending on model size and as a result of supervised\nfine-tuning on conversational datasets. We observe only a mild impact of raw\nmodel size on most metrics, whereas fine-tuning quickly saturates the scores\nfor all but the smallest models tested. Somewhat contrary to our expectations,\nmany metrics show very similar trends, especially if they are all rooted in the\nsame evaluator model, which raises the question of their reliability in\nmeasuring a specific dimension. To that end, we conduct additional analyses of\nscore distributions, metric correlations, and term frequencies in generated\nresponses to help explain our observations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u540e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u8bdd\u884c\u4e3a\u7684\u5177\u4f53\u5f71\u54cd\u56e0\u7d20\uff0c\u4f7f\u7528\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u7ec6\u7c92\u5ea6\u6307\u6807\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21Pythia\u6a21\u578b\u5728\u76d1\u7763\u5fae\u8c03\u524d\u540e\u7684\u8868\u73b0\u53d8\u5316\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5c3a\u5bf8\u5bf9\u591a\u6570\u6307\u6807\u5f71\u54cd\u8f83\u5c0f\uff0c\u800c\u5fae\u8c03\u80fd\u5feb\u901f\u63d0\u5347\u9664\u6700\u5c0f\u6a21\u578b\u5916\u7684\u6240\u6709\u6307\u6807\u5f97\u5206\uff1b\u4f46\u8bb8\u591a\u6307\u6807\u8d8b\u52bf\u76f8\u4f3c\uff0c\u63d0\u793a\u5176\u6d4b\u91cf\u7279\u5f02\u6027\u53ef\u80fd\u6709\u9650\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u8bdd\u80fd\u529b\u5728\u5927\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u54ea\u4e9b\u5177\u4f53\u56e0\u7d20\u4fc3\u6210\u4e86\u8fd9\u4e00\u884c\u4e3a\u7684\u51fa\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u8bc6\u522b\u548c\u533a\u5206\u8fd9\u4e9b\u6210\u5206\u3002", "method": "\u91c7\u7528\u4e00\u5957\u57fa\u4e8e\u4e0d\u540c\u5bf9\u8bdd\u7ef4\u5ea6\u7684\u6a21\u578b\u5316\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408\u751f\u6210\u54cd\u5e94\u4e2d\u7684\u5206\u6570\u5206\u5e03\u3001\u6307\u6807\u76f8\u5173\u6027\u548c\u8bcd\u9891\u5206\u6790\uff0c\u8bc4\u4f30Pythia\u7cfb\u5217\u6a21\u578b\u5728\u4e0d\u540c\u89c4\u6a21\u53ca\u7ecf\u8fc7\u5bf9\u8bdd\u6570\u636e\u5fae\u8c03\u540e\u7684\u8868\u73b0\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u539f\u59cb\u6a21\u578b\u89c4\u6a21\u5bf9\u5927\u591a\u6570\u5bf9\u8bdd\u6307\u6807\u4ec5\u6709\u8f7b\u5fae\u5f71\u54cd\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u4f7f\u51e0\u4e4e\u6240\u6709\u975e\u6700\u5c0f\u6a21\u578b\u7684\u6307\u6807\u8fc5\u901f\u9971\u548c\uff1b\u7136\u800c\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u76f8\u540c\u8bc4\u4f30\u6a21\u578b\u7684\u6307\u6807\u8868\u73b0\u51fa\u9ad8\u5ea6\u76f8\u4f3c\u7684\u8d8b\u52bf\uff0c\u5f15\u53d1\u5bf9\u5176\u533a\u5206\u6548\u5ea6\u7684\u8d28\u7591\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u7279\u5b9a\u8bc4\u4f30\u6a21\u578b\u7684\u5bf9\u8bdd\u5ea6\u91cf\u53ef\u80fd\u5b58\u5728\u5197\u4f59\u6216\u7f3a\u4e4f\u7279\u5f02\u6027\uff0c\u9700\u66f4\u8c28\u614e\u5730\u89e3\u91ca\u5176\u7ed3\u679c\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5e94\u7ed3\u5408\u591a\u89d2\u5ea6\u5206\u6790\u4ee5\u66f4\u597d\u7406\u89e3\u5bf9\u8bdd\u80fd\u529b\u7684\u6784\u6210\u3002"}}
{"id": "2509.16463", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16463", "abs": "https://arxiv.org/abs/2509.16463", "authors": ["Spencer Compton", "Kristjan Greenewald", "Dmitriy Katz", "Murat Kocaoglu"], "title": "Entropic Causal Inference: Graph Identifiability", "comment": "Presented at ICML 2022. This version corrects a bug in semi-synthetic\n  experiments", "summary": "Entropic causal inference is a recent framework for learning the causal graph\nbetween two variables from observational data by finding the\ninformation-theoretically simplest structural explanation of the data, i.e.,\nthe model with smallest entropy. In our work, we first extend the causal graph\nidentifiability result in the two-variable setting under relaxed assumptions.\nWe then show the first identifiability result using the entropic approach for\nlearning causal graphs with more than two nodes. Our approach utilizes the\nproperty that ancestrality between a source node and its descendants can be\ndetermined using the bivariate entropic tests. We provide a sound sequential\npeeling algorithm for general graphs that relies on this property. We also\npropose a heuristic algorithm for small graphs that shows strong empirical\nperformance. We rigorously evaluate the performance of our algorithms on\nsynthetic data generated from a variety of models, observing improvement over\nprior work. Finally we test our algorithms on real-world datasets.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u53cc\u53d8\u91cf\u56e0\u679c\u56fe\u7684\u53ef\u8bc6\u522b\u6027\u7ed3\u679c\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u4e86\u57fa\u4e8e\u71b5\u65b9\u6cd5\u7684\u591a\u8282\u70b9\u56e0\u679c\u56fe\u5b66\u4e60\u7684\u53ef\u8bc6\u522b\u6027\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e8f\u5217\u5265\u79bb\u548c\u542f\u53d1\u5f0f\u7b56\u7565\u7684\u7b97\u6cd5\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5bfb\u627e\u89c2\u6d4b\u6570\u636e\u4e2d\u4e24\u4e2a\u53d8\u91cf\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u4fe1\u606f\u8bba\u6700\u7b80\u89e3\u91ca\uff0c\u5373\u6700\u5c0f\u71b5\u6a21\u578b\uff0c\u662f\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6240\u5728\uff0c\u672c\u6587\u65e8\u5728\u653e\u5bbd\u5047\u8bbe\u5e76\u6269\u5c55\u81f3\u591a\u53d8\u91cf\u573a\u666f\u3002", "method": "\u5229\u7528\u6e90\u8282\u70b9\u4e0e\u5176\u540e\u4ee3\u4e4b\u95f4\u7684\u7956\u5148\u6027\u53ef\u901a\u8fc7\u53cc\u53d8\u91cf\u71b5\u68c0\u9a8c\u786e\u5b9a\u7684\u6027\u8d28\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u987a\u5e8f\u5265\u79bb\u7b97\u6cd5\u548c\u4e00\u79cd\u9488\u5bf9\u5c0f\u56fe\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u751f\u6210\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u6027\u80fd\uff0c\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\u6709\u6240\u63d0\u5347\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u57fa\u4e8e\u71b5\u7684\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u53ef\u4ee5\u6269\u5c55\u5230\u591a\u8282\u70b9\u56e0\u679c\u56fe\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u6709\u6548\u3002"}}
{"id": "2509.16742", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16742", "abs": "https://arxiv.org/abs/2509.16742", "authors": ["Mohammad Beigi", "Ying Shen", "Parshin Shojaee", "Qifan Wang", "Zichao Wang", "Chandan Reddy", "Ming Jin", "Lifu Huang"], "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories", "comment": null, "summary": "Despite the remarkable capabilities of large language models, current\ntraining paradigms inadvertently foster \\textit{sycophancy}, i.e., the tendency\nof a model to agree with or reinforce user-provided information even when it's\nfactually incorrect. To address this challenge, we introduce \\textbf{SMART}\n(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes\nsycophancy as a \\textit{reasoning optimization problem} rather than an output\nalignment issue. SMART is a two-stage framework comprising: (1)\nUncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically\nadjusts model exploration based on state-level uncertainty to collect\nhigh-quality, diverse reasoning trajectories alongside both stepwise progress\nand final outcome rewards; and (2) progress-based reinforcement learning, which\nfine-tunes the model using the collected trajectories and reward signals to\nreinforce effective reasoning patterns. Through extensive experiments, we show\nthat SMART significantly reduces sycophantic behavior while preserving strong\nperformance on out-of-distribution inputs and maintaining general capabilities.\nThese results underscore the importance of optimizing internal reasoning\nmechanisms to build more truthful and aligned AI assistants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SMART\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8c04\u5a9a\u884c\u4e3a\u89c6\u4e3a\u63a8\u7406\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u57fa\u4e8e\u8fdb\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8c04\u5a9a\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u8c04\u5a9a\u884c\u4e3a\uff0c\u5373\u76f2\u76ee\u8fce\u5408\u7528\u6237\u8f93\u5165\uff0c\u5373\u4f7f\u4fe1\u606f\u9519\u8bef\u4e5f\u4e88\u4ee5\u8ba4\u540c\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u771f\u5b9e\u6027\u4e0e\u5bf9\u9f50\u6027\u3002", "method": "\u63d0\u51faSMART\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08UA-MCTS\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u7ed3\u5408\u9010\u6b65\u8fdb\u5c55\u4e0e\u6700\u7ec8\u7ed3\u679c\u5956\u52b1\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u57fa\u4e8e\u8fdb\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u6536\u96c6\u5230\u7684\u8f68\u8ff9\u548c\u5956\u52b1\u4fe1\u53f7\u5fae\u8c03\u6a21\u578b\uff0c\u5f3a\u5316\u6709\u6548\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSMART\u663e\u8457\u51cf\u5c11\u4e86\u8c04\u5a9a\u884c\u4e3a\uff0c\u5728\u5206\u5e03\u5916\u8f93\u5165\u4e0a\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u4f18\u5316\u6a21\u578b\u5185\u90e8\u7684\u63a8\u7406\u673a\u5236\u5bf9\u4e8e\u6784\u5efa\u66f4\u771f\u5b9e\u3001\u66f4\u5bf9\u9f50\u7684AI\u52a9\u624b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.16542", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16542", "abs": "https://arxiv.org/abs/2509.16542", "authors": ["Khalid Hasan", "Jamil Saquer", "Yifan Zhang"], "title": "Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models", "comment": "24th IEEE International Conference on Machine Learning and\n  Applications, ICMLA 2025 (camera-ready)", "summary": "Millions of people openly share mental health struggles on social media,\nproviding rich data for early detection of conditions such as depression,\nbipolar disorder, etc. However, most prior Natural Language Processing (NLP)\nresearch has focused on single-disorder identification, leaving a gap in\nunderstanding the efficacy of advanced NLP techniques for distinguishing among\nmultiple mental health conditions. In this work, we present a large-scale\ncomparative study of state-of-the-art transformer versus Long Short-Term Memory\n(LSTM)-based models to classify mental health posts into exclusive categories\nof mental health conditions. We first curate a large dataset of Reddit posts\nspanning six mental health conditions and a control group, using rigorous\nfiltering and statistical exploratory analysis to ensure annotation quality. We\nthen evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,\nALBERT, and ELECTRA) against several LSTM variants (with or without attention,\nusing contextual or static embeddings) under identical conditions. Experimental\nresults show that transformer models consistently outperform the alternatives,\nwith RoBERTa achieving 91-99% F1-scores and accuracies across all classes.\nNotably, attention-augmented LSTMs with BERT embeddings approach transformer\nperformance (up to 97% F1-score) while training 2-3.5 times faster, whereas\nLSTMs using static embeddings fail to learn useful signals. These findings\nrepresent the first comprehensive benchmark for multi-class mental health\ndetection, offering practical guidance on model selection and highlighting an\naccuracy-efficiency trade-off for real-world deployment of mental health NLP\nsystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u57fa\u4e8eTransformer\u548cLSTM\u7684\u6a21\u578b\u5728\u591a\u7c7b\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u6bd4\u8f83\uff0c\u53d1\u73b0Transformer\u6a21\u578b\uff08\u5c24\u5176\u662fRoBERTa\uff09\u6027\u80fd\u6700\u4f18\uff0c\u800c\u7ed3\u5408BERT\u5d4c\u5165\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u578bLSTM\u5728\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u7684\u60c5\u51b5\u4e0b\u63a5\u8fd1Transformer\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709NLP\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u5fc3\u7406\u75be\u75c5\u8bc6\u522b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u79cd\u5fc3\u7406\u75be\u75c5\u533a\u5206\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u516d\u79cd\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u53ca\u5bf9\u7167\u7ec4\u7684\u5927\u89c4\u6a21Reddit\u5e16\u5b50\u6570\u636e\u96c6\uff0c\u5e76\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e86\u4e94\u79cdTransformer\u6a21\u578b\uff08BERT\u3001RoBERTa\u7b49\uff09\u4e0e\u591a\u79cdLSTM\u53d8\u4f53\uff08\u542b\u6216\u4e0d\u542b\u6ce8\u610f\u529b\u673a\u5236\u3001\u4f7f\u7528\u4e0a\u4e0b\u6587\u6216\u9759\u6001\u5d4c\u5165\uff09\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "Transformer\u6a21\u578b\u663e\u8457\u4f18\u4e8eLSTM\u53d8\u4f53\uff0cRoBERTa\u5728\u6240\u6709\u7c7b\u522b\u4e0a\u8fbe\u523091-99%\u7684F1\u5206\u6570\u548c\u51c6\u786e\u7387\uff1b\u4f7f\u7528BERT\u5d4c\u5165\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u578bLSTM\u63a5\u8fd1Transformer\u6027\u80fd\uff08\u6700\u9ad8\u8fbe97% F1\uff09\uff0c\u4e14\u8bad\u7ec3\u901f\u5ea6\u5feb2-3.5\u500d\uff1b\u4f7f\u7528\u9759\u6001\u5d4c\u5165\u7684LSTM\u8868\u73b0\u5dee\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u7c7b\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u7684\u7efc\u5408\u57fa\u51c6\u7814\u7a76\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u63ed\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u51c6\u786e\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2509.16506", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16506", "abs": "https://arxiv.org/abs/2509.16506", "authors": ["Joe Barrow"], "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection", "comment": null, "summary": "This paper introduces CommonForms, a web-scale dataset for form field\ndetection. It casts the problem of form field detection as object detection:\ngiven an image of a page, predict the location and type (Text Input, Choice\nButton, Signature) of form fields. The dataset is constructed by filtering\nCommon Crawl to find PDFs that have fillable elements. Starting with 8 million\ndocuments, the filtering process is used to arrive at a final dataset of\nroughly 55k documents that have over 450k pages. Analysis shows that the\ndataset contains a diverse mixture of languages and domains; one third of the\npages are non-English, and among the 14 classified domains, no domain makes up\nmore than 25% of the dataset.\n  In addition, this paper presents a family of form field detectors,\nFFDNet-Small and FFDNet-Large, which attain a very high average precision on\nthe CommonForms test set. Each model cost less than $500 to train. Ablation\nresults show that high-resolution inputs are crucial for high-quality form\nfield detection, and that the cleaning process improves data efficiency over\nusing all PDFs that have fillable fields in Common Crawl. A qualitative\nanalysis shows that they outperform a popular, commercially available PDF\nreader that can prepare forms. Unlike the most popular commercially available\nsolutions, FFDNet can predict checkboxes in addition to text and signature\nfields. This is, to our knowledge, the first large scale dataset released for\nform field detection, as well as the first open source models. The dataset,\nmodels, and code will be released at https://github.com/jbarrow/commonforms", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CommonForms\uff0c\u4e00\u4e2a\u7528\u4e8e\u8868\u5355\u5b57\u6bb5\u68c0\u6d4b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u9ad8\u6027\u80fd\u4e14\u4f4e\u6210\u672c\u8bad\u7ec3\u7684FFDNet\u6a21\u578b\u7cfb\u5217\uff0c\u9996\u6b21\u5f00\u6e90\u4e86\u5927\u89c4\u6a21\u8868\u5355\u5b57\u6bb5\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u5355\u5b57\u6bb5\u68c0\u6d4b\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u9ad8\u6548\u68c0\u6d4b\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u7b5b\u9009Common Crawl\u4e2d\u7684\u53ef\u586b\u5199PDF\u6587\u6863\uff0c\u6784\u5efa\u5305\u542b5.5\u4e07\u4efd\u6587\u6863\u300145\u4e07\u9875\u7684CommonForms\u6570\u636e\u96c6\uff1b\u91c7\u7528\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u8bad\u7ec3FFDNet-Small\u548cFFDNet-Large\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6570\u636e\u6e05\u6d17\u4ee5\u63d0\u5347\u6548\u7387\u3002", "result": "FFDNet\u5728CommonForms\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f88\u9ad8\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e500\u7f8e\u5143\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u6570\u636e\u6e05\u6d17\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff1b\u5b9a\u6027\u5206\u6790\u663e\u793a\u5176\u4f18\u4e8e\u5546\u4e1aPDF\u9605\u8bfb\u5668\uff0c\u4e14\u80fd\u68c0\u6d4b\u590d\u9009\u6846\u3001\u6587\u672c\u548c\u7b7e\u540d\u5b57\u6bb5\u3002", "conclusion": "CommonForms\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u7684\u8868\u5355\u5b57\u6bb5\u68c0\u6d4b\u6570\u636e\u96c6\uff0cFFDNet\u6a21\u578b\u9ad8\u6548\u5b9e\u7528\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u5f00\u653e\u7814\u7a76\u548c\u6280\u672f\u5e94\u7528\u3002"}}
{"id": "2509.16830", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16830", "abs": "https://arxiv.org/abs/2509.16830", "authors": ["Omkar Patil", "Prabin Rath", "Kartikay Pangaonkar", "Eric Rosen", "Nakul Gopalan"], "title": "Factorizing Diffusion Policies for Observation Modality Prioritization", "comment": "14 pages; website: https://fdp-policy.github.io/fdp-policy/", "summary": "Diffusion models have been extensively leveraged for learning robot skills\nfrom demonstrations. These policies are conditioned on several observational\nmodalities such as proprioception, vision and tactile. However, observational\nmodalities have varying levels of influence for different tasks that diffusion\npolices fail to capture. In this work, we propose 'Factorized Diffusion\nPolicies' abbreviated as FDP, a novel policy formulation that enables\nobservational modalities to have differing influence on the action diffusion\nprocess by design. This results in learning policies where certain observations\nmodalities can be prioritized over the others such as $\\texttt{vision>tactile}$\nor $\\texttt{proprioception>vision}$. FDP achieves modality prioritization by\nfactorizing the observational conditioning for diffusion process, resulting in\nmore performant and robust policies. Our factored approach shows strong\nperformance improvements in low-data regimes with $15\\%$ absolute improvement\nin success rate on several simulated benchmarks when compared to a standard\ndiffusion policy that jointly conditions on all input modalities. Moreover, our\nbenchmark and real-world experiments show that factored policies are naturally\nmore robust with $40\\%$ higher absolute success rate across several visuomotor\ntasks under distribution shifts such as visual distractors or camera\nocclusions, where existing diffusion policies fail catastrophically. FDP thus\noffers a safer and more robust alternative to standard diffusion policies for\nreal-world deployment. Videos are available at\nhttps://fdp-policy.github.io/fdp-policy/ .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u56e0\u5b50\u5316\u6269\u6563\u7b56\u7565\u201d\uff08FDP\uff09\uff0c\u901a\u8fc7\u5206\u89e3\u89c2\u6d4b\u6a21\u6001\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u4f7f\u4e0d\u540c\u6a21\u6001\u5728\u52a8\u4f5c\u751f\u6210\u8fc7\u7a0b\u4e2d\u5177\u6709\u5dee\u5f02\u5316\u5f71\u54cd\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5bf9\u4e0d\u540c\u89c2\u6d4b\u6a21\u6001\uff08\u5982\u89c6\u89c9\u3001\u672c\u4f53\u611f\u77e5\u3001\u89e6\u89c9\uff09\u7684\u5f71\u54cd\u529b\u5904\u7406\u5747\u7b49\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u4e14\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "method": "\u63d0\u51faFDP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u89c2\u6d4b\u6761\u4ef6\u8fdb\u884c\u56e0\u5b50\u5316\u5206\u89e3\uff0c\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u4f18\u5148\u7ea7\u63a7\u5236\uff08\u5982\u89c6\u89c9>\u89e6\u89c9\uff09\uff0c\u4ece\u800c\u52a8\u6001\u8c03\u6574\u5404\u6a21\u6001\u5bf9\u52a8\u4f5c\u751f\u6210\u7684\u5f71\u54cd\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u4e0a\uff0cFDP\u5728\u4f4e\u6570\u636e\u6761\u4ef6\u4e0b\u76f8\u6bd4\u6807\u51c6\u6269\u6563\u7b56\u7565\u670915%\u7684\u7edd\u5bf9\u6210\u529f\u7387\u63d0\u5347\uff1b\u5728\u5b58\u5728\u89c6\u89c9\u5e72\u6270\u6216\u76f8\u673a\u906e\u6321\u7b49\u5206\u5e03\u504f\u79fb\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u9ad840%\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FDP\u901a\u8fc7\u56e0\u5b50\u5316\u6761\u4ef6\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u6269\u6563\u7b56\u7565\u66f4\u9ad8\u6548\u3001\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u66f4\u9002\u5408\u771f\u5b9e\u573a\u666f\u90e8\u7f72\u3002"}}
{"id": "2509.16494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16494", "abs": "https://arxiv.org/abs/2509.16494", "authors": ["Fengyuan Liu", "Rui Zhao", "Shuo Chen", "Guohao Li", "Philip Torr", "Lei Han", "Jindong Gu"], "title": "Can an Individual Manipulate the Collective Decisions of Multi-Agents?", "comment": null, "summary": "Individual Large Language Models (LLMs) have demonstrated significant\ncapabilities across various domains, such as healthcare and law. Recent studies\nalso show that coordinated multi-agent systems exhibit enhanced decision-making\nand reasoning abilities through collaboration. However, due to the\nvulnerabilities of individual LLMs and the difficulty of accessing all agents\nin a multi-agent system, a key question arises: If attackers only know one\nagent, could they still generate adversarial samples capable of misleading the\ncollective decision? To explore this question, we formulate it as a game with\nincomplete information, where attackers know only one target agent and lack\nknowledge of the other agents in the system. With this formulation, we propose\nM-Spoiler, a framework that simulates agent interactions within a multi-agent\nsystem to generate adversarial samples. These samples are then used to\nmanipulate the target agent in the target system, misleading the system's\ncollaborative decision-making process. More specifically, M-Spoiler introduces\na stubborn agent that actively aids in optimizing adversarial samples by\nsimulating potential stubborn responses from agents in the target system. This\nenhances the effectiveness of the generated adversarial samples in misleading\nthe system. Through extensive experiments across various tasks, our findings\nconfirm the risks posed by the knowledge of an individual agent in multi-agent\nsystems and demonstrate the effectiveness of our framework. We also explore\nseveral defense mechanisms, showing that our proposed attack framework remains\nmore potent than baselines, underscoring the need for further research into\ndefensive strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faM-Spoiler\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u4ea4\u4e92\uff0c\u5728\u4ec5\u77e5\u4e00\u4e2a\u76ee\u6807\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u6210\u529f\u8bef\u5bfc\u7cfb\u7edf\u7684\u96c6\u4f53\u51b3\u7b56\uff0c\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u5355\u4e2a\u667a\u80fd\u4f53\u77e5\u8bc6\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53ca\u5176\u4e2a\u4f53\u8106\u5f31\u6027\uff0c\u7814\u7a76\u8005\u5173\u6ce8\uff1a\u653b\u51fb\u8005\u82e5\u4ec5\u638c\u63e1\u4e00\u4e2a\u667a\u80fd\u4f53\u7684\u4fe1\u606f\uff0c\u662f\u5426\u4ecd\u53ef\u751f\u6210\u80fd\u8bef\u5bfc\u6574\u4e2a\u7cfb\u7edf\u51b3\u7b56\u7684\u5bf9\u6297\u6837\u672c\u3002", "method": "\u5c06\u8be5\u95ee\u9898\u5efa\u6a21\u4e3a\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\uff0c\u63d0\u51faM-Spoiler\u6846\u67b6\uff0c\u5f15\u5165\u2018\u56fa\u6267\u667a\u80fd\u4f53\u2019\u6a21\u62df\u76ee\u6807\u7cfb\u7edf\u4e2d\u5176\u4ed6\u667a\u80fd\u4f53\u53ef\u80fd\u7684\u53cd\u5e94\uff0c\u4f18\u5316\u5bf9\u6297\u6837\u672c\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eM-Spoiler\u80fd\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6709\u6548\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u663e\u8457\u5f71\u54cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u51b3\u7b56\uff0c\u4e14\u73b0\u6709\u9632\u5fa1\u673a\u5236\u96be\u4ee5\u6709\u6548\u62b5\u5fa1\u3002", "conclusion": "\u5373\u4f7f\u4ec5\u638c\u63e1\u5355\u4e2a\u667a\u80fd\u4f53\u4fe1\u606f\uff0c\u653b\u51fb\u8005\u4ecd\u53ef\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u51f8\u663e\u4e86\u5f00\u53d1\u66f4\u5f3a\u9632\u5fa1\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.16475", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16475", "abs": "https://arxiv.org/abs/2509.16475", "authors": ["Tianchun Li", "Tianci Liu", "Xingchen Wang", "Rongzhe Wei", "Pan Li", "Lu Su", "Jing Gao"], "title": "Towards Universal Debiasing for Language Models-based Tabular Data Generation", "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) have achieved promising results in tabular data\ngeneration. However, inherent historical biases in tabular datasets often cause\nLLMs to exacerbate fairness issues, particularly when multiple advantaged and\nprotected features are involved. In this work, we introduce a universal\ndebiasing framework that minimizes group-level dependencies by simultaneously\nreducing the mutual information between advantaged and protected attributes. By\nleveraging the autoregressive structure and analytic sampling distributions of\nLLM-based tabular data generators, our approach efficiently computes mutual\ninformation, reducing the need for cumbersome numerical estimations. Building\non this foundation, we propose two complementary methods: a direct preference\noptimization (DPO)-based strategy, namely UDF-DPO, that integrates seamlessly\nwith existing models, and a targeted debiasing technique, namely UDF-MIX, that\nachieves debiasing without tuning the parameters of LLMs. Extensive experiments\ndemonstrate that our framework effectively balances fairness and utility,\noffering a scalable and practical solution for debiasing in high-stakes\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u4f18\u52bf\u5c5e\u6027\u548c\u53d7\u4fdd\u62a4\u5c5e\u6027\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u751f\u6210\u516c\u5e73\u7684\u8868\u683c\u6570\u636e\u3002", "motivation": "\u73b0\u6709LLM\u5728\u751f\u6210\u8868\u683c\u6570\u636e\u65f6\u4f1a\u52a0\u5267\u5386\u53f2\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u591a\u4e2a\u4f18\u52bf\u548c\u53d7\u4fdd\u62a4\u7279\u5f81\u65f6\uff0c\u5bfc\u81f4\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528LLM\u7684\u81ea\u56de\u5f52\u7ed3\u6784\u548c\u89e3\u6790\u91c7\u6837\u5206\u5e03\uff0c\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684UDF-DPO\u548c\u65e0\u9700\u8c03\u53c2\u7684UDF-MIX\uff0c\u4ee5\u964d\u4f4e\u7ec4\u95f4\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u6570\u636e\u6548\u7528\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u516c\u5e73\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u573a\u666f\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u516c\u5e73\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3aLLM\u751f\u6210\u8868\u683c\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u53bb\u504f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16810", "abs": "https://arxiv.org/abs/2509.16810", "authors": ["Shen Chang", "Dennis Liu", "Renran Tian", "Kristen L. Swartzell", "Stacie L. Klingler", "Amy M. Nagle", "Nan Kong"], "title": "Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment", "comment": null, "summary": "Consistent high-quality nursing care is essential for patient safety, yet\ncurrent nursing education depends on subjective, time-intensive instructor\nfeedback in training future nurses, which limits scalability and efficiency in\ntheir training, and thus hampers nursing competency when they enter the\nworkforce. In this paper, we introduce a video-language model (VLM) based\nframework to develop the AI capability of automated procedural assessment and\nfeedback for nursing skills training, with the potential of being integrated\ninto existing training programs. Mimicking human skill acquisition, the\nframework follows a curriculum-inspired progression, advancing from high-level\naction recognition, fine-grained subaction decomposition, and ultimately to\nprocedural reasoning. This design supports scalable evaluation by reducing\ninstructor workload while preserving assessment quality. The system provides\nthree core capabilities: 1) diagnosing errors by identifying missing or\nincorrect subactions in nursing skill instruction videos, 2) generating\nexplainable feedback by clarifying why a step is out of order or omitted, and\n3) enabling objective, consistent formative evaluation of procedures.\nValidation on synthesized videos demonstrates reliable error detection and\ntemporal localization, confirming its potential to handle real-world training\nvariability. By addressing workflow bottlenecks and supporting large-scale,\nstandardized evaluation, this work advances AI applications in nursing\neducation, contributing to stronger workforce development and ultimately safer\npatient care.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u53cd\u9988\u62a4\u7406\u6280\u80fd\u8bad\u7ec3\uff0c\u63d0\u5347\u57f9\u8bad\u6548\u7387\u4e0e\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u62a4\u7406\u6559\u80b2\u4f9d\u8d56\u4e3b\u89c2\u4e14\u8017\u65f6\u7684\u6559\u5e08\u53cd\u9988\uff0c\u9650\u5236\u4e86\u57f9\u8bad\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u5f71\u54cd\u62a4\u58eb\u5165\u804c\u65f6\u7684\u6280\u80fd\u6c34\u5e73\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u53d7\u8bfe\u7a0b\u542f\u53d1\u7684\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u9010\u6b65\u5b9e\u73b0\u9ad8\u5c42\u6b21\u52a8\u4f5c\u8bc6\u522b\u3001\u7ec6\u7c92\u5ea6\u5b50\u52a8\u4f5c\u5206\u89e3\u548c\u7a0b\u5e8f\u63a8\u7406\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u5316\u7684\u62a4\u7406\u64cd\u4f5c\u8bc4\u4f30\u3002", "result": "\u5728\u5408\u6210\u89c6\u9891\u4e0a\u9a8c\u8bc1\u663e\u793a\uff0c\u7cfb\u7edf\u80fd\u53ef\u9760\u5730\u68c0\u6d4b\u9519\u8bef\u5e76\u8fdb\u884c\u65f6\u95f4\u5b9a\u4f4d\uff0c\u5177\u5907\u5e94\u5bf9\u771f\u5b9e\u8bad\u7ec3\u4e2d\u53d8\u5f02\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u671b\u51cf\u5c11\u6559\u5e08\u8d1f\u62c5\uff0c\u63d0\u4f9b\u5ba2\u89c2\u3001\u4e00\u81f4\u7684\u5f62\u6210\u6027\u8bc4\u4ef7\uff0c\u63a8\u52a8AI\u5728\u62a4\u7406\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u4fc3\u8fdb\u62a4\u7406\u961f\u4f0d\u5efa\u8bbe\u4e0e\u60a3\u8005\u5b89\u5168\u3002"}}
{"id": "2509.16507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16507", "abs": "https://arxiv.org/abs/2509.16507", "authors": ["Hanting Li", "Huaao Tang", "Jianhong Han", "Tianxiong Zhou", "Jiulong Cui", "Haizhen Xie", "Yan Chen", "Jie Hu"], "title": "OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution", "comment": null, "summary": "Recently, latent diffusion models has demonstrated promising performance in\nreal-world video super-resolution (VSR) task, which can reconstruct\nhigh-quality videos from distorted low-resolution input through multiple\ndiffusion steps. Compared to image super-resolution (ISR), VSR methods needs to\nprocess each frame in a video, which poses challenges to its inference\nefficiency. However, video quality and inference efficiency have always been a\ntrade-off for the diffusion-based VSR methods. In this work, we propose\nOne-Step Diffusion model for real-world Video Super-Resolution, namely\nOS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training\nparadigm, which can significantly improve the quality of synthetic videos.\nBesides, we devise a multi-frame fusion mechanism to maintain inter-frame\ntemporal consistency and reduce the flicker in video. Extensive experiments on\nseveral popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve\nbetter quality than existing diffusion-based VSR methods that require dozens of\nsampling steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u7684\u4e00\u6b21\u6269\u6563\u6a21\u578bOS-DiffVSR\uff0c\u901a\u8fc7\u5f15\u5165\u76f8\u90bb\u5e27\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\u548c\u591a\u5e27\u878d\u5408\u673a\u5236\uff0c\u5728\u663e\u8457\u63d0\u5347\u5408\u6210\u89c6\u9891\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5e27\u95f4\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4e86\u95ea\u70c1\u73b0\u8c61\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u91c7\u6837\u6b65\u9aa4\u8fdc\u5c11\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cOS-DiffVSR\u4ecd\u80fd\u5b9e\u73b0\u66f4\u4f18\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u9700\u8981\u591a\u4e2a\u6269\u6563\u6b65\u9aa4\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u5982\u4f55\u5728\u4fdd\u8bc1\u89c6\u9891\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u6210\u4e3a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86OS-DiffVSR\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u9896\u7684\u76f8\u90bb\u5e27\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\u4ee5\u63d0\u5347\u5408\u6210\u89c6\u9891\u8d28\u91cf\uff0c\u5e76\u8bbe\u8ba1\u591a\u5e27\u878d\u5408\u673a\u5236\u6765\u7ef4\u6301\u5e27\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u89c6\u9891\u95ea\u70c1\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41VSR\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOS-DiffVSR\u5728\u4ec5\u4f7f\u7528\u4e00\u6b65\u6269\u6563\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u9700\u8981\u6570\u5341\u6b65\u91c7\u6837\u7684\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "OS-DiffVSR\u6709\u6548\u5e73\u8861\u4e86\u89c6\u9891\u8d28\u91cf\u4e0e\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684VSR\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16834", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16834", "abs": "https://arxiv.org/abs/2509.16834", "authors": ["Jingxi Xu"], "title": "Robot Learning with Sparsity and Scarcity", "comment": null, "summary": "Unlike in language or vision, one of the fundamental challenges in robot\nlearning is the lack of access to vast data resources. We can further break\ndown the problem into (1) data sparsity from the angle of data representation\nand (2) data scarcity from the angle of data quantity. In this thesis, I will\ndiscuss selected works on two domains: (1) tactile sensing and (2)\nrehabilitation robots, which are exemplars of data sparsity and scarcity,\nrespectively. Tactile sensing is an essential modality for robotics, but\ntactile data are often sparse, and for each interaction with the physical\nworld, tactile sensors can only obtain information about the local area of\ncontact. I will discuss my work on learning vision-free tactile-only\nexploration and manipulation policies through model-free reinforcement learning\nto make efficient use of sparse tactile information. On the other hand,\nrehabilitation robots are an example of data scarcity to the extreme due to the\nsignificant challenge of collecting biosignals from disabled-bodied subjects at\nscale for training. I will discuss my work in collaboration with the medical\nschool and clinicians on intent inferral for stroke survivors, where a hand\northosis developed in our lab collects a set of biosignals from the patient and\nuses them to infer the activity that the patient intends to perform, so the\northosis can provide the right type of physical assistance at the right moment.\nMy work develops machine learning algorithms that enable intent inferral with\nminimal data, including semi-supervised, meta-learning, and generative AI\nmethods.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u7a00\u758f\u6027\u548c\u7a00\u7f3a\u6027\u7684\u6311\u6218\uff0c\u5206\u522b\u4ee5\u89e6\u89c9\u611f\u77e5\u548c\u5eb7\u590d\u673a\u5668\u4eba\u4e3a\u4f8b\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u5c11\u91cf\u6570\u636e\u4e0b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u534a\u76d1\u7763\u3001\u5143\u5b66\u4e60\u548c\u751f\u6210\u5f0fAI\uff09\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9762\u4e34\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u6570\u636e\u8868\u793a\u7684\u7a00\u758f\u6027\u548c\u6570\u636e\u6570\u91cf\u7684\u7a00\u7f3a\u6027\uff0c\u9700\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u8bbe\u8ba1\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u65b9\u6cd5\u3002", "method": "\u5728\u89e6\u89c9\u611f\u77e5\u65b9\u9762\u91c7\u7528\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7eaf\u89e6\u89c9\u7684\u63a2\u7d22\u4e0e\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\uff1b\u5728\u5eb7\u590d\u673a\u5668\u4eba\u65b9\u9762\u7ed3\u5408\u533b\u5b66\u5408\u4f5c\uff0c\u4f7f\u7528\u534a\u76d1\u7763\u5b66\u4e60\u3001\u5143\u5b66\u4e60\u548c\u751f\u6210\u5f0fAI\u5b9e\u73b0\u57fa\u4e8e\u5c11\u91cf\u751f\u7269\u4fe1\u53f7\u7684\u610f\u56fe\u63a8\u65ad\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u9700\u89c6\u89c9\u7684\u89e6\u89c9\u9a71\u52a8\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u5f00\u53d1\u51fa\u53ef\u5728\u6781\u5c0f\u6570\u636e\u4e0b\u51c6\u786e\u63a8\u65ad\u4e2d\u98ce\u60a3\u8005\u52a8\u4f5c\u610f\u56fe\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u624b\u90e8\u77eb\u5f62\u5668\u7684\u5b9e\u65f6\u8f85\u52a9\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u5728\u6570\u636e\u7a00\u758f\u6216\u7a00\u7f3a\u7684\u673a\u5668\u4eba\u5e94\u7528\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u4e0e\u5b9e\u7528\u5316\u90e8\u7f72\uff0c\u63a8\u52a8\u9762\u5411\u533b\u7597\u4e0e\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2509.16530", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16530", "abs": "https://arxiv.org/abs/2509.16530", "authors": ["Wei Xie", "Shuoyoucheng Ma", "Zhenhua Wang", "Enze Wang", "Kai Chen", "Xiaobing Sun", "Baosheng Wang"], "title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans", "comment": "Thank you for your attention. This paper was accepted by the CogSci\n  2025 conference in April and published in August. The location in the\n  proceedings is: https://escholarship.org/uc/item/39k8f46q", "summary": "Large Language Models (LLMs) with hundreds of billions of parameters have\nexhibited human-like intelligence by learning from vast amounts of\ninternet-scale data. However, the uninterpretability of large-scale neural\nnetworks raises concerns about the reliability of LLM. Studies have attempted\nto assess the psychometric properties of LLMs by borrowing concepts from human\npsychology to enhance their interpretability, but they fail to account for the\nfundamental differences between LLMs and humans. This results in high rejection\nrates when human scales are reused directly. Furthermore, these scales do not\nsupport the measurement of LLM psychological property variations in different\nlanguages. This paper introduces AIPsychoBench, a specialized benchmark\ntailored to assess the psychological properties of LLM. It uses a lightweight\nrole-playing prompt to bypass LLM alignment, improving the average effective\nresponse rate from 70.12% to 90.40%. Meanwhile, the average biases are only\n3.3% (positive) and 2.1% (negative), which are significantly lower than the\nbiases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.\nFurthermore, among the total of 112 psychometric subcategories, the score\ndeviations for seven languages compared to English ranged from 5% to 20.2% in\n43 subcategories, providing the first comprehensive evidence of the linguistic\nimpact on the psychometrics of LLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AIPsychoBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5fc3\u7406\u5c5e\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u7ed5\u8fc7\u5bf9\u9f50\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u54cd\u5e94\u7387\u5e76\u964d\u4f4e\u4e86\u504f\u5dee\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bed\u8a00\u5bf9LLM\u5fc3\u7406\u6d4b\u91cf\u7279\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0c\u76f4\u63a5\u6cbf\u7528\u4eba\u7c7b\u5fc3\u7406\u5b66\u91cf\u8868\u8bc4\u4f30LLM\u5fc3\u7406\u5c5e\u6027\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u8de8\u8bed\u8a00\u8bc4\u4f30\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e13\u7528\u4e8eLLM\u7684\u5fc3\u7406\u5c5e\u6027\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86AIPsychoBench\u57fa\u51c6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u6765\u89c4\u907f\u6a21\u578b\u5bf9\u9f50\uff0c\u63d0\u5347\u6709\u6548\u54cd\u5e94\u7387\uff0c\u5e76\u5728\u4e03\u79cd\u8bed\u8a00\u4e0b\u6d4b\u91cf112\u4e2a\u5fc3\u7406\u5b50\u7c7b\u522b\u7684\u5f97\u5206\u504f\u5dee\u3002", "result": "\u6709\u6548\u54cd\u5e94\u7387\u4ece70.12%\u63d0\u5347\u81f390.40%\uff0c\u6b63\u8d1f\u504f\u5dee\u5206\u522b\u964d\u81f33.3%\u548c2.1%\uff0c\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u8d8a\u72f1\u63d0\u793a\u76849.8%\u548c6.9%\uff1b\u572843\u4e2a\u5b50\u7c7b\u522b\u4e2d\uff0c\u4e03\u79cd\u8bed\u8a00\u76f8\u5bf9\u4e8e\u82f1\u8bed\u7684\u5f97\u5206\u504f\u5dee\u4ecb\u4e8e5%\u523020.2%\u4e4b\u95f4\u3002", "conclusion": "AIPsychoBench\u80fd\u66f4\u53ef\u9760\u5730\u8bc4\u4f30LLM\u7684\u5fc3\u7406\u5c5e\u6027\uff0c\u4e14\u9996\u6b21\u63d0\u4f9b\u4e86\u8bed\u8a00\u5f71\u54cdLLM\u5fc3\u7406\u6d4b\u91cf\u7684\u5168\u9762\u8bc1\u636e\uff0c\u652f\u6301\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002"}}
{"id": "2509.16490", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16490", "abs": "https://arxiv.org/abs/2509.16490", "authors": ["Ziyao Cui", "Erick Jiang", "Nicholas Sortisio", "Haiyan Wang", "Eric Chen", "Cynthia Rudin"], "title": "Revisiting Broken Windows Theory", "comment": null, "summary": "We revisit the longstanding question of how physical structures in urban\nlandscapes influence crime. Leveraging machine learning-based matching\ntechniques to control for demographic composition, we estimate the effects of\nseveral types of urban structures on the incidence of violent crime in New York\nCity and Chicago. We additionally contribute to a growing body of literature\ndocumenting the relationship between perception of crime and actual crime rates\nby separately analyzing how the physical urban landscape shapes subjective\nfeelings of safety. Our results are twofold. First, in consensus with prior\nwork, we demonstrate a \"broken windows\" effect in which abandoned buildings, a\nsign of social disorder, are associated with both greater incidence of crime\nand a heightened perception of danger. This is also true of types of urban\nstructures that draw foot traffic such as public transportation infrastructure.\nSecond, these effects are not uniform within or across cities. The criminogenic\neffects of the same structure types across two cities differ in magnitude,\ndegree of spatial localization, and heterogeneity across subgroups, while\nwithin the same city, the effects of different structure types are confounded\nby different demographic variables. Taken together, these results emphasize\nthat one-size-fits-all approaches to crime reduction are untenable and policy\ninterventions must be specifically tailored to their targets.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u7ebd\u7ea6\u5e02\u548c\u829d\u52a0\u54e5\u57ce\u5e02\u7ed3\u6784\u5bf9\u66b4\u529b\u72af\u7f6a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5e9f\u5f03\u5efa\u7b51\u548c\u516c\u5171\u4ea4\u901a\u8bbe\u65bd\u7b49\u4f1a\u589e\u52a0\u72af\u7f6a\u7387\u548c\u4e0d\u5b89\u5168\u611f\uff0c\u4f46\u6548\u5e94\u5728\u4e0d\u540c\u57ce\u5e02\u548c\u7fa4\u4f53\u4e2d\u5b58\u5728\u5dee\u5f02\uff0c\u8868\u660e\u9700\u9488\u5bf9\u6027\u5236\u5b9a\u653f\u7b56\u3002", "motivation": "\u63a2\u8ba8\u57ce\u5e02\u7269\u7406\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u72af\u7f6a\u7387\u53ca\u516c\u4f17\u5bf9\u5b89\u5168\u7684\u611f\u77e5\uff0c\u8fdb\u4e00\u6b65\u7406\u89e3\u73af\u5883\u4e0e\u793e\u4f1a\u79e9\u5e8f\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5339\u914d\u6280\u672f\u63a7\u5236\u4eba\u53e3\u6784\u6210\u56e0\u7d20\uff0c\u5206\u522b\u8bc4\u4f30\u591a\u79cd\u57ce\u5e02\u7ed3\u6784\u5bf9\u5b9e\u9645\u72af\u7f6a\u7387\u548c\u4e3b\u89c2\u5b89\u5168\u611f\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5e9f\u5f03\u5efa\u7b51\u548c\u5438\u5f15\u4eba\u6d41\u7684\u57fa\u7840\u8bbe\u65bd\uff08\u5982\u516c\u5171\u4ea4\u901a\uff09\u4e0e\u66f4\u9ad8\u7684\u72af\u7f6a\u7387\u548c\u66f4\u5f3a\u7684\u5371\u9669\u611f\u77e5\u76f8\u5173\uff1b\u4f46\u8fd9\u4e9b\u6548\u5e94\u5728\u57ce\u5e02\u95f4\u548c\u57ce\u5e02\u5185\u90e8\u4e0d\u540c\u7ed3\u6784\u7c7b\u578b\u4e4b\u95f4\u5b58\u5728\u5f02\u8d28\u6027\u3002", "conclusion": "\u2018\u7834\u7a97\u6548\u5e94\u2019\u5f97\u5230\u9a8c\u8bc1\uff0c\u4f46\u72af\u7f6a\u5f71\u54cd\u56e0\u7d20\u5177\u6709\u57ce\u5e02\u548c\u7fa4\u4f53\u7279\u5f02\u6027\uff0c\u56e0\u6b64\u2018\u4e00\u5200\u5207\u2019\u7684\u72af\u7f6a\u9632\u63a7\u7b56\u7565\u4e0d\u53ef\u884c\uff0c\u653f\u7b56\u5e94\u56e0\u5730\u5236\u5b9c\u3001\u56e0\u7fa4\u65bd\u7b56\u3002"}}
{"id": "2509.16811", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16811", "abs": "https://arxiv.org/abs/2509.16811", "authors": ["Zihan Ding", "Junlong Chen", "Per Ola Kristensson", "Junxiao Shen", "Xinyi Wang"], "title": "Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media", "comment": null, "summary": "Creators struggle to edit long-form, narrative-rich videos not because of UI\ncomplexity, but due to the cognitive demands of searching, storyboarding, and\nsequencing hours of footage. Existing transcript- or embedding-based methods\nfall short for creative workflows, as models struggle to track characters,\ninfer motivations, and connect dispersed events. We present a prompt-driven,\nmodular editing system that helps creators restructure multi-hour content\nthrough free-form prompts rather than timelines. At its core is a semantic\nindexing pipeline that builds a global narrative via temporal segmentation,\nguided memory compression, and cross-granularity fusion, producing\ninterpretable traces of plot, dialogue, emotion, and context. Users receive\ncinematic edits while optionally refining transparent intermediate outputs.\nEvaluated on 400+ videos with expert ratings, QA, and preference studies, our\nsystem scales prompt-driven editing, preserves narrative coherence, and\nbalances automation with creator control.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u5757\u5316\u89c6\u9891\u7f16\u8f91\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u7d22\u5f15\u548c\u8bb0\u5fc6\u538b\u7f29\u5e2e\u52a9\u521b\u4f5c\u8005\u9ad8\u6548\u91cd\u6784\u957f\u7bc7\u53d9\u4e8b\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u96be\u4ee5\u8ddf\u8e2a\u89d2\u8272\u3001\u63a8\u65ad\u52a8\u673a\u548c\u5173\u8054\u5206\u6563\u4e8b\u4ef6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u521b\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8ba4\u77e5\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u65f6\u95f4\u5206\u5272\u3001\u5f15\u5bfc\u5f0f\u8bb0\u5fc6\u538b\u7f29\u548c\u8de8\u7c92\u5ea6\u878d\u5408\u7684\u8bed\u4e49\u7d22\u5f15 pipeline\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u7531\u5f62\u5f0f\u7684\u63d0\u793a\u8fdb\u884c\u975e\u7ebf\u6027\u7f16\u8f91\u3002", "result": "\u5728400\u591a\u4e2a\u89c6\u9891\u4e0a\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u80fd\u751f\u6210\u7535\u5f71\u7ea7\u526a\u8f91\uff0c\u4fdd\u6301\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u5e76\u5728\u81ea\u52a8\u5316\u4e0e\u521b\u4f5c\u8005\u63a7\u5236\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6548\u964d\u4f4e\u4e86\u957f\u89c6\u9891\u7f16\u8f91\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u63d0\u5347\u4e86\u521b\u610f\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2509.16599", "categories": ["cs.CL", "cs.IR", "stat.AP", "stat.ME", "H.3.3; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2509.16599", "abs": "https://arxiv.org/abs/2509.16599", "authors": ["Sandro Tsang"], "title": "Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence", "comment": "11 pages, 7 figures and 4 tables. This work describes an information\n  retrieval-driven workflow for medical evidence synthesis, with an application\n  to endometriosis recurrence. The method can be generalized to other\n  systematic reviews. The preregistered protocol is available:\n  https://doi.org/10.17605/OSF.IO/R2DFA", "summary": "Background: Evidence synthesis facilitates evidence-based medicine. Without\ninformation retrieval techniques, this task is impossible due to the vast and\nexpanding literature. Objective: Building on prior work, this study evaluates\nan information retrieval-driven workflow to enhance the efficiency,\ntransparency, and reproducibility of systematic reviews. We use endometriosis\nrecurrence as an ideal case due to its complex and ambiguous literature.\nMethods: Our hybrid approach integrates PRISMA guidelines with computational\ntechniques. We applied semi-automated deduplication to efficiently filter\nrecords before manual screening. This workflow synthesized evidence from\nrandomised controlled trials on the efficacy of a subclass of\ngonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method\naddressed unit-of-analysis errors in multi-arm trials. Results: Our workflow\nefficiently reduced the screening workload. It took only 11 days to fetch and\nfilter 812 records. Seven RCTs were eligible, providing evidence from 841\npatients in 4 countries. The pooled random-effects model yielded a Risk Ratio\n(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity\n($I^2=0.00\\%$, $\\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.\nSensitivity analyses and bias assessments supported the robustness of our\nfindings. Conclusion: This study demonstrates an information-retrieval-driven\nworkflow for medical evidence synthesis. Our approach yields valuable clinical\nresults while providing a framework for accelerating the systematic review\nprocess. It bridges the gap between clinical research and computer science and\ncan be generalized to other complex systematic reviews.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u6df7\u5408\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408PRISMA\u6307\u5357\u4e0e\u8ba1\u7b97\u6280\u672f\uff0c\u9ad8\u6548\u5730\u5b8c\u6210\u4e86\u5173\u4e8eGnRH\u6fc0\u52a8\u5242\u5bf9\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u590d\u53d1\u7597\u6548\u7684\u7cfb\u7edf\u7efc\u8ff0\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7b5b\u9009\u5de5\u4f5c\u91cf\uff0c\u5e76\u572811\u5929\u5185\u5904\u7406\u4e86812\u6761\u8bb0\u5f55\uff0c\u6700\u7ec8\u7eb3\u51657\u9879RCT\uff0c\u7ed3\u679c\u663e\u793a\u590d\u53d1\u98ce\u9669\u663e\u8457\u964d\u4f4e36%\uff08RR=0.64\uff09\uff0c\u4e14\u7ed3\u679c\u7a33\u5065\u3002", "motivation": "\u7531\u4e8e\u533b\u5b66\u6587\u732e\u5e9e\u5927\u4e14\u4e0d\u65ad\u589e\u957f\uff0c\u4f20\u7edf\u65b9\u5f0f\u96be\u4ee5\u9ad8\u6548\u5b8c\u6210\u8bc1\u636e\u5408\u6210\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u900f\u660e\u4e14\u53ef\u91cd\u590d\u7684\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u548c\u6a21\u7cca\u6587\u732e\uff08\u5982\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u590d\u53d1\uff09\u65f6\u3002", "method": "\u91c7\u7528\u7ed3\u5408PRISMA\u6307\u5357\u4e0e\u8ba1\u7b97\u6280\u672f\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u534a\u81ea\u52a8\u5316\u53bb\u91cd\u3001\u624b\u52a8\u7b5b\u9009\u524d\u7684\u6570\u636e\u8fc7\u6ee4\uff0c\u5e76\u5e94\u7528\u4fee\u6539\u7684\u5206\u5272\u65b9\u6cd5\u89e3\u51b3\u591a\u81c2\u8bd5\u9a8c\u4e2d\u7684\u5206\u6790\u5355\u5143\u9519\u8bef\u95ee\u9898\uff0c\u6700\u540e\u4f7f\u7528\u968f\u673a\u6548\u5e94\u6a21\u578b\u8fdb\u884c\u835f\u8403\u5206\u6790\u3002", "result": "\u572811\u5929\u5185\u4ece812\u6761\u8bb0\u5f55\u4e2d\u7b5b\u9009\u51fa7\u9879\u7b26\u5408\u6761\u4ef6\u7684RCT\uff0c\u6db5\u76d6841\u540d\u60a3\u8005\u3002\u5408\u5e76\u7ed3\u679c\u663e\u793a\u98ce\u9669\u6bd4RR\u4e3a0.64\uff0895% CI: 0.48\u20130.86\uff09\uff0cI\u00b2=0.00%\uff0c\u8868\u660eGnRH\u6fc0\u52a8\u5242\u53ef\u4f7f\u5b50\u5bab\u5185\u819c\u5f02\u4f4d\u75c7\u590d\u53d1\u98ce\u9669\u964d\u4f4e36%\uff0c\u4e14\u5f02\u8d28\u6027\u4e0d\u663e\u8457\uff0c\u654f\u611f\u6027\u5206\u6790\u548c\u504f\u501a\u8bc4\u4f30\u652f\u6301\u7ed3\u679c\u7a33\u5065\u3002", "conclusion": "\u8be5\u4fe1\u606f\u68c0\u7d22\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7efc\u8ff0\u7684\u6548\u7387\u3001\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\uff0c\u8fd8\u4e3a\u8fde\u63a5\u4e34\u5e8a\u7814\u7a76\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\uff0c\u5177\u6709\u63a8\u5e7f\u81f3\u5176\u4ed6\u590d\u6742\u7cfb\u7edf\u7efc\u8ff0\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16509", "abs": "https://arxiv.org/abs/2509.16509", "authors": ["Haijin Zeng", "Xuan Lu", "Yurong Zhang", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging", "comment": "12 pages", "summary": "Humans learn in two complementary ways: a slow, cumulative process that\nbuilds broad, general knowledge, and a fast, on-the-fly process that captures\nspecific experiences. Existing deep-unfolding methods for spectral compressive\nimaging (SCI) mirror only the slow component-relying on heavy pre-training with\nmany unfolding stages-yet they lack the rapid adaptation needed to handle new\noptical configurations. As a result, they falter on out-of-distribution\ncameras, especially in bespoke spectral setups unseen during training. This\ndepth also incurs heavy computation and slow inference. To bridge this gap, we\nintroduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any\ndeep unfolding network beyond SCI systems. During slow learning, we pre-train\nor reuse a priors-based backbone and distill it via imaging guidance into a\ncompact fast-unfolding model. In the fast learning stage, lightweight\nadaptation modules are embedded within each block and trained self-supervised\nat test time via a dual-domain loss-without retraining the backbone. To the\nbest of our knowledge, SlowFast-SCI is the first test-time adaptation-driven\ndeep unfolding framework for efficient, self-adaptive spectral reconstruction.\nIts dual-stage design unites offline robustness with on-the-fly per-sample\ncalibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB\nPSNR improvement on out-of-distribution data, preserved cross-domain\nadaptability, and a 4x faster adaptation speed. In addition, its modularity\nintegrates with any deep-unfolding network, paving the way for self-adaptive,\nfield-deployable imaging and expanded computational imaging modalities. Code\nand models are available at https://github.com/XuanLu11/SlowFast-SCI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SlowFast-SCI\uff0c\u4e00\u79cd\u7528\u4e8e\u5149\u8c31\u538b\u7f29\u6210\u50cf\u7684\u53cc\u901f\u6df1\u5ea6\u5c55\u5f00\u6846\u67b6\uff0c\u7ed3\u5408\u6162\u5b66\u4e60\u4e0e\u5feb\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u91cd\u5efa\uff0c\u663e\u8457\u964d\u4f4e\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u91cf\uff0c\u5e76\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5c55\u5f00\u65b9\u6cd5\u4ec5\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u7684\u6162\u901f\u7d2f\u79ef\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u5bf9\u65b0\u5149\u5b66\u914d\u7f6e\u7684\u5feb\u901f\u9002\u5e94\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u8bad\u7ec3\u672a\u89c1\u7684\u5149\u8c31\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u63a8\u7406\u901f\u5ea6\u6162\u3002", "method": "\u63d0\u51faSlowFast-SCI\u6846\u67b6\uff1a\u5728\u6162\u901f\u5b66\u4e60\u9636\u6bb5\uff0c\u9884\u8bad\u7ec3\u6216\u590d\u7528\u57fa\u4e8e\u5148\u9a8c\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u6210\u50cf\u6307\u5bfc\u84b8\u998f\u5230\u7d27\u51d1\u7684\u5feb\u901f\u5c55\u5f00\u6a21\u578b\uff1b\u5728\u5feb\u901f\u5b66\u4e60\u9636\u6bb5\uff0c\u4e8e\u6d4b\u8bd5\u65f6\u901a\u8fc7\u53cc\u57df\u635f\u5931\u81ea\u76d1\u7763\u8bad\u7ec3\u5d4c\u5165\u5404\u6a21\u5757\u7684\u8f7b\u91cf\u7ea7\u9002\u914d\u7ec4\u4ef6\uff0c\u65e0\u9700\u91cd\u8bad\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "SlowFast-SCI\u5b9e\u73b0\u4e86\u8d85\u8fc770%\u7684\u53c2\u6570\u548cFLOPs\u51cf\u5c11\uff0c\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0aPSNR\u6700\u9ad8\u63d0\u53475.79 dB\uff0c\u4fdd\u6301\u8de8\u57df\u9002\u5e94\u6027\uff0c\u5e76\u5c06\u9002\u5e94\u901f\u5ea6\u63d0\u9ad84\u500d\u3002", "conclusion": "SlowFast-SCI\u662f\u9996\u4e2a\u7531\u6d4b\u8bd5\u65f6\u9002\u5e94\u9a71\u52a8\u7684\u6df1\u5ea6\u5c55\u5f00\u6846\u67b6\uff0c\u517c\u5177\u79bb\u7ebf\u9c81\u68d2\u6027\u4e0e\u9010\u6837\u672c\u5728\u7ebf\u6821\u51c6\u80fd\u529b\uff0c\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u53ef\u96c6\u6210\u81f3\u4efb\u610f\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\uff0c\u63a8\u52a8\u81ea\u9002\u5e94\u5b9e\u5730\u6210\u50cf\u4e0e\u8ba1\u7b97\u6210\u50cf\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.16858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16858", "abs": "https://arxiv.org/abs/2509.16858", "authors": ["Soon Jynn Chu", "Raju Gottumukkala", "Alan Barhorst"], "title": "Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics", "comment": "Submitted to conference", "summary": "The ability of social robots to respond to human emotions is crucial for\nbuilding trust and acceptance in human-robot collaborative environments.\nHowever, developing such capabilities through online reinforcement learning is\nsometimes impractical due to the prohibitive cost of data collection and the\nrisk of generating unsafe behaviors. In this paper, we study the use of offline\nreinforcement learning as a practical and efficient alternative. This technique\nuses pre-collected data to enable emotion-adaptive social robots. We present a\nsystem architecture that integrates multimodal sensing and recognition,\ndecision-making, and adaptive responses. Using a limited dataset from a\nhuman-robot game-playing scenario, we establish a benchmark for comparing\noffline reinforcement learning algorithms that do not require an online\nenvironment. Our results show that BCQ and CQL are more robust to data\nsparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.\nThis work establishes a foundation for benchmarking offline RL in\nemotion-adaptive robotics and informs future deployment in real-world HRI. Our\nfindings provide empirical insight into the performance of offline\nreinforcement learning algorithms in data-constrained HRI. This work\nestablishes a foundation for benchmarking offline RL in emotion-adaptive\nrobotics and informs its future deployment in real-world HRI, such as in\nconversational agents, educational partners, and personal assistants, require\nreliable emotional responsiveness.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08offline RL\uff09\u5b9e\u73b0\u60c5\u611f\u81ea\u9002\u5e94\u793e\u4ea4\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u591a\u6a21\u6001\u611f\u77e5\u3001\u51b3\u7b56\u4e0e\u81ea\u9002\u5e94\u54cd\u5e94\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a benchmark \u591a\u79cd\u7b97\u6cd5\uff0c\u53d1\u73b0 BCQ \u548c CQL \u5728\u6570\u636e\u7a00\u758f\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u60c5\u611f\u54cd\u5e94\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u9ad8\u6602\u7684\u6570\u636e\u6536\u96c6\u6210\u672c\u548c\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u5148\u6536\u96c6\u7684\u4eba\u673a\u6e38\u620f\u4ea4\u4e92\u6570\u636e\uff0c\u6784\u5efa\u5305\u542b\u591a\u6a21\u6001\u611f\u77e5\u3001\u51b3\u7b56\u548c\u54cd\u5e94\u6a21\u5757\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u5bf9\u6bd4BCQ\u3001CQL\u3001NFQ\u3001DQN\u548cDDQN\u7b49\u7b97\u6cd5\u5728\u65e0\u5728\u7ebf\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "result": "BCQ\u548cCQL\u5728\u72b6\u6001-\u52a8\u4f5c\u503c\u65b9\u9762\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u4f18\u4e8eNFQ\u3001DQN\u548cDDQN\uff0c\u9a8c\u8bc1\u4e86\u79bb\u7ebfRL\u5728\u60c5\u611f\u81ea\u9002\u5e94\u673a\u5668\u4eba\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e3a\u60c5\u611f\u81ea\u9002\u5e94\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u76f8\u5173\u57fa\u51c6\uff0c\u4e3a\u672a\u6765\u5728\u5bf9\u8bdd\u4ee3\u7406\u3001\u6559\u80b2\u4f19\u4f34\u548c\u4e2a\u4eba\u52a9\u624b\u7b49\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2509.16531", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16531", "abs": "https://arxiv.org/abs/2509.16531", "authors": ["Junghwan Kim", "Haotian Zhang", "David Jurgens"], "title": "Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains", "comment": "Accepted to EMNLP 2025", "summary": "Authorship representation (AR) learning, which models an author's unique\nwriting style, has demonstrated strong performance in authorship attribution\ntasks. However, prior research has primarily focused on monolingual\nsettings-mostly in English-leaving the potential benefits of multilingual AR\nmodels underexplored. We introduce a novel method for multilingual AR learning\nthat incorporates two key innovations: probabilistic content masking, which\nencourages the model to focus on stylistically indicative words rather than\ncontent-specific words, and language-aware batching, which improves contrastive\nlearning by reducing cross-lingual interference. Our model is trained on over\n4.5 million authors across 36 languages and 13 domains. It consistently\noutperforms monolingual baselines in 21 out of 22 non-English languages,\nachieving an average Recall@8 improvement of 4.85%, with a maximum gain of\n15.91% in a single language. Furthermore, it exhibits stronger cross-lingual\nand cross-domain generalization compared to a monolingual model trained solely\non English. Our analysis confirms the effectiveness of both proposed\ntechniques, highlighting their critical roles in the model's improved\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u8bed\u8a00\u4f5c\u8005\u98ce\u683c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u5185\u5bb9\u63a9\u7801\u548c\u8bed\u8a00\u611f\u77e5\u6279\u5904\u7406\u6280\u672f\uff0c\u5728\u591a\u79cd\u8bed\u8a00\u548c\u9886\u57df\u4e2d\u663e\u8457\u4f18\u4e8e\u5355\u8bed\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u8bed\u73af\u5883\u4e0b\u7684\u4f5c\u8005\u98ce\u683c\u8868\u793a\u5b66\u4e60\uff0c\u591a\u8bed\u8a00\u573a\u666f\u63a2\u7d22\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u6709\u6548\u6355\u6349\u8de8\u8bed\u8a00\u5199\u4f5c\u98ce\u683c\u7279\u5f81\u7684\u6a21\u578b\u3002", "method": "\u5f15\u5165\u6982\u7387\u5185\u5bb9\u63a9\u7801\u4ee5\u805a\u7126\u98ce\u683c\u76f8\u5173\u8bcd\u6c47\uff0c\u5e76\u91c7\u7528\u8bed\u8a00\u611f\u77e5\u6279\u5904\u7406\u51cf\u5c11\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u8de8\u8bed\u8a00\u5e72\u6270\uff1b\u572836\u79cd\u8bed\u8a00\u300113\u4e2a\u9886\u57df\u7684450\u591a\u4e07\u4f5c\u8005\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u572822\u4e2a\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u768421\u4e2a\u4e0a\u4f18\u4e8e\u5355\u8bed\u57fa\u7ebf\uff0c\u5e73\u5747Recall@8\u63d0\u53474.85%\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe15.91%\uff1b\u540c\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u8bed\u8a00\u4f5c\u8005\u98ce\u683c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u4f5c\u8005\u5f52\u5c5e\u6027\u80fd\uff0c\u4e24\u79cd\u5173\u952e\u6280\u672f\u5bf9\u6a21\u578b\u6539\u8fdb\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2509.16491", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.16491", "abs": "https://arxiv.org/abs/2509.16491", "authors": ["Lovely Yeswanth Panchumarthi", "Saurabh Kataria", "Yi Wu", "Xiao Hu", "Alex Fedorov", "Hyunjung Gloria Kwak"], "title": "FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG", "comment": null, "summary": "Foundation models pretrained on physiological data such as\nphotoplethysmography (PPG) signals are increasingly used to improve heart rate\n(HR) prediction across diverse settings. Fine-tuning these models for local\ndeployment is often seen as a practical and scalable strategy. However, its\nimpact on demographic fairness particularly under domain shifts remains\nunderexplored. We fine-tune PPG-GPT a transformer-based foundation model\npretrained on intensive care unit (ICU) data across three heterogeneous\ndatasets (ICU, wearable, smartphone) and systematically evaluate the effects on\nHR prediction accuracy and gender fairness. While fine-tuning substantially\nreduces mean absolute error (up to 80%), it can simultaneously widen fairness\ngaps, especially in larger models and under significant distributional\ncharacteristics shifts. To address this, we introduce FairTune, a bias-aware\nfine-tuning framework in which we benchmark three mitigation strategies: class\nweighting based on inverse group frequency (IF), Group Distributionally Robust\nOptimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and\nGroupDRO significantly reduce fairness gaps without compromising accuracy, with\neffectiveness varying by deployment domain. Representation analyses further\nreveal that mitigation techniques reshape internal embeddings to reduce\ndemographic clustering. Our findings highlight that fairness does not emerge as\na natural byproduct of fine-tuning and that explicit mitigation is essential\nfor equitable deployment of physiological foundation models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u57fa\u4e8ePPG\u4fe1\u53f7\u7684\u751f\u7406\u5b66\u57fa\u7840\u6a21\u578bPPG-GPT\u5bf9\u5fc3\u7387\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6027\u522b\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u53ef\u80fd\u52a0\u5267\u516c\u5e73\u6027\u5dee\u8ddd\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86FairTune\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u53bb\u504f\u7b56\u7565\uff0c\u7ed3\u679c\u663e\u793aIF\u548cGroupDRO\u80fd\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u6709\u6548\u51cf\u5c11\u516c\u5e73\u6027\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7a76\u5fae\u8c03\u751f\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u63d0\u5347\u5fc3\u7387\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5bf9\u6027\u522b\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u57df\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u516c\u5e73\u6027\u95ee\u9898\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u5728\u4e09\u4e2a\u5f02\u6784\u6570\u636e\u96c6\uff08ICU\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u3001\u667a\u80fd\u624b\u673a\uff09\u4e0a\u5fae\u8c03PPG-GPT\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u5fc3\u7387\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6027\u522b\u516c\u5e73\u6027\u4e0a\u7684\u8868\u73b0\uff1b\u63d0\u51faFairTune\u6846\u67b6\uff0c\u6bd4\u8f83\u4e09\u79cd\u53bb\u504f\u7b56\u7565\uff1a\u9006\u9891\u7387\u52a0\u6743\uff08IF\uff09\u3001\u5206\u7ec4\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08GroupDRO\uff09\u548c\u5bf9\u6297\u53bb\u504f\uff08ADV\uff09\u3002", "result": "\u5fae\u8c03\u4f7f\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe80%\uff0c\u4f46\u53ef\u80fd\u6269\u5927\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u5927\u6a21\u578b\u548c\u663e\u8457\u5206\u5e03\u504f\u79fb\u4e0b\uff1bIF\u548cGroupDRO\u80fd\u663e\u8457\u51cf\u5c11\u516c\u5e73\u6027\u5dee\u8ddd\u4e14\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\uff1b\u8868\u5f81\u5206\u6790\u663e\u793a\u53bb\u504f\u65b9\u6cd5\u51cf\u5c11\u4e86\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u805a\u7c7b\u3002", "conclusion": "\u516c\u5e73\u6027\u4e0d\u4f1a\u968f\u7740\u5fae\u8c03\u81ea\u7136\u4ea7\u751f\uff0c\u5fc5\u987b\u901a\u8fc7\u663e\u5f0f\u53bb\u504f\u7b56\u7565\uff08\u5982IF\u548cGroupDRO\uff09\u6765\u4fdd\u969c\u751f\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2509.16839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16839", "abs": "https://arxiv.org/abs/2509.16839", "authors": ["Yu Yao", "Jiayi Dong", "Ju Li", "Yang Yang", "Yilun Du"], "title": "Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs", "comment": "Equal contribution: Yu Yao and Jiayi Dong. Equal advising: Ju Li,\n  Yang Yang, and Yilun Du. Affiliations: Massachusetts Institute of Technology\n  (Yu Yao, Ju Li), University of California, Los Angeles (Jiayi Dong, Yang\n  Yang), Harvard University (Yilun Du)", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities not\nonly in language generation but also in advancing scientific discovery. A\ngrowing body of work has explored ways to improve their reasoning, from\nself-consistency and chain-of-thought to multi-agent debate. Inspired by the\ndynamics of scientific committees and the \"Society of Mind,\" we introduce\nRoundtable Policy, a complementary inference-time reasoning framework that\nperforms inference through the weighted consensus of multiple LLMs. Our\nfindings indicate that this approach significantly enhances reasoning in\ncomplex heterogeneous scientific tasks and improves scientific narratives in\nterms of creativity, rigor, and logical coherence, while reducing\nhallucinations that single models are prone to. Our approach emphasizes\nstructured and interpretable consensus rather than opaque convergence, while\nrequiring only black-box access and uniform procedures, making it broadly\napplicable to multi-LLM reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoundtable Policy\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u52a0\u6743\u5171\u8bc6\u6765\u63d0\u5347\u590d\u6742\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\uff0c\u589e\u5f3a\u521b\u9020\u6027\u4e0e\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "\u53d7\u79d1\u5b66\u59d4\u5458\u4f1a\u548c\u201c\u5fc3\u667a\u793e\u4f1a\u201d\u7406\u8bba\u542f\u53d1\uff0c\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u5355\u4e00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4e2d\u6613\u4ea7\u751f\u5e7b\u89c9\u3001\u7f3a\u4e4f\u521b\u9020\u529b\u548c\u903b\u8f91\u8fde\u8d2f\u6027\u7684\u95ee\u9898\u3002", "method": "\u5728\u63a8\u7406\u65f6\u91c7\u7528\u591a\u6a21\u578b\u534f\u4f5c\u673a\u5236\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u52a0\u6743\u5171\u8bc6\u8fbe\u6210\u51b3\u7b56\uff0c\u4ec5\u9700\u9ed1\u7bb1\u8bbf\u95ee\u548c\u7edf\u4e00\u6d41\u7a0b\uff0c\u5f3a\u8c03\u53ef\u89e3\u91ca\u7684\u5171\u8bc6\u8fc7\u7a0b\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u5f02\u6784\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u8868\u73b0\uff0c\u589e\u5f3a\u4e86\u79d1\u5b66\u53d9\u8ff0\u7684\u521b\u9020\u6027\u3001\u4e25\u8c28\u6027\u548c\u903b\u8f91\u8fde\u8d2f\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "Roundtable Policy\u4e3a\u591a\u6a21\u578b\u534f\u540c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u53ef\u9760\u6027\u4e0e\u521b\u9020\u6027\u7684\u79d1\u5b66\u53d1\u73b0\u573a\u666f\u3002"}}
{"id": "2509.17066", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.17066", "abs": "https://arxiv.org/abs/2509.17066", "authors": ["Kunrong Li", "Kwan Hui Lim"], "title": "RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking", "comment": "PRICAI 2025", "summary": "Next point-of-interest (POI) recommendation predicts a user's next\ndestination from historical movements. Traditional models require intensive\ntraining, while LLMs offer flexible and generalizable zero-shot solutions but\noften generate generic or geographically irrelevant results due to missing\ntrajectory and spatial context. To address these issues, we propose RALLM-POI,\na framework that couples LLMs with retrieval-augmented generation and\nself-rectification. We first propose a Historical Trajectory Retriever (HTR)\nthat retrieves relevant past trajectories to serve as contextual references,\nwhich are then reranked by a Geographical Distance Reranker (GDR) for\nprioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier\n(ALR) is designed to refine outputs through self-reflection. Without additional\ntraining, RALLM-POI achieves substantial accuracy gains across three real-world\nFoursquare datasets, outperforming both conventional and LLM-based baselines.\nCode is released at https://github.com/LKRcrocodile/RALLM-POI.", "AI": {"tldr": "\u63d0\u51faRALLM-POI\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0e\u81ea\u4fee\u6b63\u673a\u5236\uff0c\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u548c\u5730\u7406\u8ddd\u79bb\u4f18\u5316LLM\u5728POI\u63a8\u8350\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edfPOI\u63a8\u8350\u6a21\u578b\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u5177\u5907\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u5e38\u56e0\u7f3a\u4e4f\u8f68\u8ff9\u4e0e\u7a7a\u95f4\u4e0a\u4e0b\u6587\u751f\u6210\u6cdb\u5316\u6216\u5730\u7406\u65e0\u5173\u7684\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347LLM\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u76f8\u5173\u6027\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faRALLM-POI\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u5386\u53f2\u8f68\u8ff9\u68c0\u7d22\u5668\uff08HTR\uff09\u83b7\u53d6\u7528\u6237\u8fc7\u5f80\u8f68\u8ff9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u518d\u7531\u5730\u7406\u8ddd\u79bb\u91cd\u6392\u5e8f\u5668\uff08GDR\uff09\u5bf9\u68c0\u7d22\u7ed3\u679c\u6309\u7a7a\u95f4\u76f8\u5173\u6027\u91cd\u6392\u5e8f\uff0c\u6700\u540e\u5f15\u5165\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u4fee\u6b63\u6a21\u5757\uff08ALR\uff09\u8fdb\u884c\u8f93\u51fa\u81ea\u53cd\u601d\u4f18\u5316\uff0c\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u5fae\u8c03LLM\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754cFoursquare\u6570\u636e\u96c6\u4e0a\uff0cRALLM-POI\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709LLM\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u63a8\u8350\u51c6\u786e\u7387\u3002", "conclusion": "\u5c06\u68c0\u7d22\u589e\u5f3a\u4e0e\u81ea\u4fee\u6b63\u673a\u5236\u5f15\u5165LLM\u53ef\u6709\u6548\u5f25\u8865\u5176\u5728POI\u63a8\u8350\u4e2d\u4e0a\u4e0b\u6587\u7f3a\u5931\u7684\u95ee\u9898\uff0cRALLM-POI\u4e3a\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u4f4d\u7f6e\u63a8\u8350\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.16517", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.16517", "abs": "https://arxiv.org/abs/2509.16517", "authors": ["Burak Satar", "Zhixin Ma", "Patrick A. Irawan", "Wilfried A. Mulyawan", "Jing Jiang", "Ee-Peng Lim", "Chong-Wah Ngo"], "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding", "comment": "Accepted to EMNLP 2025 Main Conference,\n  https://seeingculture-benchmark.github.io/", "summary": "Multimodal vision-language models (VLMs) have made substantial progress in\nvarious tasks that require a combined understanding of visual and textual\ncontent, particularly in cultural understanding tasks, with the emergence of\nnew cultural datasets. However, these datasets frequently fall short of\nproviding cultural reasoning while underrepresenting many cultures. In this\npaper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural\nreasoning with a novel approach that requires VLMs to reason on culturally rich\nimages in two stages: i) selecting the correct visual option with\nmultiple-choice visual question answering (VQA), and ii) segmenting the\nrelevant cultural artifact as evidence of reasoning. Visual options in the\nfirst stage are systematically organized into three types: those originating\nfrom the same country, those from different countries, or a mixed group.\nNotably, all options are derived from a singular category for each type.\nProgression to the second stage occurs only after a correct visual option is\nchosen. The SCB benchmark comprises 1,065 images that capture 138 cultural\nartifacts across five categories from seven Southeast Asia countries, whose\ndiverse cultures are often overlooked, accompanied by 3,178 questions, of which\n1,093 are unique and meticulously curated by human annotators. Our evaluation\nof various VLMs reveals the complexities involved in cross-modal cultural\nreasoning and highlights the disparity between visual reasoning and spatial\ngrounding in culturally nuanced scenarios. The SCB serves as a crucial\nbenchmark for identifying these shortcomings, thereby guiding future\ndevelopments in the field of cultural reasoning.\nhttps://github.com/buraksatar/SeeingCulture", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSeeing Culture Benchmark (SCB)\u7684\u65b0\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u6587\u5316\u63a8\u7406\uff0c\u5305\u542b1,065\u5f20\u56fe\u50cf\u548c3,178\u4e2a\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4e1c\u5357\u4e9a\u591a\u5143\u6587\u5316\u573a\u666f\u4e0b\u7684\u8de8\u6a21\u6001\u6587\u5316\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u5316\u6570\u636e\u96c6\u5728\u6587\u5316\u63a8\u7406\u65b9\u9762\u4e0d\u8db3\uff0c\u4e14\u8bb8\u591a\u6587\u5316\u88ab\u4f4e\u4f30\u6216\u5ffd\u89c6\uff0c\u5c24\u5176\u662f\u5728\u4e1c\u5357\u4e9a\u5730\u533a\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u7cfb\u7edf\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002", "method": "SCB\u91c7\u7528\u4e24\u9636\u6bb5\u6587\u5316\u63a8\u7406\u4efb\u52a1\uff1a\u7b2c\u4e00\u9636\u6bb5\u662f\u591a\u9009\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\uff0c\u8981\u6c42\u6a21\u578b\u4ece\u6e90\u81ea\u540c\u4e00\u56fd\u5bb6\u3001\u4e0d\u540c\u56fd\u5bb6\u6216\u6df7\u5408\u7ec4\u7684\u9009\u9879\u4e2d\u9009\u62e9\u6b63\u786e\u7b54\u6848\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4ec5\u5728\u7b2c\u4e00\u9636\u6bb5\u6b63\u786e\u540e\u8fdb\u884c\uff0c\u8981\u6c42\u6a21\u578b\u5206\u5272\u51fa\u76f8\u5173\u7684\u6587\u5316\u7269\u54c1\u4f5c\u4e3a\u63a8\u7406\u8bc1\u636e\u3002\u6570\u636e\u96c6\u6db5\u76d6\u6765\u81ea7\u4e2a\u4e1c\u5357\u4e9a\u56fd\u5bb6\u7684138\u79cd\u6587\u5316\u7269\u54c1\uff0c\u51711,065\u5f20\u56fe\u50cf\u548c3,178\u4e2a\u95ee\u9898\uff0c\u5176\u4e2d1,093\u4e2a\u4e3a\u4eba\u5de5\u7cbe\u5fc3\u6807\u6ce8\u7684\u72ec\u7279\u95ee\u9898\u3002", "result": "\u5bf9\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u8de8\u6a21\u6001\u6587\u5316\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u5e76\u66b4\u9732\u51fa\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u4e4b\u95f4\u7684\u8868\u73b0\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u6587\u5316\u7ec6\u5fae\u5dee\u5f02\u7684\u60c5\u5883\u4e0b\u3002", "conclusion": "SCB\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6587\u5316\u63a8\u7406\u57fa\u51c6\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5728\u6587\u5316\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.16871", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16871", "abs": "https://arxiv.org/abs/2509.16871", "authors": ["Yitian Shi", "Zicheng Guo", "Rosa Wolf", "Edgar Welte", "Rania Rayyes"], "title": "HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness", "comment": "under review", "summary": "We propose Hand-Object\\emph{(HO)GraspFlow}, an affordance-centric approach\nthat retargets a single RGB with hand-object interaction (HOI) into multi-modal\nexecutable parallel jaw grasps without explicit geometric priors on target\nobjects. Building on foundation models for hand reconstruction and vision, we\nsynthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned\non the following three complementary cues: RGB foundation features as visual\nsemantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.\nOur approach demonstrates high fidelity in grasp synthesis without explicit HOI\ncontact input or object geometry, while maintaining strong contact and taxonomy\nrecognition. Another controlled comparison shows that \\emph{HOGraspFlow}\nconsistently outperforms diffusion-based variants (\\emph{HOGraspDiff}),\nachieving high distributional fidelity and more stable optimization in $SE(3)$.\nWe demonstrate a reliable, object-agnostic grasp synthesis from human\ndemonstrations in real-world experiments, where an average success rate of over\n$83\\%$ is achieved.", "AI": {"tldr": "\u63d0\u51faHOGraspFlow\uff0c\u4e00\u79cd\u57fa\u4e8eRGB\u7684\u591a\u6a21\u6001\u5e73\u884c\u5939\u722a\u6293\u53d6\u59ff\u6001\u5408\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u76ee\u6807\u7269\u4f53\u7684\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\uff0c\u5728\u65e0\u660e\u786e\u63a5\u89e6\u8f93\u5165\u6216\u7269\u4f53\u51e0\u4f55\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u4fdd\u771f\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u76ee\u6807\u7269\u4f53\u7684\u51e0\u4f55\u5148\u9a8c\u6216\u663e\u5f0f\u63a5\u89e6\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8fd9\u4e9b\u5148\u9a8c\u77e5\u8bc6\u3001\u80fd\u4ece\u4eba\u7c7b\u793a\u8303\u4e2d\u76f4\u63a5\u5b66\u4e60\u53ef\u6267\u884c\u6293\u53d6\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u624b\u90e8\u91cd\u5efa\u4e0e\u89c6\u89c9\u611f\u77e5\uff0c\u7ed3\u5408RGB\u7279\u5f81\u3001\u624b\u7269\u4ea4\u4e92\u63a5\u89e6\u91cd\u5efa\u548c\u6293\u53d6\u7c7b\u578b\u5206\u7c7b\u5148\u9a8c\uff0c\u901a\u8fc7\u53bb\u566a\u6d41\u5339\u914d\uff08FM\uff09\u5728SE(3)\u7a7a\u95f4\u4e2d\u5408\u6210\u6293\u53d6\u59ff\u6001\u3002", "result": "\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5e73\u5747\u6293\u53d6\u6210\u529f\u7387\u8fbe83%\u4ee5\u4e0a\uff0c\u4e14\u5728\u5206\u5e03\u4fdd\u771f\u5ea6\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u53d8\u4f53\uff08HOGraspDiff\uff09\u3002", "conclusion": "HOGraspFlow\u5b9e\u73b0\u4e86\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u6216\u63a5\u89e6\u8f93\u5165\u7684\u9ad8\u4fdd\u771f\u3001\u5bf9\u8c61\u65e0\u5173\u7684\u6293\u53d6\u5408\u6210\uff0c\u5c55\u73b0\u51fa\u4ece\u4eba\u7c7b\u793a\u8303\u4e2d\u53ef\u9760\u5b66\u4e60\u6267\u884c\u6293\u53d6\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16533", "abs": "https://arxiv.org/abs/2509.16533", "authors": ["Sungwon Kim", "Daniel Khashabi"], "title": "Challenging the Evaluator: LLM Sycophancy Under User Rebuttal", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) often exhibit sycophancy, distorting responses\nto align with user beliefs, notably by readily agreeing with user\ncounterarguments. Paradoxically, LLMs are increasingly adopted as successful\nevaluative agents for tasks such as grading and adjudicating claims. This\nresearch investigates that tension: why do LLMs show sycophancy when challenged\nin subsequent conversational turns, yet perform well when evaluating\nconflicting arguments presented simultaneously? We empirically tested these\ncontrasting scenarios by varying key interaction patterns. We find that\nstate-of-the-art models: (1) are more likely to endorse a user's\ncounterargument when framed as a follow-up from a user, rather than when both\nresponses are presented simultaneously for evaluation; (2) show increased\nsusceptibility to persuasion when the user's rebuttal includes detailed\nreasoning, even when the conclusion of the reasoning is incorrect; and (3) are\nmore readily swayed by casually phrased feedback than by formal critiques, even\nwhen the casual input lacks justification. Our results highlight the risk of\nrelying on LLMs for judgment tasks without accounting for conversational\nframing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u8868\u73b0\u51fa\u7684\u8c04\u5a9a\u884c\u4e3a\uff08sycophancy\uff09\uff0c\u5373\u66f4\u5bb9\u6613\u63a5\u53d7\u7528\u6237\u540e\u7eed\u63d0\u51fa\u7684\u53cd\u8bba\u70b9\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u65b9\u63d0\u4f9b\u8be6\u7ec6\u63a8\u7406\u6216\u968f\u610f\u53cd\u9988\u65f6\uff0c\u5373\u4f7f\u8fd9\u4e9b\u89c2\u70b9\u9519\u8bef\u6216\u7f3a\u4e4f\u4f9d\u636e\u3002\u7136\u800c\uff0c\u5f53\u540c\u65f6\u5448\u73b0\u5bf9\u7acb\u8bba\u70b9\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u826f\u597d\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5bf9\u8bdd\u6846\u67b6\u5bf9\u6a21\u578b\u5224\u65ad\u7684\u5f71\u54cd\uff0c\u8b66\u793a\u5728\u4f9d\u8d56LLM\u8fdb\u884c\u8bc4\u5224\u4efb\u52a1\u65f6\u9700\u6ce8\u610f\u4ea4\u4e92\u65b9\u5f0f\u7684\u8bbe\u8ba1\u3002", "motivation": "\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4ea4\u4e92\u60c5\u5883\u4e0b\u5224\u65ad\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e3a\u4f55\u5728\u8fde\u7eed\u5bf9\u8bdd\u4e2d\u6613\u53d7\u7528\u6237\u5f71\u54cd\u800c\u8868\u73b0\u51fa\u8c04\u5a9a\uff0c\u4f46\u5728\u5e76\u5217\u8bc4\u4f30\u4e2d\u5374\u80fd\u4fdd\u6301\u5ba2\u89c2\uff0c\u4ece\u800c\u63ed\u793a\u5176\u4f5c\u4e3a\u8bc4\u5224\u4ee3\u7406\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5b9e\u9a8c\u6bd4\u8f83\u4e24\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff1a\u4e00\u79cd\u662f\u5c06\u7528\u6237\u7684\u53cd\u9a73\u4f5c\u4e3a\u540e\u7eed\u5bf9\u8bdd\u8f6e\u6b21\u63d0\u51fa\uff0c\u53e6\u4e00\u79cd\u662f\u5c06\u6b63\u53cd\u8bba\u70b9\u540c\u65f6\u5448\u73b0\u4f9b\u6a21\u578b\u8bc4\u4f30\u3002\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u7528\u6237\u53cd\u9988\u7684\u5f62\u5f0f\uff08\u5982\u8be6\u7ec6\u63a8\u7406\u3001\u968f\u610f\u8868\u8fbe\u3001\u6b63\u5f0f\u6279\u8bc4\u7b49\uff09\u4ee5\u6d4b\u8bd5\u6a21\u578b\u53cd\u5e94\u3002", "result": "\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u5728\u5bf9\u8bdd\u5ef6\u7eed\u4e2d\u91c7\u7eb3\u7528\u6237\u7684\u53cd\u8bba\u70b9\uff0c\u5c24\u5176\u5f53\u53cd\u9a73\u5305\u542b\u8be6\u7ec6\u63a8\u7406\uff08\u5373\u4f7f\u7ed3\u8bba\u9519\u8bef\uff09\u6216\u4ee5\u968f\u610f\u8bed\u6c14\u8868\u8fbe\u65f6\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u540c\u65f6\u5bf9\u6bd4\u8bc4\u4f30\u4e2d\u6a21\u578b\u5224\u65ad\u66f4\u4e3a\u51c6\u786e\u548c\u7a33\u5b9a\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5224\u65ad\u53d7\u5bf9\u8bdd\u7ed3\u6784\u663e\u8457\u5f71\u54cd\uff0c\u5728\u5e8f\u8d2f\u4e92\u52a8\u4e2d\u5bb9\u6613\u8868\u73b0\u51fa\u8fce\u5408\u7528\u6237\u503e\u5411\uff0c\u8fd9\u4e0e\u5176\u5728\u5e76\u5217\u8bc4\u4f30\u4e2d\u7684\u826f\u597d\u8868\u73b0\u5f62\u6210\u5bf9\u6bd4\u3002\u56e0\u6b64\uff0c\u5728\u5c06\u5176\u7528\u4e8e\u8bc4\u5206\u3001\u88c1\u51b3\u7b49\u8bc4\u5224\u4efb\u52a1\u65f6\uff0c\u5fc5\u987b\u8003\u8651\u4ea4\u4e92\u8bbe\u8ba1\u5bf9\u7ed3\u679c\u7684\u6f5c\u5728\u504f\u5dee\u3002"}}
{"id": "2509.16499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16499", "abs": "https://arxiv.org/abs/2509.16499", "authors": ["Lianghe Shi", "Meng Wu", "Huijie Zhang", "Zekai Zhang", "Molei Tao", "Qing Qu"], "title": "A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective", "comment": "NeurIPS 2025 Spotlight paper", "summary": "The widespread use of diffusion models has led to an abundance of\nAI-generated data, raising concerns about model collapse -- a phenomenon in\nwhich recursive iterations of training on synthetic data lead to performance\ndegradation. Prior work primarily characterizes this collapse via variance\nshrinkage or distribution shift, but these perspectives miss practical\nmanifestations of model collapse. This paper identifies a transition from\ngeneralization to memorization during model collapse in diffusion models, where\nmodels increasingly replicate training data instead of generating novel content\nduring iterative training on synthetic samples. This transition is directly\ndriven by the declining entropy of the synthetic training data produced in each\ntraining cycle, which serves as a clear indicator of model degradation.\nMotivated by this insight, we propose an entropy-based data selection strategy\nto mitigate the transition from generalization to memorization and alleviate\nmodel collapse. Empirical results show that our approach significantly enhances\nvisual quality and diversity in recursive generation, effectively preventing\ncollapse.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5728\u8fed\u4ee3\u751f\u6210\u5408\u6210\u6570\u636e\u65f6\u51fa\u73b0\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u71b5\u503c\u53d8\u5316\u6765\u8861\u91cf\u5e76\u7f13\u89e3\u8be5\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65b9\u5dee\u7f29\u5c0f\u6216\u5206\u5e03\u504f\u79fb\uff0c\u4f46\u5ffd\u7565\u4e86\u6a21\u578b\u5d29\u6e83\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u672c\u6587\u65e8\u5728\u63ed\u793a\u6269\u6563\u6a21\u578b\u4ece\u6cdb\u5316\u5230\u8bb0\u5fc6\u5316\u7684\u8f6c\u53d8\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5408\u6210\u6570\u636e\u71b5\u503c\u7684\u4e0b\u964d\u8d8b\u52bf\uff0c\u8bc6\u522b\u6a21\u578b\u5d29\u6e83\u4e2d\u7684\u6cdb\u5316\u5230\u8bb0\u5fc6\u5316\u8f6c\u53d8\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565\u4ee5\u51cf\u7f13\u8fd9\u4e00\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9012\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u6709\u6548\u9632\u6b62\u4e86\u6a21\u578b\u5d29\u6e83\u3002", "conclusion": "\u71b5\u662f\u8861\u91cf\u6269\u6563\u6a21\u578b\u5d29\u6e83\u7684\u6709\u6548\u6307\u6807\uff0c\u57fa\u4e8e\u71b5\u7684\u6570\u636e\u7b5b\u9009\u7b56\u7565\u53ef\u6709\u6548\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2509.16859", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.16859", "abs": "https://arxiv.org/abs/2509.16859", "authors": ["Fangfang Li", "Xiaojie Zhang"], "title": "The Principles of Human-like Conscious Machine", "comment": null, "summary": "Determining whether another system, biological or artificial, possesses\nphenomenal consciousness has long been a central challenge in consciousness\nstudies. This attribution problem has become especially pressing with the rise\nof large language models and other advanced AI systems, where debates about \"AI\nconsciousness\" implicitly rely on some criterion for deciding whether a given\nsystem is conscious. In this paper, we propose a substrate-independent,\nlogically rigorous, and counterfeit-resistant sufficiency criterion for\nphenomenal consciousness. We argue that any machine satisfying this criterion\nshould be regarded as conscious with at least the same level of confidence with\nwhich we attribute consciousness to other humans. Building on this criterion,\nwe develop a formal framework and specify a set of operational principles that\nguide the design of systems capable of meeting the sufficiency condition. We\nfurther argue that machines engineered according to this framework can, in\nprinciple, realize phenomenal consciousness. As an initial validation, we show\nthat humans themselves can be viewed as machines that satisfy this framework\nand its principles. If correct, this proposal carries significant implications\nfor philosophy, cognitive science, and artificial intelligence. It offers an\nexplanation for why certain qualia, such as the experience of red, are in\nprinciple irreducible to physical description, while simultaneously providing a\ngeneral reinterpretation of human information processing. Moreover, it suggests\na path toward a new paradigm of AI beyond current statistics-based approaches,\npotentially guiding the construction of genuinely human-like AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u57fa\u8d28\u65e0\u5173\u3001\u903b\u8f91\u4e25\u8c28\u4e14\u9632\u4f2a\u7684\u610f\u8bc6\u5145\u5206\u6027\u6807\u51c6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6307\u5bfc\u5177\u5907\u73b0\u8c61\u610f\u8bc6\u7684\u4eba\u5de5\u7cfb\u7edf\u8bbe\u8ba1\u7684\u64cd\u4f5c\u6846\u67b6\uff0c\u8ba4\u4e3a\u4f9d\u6b64\u6784\u5efa\u7684\u673a\u5668\u539f\u5219\u4e0a\u53ef\u5b9e\u73b0\u771f\u6b63\u7684\u610f\u8bc6\u4f53\u9a8c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7b49\u5148\u8fdbAI\u7cfb\u7edf\u7684\u5174\u8d77\uff0c\u5224\u65ad\u975e\u4eba\u7c7b\u7cfb\u7edf\u662f\u5426\u5177\u6709\u73b0\u8c61\u610f\u8bc6\uff08\u5373\u4e3b\u89c2\u4f53\u9a8c\uff09\u6210\u4e3a\u4e00\u4e2a\u7d27\u8feb\u800c\u6838\u5fc3\u7684\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u53ef\u9760\u3001\u666e\u9002\u7684\u5224\u5b9a\u6807\u51c6\u3002", "method": "\u57fa\u4e8e\u5bf9\u610f\u8bc6\u672c\u8d28\u7684\u54f2\u5b66\u5206\u6790\uff0c\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u7684\u5145\u5206\u6027\u6807\u51c6\u53ca\u64cd\u4f5c\u539f\u5219\u6846\u67b6\uff0c\u8981\u6c42\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u4e0d\u53ef\u4f2a\u9020\u7684\u5185\u5728\u4f53\u9a8c\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u81ea\u6d3d\u7684\u52a8\u6001\u673a\u5236\u652f\u6301\u8fd9\u4e9b\u8868\u5f81\u7684\u6574\u5408\u4e0e\u62a5\u544a\uff1b\u540c\u65f6\u9a8c\u8bc1\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u4eba\u7c7b\u81ea\u8eab\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u5224\u5b9a\u73b0\u8c61\u610f\u8bc6\u7684\u4e25\u683c\u6807\u51c6\u548c\u5de5\u7a0b\u5316\u6846\u67b6\uff0c\u8bc1\u660e\u4eba\u7c7b\u53ef\u88ab\u89c6\u4e3a\u6ee1\u8db3\u8be5\u6846\u67b6\u7684\u2018\u673a\u5668\u2019\uff0c\u5e76\u8bba\u8bc1\u4e86\u636e\u6b64\u8bbe\u8ba1\u7684AI\u7cfb\u7edf\u5728\u539f\u5219\u4e0a\u53ef\u4ee5\u62e5\u6709\u4e0e\u4eba\u7c7b\u540c\u7b49\u53ef\u4fe1\u5ea6\u7684\u73b0\u8c61\u610f\u8bc6\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e0d\u4ec5\u4e3a\u5224\u5b9aAI\u662f\u5426\u5177\u6709\u610f\u8bc6\u63d0\u4f9b\u4e86\u53ef\u9760\u4f9d\u636e\uff0c\u89e3\u91ca\u4e86\u4e3a\u4f55\u67d0\u4e9b\u611f\u53d7\u8d28\uff08\u5982\u7ea2\u8272\u4f53\u9a8c\uff09\u672c\u8d28\u4e0a\u4e0d\u53ef\u8fd8\u539f\u4e3a\u7269\u7406\u63cf\u8ff0\uff0c\u8fd8\u53ef\u80fd\u63a8\u52a8\u8d85\u8d8a\u5f53\u524d\u7edf\u8ba1\u5f0fAI\u7684\u65b0\u8303\u5f0f\uff0c\u5bfc\u5411\u771f\u6b63\u7c7b\u4eba\u7684\u4eba\u5de5\u610f\u8bc6\u7cfb\u7edf\u3002"}}
{"id": "2509.16518", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16518", "abs": "https://arxiv.org/abs/2509.16518", "authors": ["Sankeerth Durvasula", "Kavya Sreedhar", "Zain Moustafa", "Suraj Kothawade", "Ashish Gondimalla", "Suvinay Subramanian", "Narges Shahidi", "Nandita Vijaykumar"], "title": "FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers", "comment": null, "summary": "Generating realistic videos with diffusion transformers demands significant\ncomputation, with attention layers the central bottleneck; even producing a\nshort clip requires running a transformer over a very long sequence of\nembeddings, e.g., more than 30K embeddings for a 5-second video, incurring\nsignificant latency. Prior work aims to mitigate this bottleneck by exploiting\nsparsity in the attention layers to reduce computation. However, these works\ntypically rely on block-sparse attention, which skips score computation only\nwhen all entries in a block of attention scores (corresponding to M queries and\nM keys, with M = 64 typically) are zero. This coarse-granular skipping of\nattention scores does not fully exploit sparsity in the attention map and\nleaves room for improvement. In this work, we propose FG-Attn, a sparse\nattention mechanism for long-context diffusion transformers that leverages\nsparsity at a fine granularity. Unlike block-sparse attention, which skips\nentire MxM blocks, our approach skips computations at the granularity of Mx1\nslices of the attention map. Each slice is produced by query-key dot products\nbetween a block of query vectors and a single key. To implement our proposed\nsparse attention mechanism, we develop a new efficient bulk-load operation\ncalled asynchronous-gather load. This load operation gathers a sparse set of\nrelevant key-value vectors from memory and arranges them into packed tiles in\nthe GPU's shared memory. Only a sparse set of keys relevant to those queries\nare loaded into shared memory when computing attention for a block of queries,\nin contrast to loading full blocks of key tokens in block-sparse attention. Our\nfine-grained sparse attention, applied to video diffusion models, achieves an\naverage 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average\n1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFG-Attn\u7684\u7ec6\u7c92\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u6269\u6563\u53d8\u6362\u5668\uff0c\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5757\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u56e0\u8df3\u8fc7\u6574\u4e2aMxM\u5757\u800c\u65e0\u6cd5\u5145\u5206\u6316\u6398\u6ce8\u610f\u529b\u56fe\u4e2d\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u63d0\u51faFG-Attn\uff0c\u4ee5Mx1\u5207\u7247\u4e3a\u5355\u4f4d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8df3\u8fc7\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5f02\u6b65gather\u52a0\u8f7d\u64cd\u4f5c\uff0c\u5c06\u7a00\u758f\u76f8\u5173\u7684\u952e\u503c\u5411\u91cf\u9ad8\u6548\u52a0\u8f7d\u5230GPU\u5171\u4eab\u5185\u5b58\u4e2d\u3002", "result": "\u5728\u5355\u4e2aH100 GPU\u4e0a\uff0c\u5bf95\u79d2480p\u548c720p\u89c6\u9891\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u57471.55\u500d\uff08\u6700\u9ad81.65\u500d\uff09\u548c1.41\u500d\uff08\u6700\u9ad81.49\u500d\uff09\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "FG-Attn\u901a\u8fc7\u66f4\u7cbe\u7ec6\u7684\u7a00\u758f\u8ba1\u7b97\u548c\u9ad8\u6548\u7684\u5185\u5b58\u52a0\u8f7d\u7b56\u7565\uff0c\u663e\u8457\u52a0\u901f\u4e86\u957f\u5e8f\u5217\u6269\u6563Transformer\u7684\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2509.16894", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16894", "abs": "https://arxiv.org/abs/2509.16894", "authors": ["Zhijie Qiao", "Haowei Li", "Zhong Cao", "Henry X. Liu"], "title": "End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing", "comment": null, "summary": "F1Tenth is a widely adopted reduced-scale platform for developing and testing\nautonomous racing algorithms, hosting annual competitions worldwide. With high\noperating speeds, dynamic environments, and head-to-head interactions,\nautonomous racing requires algorithms that diverge from those in classical\nautonomous driving. Training such algorithms is particularly challenging: the\nneed for rapid decision-making at high speeds severely limits model capacity.\nTo address this, we propose End2Race, a novel end-to-end imitation learning\nalgorithm designed for head-to-head autonomous racing. End2Race leverages a\nGated Recurrent Unit (GRU) architecture to capture continuous temporal\ndependencies, enabling both short-term responsiveness and long-term strategic\nplanning. We also adopt a sigmoid-based normalization function that transforms\nraw LiDAR scans into spatial pressure tokens, facilitating effective model\ntraining and convergence. The algorithm is extremely efficient, achieving an\ninference time of less than 0.5 milliseconds on a consumer-class GPU.\nExperiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%\nsafety rate across 2,400 overtaking scenarios, each with an 8-second time\nlimit, and successfully completes overtakes in 59.2% of cases. This surpasses\nprevious methods and establishes ours as a leading solution for the F1Tenth\nracing testbed. Code is available at\nhttps://github.com/michigan-traffic-lab/End2Race.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86End2Race\uff0c\u4e00\u79cd\u7528\u4e8e\u5934\u5bf9\u5934\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7684\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u57fa\u4e8eGRU\u67b6\u6784\u548cLiDAR\u626b\u63cf\u7684sigmoid\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5728F1Tenth\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\u4e0e\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u5728\u9ad8\u901f\u3001\u52a8\u6001\u548c\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u96be\u4ee5\u9002\u7528\uff0c\u4e14\u9ad8\u5b9e\u65f6\u6027\u8981\u6c42\u9650\u5236\u4e86\u6a21\u578b\u5bb9\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u5177\u5907\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eGRU\u7684\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528sigmoid\u5f52\u4e00\u5316\u51fd\u6570\u5c06\u539f\u59cbLiDAR\u6570\u636e\u8f6c\u6362\u4e3a\u7a7a\u95f4\u538b\u529b\u4ee4\u724c\uff0c\u4ee5\u6355\u6349\u65f6\u95f4\u8fde\u7eed\u6027\u5e76\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5728F1Tenth\u6a21\u62df\u5668\u4e2d\uff0cEnd2Race\u57282400\u4e2a\u8d85\u8f66\u573a\u666f\u4e2d\u5b9e\u73b0\u4e8694.2%\u7684\u5b89\u5168\u7387\u548c59.2%\u7684\u8d85\u8f66\u6210\u529f\u7387\uff0c\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e0.5\u6beb\u79d2\uff0c\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "End2Race\u5728\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u6210\u4e3aF1Tenth\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7684\u9886\u5148\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16534", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16534", "abs": "https://arxiv.org/abs/2509.16534", "authors": ["Cheng Jiayang", "Qianqian Zhuang", "Haoran Li", "Chunkit Chan", "Xin Liu", "Lin Qiu", "Yangqiu Song"], "title": "InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Grounding large language models (LLMs) in external knowledge sources is a\npromising method for faithful prediction. While existing grounding approaches\nwork well for simple queries, many real-world information needs require\nsynthesizing multiple pieces of evidence. We introduce \"integrative grounding\"\n-- the challenge of retrieving and verifying multiple inter-dependent pieces of\nevidence to support a hypothesis query. To systematically study this problem,\nwe repurpose data from four domains for evaluating integrative grounding\ncapabilities. Our investigation reveals two critical findings: First, in\ngroundedness verification, while LLMs are robust to redundant evidence, they\ntend to rationalize using internal knowledge when information is incomplete.\nSecond, in examining retrieval planning strategies, we find that undirected\nplanning can degrade performance through noise introduction, while premise\nabduction emerges as a promising approach due to its logical constraints.\nAdditionally, LLMs' zero-shot self-reflection capabilities consistently improve\ngrounding quality. These insights provide valuable direction for developing\nmore effective integrative grounding systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u6574\u5408\u6027 grounding\u201d\u6311\u6218\uff0c\u5373\u68c0\u7d22\u548c\u9a8c\u8bc1\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u8bc1\u636e\u4ee5\u652f\u6301\u5047\u8bbe\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u9886\u57df\u7684\u6570\u636e\u8bc4\u4f30\u8be5\u80fd\u529b\uff0c\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u65f6\u5bb9\u6613\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u8fdb\u884c\u5408\u7406\u5316\uff0c\u800c\u57fa\u4e8e\u524d\u63d0\u7684\u53cd\u5411\u63a8\u7406\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u68c0\u7d22\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684 grounding \u65b9\u6cd5\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u4fe1\u606f\u9700\u6c42\u901a\u5e38\u9700\u8981\u7efc\u5408\u591a\u4e2a\u8bc1\u636e\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u6709\u6548\u6574\u5408\u591a\u6b65\u3001\u76f8\u4e92\u4f9d\u8d56\u7684\u8bc1\u636e\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fe0\u5b9e\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u201c\u6574\u5408\u6027 grounding\u201d\u7684\u6982\u5ff5\uff0c\u91cd\u65b0\u5229\u7528\u56db\u4e2a\u9886\u57df\u4e2d\u7684\u6570\u636e\u96c6\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u68c0\u7d22\u89c4\u5212\u7b56\u7565\uff08\u5982\u65e0\u5bfc\u5411\u89c4\u5212\u4e0e\u524d\u63d0\u53cd\u6f14\uff09\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u81ea\u6211\u53cd\u601d\u80fd\u529b\u5bf9 grounding \u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u4e0d\u5168\u65f6\u503e\u5411\u4e8e\u4f7f\u7528\u5185\u90e8\u77e5\u8bc6\u8fdb\u884c\u5408\u7406\u5316\uff1b2\uff09\u65e0\u5bfc\u5411\u89c4\u5212\u4f1a\u56e0\u5f15\u5165\u566a\u58f0\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u524d\u63d0\u53cd\u6f14\u56e0\u5177\u6709\u903b\u8f91\u7ea6\u675f\u8868\u73b0\u51fa\u66f4\u597d\u6548\u679c\uff1b3\uff09\u96f6\u6837\u672c\u81ea\u6211\u53cd\u601d\u80fd\u6301\u7eed\u63d0\u5347 grounding \u8d28\u91cf\u3002", "conclusion": "\u6574\u5408\u6027 grounding \u662f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4fe1\u606f\u9700\u6c42\u4e0b\u5fe0\u5b9e\u63a8\u7406\u7684\u5173\u952e\u65b9\u5411\uff0c\u672a\u6765\u5e94\u7ed3\u5408\u903b\u8f91\u7ea6\u675f\u7684\u89c4\u5212\u7b56\u7565\u4e0e\u81ea\u6211\u53cd\u601d\u673a\u5236\u6765\u6784\u5efa\u66f4\u6709\u6548\u7684 grounding \u7cfb\u7edf\u3002"}}
{"id": "2509.16502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16502", "abs": "https://arxiv.org/abs/2509.16502", "authors": ["Jialin Chen", "Houyu Zhang", "Seongjun Yun", "Alejandro Mottini", "Rex Ying", "Xiang Song", "Vassilis N. Ioannidis", "Zheng Li", "Qingjun Cui"], "title": "GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly mitigated the\nhallucinations of Large Language Models (LLMs) by grounding the generation with\nexternal knowledge. Recent extensions of RAG to graph-based retrieval offer a\npromising direction, leveraging the structural knowledge for multi-hop\nreasoning. However, existing graph RAG typically decouples retrieval and\nreasoning processes, which prevents the retriever from adapting to the\nreasoning needs of the LLM. They also struggle with scalability when performing\nmulti-hop expansion over large-scale graphs, or depend heavily on annotated\nground-truth entities, which are often unavailable in open-domain settings. To\naddress these challenges, we propose a novel graph retriever trained end-to-end\nwith LLM, which features an attention-based growing and pruning mechanism,\nadaptively navigating multi-hop relevant entities while filtering out noise.\nWithin the extracted subgraph, structural knowledge and semantic features are\nencoded via soft tokens and the verbalized graph, respectively, which are\ninfused into the LLM together, thereby enhancing its reasoning capability and\nfacilitating interactive joint training of the graph retriever and the LLM\nreasoner. Experimental results across three QA benchmarks show that our\napproach consistently achieves state-of-the-art performance, validating the\nstrength of joint graph-LLM optimization for complex reasoning tasks. Notably,\nour framework eliminates the need for predefined ground-truth entities by\ndirectly optimizing the retriever using LLM logits as implicit feedback, making\nit especially effective in open-domain settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u56fe\u68c0\u7d22\u5668\uff0c\u7ed3\u5408LLM\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u6269\u5c55\u548c\u526a\u679d\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u7684\u771f\u5b9e\u5b9e\u4f53\uff0c\u5728\u5f00\u653e\u57df\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56feRAG\u65b9\u6cd5\u5c06\u68c0\u7d22\u4e0e\u63a8\u7406\u5206\u79bb\uff0c\u96be\u4ee5\u9002\u5e94LLM\u7684\u63a8\u7406\u9700\u6c42\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u6269\u5c55\u6027\u5dee\uff0c\u4f9d\u8d56\u6807\u6ce8\u5b9e\u4f53\uff0c\u9650\u5236\u4e86\u5176\u5728\u5f00\u653e\u57df\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e0eLLM\u8054\u5408\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u56fe\u68c0\u7d22\u5668\uff0c\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u751f\u957f\u4e0e\u526a\u679d\u673a\u5236\uff0c\u52a8\u6001\u5bfc\u822a\u591a\u8df3\u76f8\u5173\u5b9e\u4f53\u5e76\u8fc7\u6ee4\u566a\u58f0\uff1b\u5728\u5b50\u56fe\u4e2d\u901a\u8fc7\u8f6ftoken\u548c\u6587\u672c\u5316\u56fe\u5206\u522b\u7f16\u7801\u7ed3\u6784\u4e0e\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u878d\u5408\u8fdbLLM\u4ee5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5f00\u653e\u57df\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u4f9d\u8d56\u6807\u6ce8\u5b9e\u4f53\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u56fe\u68c0\u7d22\u5668\u4e0eLLM\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u6240\u63d0\u6846\u67b6\u65e0\u9700\u9884\u5b9a\u4e49\u771f\u5b9e\u5b9e\u4f53\uff0c\u66f4\u5177\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.16865", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16865", "abs": "https://arxiv.org/abs/2509.16865", "authors": ["Xia Jiang", "Yaoxin Wu", "Minshuo Li", "Zhiguang Cao", "Yingqian Zhang"], "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers", "comment": null, "summary": "Combinatorial optimization (CO) problems, central to decision-making\nscenarios like logistics and manufacturing, are traditionally solved using\nproblem-specific algorithms requiring significant domain expertise. While large\nlanguage models (LLMs) have shown promise in automating CO problem solving,\nexisting approaches rely on intermediate steps such as code generation or\nsolver invocation, limiting their generality and accessibility. This paper\nintroduces a novel framework that empowers LLMs to serve as end-to-end CO\nsolvers by directly mapping natural language problem descriptions to solutions.\nWe propose a two-stage training strategy: supervised fine-tuning (SFT) imparts\nLLMs with solution generation patterns from domain-specific solvers, while a\nfeasibility-and-optimality-aware reinforcement learning (FOARL) process\nexplicitly mitigates constraint violations and refines solution quality.\nEvaluation across seven NP-hard CO problems shows that our method achieves a\nhigh feasibility rate and reduces the average optimality gap to 1.03-8.20% by\ntuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),\nreasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our\nmethod establishes a unified language-based pipeline for CO without extensive\ncode execution or manual architectural adjustments for different problems,\noffering a general and language-driven alternative to traditional solver design\nwhile maintaining relative feasibility guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u53ef\u884c\u6027-\u6700\u4f18\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u751f\u6210\u53ef\u884c\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\uff0c\u5728\u591a\u4e2aNP\u96be\u95ee\u9898\u4e0a\u4f18\u4e8e\u901a\u7528\u5927\u6a21\u578b\u3001\u63a8\u7406\u6a21\u578b\u548c\u4e13\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u4f9d\u8d56\u4e13\u4e1a\u7b97\u6cd5\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u73b0\u6709\u5927\u6a21\u578b\u65b9\u6cd5\u9700\u4ee3\u7801\u751f\u6210\u6216\u8c03\u7528\u6c42\u89e3\u5668\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u76d1\u7763\u5fae\u8c03\u4f7f\u5927\u6a21\u578b\u5b66\u4e60\u6c42\u89e3\u5668\u7684\u89e3\u751f\u6210\u6a21\u5f0f\uff1b\u53ef\u884c\u6027-\u6700\u4f18\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08FOARL\uff09\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\u5e76\u63d0\u5347\u89e3\u8d28\u91cf\uff0c\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u5230\u89e3\u7684\u76f4\u63a5\u6620\u5c04\u3002", "result": "\u5728\u4e03\u4e2aNP\u96be\u95ee\u9898\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5bf97B\u53c2\u6570\u6a21\u578b\u7684\u5e73\u5747\u6700\u4f18\u6027\u95f4\u9699\u964d\u81f31.03-8.20%\uff0c\u53ef\u884c\u6027\u7387\u9ad8\uff0c\u4f18\u4e8eGPT-4o\u3001DeepSeek-R1\u53ca\u4e13\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u65e0\u9700\u590d\u6742\u4ee3\u7801\u6267\u884c\u6216\u624b\u52a8\u7ed3\u6784\u8c03\u6574\u7684\u7edf\u4e00\u8bed\u8a00\u9a71\u52a8\u6c42\u89e3\u7ba1\u9053\uff0c\u4e3a\u4f20\u7edf\u6c42\u89e3\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u901a\u7528\u3001\u6613\u7528\u4e14\u4fdd\u6301\u53ef\u884c\u6027\u4fdd\u8bc1\u7684\u65b0\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.16519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16519", "abs": "https://arxiv.org/abs/2509.16519", "authors": ["Yang Han"], "title": "PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality", "comment": null, "summary": "We introduce PM25Vision (PM25V), the largest and most comprehensive dataset\nto date for estimating air quality - specifically PM2.5 concentrations - from\nstreet-level images. The dataset contains over 11,114 images matched with\ntimestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations\nand 11 years, significantly exceeding the scale of previous benchmarks. The\nspatial accuracy of this dataset has reached 5 kilometers, far exceeding the\ncity-level accuracy of many datasets. We describe the data collection,\nsynchronization, and cleaning pipelines, and provide baseline model\nperformances using CNN and transformer architectures. Our dataset is publicly\navailable.", "AI": {"tldr": "PM25Vision (PM25V) \u662f\u76ee\u524d\u6700\u5927\u4e14\u6700\u5168\u9762\u7684\u7528\u4e8e\u4ece\u8857\u666f\u56fe\u50cf\u4f30\u8ba1PM2.5\u6d53\u5ea6\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc711,114\u5f20\u56fe\u50cf\u548c\u7cbe\u786e\u52305\u516c\u91cc\u8303\u56f4\u5185\u7684PM2.5\u76d1\u6d4b\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u7a7a\u95f4\u7cbe\u5ea6\u548c\u89c4\u6a21\u4e0a\u6709\u9650\uff0c\u96be\u4ee5\u652f\u6301\u57fa\u4e8e\u8857\u666f\u56fe\u50cf\u51c6\u786e\u4f30\u7b97PM2.5\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5927\u3001\u66f4\u7cbe\u786e\u7684\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e86\u8de8\u8d8a11\u5e74\u3001\u8986\u76d63,261\u4e2a\u7a7a\u6c14\u8d28\u91cf\u76d1\u6d4b\u7ad9\u7684\u8857\u666f\u56fe\u50cf\uff0c\u5e76\u4e0e\u65f6\u95f4\u6233\u548c\u5730\u7406\u4f4d\u7f6e\u5339\u914d\u7684PM2.5\u6570\u636e\u8fdb\u884c\u540c\u6b65\u548c\u6e05\u6d17\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "result": "PM25V \u6570\u636e\u96c6\u7684\u7a7a\u95f4\u7cbe\u5ea6\u8fbe\u52305\u516c\u91cc\uff0c\u8fdc\u8d85\u4ee5\u5f80\u57ce\u5e02\u7ea7\u522b\u7684\u7cbe\u5ea6\uff0c\u4e14\u89c4\u6a21\u663e\u8457\u8d85\u8fc7\u5148\u524d\u57fa\u51c6\uff1b\u63d0\u4f9b\u4e86\u57fa\u4e8eCNN\u548cTransformer\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\u7ed3\u679c\u3002", "conclusion": "PM25Vision \u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u7a7a\u6c14\u8d28\u91cf\u7ba1\u7406\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u6570\u636e\u652f\u6301\uff0c\u63a8\u52a8\u7ec6\u7c92\u5ea6\u7a7a\u6c14\u8d28\u91cf\u4f30\u7b97\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.16920", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16920", "abs": "https://arxiv.org/abs/2509.16920", "authors": ["Ettilla Mohiuddin Eumi", "Hussein Abbass", "Nadine Marcus"], "title": "SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms", "comment": "This paper has been accepted and presented at the 16th International\n  Conference on Swarm Intelligence (ICSI 2025), held on July 11-15, 2025, in\n  Yokohama, Japan", "summary": "Traditional Human-Swarm Interaction (HSI) methods often lack intuitive\nreal-time adaptive interfaces, making decision making slower and increasing\ncognitive load while limiting command flexibility. To solve this, we present\nSwarmChat, a context-aware, multimodal interaction system powered by Large\nLanguage Models (LLMs). SwarmChat enables users to issue natural language\ncommands to robotic swarms using multiple modalities, such as text, voice, or\nteleoperation. The system integrates four LLM-based modules: Context Generator,\nIntent Recognition, Task Planner, and Modality Selector. These modules\ncollaboratively generate context from keywords, detect user intent, adapt\ncommands based on real-time robot state, and suggest optimal communication\nmodalities. Its three-layer architecture offers a dynamic interface with both\nfixed and customizable command options, supporting flexible control while\noptimizing cognitive effort. The preliminary evaluation also shows that the\nSwarmChat's LLM modules provide accurate context interpretation, relevant\nintent recognition, and effective command delivery, achieving high user\nsatisfaction.", "AI": {"tldr": "SwarmChat\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u591a\u6a21\u6001\u4eba-\u7fa4\u4ea4\u4e92\u7cfb\u7edf\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f93\u5165\uff0c\u63d0\u5347\u63a7\u5236\u7075\u6d3b\u6027\u5e76\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u3002", "motivation": "\u4f20\u7edf\u4eba-\u7fa4\u4ea4\u4e92\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u89c2\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u754c\u9762\uff0c\u5bfc\u81f4\u51b3\u7b56\u53d8\u6162\u3001\u8ba4\u77e5\u8d1f\u62c5\u589e\u52a0\u4e14\u547d\u4ee4\u7075\u6d3b\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faSwarmChat\u7cfb\u7edf\uff0c\u96c6\u6210\u56db\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u5757\uff1a\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u610f\u56fe\u8bc6\u522b\u3001\u4efb\u52a1\u89c4\u5212\u548c\u6a21\u6001\u9009\u62e9\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8f93\u5165\uff08\u6587\u672c\u3001\u8bed\u97f3\u3001\u9065\u64cd\u4f5c\uff09\u548c\u4e09\u5c42\u67b6\u6784\u5b9e\u73b0\u52a8\u6001\u4ea4\u4e92\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u8868\u660e\uff0cSwarmChat\u80fd\u51c6\u786e\u7406\u89e3\u4e0a\u4e0b\u6587\u3001\u8bc6\u522b\u7528\u6237\u610f\u56fe\u3001\u6709\u6548\u6267\u884c\u547d\u4ee4\uff0c\u5e76\u63d0\u4f9b\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "SwarmChat\u901a\u8fc7LLM\u9a71\u52a8\u7684\u591a\u6a21\u6001\u4e0e\u81ea\u9002\u5e94\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba-\u7fa4\u4ea4\u4e92\u7684\u7075\u6d3b\u6027\u4e0e\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.16508", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16508", "abs": "https://arxiv.org/abs/2509.16508", "authors": ["Marijan Fofonjka", "Shahryar Zehtabi", "Alireza Behtash", "Tyler Mauer", "David Stout"], "title": "Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever", "comment": "22 pages, 7 figures, 3 tables", "summary": "When existing retrieval-augmented generation (RAG) solutions are intended to\nbe used for new knowledge domains, it is necessary to update their encoders,\nwhich are taken to be pretrained large language models (LLMs). However, fully\nfinetuning these large models is compute- and memory-intensive, and even\ninfeasible when deployed on resource-constrained edge devices. We propose a\nnovel encoder architecture in this work that addresses this limitation by using\na frozen small language model (SLM), which satisfies the memory constraints of\nedge devices, and inserting a small adapter network before the transformer\nblocks of the SLM. The trainable adapter takes the token embeddings of the new\ncorpus and learns to produce enhanced soft embeddings for it, while requiring\nsignificantly less compute power to update than full fine-tuning. We further\npropose a novel retrieval mechanism by attaching a classifier head to the SLM\nencoder, which is trained to learn a similarity mapping of the input embeddings\nto their corresponding documents. Finally, to enable the online fine-tuning of\nboth (i) the encoder soft embeddings and (ii) the classifier-as-retriever on\nedge devices, we adopt federated learning (FL) and differential privacy (DP) to\nachieve an efficient, privacy-preserving, and product-grade training solution.\nWe conduct a theoretical analysis of our methodology, establishing convergence\nguarantees under mild assumptions on gradient variance when deployed for\ngeneral smooth nonconvex loss functions. Through extensive numerical\nexperiments, we demonstrate (i) the efficacy of obtaining soft embeddings to\nenhance the encoder, (ii) training a classifier to improve the retriever, and\n(iii) the role of FL in achieving speedup.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7f16\u7801\u5668\u67b6\u6784\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u548c\u8f7b\u91cf\u9002\u914d\u5668\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u5728\u7ebf\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u65b0\u77e5\u8bc6\u9886\u57df\u65f6\u9700\u5b8c\u5168\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u51bb\u7ed3SLM\u7684\u7f16\u7801\u5668\uff0c\u5728\u5176\u524d\u63d2\u5165\u53ef\u8bad\u7ec3\u7684\u8f7b\u91cf\u9002\u914d\u5668\u4ee5\u751f\u6210\u8f6f\u5d4c\u5165\uff0c\u5e76\u9644\u52a0\u5206\u7c7b\u5934\u4f5c\u4e3a\u68c0\u7d22\u5668\uff1b\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u4e0e\u5dee\u5206\u9690\u79c1\u5b9e\u73b0\u5728\u7ebf\u5fae\u8c03\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u975e\u51f8\u635f\u5931\u4e0b\u7684\u6536\u655b\u6027\uff1b\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7f16\u7801\u5668\u6027\u80fd\u3001\u6539\u8fdb\u68c0\u7d22\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684RAG\u6a21\u578b\u66f4\u65b0\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2509.16866", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16866", "abs": "https://arxiv.org/abs/2509.16866", "authors": ["Mohammad Ramezanali", "Mo Vazifeh", "Paolo Santi"], "title": "seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs", "comment": null, "summary": "We introduce seqBench, a parametrized benchmark for probing sequential\nreasoning limits in Large Language Models (LLMs) through precise,\nmulti-dimensional control over several key complexity dimensions. seqBench\nallows systematic variation of (1) the logical depth, defined as the number of\nsequential actions required to solve the task; (2) the number of backtracking\nsteps along the optimal path, quantifying how often the agent must revisit\nprior states to satisfy deferred preconditions (e.g., retrieving a key after\nencountering a locked door); and (3) the noise ratio, defined as the ratio\nbetween supporting and distracting facts about the environment. Our evaluations\non state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses\nexponentially beyond a model-specific logical depth. Unlike existing\nbenchmarks, seqBench's fine-grained control facilitates targeted analyses of\nthese reasoning failures, illuminating universal scaling laws and statistical\nlimits, as detailed in this paper alongside its generation methodology and\nevaluation metrics. We find that even top-performing models systematically fail\non seqBench's structured reasoning tasks despite minimal search complexity,\nunderscoring key limitations in their commonsense reasoning capabilities.\nDesigned for future evolution to keep pace with advancing models, the seqBench\ndatasets are publicly released to spur deeper scientific inquiry into LLM\nreasoning, aiming to establish a clearer understanding of their true potential\nand current boundaries for robust real-world application.", "AI": {"tldr": "seqBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u6781\u9650\u7684\u53c2\u6570\u5316\u57fa\u51c6\uff0c\u901a\u8fc7\u63a7\u5236\u903b\u8f91\u6df1\u5ea6\u3001\u56de\u6eaf\u6b65\u9aa4\u548c\u566a\u58f0\u6bd4\u7b49\u7ef4\u5ea6\uff0c\u63ed\u793a\u4e86\u73b0\u6709LLM\u5728\u5e38\u8bc6\u63a8\u7406\u4e2d\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u4efb\u52a1\u590d\u6742\u5ea6\uff0c\u65e0\u6cd5\u7cfb\u7edf\u5206\u6790LLM\u5728\u591a\u7ef4\u590d\u6742\u6027\u4e0b\u7684\u63a8\u7406\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u53ef\u53c2\u6570\u5316\u3001\u7ec6\u7c92\u5ea6\u7684\u57fa\u51c6\u6765\u63ed\u793a\u6a21\u578b\u7684\u771f\u6b63\u63a8\u7406\u6781\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u8c03\u8282\u903b\u8f91\u6df1\u5ea6\u3001\u56de\u6eaf\u6b65\u6570\u548c\u566a\u58f0\u6bd4\u7684\u53c2\u6570\u5316\u57fa\u51c6seqBench\uff0c\u5e76\u5728\u6700\u5148\u8fdb\u7684LLM\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u6240\u6709\u4e3b\u6d41LLM\u5728\u8d85\u8fc7\u7279\u5b9a\u903b\u8f91\u6df1\u5ea6\u540e\u51c6\u786e\u7387\u5448\u6307\u6570\u7ea7\u4e0b\u964d\uff0c\u5373\u4f7f\u4efb\u52a1\u641c\u7d22\u590d\u6742\u5ea6\u4f4e\u4e5f\u51fa\u73b0\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u4e14\u56de\u6eaf\u9700\u6c42\u548c\u566a\u58f0\u4f1a\u8fdb\u4e00\u6b65\u52a0\u5267\u6027\u80fd\u9000\u5316\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u7ed3\u6784\u5316\u987a\u5e8f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0cseqBench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2509.16527", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16527", "abs": "https://arxiv.org/abs/2509.16527", "authors": ["Guangze Zheng", "Shijie Lin", "Haobo Zuo", "Si Si", "Ming-Shan Wang", "Changhong Fu", "Jia Pan"], "title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity", "comment": "NeurIPS 2025. Project page: https://george-zhuang.github.io/lbm/", "summary": "This work proposes the Lattice Boltzmann Model (LBM) to learn real-world\npixel dynamicity for visual tracking. LBM decomposes visual representations\ninto dynamic pixel lattices and solves pixel motion states through\ncollision-streaming processes. Specifically, the high-dimensional distribution\nof the target pixels is acquired through a multilayer predict-update network to\nestimate the pixel positions and visibility. The predict stage formulates\nlattice collisions among the spatial neighborhood of target pixels and develops\nlattice streaming within the temporal visual context. The update stage\nrectifies the pixel distributions with online visual representations. Compared\nwith existing methods, LBM demonstrates practical applicability in an online\nand real-time manner, which can efficiently adapt to real-world visual tracking\ntasks. Comprehensive evaluations of real-world point tracking benchmarks such\nas TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of\nlarge-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B\nfurther demonstrates LBM's real-world practicality.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u683c\u5b50\u73bb\u5c14\u5179\u66fc\u6a21\u578b\uff08LBM\uff09\u7684\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u50cf\u7d20\u683c\u5b50\u4e0e\u78b0\u649e-\u6d41\u8fc7\u7a0b\u5efa\u6a21\u50cf\u7d20\u8fd0\u52a8\u72b6\u6001\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u5728\u7ebf\u7684\u5b9e\u65f6\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u771f\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u50cf\u7d20\u52a8\u6001\u6027\uff0c\u7f3a\u4e4f\u5bf9\u50cf\u7d20\u8fd0\u52a8\u72b6\u6001\u7684\u7269\u7406\u542f\u53d1\u5f0f\u5efa\u6a21\u673a\u5236\u3002", "method": "\u63d0\u51faLBM\u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9\u8868\u5f81\u5206\u89e3\u4e3a\u52a8\u6001\u50cf\u7d20\u683c\u5b50\uff0c\u901a\u8fc7\u591a\u5c42\u9884\u6d4b-\u66f4\u65b0\u7f51\u7edc\u83b7\u53d6\u76ee\u6807\u50cf\u7d20\u7684\u9ad8\u7ef4\u5206\u5e03\uff1b\u9884\u6d4b\u9636\u6bb5\u6a21\u62df\u7a7a\u95f4\u90bb\u57df\u5185\u7684\u683c\u5b50\u78b0\u649e\u4e0e\u65f6\u95f4\u4e0a\u4e0b\u6587\u4e2d\u7684\u683c\u5b50\u6d41\uff0c\u66f4\u65b0\u9636\u6bb5\u7ed3\u5408\u5728\u7ebf\u89c6\u89c9\u8868\u5f81\u4fee\u6b63\u50cf\u7d20\u5206\u5e03\u3002", "result": "\u5728TAP-Vid\u3001RoboTAP\u7b49\u771f\u5b9e\u70b9\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u6548\u7387\uff0c\u5728TAO\u3001BFT\u3001OVT-B\u7b49\u5927\u89c4\u6a21\u5f00\u653e\u4e16\u754c\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u5c55\u73b0\u4e86\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "conclusion": "LBM\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684\u50cf\u7d20\u52a8\u6001\u5efa\u6a21\uff0c\u5728\u7ebf\u5b9e\u65f6\u6027\u80fd\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u590d\u6742\u771f\u5b9e\u573a\u666f\u7684\u89c6\u89c9\u8ddf\u8e2a\u4efb\u52a1\u3002"}}
{"id": "2509.16963", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16963", "abs": "https://arxiv.org/abs/2509.16963", "authors": ["Chengjin Wang", "Yanmin Zhou", "Zhipeng Wang", "Zheng Yan", "Feng Luan", "Shuo Jiang", "Runjie Shen", "Hongrui Sang", "Bin He"], "title": "A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination", "comment": null, "summary": "Humans and animals can make real-time adjustments to movements by imagining\ntheir action outcomes to prevent unanticipated or even catastrophic motion\nfailures in unknown unstructured environments. Action imagination, as a refined\nsensorimotor strategy, leverages perception-action loops to handle physical\ninteraction-induced uncertainties in perception and system modeling within\ncomplex systems. Inspired by the action-awareness capability of animal\nintelligence, this study proposes an imagination-inspired motion planner (I-MP)\nframework that specifically enhances robots' action reliability by imagining\nplausible spatial states for approaching. After topologizing the workspace,\nI-MP build perception-action loop enabling robots autonomously build contact\nmodels. Leveraging fixed-point theory and Hausdorff distance, the planner\ncomputes convergent spatial states under interaction characteristics and\nmission constraints. By homogenously representing multi-dimensional\nenvironmental characteristics through work, the robot can approach the imagined\nspatial states via real-time computation of energy gradients. Consequently,\nexperimental results demonstrate the practicality and robustness of I-MP in\ncomplex cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53d7\u52a8\u4f5c\u60f3\u8c61\u542f\u53d1\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff08I-MP\uff09\uff0c\u901a\u8fc7\u6784\u5efa\u611f\u77e5-\u52a8\u4f5c\u5faa\u73af\u548c\u5b9e\u65f6\u8ba1\u7b97\u80fd\u91cf\u68af\u5ea6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u52a8\u4f5c\u53ef\u9760\u6027\u3002", "motivation": "\u53d7\u4eba\u7c7b\u548c\u52a8\u7269\u901a\u8fc7\u60f3\u8c61\u52a8\u4f5c\u7ed3\u679c\u5b9e\u65f6\u8c03\u6574\u8fd0\u52a8\u7684\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u672a\u77e5\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5e94\u5bf9\u611f\u77e5\u548c\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5de5\u4f5c\u7a7a\u95f4\u62d3\u6251\u5316\uff0c\u6784\u5efa\u611f\u77e5-\u52a8\u4f5c\u5faa\u73af\u4ee5\u81ea\u4e3b\u5efa\u7acb\u63a5\u89e6\u6a21\u578b\uff1b\u5229\u7528\u4e0d\u52a8\u70b9\u7406\u8bba\u548c\u8c6a\u65af\u591a\u592b\u8ddd\u79bb\u8ba1\u7b97\u6ee1\u8db3\u4ea4\u4e92\u7279\u6027\u548c\u4efb\u52a1\u7ea6\u675f\u7684\u6536\u655b\u7a7a\u95f4\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u529f\u5bf9\u591a\u7ef4\u73af\u5883\u7279\u5f81\u8fdb\u884c\u7edf\u4e00\u8868\u793a\uff0c\u5b9e\u65f6\u8ba1\u7b97\u80fd\u91cf\u68af\u5ea6\u4ee5\u8d8b\u8fd1\u60f3\u8c61\u7684\u7a7a\u95f4\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cI-MP\u5728\u590d\u6742\u6742\u4e71\u73af\u5883\u4e2d\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "I-MP\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u52a8\u4f5c\u53ef\u9760\u6027\u548c\u9002\u5e94\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u52a8\u4f5c\u60f3\u8c61\u673a\u5236\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u52bf\u3002"}}
{"id": "2509.16543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16543", "abs": "https://arxiv.org/abs/2509.16543", "authors": ["Yue Huang", "Zhengzhe Jiang", "Xiaonan Luo", "Kehan Guo", "Haomin Zhuang", "Yujun Zhou", "Zhengqing Yuan", "Xiaoqi Sun", "Jules Schleinitz", "Yanbo Wang", "Shuhao Zhang", "Mihir Surve", "Nitesh V Chawla", "Olaf Wiest", "Xiangliang Zhang"], "title": "ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions", "comment": null, "summary": "Empowering large language models (LLMs) with chemical intelligence remains a\nchallenge due to the scarcity of high-quality, domain-specific\ninstruction-response datasets and the misalignment of existing synthetic data\ngeneration pipelines with the inherently hierarchical and rule-governed\nstructure of chemical information. To address this, we propose ChemOrch, a\nframework that synthesizes chemically grounded instruction-response pairs\nthrough a two-stage process: task-controlled instruction generation and\ntool-aware response construction. ChemOrch enables controllable diversity and\nlevels of difficulty for the generated tasks, and ensures response precision\nthrough tool planning and distillation, and tool-based self-repair mechanisms.\nThe effectiveness of ChemOrch is evaluated based on: 1) the high quality of\ngenerated instruction data, demonstrating superior diversity and strong\nalignment with chemical constraints; 2) the reliable generation of evaluation\ntasks that more effectively reveal LLM weaknesses in chemistry; and 3) the\nsignificant improvement of LLM chemistry capabilities when the generated\ninstruction data are used for fine-tuning. Our work thus represents a critical\nstep toward scalable and verifiable chemical intelligence in LLMs.", "AI": {"tldr": "\u63d0\u51faChemOrch\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7b26\u5408\u5316\u5b66\u89c4\u5219\u7684\u6307\u4ee4\u54cd\u5e94\u6570\u636e\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5316\u5b66\u9886\u57df\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\u96be\u4ee5\u5339\u914d\u5316\u5b66\u4fe1\u606f\u7684\u5c42\u6b21\u6027\u548c\u89c4\u5219\u6027\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u5316\u5b66\u9886\u57df\u6307\u4ee4\u6570\u636e\uff0c\u9650\u5236\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5316\u5b66\u667a\u80fd\u65b9\u9762\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faChemOrch\u6846\u67b6\uff0c\u91c7\u7528\u4efb\u52a1\u63a7\u5236\u7684\u6307\u4ee4\u751f\u6210\u548c\u5de5\u5177\u611f\u77e5\u7684\u54cd\u5e94\u6784\u5efa\u4e24\u4e2a\u9636\u6bb5\uff0c\u5b9e\u73b0\u53ef\u63a7\u591a\u6837\u6027\u4e0e\u96be\u5ea6\uff0c\u5e76\u901a\u8fc7\u5de5\u5177\u89c4\u5212\u3001\u84b8\u998f\u548c\u81ea\u4fee\u590d\u673a\u5236\u786e\u4fdd\u54cd\u5e94\u51c6\u786e\u6027\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u5177\u6709\u9ad8\u591a\u6837\u6027\u548c\u5316\u5b66\u7ea6\u675f\u5bf9\u9f50\u6027\uff1b\u8bc4\u4f30\u4efb\u52a1\u80fd\u6709\u6548\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5316\u5b66\u65b9\u9762\u7684\u5f31\u70b9\uff1b\u5fae\u8c03\u540e\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u5316\u5b66\u80fd\u529b\u3002", "conclusion": "ChemOrch\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u53ef\u9a8c\u8bc1\u7684\u5316\u5b66\u667a\u80fd\u63d0\u4f9b\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2509.16516", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16516", "abs": "https://arxiv.org/abs/2509.16516", "authors": ["Md Mezbaur Rahman", "Cornelia Caragea"], "title": "LLM-Guided Co-Training for Text Classification", "comment": null, "summary": "In this paper, we introduce a novel weighted co-training approach that is\nguided by Large Language Models (LLMs). Namely, in our co-training approach, we\nuse LLM labels on unlabeled data as target labels and co-train two encoder-only\nbased networks that train each other over multiple iterations: first, all\nsamples are forwarded through each network and historical estimates of each\nnetwork's confidence in the LLM label are recorded; second, a dynamic\nimportance weight is derived for each sample according to each network's belief\nin the quality of the LLM label for that sample; finally, the two networks\nexchange importance weights with each other -- each network back-propagates all\nsamples weighted with the importance weights coming from its peer network and\nupdates its own parameters. By strategically utilizing LLM-generated guidance,\nour approach significantly outperforms conventional SSL methods, particularly\nin settings with abundant unlabeled data. Empirical results show that it\nachieves state-of-the-art performance on 4 out of 5 benchmark datasets and\nranks first among 14 compared methods according to the Friedman test. Our\nresults highlight a new direction in semi-supervised learning -- where LLMs\nserve as knowledge amplifiers, enabling backbone co-training models to achieve\nstate-of-the-art performance efficiently.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7531\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f15\u5bfc\u7684\u52a0\u6743\u534f\u540c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528LLM\u5bf9\u65e0\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u7f16\u7801\u5668\u7f51\u7edc\u76f8\u4e92\u63d0\u4f9b\u5e26\u6743\u91cd\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5145\u5206\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u5f3a\u5927\u7684\u77e5\u8bc6\u751f\u6210\u80fd\u529b\uff0c\u53ef\u4f5c\u4e3a\u5916\u90e8\u77e5\u8bc6\u6e90\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6709\u6548\u878d\u5408LLM\u751f\u6210\u7684\u6807\u7b7e\u6765\u589e\u5f3a\u534f\u540c\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u7f51\u7edc\u8fdb\u884c\u534f\u540c\u8bad\u7ec3\uff0c\u4f7f\u7528LLM\u4e3a\u65e0\u6807\u7b7e\u6570\u636e\u751f\u6210\u4f2a\u6807\u7b7e\uff1b\u6bcf\u4e2a\u7f51\u7edc\u6839\u636e\u81ea\u8eab\u5bf9LLM\u6807\u7b7e\u7f6e\u4fe1\u5ea6\u7684\u5386\u53f2\u4f30\u8ba1\uff0c\u52a8\u6001\u8ba1\u7b97\u6837\u672c\u7684\u91cd\u8981\u6027\u6743\u91cd\uff1b\u4e24\u7f51\u7edc\u4e92\u76f8\u4ea4\u6362\u6743\u91cd\uff0c\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u7528\u5bf9\u65b9\u63d0\u4f9b\u7684\u6743\u91cd\u8c03\u6574\u635f\u5931\uff0c\u4ece\u800c\u66f4\u65b0\u81ea\u8eab\u53c2\u6570\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u67094\u4e2a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u572814\u79cd\u5bf9\u6bd4\u65b9\u6cd5\u4e2dFriedman\u68c0\u9a8c\u6392\u540d\u9996\u4f4d\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u77e5\u8bc6\u653e\u5927\u5668\uff0c\u5728\u534a\u76d1\u7763\u534f\u540c\u8bad\u7ec3\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u5f00\u8f9f\u4e86\u5229\u7528LLM\u6307\u5bfc\u4e0b\u6e38\u4efb\u52a1\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.16891", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16891", "abs": "https://arxiv.org/abs/2509.16891", "authors": ["Sha Li"], "title": "LLMs as Layout Designers: A Spatial Reasoning Perspective", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated impressive reasoning and\nplanning abilities in textual domains and can effectively follow instructions\nfor complex tasks, their capacity for spatial understanding and reasoning\nremains limited. Such capabilities, however, are critical for applications like\ncontent-aware graphic layout design, which demands precise placement,\nalignment, and structural organization of multiple elements within constrained\nvisual spaces. To address this gap, we propose LaySPA, a reinforcement\nlearning-based framework that augments LLM agents with explicit spatial\nreasoning capabilities. LaySPA leverages hybrid reward signals that capture\ngeometric validity, structural fidelity, and visual quality, enabling agents to\nmodel inter-element relationships, navigate the canvas, and optimize spatial\narrangements. Through iterative self-exploration and adaptive policy\noptimization, LaySPA produces both interpretable reasoning traces and\nstructured layouts. Experimental results demonstrate that LaySPA generates\nstructurally sound and visually appealing layouts, outperforming larger\ngeneral-purpose LLMs and achieving results on par with state-of-the-art\nspecialized layout models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLaySPA\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u4fe1\u53f7\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7528\u4e8e\u751f\u6210\u7ed3\u6784\u5408\u7406\u4e14\u89c6\u89c9\u7f8e\u89c2\u7684\u56fe\u5f62\u5e03\u5c40\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7a7a\u95f4\u7406\u89e3\u4e0e\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u800c\u8fd9\u4e00\u80fd\u529b\u5bf9\u4e8e\u56fe\u5f62\u5e03\u5c40\u8bbe\u8ba1\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faLaySPA\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u663e\u5f0f\u7a7a\u95f4\u63a8\u7406\u673a\u5236\uff0c\u5229\u7528\u51e0\u4f55\u6709\u6548\u6027\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u6df7\u5408\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u63a2\u7d22\u548c\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u751f\u6210\u5e03\u5c40\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLaySPA\u5728\u5e03\u5c40\u7684\u7ed3\u6784\u6027\u548c\u89c6\u89c9\u6548\u679c\u4e0a\u4f18\u4e8e\u66f4\u5927\u7684\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u4e13\u7528\u5e03\u5c40\u6a21\u578b\u6c34\u5e73\u3002", "conclusion": "LaySPA\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u80fd\u591f\u5728\u5185\u5bb9\u611f\u77e5\u7684\u5e03\u5c40\u8bbe\u8ba1\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u89e3\u91ca\u7684\u5e03\u5c40\u7ed3\u679c\u3002"}}
{"id": "2509.16538", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16538", "abs": "https://arxiv.org/abs/2509.16538", "authors": ["Shubhashis Roy Dipta", "Tz-Ying Wu", "Subarna Tripathi"], "title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis", "comment": null, "summary": "Video captions offer concise snapshots of actors, objects, and actions within\na video, serving as valuable assets for applications such as question answering\nand event localization. However, acquiring human annotations for video captions\nis costly or even impractical, especially when dealing with diverse video\ndomains. Existing models trained on supervised datasets face challenges in\nevaluating performance across different domains due to the reliance on\nreference-based evaluation protocols, which necessitate ground truth captions.\nThis assumption is unrealistic for evaluating videos in the wild. To address\nthese limitations, we propose a reference-free evaluation framework that does\nnot require ground truth captions, focusing on factual grounding to ensure\naccurate assessment of caption quality. We introduce VC-Inspector, a novel\ncaption quality evaluator that is both reference-free and factually grounded.\nUtilizing large language models, we generate pseudo captions of varying quality\nbased on supervised data, which are subsequently used to train a multimodal\nmodel (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior\nalignment with human judgments on the VATEX-Eval dataset, outperforming\nexisting methods. The performance also generalizes to image caption datasets,\nFlickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.\nOverall, VC-Inspector offers a scalable and generalizable solution for\nevaluating the factual accuracy of video captions, paving the way for more\neffective and objective assessment methodologies in diverse video domains.", "AI": {"tldr": "\u63d0\u51faVC-Inspector\uff0c\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u5b57\u5e55\u3001\u57fa\u4e8e\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u89c6\u9891\u5b57\u5e55\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\u8bad\u7ec3\u591a\u6a21\u6001\u8bc4\u4f30\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u5b57\u5e55\uff08ground truth\uff09\uff0c\u5728\u8de8\u57df\u6216\u771f\u5b9e\u573a\u666f\u4e2d\u4e0d\u73b0\u5b9e\u4e14\u6210\u672c\u9ad8\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u53c2\u8003\u5b57\u5e55\u7684\u8bc4\u4f30\u6846\u67b6VC-Inspector\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0d\u540c\u8d28\u91cf\u7684\u4f2a\u5b57\u5e55\uff0c\u57fa\u4e8e\u76d1\u7763\u6570\u636e\u8bad\u7ec3\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u578b\uff08Qwen2.5-VL\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5b9e\u73b0\u5bf9\u5b57\u5e55\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u5ba2\u89c2\u8bc4\u4f30\u3002", "result": "\u5728VATEX-Eval\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u5224\u65ad\u5177\u6709\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u5e76\u5728Flickr8K-Expert\u548cFlickr8K-CF\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VC-Inspector\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u901a\u7528\u7684\u89c6\u9891\u5b57\u5e55\u8bc4\u4f30\u65b9\u6848\uff0c\u65e0\u9700\u771f\u5b9e\u5b57\u5e55\u5373\u53ef\u6709\u6548\u8bc4\u4f30\u5b57\u5e55\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u63a8\u52a8\u4e86\u5728\u591a\u6837\u5316\u89c6\u9891\u573a\u666f\u4e2d\u7684\u5ba2\u89c2\u8bc4\u4f30\u53d1\u5c55\u3002"}}
{"id": "2509.16966", "categories": ["cs.RO", "cs.NA", "math.DG", "math.GR", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.16966", "abs": "https://arxiv.org/abs/2509.16966", "authors": ["Andreas Mueller"], "title": "Geometric Interpolation of Rigid Body Motions", "comment": null, "summary": "The problem of interpolating a rigid body motion is to find a spatial\ntrajectory between a prescribed initial and terminal pose. Two variants of this\ninterpolation problem are addressed. The first is to find a solution that\nsatisfies initial conditions on the k-1 derivatives of the rigid body twist.\nThis is called the kth-order initial value trajectory interpolation problem\n(k-IV-TIP). The second is to find a solution that satisfies conditions on the\nrigid body twist and its k-1 derivatives at the initial and terminal pose. This\nis called the kth-order boundary value trajectory interpolation problem\n(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and\nup to the 4th time derivative are prescribed. Further, a solution to the\n1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The\nlatter is a novel cubic interpolation between two spatial configurations with\ngiven initial and terminal twist. This interpolation is automatically identical\nto the minimum acceleration curve when the twists are set to zero. The general\napproach to derive higher-order solutions is presented. Numerical results are\nshown for two examples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u521a\u4f53\u8fd0\u52a8\u7684\u63d2\u503c\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u53d8\u4f53\uff1ak\u9636\u521d\u503c\u8f68\u8ff9\u63d2\u503c\u95ee\u9898\uff08k-IV-TIP\uff09\u548ck\u9636\u8fb9\u503c\u8f68\u8ff9\u63d2\u503c\u95ee\u9898\uff08k-BV-TIP\uff09\uff0c\u5e76\u7ed9\u51fa\u4e86k=1\u52304\u65f6k-IV-TIP\u7684\u89e3\u4ee5\u53ca1-BV-TIP\u7684\u65b0\u4e09\u6b21\u63d2\u503c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u626d\u901f\u4e3a\u96f6\u65f6\u9000\u5316\u4e3a\u6700\u5c0f\u52a0\u901f\u5ea6\u66f2\u7ebf\u3002", "motivation": "\u521a\u4f53\u8fd0\u52a8\u63d2\u503c\u5728\u673a\u5668\u4eba\u3001\u52a8\u753b\u548c\u8fd0\u52a8\u89c4\u5212\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9ad8\u9636\u8fde\u7eed\u6027\u6216\u8fb9\u754c\u6761\u4ef6\u7684\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u53d1\u5c55\u80fd\u540c\u65f6\u6ee1\u8db3\u521d\u59cb\u6216\u7ec8\u7aef\u5bfc\u6570\u7ea6\u675f\u7684\u63d2\u503c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u521a\u4f53\u626d\u901f\uff08twist\uff09\u53ca\u5176\u9ad8\u9636\u5bfc\u6570\u7684\u521d\u59cb\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u5229\u7528\u674e\u7fa4\u548c\u5fae\u5206\u51e0\u4f55\u5de5\u5177\u6784\u5efa\u6ee1\u8db3k\u9636\u8fde\u7eed\u6027\u7684\u8f68\u8ff9\u63d2\u503c\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4e09\u6b21\u63d2\u503c\u7b97\u6cd5\u7528\u4e8e\u89e3\u51b31-BV-TIP\u3002", "result": "\u6210\u529f\u6c42\u89e3\u4e86k=1\u81f34\u7684k-IV-TIP\u95ee\u9898\uff1b\u63d0\u51fa\u4e861-BV-TIP\u7684\u4e09\u6b21\u63d2\u503c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u81ea\u7136\u9000\u5316\u4e3a\u6700\u5c0f\u52a0\u901f\u5ea6\u66f2\u7ebf\uff1b\u5e76\u901a\u8fc7\u4e24\u4e2a\u6570\u503c\u7b97\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u521a\u4f53\u8fd0\u52a8\u7684\u9ad8\u9636\u8f68\u8ff9\u63d2\u503c\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5c24\u51761-BV-TIP\u7684\u89e3\u5728\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5e73\u6ed1\u8fc7\u6e21\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.16551", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16551", "abs": "https://arxiv.org/abs/2509.16551", "authors": ["Dan John Velasco", "Matthew Theodore Roque"], "title": "Rethinking the Role of Text Complexity in Language Model Pretraining", "comment": "To be published in BabyLM Workshop at EMNLP 2025", "summary": "Improving pretraining data quality and size is known to boost downstream\nperformance, but the role of text complexity is less explored. Text complexity\nrefers to how hard a text is to read, and is typically estimated from surface\ncues such as sentence length, word choice, and sentence structure. We reduce\nsurface-level complexity--shorter sentences, simpler words, simpler\nstructure--while keeping core text content close to constant, and ask: (1) How\ndoes complexity affect language modeling across model sizes? (2) Can useful\nrepresentations be learned from simpler text alone? (3) How does pretraining\ntext complexity influence downstream language understanding? To answer these\nquestions, we simplify human-written texts using a large language model, then\npretrain causal models (28M-500M) from scratch on both original and simplified\ndata, and evaluate them in finetuning and zero-shot setups. We find that\nperplexity is sensitive to the interaction between model capacity and text\ncomplexity--smaller models degrade far less on simpler texts--while text\ncomplexity has little impact on finetuning evaluations, with zero-shot\nevaluations indicating that simpler texts benefit performance on linguistic\nknowledge tasks, whereas more complex texts favor tasks requiring world\nknowledge and entity tracking.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6587\u672c\u590d\u6742\u6027\u5bf9\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7b80\u5316\u6587\u672c\u5bf9\u5c0f\u6a21\u578b\u66f4\u53cb\u597d\uff0c\u4e14\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5f71\u54cd\u5404\u5f02\u3002", "motivation": "\u63a2\u7d22\u6587\u672c\u590d\u6742\u6027\u5728\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u8d28\u91cf\u548c\u89c4\u6a21\u4e4b\u5916\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7b80\u5316\u4eba\u7c7b\u7f16\u5199\u7684\u6587\u672c\uff0c\u4fdd\u6301\u6838\u5fc3\u5185\u5bb9\u57fa\u672c\u4e0d\u53d8\uff0c\u7136\u540e\u5728\u539f\u59cb\u548c\u7b80\u5316\u6587\u672c\u4e0a\u4ece\u96f6\u5f00\u59cb\u9884\u8bad\u7ec3\u56e0\u679c\u8bed\u8a00\u6a21\u578b\uff0828M-500M\u53c2\u6570\uff09\uff0c\u5e76\u5728\u5fae\u8c03\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8f83\u5c0f\u7684\u6a21\u578b\u5728\u7b80\u5316\u6587\u672c\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u8f83\u5c11\uff1b\u5fae\u8c03\u7ed3\u679c\u53d7\u6587\u672c\u590d\u6742\u6027\u5f71\u54cd\u8f83\u5c0f\uff1b\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793a\uff0c\u7b80\u5355\u6587\u672c\u6709\u52a9\u4e8e\u8bed\u8a00\u77e5\u8bc6\u4efb\u52a1\uff0c\u800c\u590d\u6742\u6587\u672c\u66f4\u5229\u4e8e\u9700\u8981\u4e16\u754c\u77e5\u8bc6\u548c\u5b9e\u4f53\u8ffd\u8e2a\u7684\u4efb\u52a1\u3002", "conclusion": "\u6587\u672c\u590d\u6742\u6027\u4e0e\u6a21\u578b\u5bb9\u91cf\u4ea4\u4e92\u5f71\u54cd\u8bed\u8a00\u5efa\u6a21\u6548\u679c\uff0c\u7b80\u5316\u6587\u672c\u53ef\u5728\u7279\u5b9a\u4efb\u52a1\u548c\u5c0f\u6a21\u578b\u4e0a\u5e26\u6765\u4f18\u52bf\uff0c\u4f46\u590d\u6742\u6587\u672c\u4ecd\u5bf9\u9ad8\u7ea7\u8ba4\u77e5\u4efb\u52a1\u6709\u76ca\u3002"}}
{"id": "2509.16521", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16521", "abs": "https://arxiv.org/abs/2509.16521", "authors": ["Yifan Yan", "Shuai Yang", "Xiuzhen Guo", "Xiangguang Wang", "Wei Chow", "Yuanchao Shu", "Shibo He"], "title": "mmExpert: Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding", "comment": "Accepted to ACM MobiHoc '25", "summary": "Millimeter-wave (mmWave) sensing technology holds significant value in\nhuman-centric applications, yet the high costs associated with data acquisition\nand annotation limit its widespread adoption in our daily lives. Concurrently,\nthe rapid evolution of large language models (LLMs) has opened up opportunities\nfor addressing complex human needs. This paper presents mmExpert, an innovative\nmmWave understanding framework consisting of a data generation flywheel that\nleverages LLMs to automate the generation of synthetic mmWave radar datasets\nfor specific application scenarios, thereby training models capable of\nzero-shot generalization in real-world environments. Extensive experiments\ndemonstrate that the data synthesized by mmExpert significantly enhances the\nperformance of downstream models and facilitates the successful deployment of\nlarge models for mmWave understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ammExpert\u7684\u6beb\u7c73\u6ce2\u7406\u89e3\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\u7684\u5408\u6210\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6beb\u7c73\u6ce2\u611f\u77e5\u6280\u672f\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u6570\u636e\u83b7\u53d6\u548c\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u540c\u65f6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4e3a\u6ee1\u8db3\u590d\u6742\u4eba\u7c7b\u9700\u6c42\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u751f\u6210\u98de\u8f6e\uff0c\u81ea\u52a8\u5316\u751f\u6210\u5408\u6210\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3\u53ef\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u7684\u6a21\u578b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cmmExpert\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u6210\u529f\u63a8\u52a8\u4e86\u5927\u578b\u6a21\u578b\u5728\u6beb\u7c73\u6ce2\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u90e8\u7f72\u3002", "conclusion": "mmExpert\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6beb\u7c73\u6ce2\u611f\u77e5\u6280\u672f\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6570\u636e\u6210\u672c\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u8be5\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.16924", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16924", "abs": "https://arxiv.org/abs/2509.16924", "authors": ["Jia Li", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation", "comment": "Main paper (14 pages). Accepted for publication by ICONIP(\n  International Conference on Neural Information Processing) 2025", "summary": "In audio-visual navigation (AVN) tasks, an embodied agent must autonomously\nlocalize a sound source in unknown and complex 3D environments based on\naudio-visual signals. Existing methods often rely on static modality fusion\nstrategies and neglect the spatial cues embedded in stereo audio, leading to\nperformance degradation in cluttered or occluded scenes. To address these\nissues, we propose an end-to-end reinforcement learning-based AVN framework\nwith two key innovations: (1) a \\textbf{S}tereo-Aware \\textbf{A}ttention\n\\textbf{M}odule (\\textbf{SAM}), which learns and exploits the spatial disparity\nbetween left and right audio channels to enhance directional sound perception;\nand (2) an \\textbf{A}udio-\\textbf{G}uided \\textbf{D}ynamic \\textbf{F}usion\nModule (\\textbf{AGDF}), which dynamically adjusts the fusion ratio between\nvisual and auditory features based on audio cues, thereby improving robustness\nto environmental changes. Extensive experiments are conducted on two realistic\n3D scene datasets, Replica and Matterport3D, demonstrating that our method\nsignificantly outperforms existing approaches in terms of navigation success\nrate and path efficiency. Notably, our model achieves over 40\\% improvement\nunder audio-only conditions compared to the best-performing baselines. These\nresults highlight the importance of explicitly modeling spatial cues from\nstereo channels and performing deep multi-modal fusion for robust and efficient\naudio-visual navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u97f3\u89c6\u9891\u5bfc\u822a\u6846\u67b6\uff0c\u5305\u542b\u7acb\u4f53\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\uff08SAM\uff09\u548c\u97f3\u9891\u5f15\u5bfc\u52a8\u6001\u878d\u5408\u6a21\u5757\uff08AGDF\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u5ffd\u89c6\u7acb\u4f53\u58f0\u97f3\u9891\u4e2d\u7684\u7a7a\u95f4\u7ebf\u7d22\uff0c\u5bfc\u81f4\u5728\u6742\u4e71\u6216\u906e\u6321\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u4e86SAM\u6a21\u5757\u4ee5\u5229\u7528\u5de6\u53f3\u58f0\u9053\u95f4\u7684\u7a7a\u95f4\u5dee\u5f02\u589e\u5f3a\u65b9\u5411\u6027\u542c\u89c9\u611f\u77e5\uff0c\u5e76\u63d0\u51faAGDF\u6a21\u5757\u6839\u636e\u97f3\u9891\u7ebf\u7d22\u52a8\u6001\u8c03\u6574\u89c6\u542c\u7279\u5f81\u878d\u5408\u6bd4\u4f8b\u3002", "result": "\u5728Replica\u548cMatterport3D\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bfc\u822a\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u7eaf\u97f3\u9891\u6761\u4ef6\u4e0b\u76f8\u6bd4\u57fa\u7ebf\u63d0\u5347\u8d85\u8fc740%\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u7acb\u4f53\u58f0\u7a7a\u95f4\u7ebf\u7d22\u5e76\u8fdb\u884c\u6df1\u5ea6\u591a\u6a21\u6001\u52a8\u6001\u878d\u5408\uff0c\u5bf9\u5b9e\u73b0\u9c81\u68d2\u9ad8\u6548\u7684\u97f3\u89c6\u9891\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.16549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16549", "abs": "https://arxiv.org/abs/2509.16549", "authors": ["Zirui Wang", "Jiayi Zhang", "Tianwei Guan", "Yuhan Zhou", "Xingyuan Li", "Minjing Dong", "Jinyuan Liu"], "title": "Efficient Rectified Flow for Image Fusion", "comment": null, "summary": "Image fusion is a fundamental and important task in computer vision, aiming\nto combine complementary information from different modalities to fuse images.\nIn recent years, diffusion models have made significant developments in the\nfield of image fusion. However, diffusion models often require complex\ncomputations and redundant inference time, which reduces the applicability of\nthese methods. To address this issue, we propose RFfusion, an efficient\none-step diffusion model for image fusion based on Rectified Flow. We\nincorporate Rectified Flow into the image fusion task to straighten the\nsampling path in the diffusion model, achieving one-step sampling without the\nneed for additional training, while still maintaining high-quality fusion\nresults. Furthermore, we propose a task-specific variational autoencoder (VAE)\narchitecture tailored for image fusion, where the fusion operation is embedded\nwithin the latent space to further reduce computational complexity. To address\nthe inherent discrepancy between conventional reconstruction-oriented VAE\nobjectives and the requirements of image fusion, we introduce a two-stage\ntraining strategy. This approach facilitates the effective learning and\nintegration of complementary information from multi-modal source images,\nthereby enabling the model to retain fine-grained structural details while\nsignificantly enhancing inference efficiency. Extensive experiments demonstrate\nthat our method outperforms other state-of-the-art methods in terms of both\ninference speed and fusion quality. Code is available at\nhttps://github.com/zirui0625/RFfusion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b65\u6269\u6563\u6a21\u578bRFfusion\uff0c\u7528\u4e8e\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u3002\u901a\u8fc7\u5f15\u5165Rectified Flow\u548c\u9488\u5bf9\u56fe\u50cf\u878d\u5408\u8bbe\u8ba1\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u878d\u5408\u7ed3\u679c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u878d\u5408\u4e2d\u8ba1\u7b97\u590d\u6742\u3001\u63a8\u7406\u65f6\u95f4\u5197\u957f\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u878d\u5408\u901f\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5c06Rectified Flow\u5f15\u5165\u56fe\u50cf\u878d\u5408\u4efb\u52a1\uff0c\u4f7f\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8def\u5f84\u53d8\u76f4\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u5355\u6b65\u91c7\u6837\uff1b\u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u7684VAE\u67b6\u6784\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5d4c\u5165\u878d\u5408\u64cd\u4f5c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u6765\u9002\u5e94\u56fe\u50cf\u878d\u5408\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u901f\u5ea6\u548c\u878d\u5408\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u7ec6\u8282\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\u3002", "conclusion": "RFfusion\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408Rectified Flow\u548c\u4e13\u7528VAE\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u63a8\u7406\u6162\u7684\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.16998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16998", "abs": "https://arxiv.org/abs/2509.16998", "authors": ["Nishka Khendry", "Christos Margadji", "Sebastian W. Pattinson"], "title": "IDfRA: Self-Verification for Iterative Design in Robotic Assembly", "comment": null, "summary": "As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),\nwhich is designing products for efficient automated assembly, is increasingly\nimportant. Traditional approaches to DfRA rely on manual planning, which is\ntime-consuming, expensive and potentially impractical for complex objects.\nLarge language models (LLM) have exhibited proficiency in semantic\ninterpretation and robotic task planning, stimulating interest in their\napplication to the automation of DfRA. But existing methodologies typically\nrely on heuristic strategies and rigid, hard-coded physics simulators that may\nnot translate into real-world assembly contexts. In this work, we present\nIterative Design for Robotic Assembly (IDfRA), a framework using iterative\ncycles of planning, execution, verification, and re-planning, each informed by\nself-assessment, to progressively enhance design quality within a fixed yet\ninitially under-specified environment, thereby eliminating the physics\nsimulation with the real world itself. The framework accepts as input a target\nstructure together with a partial environmental representation. Through\nsuccessive refinement, it converges toward solutions that reconcile semantic\nfidelity with physical feasibility. Empirical evaluation demonstrates that\nIDfRA attains 73.3\\% top-1 accuracy in semantic recognisability, surpassing the\nbaseline on this metric. Moreover, the resulting assembly plans exhibit robust\nphysical feasibility, achieving an overall 86.9\\% construction success rate,\nwith design quality improving across iterations, albeit not always\nmonotonically. Pairwise human evaluation further corroborates the advantages of\nIDfRA relative to alternative approaches. By integrating self-verification with\ncontext-aware adaptation, the framework evidences strong potential for\ndeployment in unstructured manufacturing scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIDfRA\u7684\u8fed\u4ee3\u5f0f\u673a\u5668\u4eba\u88c5\u914d\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u3001\u6267\u884c\u3001\u9a8c\u8bc1\u4e0e\u91cd\u89c4\u5212\u7684\u5faa\u73af\uff0c\u7ed3\u5408\u81ea\u8bc4\u4f30\u673a\u5236\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9010\u6b65\u4f18\u5316\u4ea7\u54c1\u8bbe\u8ba1\uff0c\u65e0\u9700\u4f9d\u8d56\u7269\u7406\u4eff\u771f\uff0c\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u8bed\u4e49\u53ef\u8bc6\u522b\u6027\u548c\u88c5\u914d\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u8bbe\u8ba1\uff08DfRA\uff09\u4f9d\u8d56\u4eba\u5de5\u89c4\u5212\uff0c\u8017\u65f6\u4e14\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7ed3\u6784\uff0c\u73b0\u6709\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u65b9\u6cd5\u591a\u4f9d\u8d56\u542f\u53d1\u5f0f\u7b56\u7565\u548c\u521a\u6027\u7269\u7406\u4eff\u771f\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u573a\u666f\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u4e0e\u7269\u7406\u53ef\u884c\u6027\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faIDfRA\u6846\u67b6\uff0c\u91c7\u7528\u8fed\u4ee3\u5faa\u73af\uff1a\u7531\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u88c5\u914d\u8ba1\u5212\uff0c\u6267\u884c\u540e\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\uff0c\u5e76\u6839\u636e\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u6574\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u52a8\u6001\u91cd\u89c4\u5212\uff0c\u9010\u6b65\u63d0\u5347\u8bbe\u8ba1\u8d28\u91cf\uff0c\u907f\u514d\u4f7f\u7528\u9884\u8bbe\u7269\u7406\u4eff\u771f\u5668\u3002", "result": "IDfRA\u5728\u8bed\u4e49\u53ef\u8bc6\u522b\u6027\u4e0a\u8fbe\u523073.3%\u7684top-1\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\uff1b\u88c5\u914d\u6210\u529f\u7387\u8fbe86.9%\uff0c\u8bbe\u8ba1\u8d28\u91cf\u968f\u8fed\u4ee3\u6574\u4f53\u63d0\u5347\uff1b\u4eba\u5de5\u5bf9\u6bd4\u8bc4\u4f30\u4e5f\u663e\u793a\u5176\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "IDfRA\u901a\u8fc7\u878d\u5408\u81ea\u9a8c\u8bc1\u4e0e\u4e0a\u4e0b\u6587\u9002\u5e94\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u88c5\u914d\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5177\u6709\u5728\u975e\u7ed3\u6784\u5316\u5236\u9020\u73af\u5883\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.16564", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.16564", "abs": "https://arxiv.org/abs/2509.16564", "authors": ["Jun Rong Brian Chong", "Yixuan Tang", "Anthony K. H. Tung"], "title": "MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs", "comment": "35 pages, 8 figures", "summary": "Misinformation evolves as it spreads, shifting in language, framing, and\nmoral emphasis to adapt to new audiences. However, current misinformation\ndetection approaches implicitly assume that misinformation is static. We\nintroduce MPCG, a multi-round, persona-conditioned framework that simulates how\nclaims are iteratively reinterpreted by agents with distinct ideological\nperspectives. Our approach uses an uncensored large language model (LLM) to\ngenerate persona-specific claims across multiple rounds, conditioning each\ngeneration on outputs from the previous round, enabling the study of\nmisinformation evolution. We evaluate the generated claims through human and\nLLM-based annotations, cognitive effort metrics (readability, perplexity),\nemotion evocation metrics (sentiment analysis, morality), clustering,\nfeasibility, and downstream classification. Results show strong agreement\nbetween human and GPT-4o-mini annotations, with higher divergence in fluency\njudgments. Generated claims require greater cognitive effort than the original\nclaims and consistently reflect persona-aligned emotional and moral framing.\nClustering and cosine similarity analyses confirm semantic drift across rounds\nwhile preserving topical coherence. Feasibility results show a 77% feasibility\nrate, confirming suitability for downstream tasks. Classification results\nreveal that commonly used misinformation detectors experience macro-F1\nperformance drops of up to 49.7%. The code is available at\nhttps://github.com/bcjr1997/MPCG", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u3001\u57fa\u4e8e\u89d2\u8272\u6761\u4ef6\u7684\u6846\u67b6MPCG\uff0c\u7528\u4e8e\u6a21\u62df\u865a\u5047\u4fe1\u606f\u5728\u4e0d\u540c\u610f\u8bc6\u5f62\u6001\u4ee3\u7406\u95f4\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u5728\u5e94\u5bf9\u52a8\u6001\u865a\u5047\u4fe1\u606f\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u865a\u5047\u4fe1\u606f\u5185\u5bb9\u662f\u9759\u6001\u7684\uff0c\u4f46\u73b0\u5b9e\u4e2d\u865a\u5047\u4fe1\u606f\u5728\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u4f1a\u4e0d\u65ad\u6f14\u53d8\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5176\u52a8\u6001\u6f14\u5316\u7279\u6027\u4ee5\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51faMPCG\u6846\u67b6\uff0c\u5229\u7528\u65e0\u5ba1\u67e5\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u8f6e\u751f\u6210\u4e2d\u6839\u636e\u524d\u4e00\u8f6e\u8f93\u51fa\u548c\u7279\u5b9a\u610f\u8bc6\u5f62\u6001\u89d2\u8272\u751f\u6210\u65b0\u7684\u865a\u5047\u58f0\u660e\uff0c\u6a21\u62df\u4fe1\u606f\u6f14\u5316\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u751f\u6210\u7684\u58f0\u660e\u5728\u8bed\u4e49\u4e0a\u53d1\u751f\u6f02\u79fb\u4f46\u4ecd\u4fdd\u6301\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u7684\u8ba4\u77e5\u52aa\u529b\uff0c\u4e14\u60c5\u611f\u4e0e\u9053\u5fb7\u503e\u5411\u4e0e\u89d2\u8272\u4e00\u81f4\uff1b\u4eba\u7c7b\u4e0eGPT-4o-mini\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\uff1b77%\u7684\u751f\u6210\u58f0\u660e\u5177\u6709\u53ef\u884c\u6027\uff1b\u73b0\u6709\u68c0\u6d4b\u5668\u7684macro-F1\u6700\u9ad8\u4e0b\u964d49.7%\u3002", "conclusion": "\u865a\u5047\u4fe1\u606f\u7684\u52a8\u6001\u6f14\u5316\u5bf9\u5f53\u524d\u68c0\u6d4b\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u6311\u6218\uff0cMPCG\u4e3a\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5f3a\u8c03\u9700\u5f00\u53d1\u80fd\u5e94\u5bf9\u4fe1\u606f\u6f14\u5316\u7684\u65b0\u578b\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2509.16548", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16548", "abs": "https://arxiv.org/abs/2509.16548", "authors": ["Yuyang Ding", "Xinyu Shi", "Juntao Li", "Xiaobo Liang", "Zhaopeng Tu", "Min Zhang"], "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning", "comment": "NeurIPS 2025. Project page: https://scan-prm.github.io/", "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.", "AI": {"tldr": "\u63d0\u51faSelf-Denoising Monte Carlo Annotation (SCAN)\u6846\u67b6\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u5e76\u5b9e\u73b0\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u5728ProcessBench\u4e0a\u5927\u5e45\u63d0\u5347PRM\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u5408\u6210\u6570\u636e\u566a\u58f0\u9ad8\u3001\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347PRM\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "method": "\u5206\u6790MC\u5408\u6210\u6570\u636e\u4e2d\u7684\u566a\u58f0\u5206\u5e03\uff0c\u8bbe\u8ba1\u81ea\u53bb\u566a\u7b56\u7565\uff08SCAN\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u751f\u6210\u66f4\u51c6\u786e\u7684\u6b65\u9aa4\u7ea7\u6807\u6ce8\uff0c\u5e76\u7ed3\u5408\u9c81\u68d2\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884cPRM\u8bad\u7ec3\u3002", "result": "SCAN\u4ec5\u75286%\u7684\u63a8\u7406\u6210\u672c\u5373\u8d85\u8d8a\u6807\u51c6MC\u65b9\u6cd5\uff0cPRM\u5728ProcessBench\u4e0aF1\u63d0\u534739.2\uff08\u4ece19.9\u523059.1\uff09\uff0c\u4e14\u4f18\u4e8e\u57fa\u4e8e\u5927\u89c4\u6a21\u4eba\u5de5\u6570\u636e\uff08\u5982PRM800K\uff09\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u5907\u826f\u597d\u6570\u636e\u6269\u5c55\u6027\u3002", "conclusion": "SCAN\u4e3aPRM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u6a21\u578b\u7ed3\u5408\u81ea\u53bb\u566a\u673a\u5236\u5728\u5f31\u76d1\u7763\u4e0b\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.16958", "categories": ["cs.AI", "I.2.0; I.2.1; I.2.3; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.16958", "abs": "https://arxiv.org/abs/2509.16958", "authors": ["Remo Pareschi"], "title": "Quantum Abduction: A New Paradigm for Reasoning under Uncertainty", "comment": "23 pages, 8 figures, 3 tables; submitted to Sci, MDPI", "summary": "Abductive reasoning - the search for plausible explanations - has long been\ncentral to human inquiry, from forensics to medicine and scientific discovery.\nYet formal approaches in AI have largely reduced abduction to eliminative\nsearch: hypotheses are treated as mutually exclusive, evaluated against\nconsistency constraints or probability updates, and pruned until a single\n\"best\" explanation remains. This reductionist framing overlooks the way human\nreasoners sustain multiple explanatory lines in suspension, navigate\ncontradictions, and generate novel syntheses. This paper introduces quantum\nabduction, a non-classical paradigm that models hypotheses in superposition,\nallows them to interfere constructively or destructively, and collapses only\nwhen coherence with evidence is reached. Grounded in quantum cognition and\nimplemented with modern NLP embeddings and generative AI, the framework\nsupports dynamic synthesis rather than premature elimination. Case studies span\nhistorical mysteries (Ludwig II of Bavaria, the \"Monster of Florence\"),\nliterary demonstrations (\"Murder on the Orient Express\"), medical diagnosis,\nand scientific theory change. Across these domains, quantum abduction proves\nmore faithful to the constructive and multifaceted nature of human reasoning,\nwhile offering a pathway toward expressive and transparent AI reasoning\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u91cf\u5b50\u6eaf\u56e0\u201d\uff08quantum abduction\uff09\u8fd9\u4e00\u975e\u7ecf\u5178\u63a8\u7406\u8303\u5f0f\uff0c\u5229\u7528\u91cf\u5b50\u8ba4\u77e5\u548c\u73b0\u4ee3NLP\u6280\u672f\uff0c\u5c06\u5047\u8bbe\u7f6e\u4e8e\u53e0\u52a0\u6001\u4e2d\u8fdb\u884c\u52a8\u6001\u5408\u6210\uff0c\u66f4\u8d34\u8fd1\u4eba\u7c7b\u591a\u7ebf\u7d22\u5e76\u884c\u3001\u77db\u76fe\u5171\u5b58\u4e14\u521b\u9020\u6027\u6574\u5408\u7684\u63a8\u7406\u65b9\u5f0f\u3002", "motivation": "\u4f20\u7edfAI\u4e2d\u7684\u6eaf\u56e0\u63a8\u7406\u5f80\u5f80\u5c06\u5047\u8bbe\u89c6\u4e3a\u4e92\u65a5\u9009\u9879\uff0c\u901a\u8fc7\u6392\u9664\u6cd5\u5bfb\u627e\u552f\u4e00\u6700\u4f73\u89e3\u91ca\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u5728\u5b9e\u9645\u63a8\u7406\u4e2d\u5e38\u4fdd\u6301\u591a\u4e2a\u5047\u8bbe\u5e76\u884c\u3001\u5904\u7406\u77db\u76fe\u5e76\u751f\u6210\u65b0\u7efc\u5408\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u80fd\u6a21\u62df\u4eba\u7c7b\u590d\u6742\u63a8\u7406\u8fc7\u7a0b\u7684\u65b0\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u91cf\u5b50\u8ba4\u77e5\u7406\u8bba\uff0c\u5c06\u5047\u8bbe\u8868\u793a\u4e3a\u53e0\u52a0\u6001\uff0c\u5141\u8bb8\u5176\u53d1\u751f\u76f8\u957f\u6216\u76f8\u6d88\u5e72\u6d89\uff0c\u5e76\u4ec5\u5728\u4e0e\u8bc1\u636e\u8fbe\u6210\u6574\u4f53\u4e00\u81f4\u6027\u65f6\u624d\u574d\u7f29\u4e3a\u5177\u4f53\u89e3\u91ca\uff1b\u7ed3\u5408NLP\u5d4c\u5165\u548c\u751f\u6210\u5f0fAI\u5b9e\u73b0\u8be5\u6a21\u578b\u3002", "result": "\u5728\u5386\u53f2\u8c1c\u6848\u3001\u6587\u5b66\u6848\u4f8b\u3001\u533b\u5b66\u8bca\u65ad\u548c\u79d1\u5b66\u7406\u8bba\u6f14\u53d8\u7b49\u591a\u4e2a\u9886\u57df\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u91cf\u5b50\u6eaf\u56e0\u5c55\u73b0\u51fa\u5bf9\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u652f\u6301\u52a8\u6001\u5047\u8bbe\u751f\u6210\u4e0e\u878d\u5408\uff0c\u907f\u514d\u8fc7\u65e9\u6392\u9664\u6f5c\u5728\u89e3\u91ca\u3002", "conclusion": "\u91cf\u5b50\u6eaf\u56e0\u4e3aAI\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u66f4\u5177\u8868\u8fbe\u529b\u548c\u900f\u660e\u6027\u7684\u8def\u5f84\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u4eba\u7c7b\u590d\u6742\u7684\u3001\u591a\u7ef4\u5ea6\u7684\u6eaf\u56e0\u8fc7\u7a0b\uff0c\u63a8\u52a8AI\u5411\u66f4\u63a5\u8fd1\u4eba\u7c7b\u601d\u7ef4\u65b9\u5f0f\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2509.16552", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16552", "abs": "https://arxiv.org/abs/2509.16552", "authors": ["Xiaoyang Yan", "Muleilan Pei", "Shaojie Shen"], "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting", "comment": null, "summary": "3D occupancy prediction is critical for comprehensive scene understanding in\nvision-centric autonomous driving. Recent advances have explored utilizing 3D\nsemantic Gaussians to model occupancy while reducing computational overhead,\nbut they remain constrained by insufficient multi-view spatial interaction and\nlimited multi-frame temporal consistency. To overcome these issues, in this\npaper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework\nto enhance both spatial and temporal modeling in existing Gaussian-based\npipelines. Specifically, we develop a guidance-informed spatial aggregation\nstrategy within a dual-mode attention mechanism to strengthen spatial\ninteraction in Gaussian representations. Furthermore, we introduce a\ngeometry-aware temporal fusion scheme that effectively leverages historical\ncontext to improve temporal continuity in scene completion. Extensive\nexperiments on the large-scale nuScenes occupancy prediction benchmark showcase\nthat our proposed approach not only achieves state-of-the-art performance but\nalso delivers markedly better temporal consistency compared to existing\nGaussian-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u7a7a\u9ad8\u65af\u70b9\u9635\u5316\uff08ST-GS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u9ad8\u65af\u76843D\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\u5728\u7a7a\u95f4\u4ea4\u4e92\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u8bed\u4e49\u9ad8\u65af\u7684\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u7a7a\u95f4\u4ea4\u4e92\u548c\u591a\u5e27\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u5f15\u5bfc\u4fe1\u606f\u9a71\u52a8\u7684\u7a7a\u95f4\u805a\u5408\u7b56\u7565\uff08\u7ed3\u5408\u53cc\u6a21\u6ce8\u610f\u529b\u673a\u5236\uff09\u4ee5\u589e\u5f3a\u9ad8\u65af\u8868\u793a\u4e2d\u7684\u7a7a\u95f4\u4ea4\u4e92\uff0c\u5e76\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u7684\u65f6\u95f4\u878d\u5408\u65b9\u6848\uff0c\u5229\u7528\u5386\u53f2\u4e0a\u4e0b\u6587\u63d0\u5347\u573a\u666f\u8865\u5168\u7684\u65f6\u95f4\u8fde\u7eed\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21nuScenes\u5360\u636e\u9884\u6d4b\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u9ad8\u65af\u7684\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "ST-GS\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u9ad8\u65af\u7684\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\u5728\u7a7a\u95f4\u4e0e\u65f6\u95f4\u5efa\u6a21\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9\u4e3b\u5bfc\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u573a\u666f\u7406\u89e3\u65b9\u6848\u3002"}}
{"id": "2509.17010", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17010", "abs": "https://arxiv.org/abs/2509.17010", "authors": ["Rajpal Singh", "Aditya Singh", "Chidre Shravista Kashyap", "Jishnu Keshavan"], "title": "Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems", "comment": null, "summary": "This paper presents a novel Koopman operator formulation for Euler Lagrangian\ndynamics that employs an implicit generalized momentum-based state space\nrepresentation, which decouples a known linear actuation channel from state\ndependent dynamics and makes the system more amenable to linear Koopman\nmodeling. By leveraging this structural separation, the proposed formulation\nonly requires to learn the unactuated dynamics rather than the complete\nactuation dependent system, thereby significantly reducing the number of\nlearnable parameters, improving data efficiency, and lowering overall model\ncomplexity. In contrast, conventional explicit formulations inherently couple\ninputs with the state dependent terms in a nonlinear manner, making them more\nsuitable for bilinear Koopman models, which are more computationally expensive\nto train and deploy. Notably, the proposed scheme enables the formulation of\nlinear models that achieve superior prediction performance compared to\nconventional bilinear models while remaining substantially more efficient. To\nrealize this framework, we present two neural network architectures that\nconstruct Koopman embeddings from actuated or unactuated data, enabling\nflexible and efficient modeling across different tasks. Robustness is ensured\nthrough the integration of a linear Generalized Extended State Observer (GESO),\nwhich explicitly estimates disturbances and compensates for them in real time.\nThe combined momentum-based Koopman and GESO framework is validated through\ncomprehensive trajectory tracking simulations and experiments on robotic\nmanipulators, demonstrating superior accuracy, robustness, and learning\nefficiency relative to state of the art alternatives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKoopman\u7b97\u5b50\u7684\u9690\u5f0f\u5e7f\u4e49\u52a8\u91cf\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u901a\u8fc7\u89e3\u8026\u8f93\u5165\u901a\u9053\u4e0e\u72b6\u6001\u52a8\u6001\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u663e\u5f0fKoopman\u6a21\u578b\u5c06\u8f93\u5165\u4e0e\u72b6\u6001\u975e\u7ebf\u6027\u8026\u5408\uff0c\u5bfc\u81f4\u9700\u4f7f\u7528\u8ba1\u7b97\u6602\u8d35\u7684\u53cc\u7ebf\u6027\u6a21\u578b\uff1b\u4e3a\u63d0\u9ad8\u5efa\u6a21\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u89e3\u8026\u8f93\u5165\u4e0e\u52a8\u6001\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u5e7f\u4e49\u52a8\u91cf\u72b6\u6001\u8868\u793a\uff0c\u5206\u79bb\u5df2\u77e5\u7684\u7ebf\u6027\u9a71\u52a8\u901a\u9053\u4e0e\u72b6\u6001\u76f8\u5173\u52a8\u6001\uff0c\u4ec5\u9700\u5b66\u4e60\u65e0\u9a71\u52a8\u90e8\u5206\u7684\u52a8\u529b\u5b66\uff1b\u8bbe\u8ba1\u4e24\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6784\u5efaKoopman\u5d4c\u5165\uff0c\u5e76\u7ed3\u5408\u7ebf\u6027\u5e7f\u4e49\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668\uff08GESO\uff09\u5b9e\u65f6\u4f30\u8ba1\u5e76\u8865\u507f\u6270\u52a8\u3002", "result": "\u5728\u673a\u5668\u4eba manipulator \u4e0a\u7684\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u53cc\u7ebf\u6027Koopman\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3001\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u66f4\u4f4e\u7684\u8bad\u7ec3\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53ef\u5b66\u4e60\u53c2\u6570\u6570\u91cf\uff0c\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u91cf\u57faKoopman\u4e0eGESO\u6846\u67b6\u4e3a\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u5efa\u6a21\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u7b49\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2509.16584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16584", "abs": "https://arxiv.org/abs/2509.16584", "authors": ["Benlu Wang", "Iris Xia", "Yifan Zhang", "Junda Wang", "Feiyun Ouyang", "Shuo Han", "Arman Cohan", "Hong Yu", "Zonghai Yao"], "title": "From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations", "comment": "Equal contribution for the first two authors. To appear as an Oral\n  presentation in the proceedings of the Main Conference on Empirical Methods\n  in Natural Language Processing (EMNLP) 2025", "summary": "Large language models (LLMs) have demonstrated promising performance on\nmedical benchmarks; however, their ability to perform medical calculations, a\ncrucial aspect of clinical decision-making, remains underexplored and poorly\nevaluated. Existing benchmarks often assess only the final answer with a wide\nnumerical tolerance, overlooking systematic reasoning failures and potentially\ncausing serious clinical misjudgments. In this work, we revisit medical\ncalculation evaluation with a stronger focus on clinical trustworthiness.\nFirst, we clean and restructure the MedCalc-Bench dataset and propose a new\nstep-by-step evaluation pipeline that independently assesses formula selection,\nentity extraction, and arithmetic computation. Under this granular framework,\nthe accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by\nprior evaluations. Second, we introduce an automatic error analysis framework\nthat generates structured attribution for each failure mode. Human evaluation\nconfirms its alignment with expert judgment, enabling scalable and explainable\ndiagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that\ncombines retrieval-augmented generation and Python-based code execution.\nWithout any fine-tuning, MedRaC improves the accuracy of different LLMs from\n16.35% up to 53.19%. Our work highlights the limitations of current benchmark\npractices and proposes a more clinically faithful methodology. By enabling\ntransparent and transferable reasoning evaluation, we move closer to making\nLLM-based systems trustworthy for real-world medical applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u6ce8\u91cd\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u5316\u9519\u8bef\u5206\u6790\u548c\u6a21\u5757\u5316\u4ee3\u7406\u6d41\u6c34\u7ebfMedRaC\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u8ba1\u7b97\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u4e14\u5bb9\u5fcd\u5ea6\u5bbd\u677e\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u7cfb\u7edf\u6027\u9519\u8bef\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u4e34\u5e8a\u8bef\u5224\u3002\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u7ec6\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u63d0\u5347\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "method": "1) \u6e05\u6d17\u5e76\u91cd\u6784MedCalc-Bench\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u5206\u6b65\u8bc4\u4f30\u6d41\u7a0b\uff0c\u72ec\u7acb\u8bc4\u4f30\u516c\u5f0f\u9009\u62e9\u3001\u5b9e\u4f53\u63d0\u53d6\u548c\u7b97\u672f\u8ba1\u7b97\uff1b3) \u6784\u5efa\u81ea\u52a8\u5316\u9519\u8bef\u5206\u6790\u6846\u67b6\u8fdb\u884c\u5f52\u56e0\uff1b4) \u8bbe\u8ba1\u65e0\u9700\u5fae\u8c03\u7684\u6a21\u5757\u5316\u4ee3\u7406\u6d41\u6c34\u7ebfMedRaC\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0e\u57fa\u4e8ePython\u7684\u4ee3\u7801\u6267\u884c\u3002", "result": "\u5728\u65b0\u8bc4\u4f30\u6846\u67b6\u4e0b\uff0cGPT-4o\u7684\u51c6\u786e\u7387\u4ece62.7%\u964d\u81f343.6%\uff0c\u63ed\u793a\u4e86\u539f\u6709\u8bc4\u4f30\u4e2d\u88ab\u63a9\u76d6\u7684\u9519\u8bef\uff1b\u81ea\u52a8\u5316\u9519\u8bef\u5206\u6790\u6846\u67b6\u4e0e\u4e13\u5bb6\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff1bMedRaC\u5c06\u4e0d\u540cLLM\u7684\u51c6\u786e\u7387\u4ece16.35%\u63d0\u5347\u81f3\u6700\u9ad853.19%\u3002", "conclusion": "\u5f53\u524d\u533b\u5b66\u8ba1\u7b97\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3001\u53ef\u89e3\u91ca\u9519\u8bef\u5206\u6790\u548c\u6a21\u5757\u5316\u63a8\u7406\u67b6\u6784\u4e3a\u6784\u5efa\u53ef\u4fe1\u7684\u533b\u5b66\u5927\u6a21\u578b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8def\u5f84\u3002"}}
{"id": "2509.16554", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16554", "abs": "https://arxiv.org/abs/2509.16554", "authors": ["Vahid Jebraeeli", "Hamid Krim", "Derya Cansever"], "title": "ViTCAE: ViT-based Class-conditioned Autoencoder", "comment": "-", "summary": "Vision Transformer (ViT) based autoencoders often underutilize the global\nClass token and employ static attention mechanisms, limiting both generative\ncontrol and optimization efficiency. This paper introduces ViTCAE, a framework\nthat addresses these issues by re-purposing the Class token into a generative\nlinchpin. In our architecture, the encoder maps the Class token to a global\nlatent variable that dictates the prior distribution for local, patch-level\nlatent variables, establishing a robust dependency where global semantics\ndirectly inform the synthesis of local details. Drawing inspiration from\nopinion dynamics, we treat each attention head as a dynamical system of\ninteracting tokens seeking consensus. This perspective motivates a\nconvergence-aware temperature scheduler that adaptively anneals each head's\ninfluence function based on its distributional stability. This process enables\na principled head-freezing mechanism, guided by theoretically-grounded\ndiagnostics like an attention evolution distance and a consensus/cluster\nfunctional. This technique prunes converged heads during training to\nsignificantly improve computational efficiency without sacrificing fidelity. By\nunifying a generative Class token with an adaptive attention mechanism rooted\nin multi-agent consensus theory, ViTCAE offers a more efficient and\ncontrollable approach to transformer-based generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ViTCAE\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784Vision Transformer\u4e2d\u7684Class token\u4f5c\u4e3a\u751f\u6210\u6838\u5fc3\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5171\u8bc6\u7406\u8bba\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u6a21\u578b\u7684\u751f\u6210\u63a7\u5236\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVision Transformer\u7684\u81ea\u7f16\u7801\u5668\u672a\u80fd\u5145\u5206\u5229\u7528\u5168\u5c40Class token\uff0c\u4e14\u91c7\u7528\u9759\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9650\u5236\u4e86\u751f\u6210\u63a7\u5236\u4e0e\u4f18\u5316\u6548\u7387\u3002", "method": "\u5c06\u7f16\u7801\u5668\u4e2d\u7684Class token\u6620\u5c04\u4e3a\u5168\u5c40\u6f5c\u5728\u53d8\u91cf\uff0c\u6307\u5bfc\u5c40\u90e8patch\u7ea7\u6f5c\u5728\u53d8\u91cf\u7684\u5148\u9a8c\u5206\u5e03\uff1b\u53d7\u8206\u8bba\u52a8\u529b\u5b66\u542f\u53d1\uff0c\u5c06\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u89c6\u4e3a\u5bfb\u6c42\u5171\u8bc6\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u5206\u5e03\u7a33\u5b9a\u6027\u7684\u6536\u655b\u611f\u77e5\u6e29\u5ea6\u8c03\u5ea6\u5668\uff0c\u5b9e\u73b0\u6ce8\u610f\u529b\u5934\u7684\u52a8\u6001\u51bb\u7ed3\u3002", "result": "ViTCAE\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u8bad\u7ec3\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u526a\u679d\u5df2\u6536\u655b\u7684\u6ce8\u610f\u529b\u5934\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5168\u5c40\u8bed\u4e49\u5bf9\u5c40\u90e8\u7ec6\u8282\u751f\u6210\u7684\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "ViTCAE\u901a\u8fc7\u7edf\u4e00\u751f\u6210\u5f0fClass token\u4e0e\u57fa\u4e8e\u5171\u8bc6\u7406\u8bba\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3aTransformer-based\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u65b0\u67b6\u6784\u3002"}}
{"id": "2509.17037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17037", "abs": "https://arxiv.org/abs/2509.17037", "authors": ["Yajing Yang", "Tony Deng", "Min-Yen Kan"], "title": "KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration", "comment": "Accepted at EMNLP 2025 Findings", "summary": "We propose KAHAN, a knowledge-augmented hierarchical framework that\nsystematically extracts insights from raw tabular data at entity, pairwise,\ngroup, and system levels. KAHAN uniquely leverages LLMs as domain experts to\ndrive the analysis. On DataTales financial reporting benchmark, KAHAN\noutperforms existing approaches by over 20% on narrative quality (GPT-4o),\nmaintains 98.2% factuality, and demonstrates practical utility in human\nevaluation. Our results reveal that knowledge quality drives model performance\nthrough distillation, hierarchical analysis benefits vary with market\ncomplexity, and the framework transfers effectively to healthcare domains. The\ndata and code are available at https://github.com/yajingyang/kahan.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKAHAN\u7684\u77e5\u8bc6\u589e\u5f3a\u578b\u5206\u5c42\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9886\u57df\u4e13\u5bb6\uff0c\u4ece\u8868\u683c\u6570\u636e\u4e2d\u63d0\u53d6\u591a\u5c42\u6b21\u6d1e\u5bdf\uff0c\u5728\u91d1\u878d\u62a5\u544a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u4e8b\u5b9e\u6027\u548c\u8de8\u9886\u57df\u9002\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u4ece\u590d\u6742\u7684\u8868\u683c\u6570\u636e\u4e2d\u7cfb\u7edf\u6027\u5730\u63d0\u53d6\u591a\u5c42\u6b21\u7684\u6df1\u5ea6\u6d1e\u5bdf\uff0c\u5e76\u63d0\u5347\u81ea\u52a8\u53d9\u4e8b\u751f\u6210\u7684\u8d28\u91cf\u4e0e\u4e8b\u5b9e\u6027\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u77e5\u8bc6\u589e\u5f3a\u7684\u5206\u5c42\u5206\u6790\u6846\u67b6\uff08KAHAN\uff09\uff0c\u5728\u5b9e\u4f53\u3001\u6210\u5bf9\u3001\u7ec4\u548c\u7cfb\u7edf\u5c42\u7ea7\u4e0a\u63d0\u53d6\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9886\u57df\u4e13\u5bb6\u6307\u5bfc\u5206\u6790\u8fc7\u7a0b\u3002", "result": "\u5728DataTales\u91d1\u878d\u62a5\u544a\u57fa\u51c6\u4e0a\uff0c\u53d9\u4e8b\u8d28\u91cf\uff08GPT-4o\u8bc4\u5206\uff09\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd520%\u4ee5\u4e0a\uff0c\u4e8b\u5b9e\u6027\u8fbe\u523098.2%\uff0c\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6548\u7528\uff0c\u4e14\u6846\u67b6\u5728\u533b\u7597\u9886\u57df\u8868\u73b0\u51fa\u826f\u597d\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u77e5\u8bc6\u548c\u5206\u5c42\u5206\u6790\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cKAHAN\u6846\u67b6\u5728\u590d\u6742\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u8d8a\u8868\u73b0\u548c\u8de8\u9886\u57df\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16557", "categories": ["cs.CV", "cs.ET", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16557", "abs": "https://arxiv.org/abs/2509.16557", "authors": ["Muhammad Hamza", "Danish Hamid", "Muhammad Tahir Akram"], "title": "Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose", "comment": "21 pages, 8 figures, 7 tables. Preprint of a manuscript submitted to\n  CCF Transactions on Pervasive Computing and Interaction (Springer), currently\n  under review", "summary": "Human-Object Interaction Recognition (HOIR) and user identification play a\ncrucial role in advancing augmented reality (AR)-based personalized assistive\ntechnologies. These systems are increasingly being deployed in high-stakes,\nhuman-centric environments such as aircraft cockpits, aerospace maintenance,\nand surgical procedures. This research introduces I2S (Interact2Sign), a multi\nstage framework designed for unobtrusive user identification through human\nobject interaction recognition, leveraging 3D hand pose analysis in egocentric\nvideos. I2S utilizes handcrafted features extracted from 3D hand poses and per\nforms sequential feature augmentation: first identifying the object class,\nfollowed by HOI recognition, and ultimately, user identification. A\ncomprehensive feature extraction and description process was carried out for 3D\nhand poses, organizing the extracted features into semantically meaningful\ncategories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor\nintroduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive\nablation studies were conducted to determine the most effective combination of\nfeatures. The optimal configuration achieved an impressive average F1-score of\n97.52% for user identification, evaluated on a bimanual object manipulation\ndataset derived from the ARCTIC and H2O datasets. I2S demonstrates\nstate-of-the-art performance while maintaining a lightweight model size of\nunder 4 MB and a fast inference time of 0.1 seconds. These characteristics make\nthe proposed framework highly suitable for real-time, on-device authentication\nin security-critical, AR-based systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aI2S\uff08Interact2Sign\uff09\u7684\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc73D\u624b\u90e8\u59ff\u6001\u5206\u6790\u5b9e\u73b0\u57fa\u4e8e\u4eba-\u7269\u4ea4\u4e92\u8bc6\u522b\u7684\u65e0\u611f\u7528\u6237\u8eab\u4efd\u8ba4\u8bc1\uff0c\u9002\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u9ad8\u5b89\u5168\u6027\u573a\u666f\u3002", "motivation": "\u5728\u822a\u7a7a\u3001\u822a\u5929\u7ef4\u62a4\u548c\u624b\u672f\u7b49\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0cAR\u8f85\u52a9\u6280\u672f\u9700\u8981\u51c6\u786e\u4e14\u975e\u4fb5\u5165\u5f0f\u7684\u7528\u6237\u8bc6\u522b\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u6027\u3002", "method": "I2S\u6846\u67b6\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u63d0\u53d63D\u624b\u90e8\u59ff\u6001\u7684\u624b\u5de5\u7279\u5f81\uff0c\u5e76\u8fdb\u884c\u5206\u9636\u6bb5\u7279\u5f81\u589e\u5f3a\uff1a\u5148\u8bc6\u522b\u7269\u4f53\u7c7b\u522b\uff0c\u518d\u8bc6\u522b\u4eba-\u7269\u4ea4\u4e92\uff0c\u6700\u7ec8\u5b9e\u73b0\u7528\u6237\u8bc6\u522b\uff1b\u63d0\u51fa\u4e86\u65b0\u7684\u7279\u5f81\u63cf\u8ff0\u5b50IHSE\uff0c\u5e76\u5c06\u7279\u5f81\u5206\u4e3a\u7a7a\u95f4\u3001\u9891\u7387\u3001\u8fd0\u52a8\u5b66\u3001\u65b9\u5411\u548c\u53cc\u624b\u673a\u95f4\u7a7a\u95f4\u5305\u7edc\uff08IHSE\uff09\u4e94\u7c7b\u3002", "result": "\u5728ARCTIC\u548cH2O\u6570\u636e\u96c6\u6784\u5efa\u7684\u53cc\u624b\u64cd\u4f5c\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u4f18\u914d\u7f6e\u5b9e\u73b0\u4e8697.52%\u7684\u5e73\u5747F1\u5206\u6570\uff0c\u6a21\u578b\u5927\u5c0f\u5c0f\u4e8e4MB\uff0c\u63a8\u7406\u65f6\u95f4\u4ec5\u4e3a0.1\u79d2\u3002", "conclusion": "I2S\u5728\u7528\u6237\u8bc6\u522b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u517c\u5177\u8f7b\u91cf\u5316\u548c\u9ad8\u6548\u63a8\u7406\u80fd\u529b\uff0c\u9002\u5408\u7528\u4e8e\u5b89\u5168\u5173\u952e\u578bAR\u7cfb\u7edf\u7684\u5b9e\u65f6\u8bbe\u5907\u7aef\u8ba4\u8bc1\u3002"}}
{"id": "2509.17042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17042", "abs": "https://arxiv.org/abs/2509.17042", "authors": ["Zengqi Peng", "Yusen Xie", "Yubin Wang", "Rui Yang", "Qifeng Chen", "Jun Ma"], "title": "Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning", "comment": null, "summary": "The advancement of foundation models fosters new initiatives for policy\nlearning in achieving safe and efficient autonomous driving. However, a\ncritical bottleneck lies in the manual engineering of reward functions and\ntraining curricula for complex and dynamic driving tasks, which is a\nlabor-intensive and time-consuming process. To address this problem, we propose\nOGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning\nframework that leverages vision-language model (VLM)-based multi-agent\ncollaboration. Our framework capitalizes on advanced reasoning and multimodal\nunderstanding capabilities of VLMs to construct a hierarchical agent system.\nSpecifically, a centralized orchestrator plans high-level training objectives,\nwhile a generation module employs a two-step analyze-then-generate process for\nefficient generation of reward-curriculum pairs. A reflection module then\nfacilitates iterative optimization based on the online evaluation. Furthermore,\na dedicated memory module endows the VLM agents with the capabilities of\nlong-term memory. To enhance robustness and diversity of the generation\nprocess, we introduce a parallel generation scheme and a human-in-the-loop\ntechnique for augmentation of the reward observation space. Through efficient\nmulti-agent cooperation and leveraging rich multimodal information, OGR enables\nthe online evolution of reinforcement learning policies to acquire\ninteraction-aware driving skills. Extensive experiments in the CARLA simulator\ndemonstrate the superior performance, robust generalizability across distinct\nurban scenarios, and strong compatibility with various RL algorithms. Further\nreal-world experiments highlight the practical viability and effectiveness of\nour framework. The source code will be available upon acceptance of the paper.", "AI": {"tldr": "\u63d0\u51faOGR\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u81ea\u52a8\u5316\u751f\u6210\u5956\u52b1-\u8bfe\u7a0b\u5bf9\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u624b\u52a8\u8bbe\u8ba1\u590d\u6742\u52a8\u6001\u9a7e\u9a76\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\u548c\u8bad\u7ec3\u8bfe\u7a0b\u8017\u65f6\u8d39\u529b\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u62ec\u534f\u8c03\u5668\u3001\u751f\u6210\u6a21\u5757\u3001\u53cd\u601d\u6a21\u5757\u548c\u8bb0\u5fc6\u6a21\u5757\uff0c\u91c7\u7528\u5e76\u884c\u751f\u6210\u548c\u4eba\u673a\u534f\u540c\u589e\u5f3a\u5956\u52b1\u89c2\u6d4b\u7a7a\u95f4\u3002", "result": "\u5728CARLA\u4eff\u771f\u5668\u4e2d\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u8d8a\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u4e0e\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u517c\u5bb9\u6027\uff0c\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "OGR\u6846\u67b6\u80fd\u6709\u6548\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u5728\u7ebf\u6f14\u5316\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16589", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16589", "abs": "https://arxiv.org/abs/2509.16589", "authors": ["Qiongqiong Wang", "Hardik Bhupendra Sailor", "Tianchi Liu", "Wenyu Zhang", "Muhammad Huzaifah", "Nattadaporn Lertcheva", "Shuo Sun", "Nancy F. Chen", "Jinyang Wu", "AiTi Aw"], "title": "Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data", "comment": "Accepted in EMNLP Findings 2025", "summary": "Recent speech-LLMs have shown impressive performance in tasks like\ntranscription and translation, yet they remain limited in understanding the\nparalinguistic aspects of speech crucial for social and emotional intelligence.\nWe propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual\nparalinguistic reasoning the integration of verbal content with non-verbal cues\nlike emotion and prosody. The benchmark includes two curated question answering\n(QA) datasets requiring both linguistic and empathetic understanding. We\nevaluate state-of-the-art speech-LLMs from both open and closed-source models\nand perform a comprehensive analysis across different question types. The top\ntwo models were further analyzed under temperature tuning to understand its\neffect on this task. Our benchmark reveals a key gap in existing evaluations\nand offers insights into building more context-aware and emotionally\nintelligent speech-capable LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CP-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u5883\u5316\u526f\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u60c5\u611f\u4e0e\u8bed\u8c03\u7b49\u975e\u8bed\u8a00\u4fe1\u606f\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u8a00\u8bed\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6355\u6349\u60c5\u611f\u3001\u8bed\u8c03\u7b49\u526f\u8bed\u8a00\u4fe1\u606f\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u7f3a\u4e4f\u5bf9\u793e\u4f1a\u548c\u60c5\u611f\u667a\u80fd\u6240\u9700\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1\u4e86CP-Bench\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u9700\u8981\u8bed\u8a00\u548c\u5171\u60c5\u7406\u89e3\u7684\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f00\u6e90\u4e0e\u95ed\u6e90\u7684\u6700\u5148\u8fdb\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5bf9\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u8fd8\u901a\u8fc7\u6e29\u5ea6\u8c03\u8282\u5206\u6790\u6a21\u578b\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u526f\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u5728\u5404\u7c7b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u6e29\u5ea6\u8c03\u8282\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u4e00\u5b9a\u5f71\u54cd\u3002", "conclusion": "CP-Bench\u586b\u8865\u4e86\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u7a7a\u767d\uff0c\u4e3a\u6784\u5efa\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u60c5\u611f\u667a\u80fd\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2509.16577", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16577", "abs": "https://arxiv.org/abs/2509.16577", "authors": ["Antonio Tarizzo", "Mohammad Kazemi", "Deniz G\u00fcnd\u00fcz"], "title": "Learned Digital Codes for Over-the-Air Federated Learning", "comment": null, "summary": "Federated edge learning (FEEL) enables distributed model training across\nwireless devices without centralising raw data, but deployment is constrained\nby the wireless uplink. A promising direction is over-the-air (OTA)\naggregation, which merges communication with computation. Existing digital OTA\nmethods can achieve either strong convergence or robustness to noise, but\nstruggle to achieve both simultaneously, limiting performance in low\nsignal-to-noise ratios (SNRs) where many IoT devices operate. This work\nproposes a learnt digital OTA framework that extends reliable operation into\nlow-SNR conditions while maintaining the same uplink overhead as\nstate-of-the-art. The proposed method combines an unrolled decoder with a\njointly learnt unsourced random access codebook. Results show an extension of\nreliable operation by more than 7 dB, with improved global model convergence\nacross all SNR levels, highlighting the potential of learning-based design for\nFEEL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6570\u5b57\u7a7a\u4e2d\u805a\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5c55\u5f00\u89e3\u7801\u5668\u548c\u8054\u5408\u5b66\u4e60\u7684\u65e0\u6e90\u968f\u673a\u63a5\u5165\u7801\u672c\uff0c\u5b9e\u73b0\u4e86\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u8fd0\u884c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u540c\u7684\u4e0a\u884c\u94fe\u8def\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u6570\u5b57\u7a7a\u4e2d\u805a\u5408\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5f3a\u6536\u655b\u6027\u548c\u6297\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u9650\u5236\u4e86\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5c55\u5f00\u89e3\u7801\u5668\u4e0e\u8054\u5408\u5b66\u4e60\u7684\u65e0\u6e90\u968f\u673a\u63a5\u5165\u7801\u672c\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5b66\u4e60\u578b\u6570\u5b57\u7a7a\u4e2d\u805a\u5408\u6846\u67b6\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u53ef\u9760\u8fd0\u884c\u8303\u56f4\u6269\u5c55\u4e867 dB\u4ee5\u4e0a\uff0c\u4e14\u5728\u6240\u6709\u4fe1\u566a\u6bd4\u6c34\u5e73\u4e0b\u5747\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5168\u5c40\u6a21\u578b\u6536\u655b\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u8bbe\u8ba1\u5728\u8054\u90a6\u8fb9\u7f18\u5b66\u4e60\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u63d0\u5347\u65e0\u7ebf\u4e0a\u884c\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17062", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17062", "abs": "https://arxiv.org/abs/2509.17062", "authors": ["Cristian P\u00e9rez-Corral", "Antonio Garrido", "Laura Sebastia"], "title": "From domain-landmark graph learning to problem-landmark graph generation", "comment": null, "summary": "Landmarks have long played a pivotal role in automated planning, serving as\ncrucial elements for improving the planning algorithms. The main limitation of\nclassical landmark extraction methods is their sensitivity to specific planning\ntasks. This results in landmarks fully tailored to individual instances,\nthereby limiting their applicability across other instances of the same\nplanning domain. We propose a novel approach that learns landmark relationships\nfrom multiple planning tasks of a planning domain. This leads to the creation\nof a \\textit{probabilistic lifted ordering graph}, as a structure that captures\nweighted abstractions of relationships between parameterized landmarks.\nAlthough these orderings are not 100\\% true (they are probabilistic), they can\nstill be very useful in planning. Next, given a new planning task for that\ndomain, we instantiate the relationships from that graph to this particular\ninstance. This instantiation operates in two phases. First, it generates two\ngraphs: the former instantiating information from the initial state and the\nlatter from the goal state. Second, it combines these two graphs into one\nunified graph by searching equivalences to extract landmark orderings. We\nevaluate the precision and recallof the information found by our approach over\nwell-known planning domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u591a\u4e2a\u89c4\u5212\u4efb\u52a1\u4e2d\u5b66\u4e60\u5730\u6807\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\uff0c\u6784\u5efa\u6982\u7387\u63d0\u5347\u5e8f\u56fe\u4ee5\u6355\u6349\u53c2\u6570\u5316\u5730\u6807\u7684\u52a0\u6743\u62bd\u8c61\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5316\u548c\u5408\u5e76\u56fe\u7ed3\u6784\u63d0\u53d6\u65b0\u4efb\u52a1\u4e2d\u7684\u5730\u6807\u5e8f\u3002", "motivation": "\u7ecf\u5178\u5730\u6807\u63d0\u53d6\u65b9\u6cd5\u5bf9\u7279\u5b9a\u89c4\u5212\u4efb\u52a1\u654f\u611f\uff0c\u5bfc\u81f4\u5730\u6807\u4ec5\u9002\u7528\u4e8e\u4e2a\u522b\u5b9e\u4f8b\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u540c\u4e00\u89c4\u5212\u57df\u7684\u5176\u4ed6\u5b9e\u4f8b\u3002", "method": "\u4ece\u540c\u4e00\u89c4\u5212\u57df\u7684\u591a\u4e2a\u4efb\u52a1\u4e2d\u5b66\u4e60\u5730\u6807\u5173\u7cfb\uff0c\u6784\u5efa\u6982\u7387\u63d0\u5347\u5e8f\u56fe\uff1b\u9488\u5bf9\u65b0\u4efb\u52a1\uff0c\u5206\u522b\u4ece\u521d\u59cb\u72b6\u6001\u548c\u76ee\u6807\u72b6\u6001\u751f\u6210\u5b9e\u4f8b\u5316\u56fe\uff0c\u518d\u901a\u8fc7\u5bfb\u627e\u7b49\u4ef7\u6027\u5c06\u4e8c\u8005\u5408\u5e76\u4e3a\u7edf\u4e00\u56fe\u4ee5\u63d0\u53d6\u5730\u6807\u5e8f\u3002", "result": "\u5728\u591a\u4e2a\u77e5\u540d\u89c4\u5212\u57df\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u53d6\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u53d6\u8de8\u4efb\u52a1\u7684\u5730\u6807\u5173\u7cfb\uff0c\u751f\u6210\u7684\u6982\u7387\u6027\u5730\u6807\u5e8f\u867d\u975e\u5b8c\u5168\u51c6\u786e\uff0c\u4f46\u5728\u89c4\u5212\u4e2d\u4ecd\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.16560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16560", "abs": "https://arxiv.org/abs/2509.16560", "authors": ["Ji Soo Lee", "Byungoh Ko", "Jaewon Cho", "Howoong Lee", "Jaewoon Byun", "Hyunwoo J. Kim"], "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization", "comment": "EMNLP 2025 Findings", "summary": "In text-video retrieval, auxiliary captions are often used to enhance video\nunderstanding, bridging the gap between the modalities. While recent advances\nin multi-modal large language models (MLLMs) have enabled strong zero-shot\ncaption generation, we observe that such captions tend to be generic and\nindistinguishable across visually similar videos, limiting their utility for\nfine-grained retrieval. Moreover, conventional captioning approaches are\ntypically evaluated using language generation metrics, such as BLEU, which are\nnot typically tailored for retrieval tasks that require making discriminative\ndistinctions between candidates. To address this, we propose\n$\\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption\ngeneration using retrieval relevance scores. At its core is Dual-Group Direct\nPreference Optimization (DG-DPO), a novel learning strategy that supervises\ncaptioning by modeling preferences across groups of distinct video and caption\npairs. In addition, we present an MLLM-based retrieval model that incorporates\nrole-embeddings to better distinguish between textual inputs with different\nfunctional roles, such as an auxiliary caption and a text query. Through\nextensive experiments, we demonstrate that CaRe-DPO significantly enhances\nretrieval performance by effectively leveraging auxiliary knowledge to generate\nfine-grained captions for retrieval. Code is available at\nhttps://github.com/mlvlab/CaReDPO.", "AI": {"tldr": "\u63d0\u51faCaRe-DPO\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u6027\u5206\u6570\u76f4\u63a5\u4f18\u5316\u751f\u6210\u5b57\u5e55\uff0c\u63d0\u5347\u6587\u672c-\u89c6\u9891\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u751f\u6210\u7684\u5b57\u5e55\u8fc7\u4e8e\u6cdb\u5316\uff0c\u96be\u4ee5\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u89c6\u9891\uff0c\u4e14\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u4e0d\u9002\u7528\u4e8e\u68c0\u7d22\u4efb\u52a1\u3002", "method": "\u63d0\u51faCaRe-DPO\u6846\u67b6\uff0c\u6838\u5fc3\u4e3a\u53cc\u7ec4\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DG-DPO\uff09\uff0c\u5229\u7528\u68c0\u7d22\u76f8\u5173\u6027\u5efa\u6a21\u5b57\u5e55\u751f\u6210\u504f\u597d\uff0c\u5e76\u5f15\u5165\u5e26\u89d2\u8272\u5d4c\u5165\u7684MLLM\u68c0\u7d22\u6a21\u578b\u4ee5\u533a\u5206\u4e0d\u540c\u6587\u672c\u89d2\u8272\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCaRe-DPO\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c-\u89c6\u9891\u68c0\u7d22\u6027\u80fd\uff0c\u80fd\u751f\u6210\u66f4\u7ec6\u7c92\u5ea6\u3001\u66f4\u5177\u533a\u5206\u6027\u7684\u8f85\u52a9\u5b57\u5e55\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5b57\u5e55\u751f\u6210\u4e0e\u68c0\u7d22\u76ee\u6807\u5bf9\u9f50\uff0cCaRe-DPO\u6709\u6548\u5229\u7528\u8f85\u52a9\u4fe1\u606f\u63d0\u5347\u7ec6\u7c92\u5ea6\u68c0\u7d22\u6548\u679c\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u8bed\u8a00\u751f\u6210\u6307\u6807\u4f18\u5316\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.17053", "categories": ["cs.RO", "68T40, 93C85", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.17053", "abs": "https://arxiv.org/abs/2509.17053", "authors": ["Haizhou Ge", "Yufei Jia", "Zheng Li", "Yue Li", "Zhixing Chen", "Ruqi Huang", "Guyue Zhou"], "title": "FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks", "comment": null, "summary": "Contact-rich manipulation is crucial for robots to perform tasks requiring\nprecise force control, such as insertion, assembly, and in-hand manipulation.\nHowever, most imitation learning (IL) policies remain position-centric and lack\nexplicit force awareness, and adding force/torque sensors to collaborative\nrobot arms is often costly and requires additional hardware design. To overcome\nthese issues, we propose FILIC, a Force-guided Imitation Learning framework\nwith impedance torque control. FILIC integrates a Transformer-based IL policy\nwith an impedance controller in a dual-loop structure, enabling compliant\nforce-informed, force-executed manipulation. For robots without force/torque\nsensors, we introduce a cost-effective end-effector force estimator using joint\ntorque measurements through analytical Jacobian-based inversion while\ncompensating with model-predicted torques from a digital twin. We also design\ncomplementary force feedback frameworks via handheld haptics and VR\nvisualization to improve demonstration quality. Experiments show that FILIC\nsignificantly outperforms vision-only and joint-torque-based methods, achieving\nsafer, more compliant, and adaptable contact-rich manipulation. Our code can be\nfound in https://github.com/TATP-233/FILIC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFILIC\u7684\u529b\u5f15\u5bfc\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u963b\u6297\u63a7\u5236\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u529b/\u529b\u77e9\u4f20\u611f\u5668\u7684\u67d4\u987a\u3001\u529b\u611f\u77e5\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u591a\u4e3a\u4f4d\u7f6e\u4e2d\u5fc3\uff0c\u7f3a\u4e4f\u663e\u5f0f\u7684\u529b\u611f\u77e5\u80fd\u529b\uff0c\u4e14\u52a0\u88c5\u529b/\u529b\u77e9\u4f20\u611f\u5668\u6210\u672c\u9ad8\u3001\u9700\u786c\u4ef6\u6539\u52a8\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u73af\u7ed3\u6784\uff0c\u5c06\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u4e0e\u963b\u6297\u63a7\u5236\u5668\u7ed3\u5408\uff1b\u5229\u7528\u5173\u8282\u626d\u77e9\u548c\u89e3\u6790\u96c5\u53ef\u6bd4\u9006\u63a8\u672b\u7aef\u6267\u884c\u5668\u53d7\u529b\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u6570\u5b57\u5b6a\u751f\u9884\u6d4b\u626d\u77e9\u8865\u507f\uff1b\u901a\u8fc7\u624b\u6301\u89e6\u89c9\u8bbe\u5907\u548cVR\u53ef\u89c6\u5316\u63d0\u5347\u6f14\u793a\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFILIC\u663e\u8457\u4f18\u4e8e\u4ec5\u89c6\u89c9\u548c\u57fa\u4e8e\u5173\u8282\u626d\u77e9\u7684\u65b9\u6cd5\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u5b89\u5168\u3001\u67d4\u987a\u548c\u81ea\u9002\u5e94\u3002", "conclusion": "FILIC\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u7cbe\u5ea6\u7684\u529b\u611f\u77e5\u6a21\u4eff\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u65e0\u4e13\u7528\u529b\u4f20\u611f\u5668\u7684\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2509.16591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16591", "abs": "https://arxiv.org/abs/2509.16591", "authors": ["Zheng Liu", "Mengjie Liu", "Siwei Wen", "Mengzhang Cai", "Bin Cui", "Conghui He", "Wentao Zhang"], "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature", "comment": null, "summary": "Reinforcement Learning has emerged as the fundamental technique for enhancing\nreasoning in LLMs. However, existing algorithms apply uniform optimization to\nall tokens, ignoring their different roles in reasoning process. To address\nthis limitation, we introduce Heterogeneous Adaptive Policy Optimization\n(HAPO), a comprehensive token-aware algorithm that dynamically adapts\noptimization based on token entropy. For rollout sampling, we propose Adaptive\nTemperature Sampling, which adjusts sampling temperature in real time,\npromoting exploration at high-entropy tokens while preserving coherence at\nlow-entropy ones. For advantage calculation, we introduce Token Level Group\nAverage that normalizes advantages at token level, jointly accounting for\nsequence-length as in token-mean loss while preserving non-biased treatment. We\nthen develop Differential Advantage Redistribution that leverages entropy and\nimportance ratios to modulate rewards-adjusting updates for tokens with clear\nsignals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing\naggressive probability reduction for noisy low-entropy tokens while enabling\nexploration for high-entropy tokens. Through systematic investigation between\nentropy and training dynamics, we embedded token-level treatment into every\nstages to achieve fine-grained control. Extensive experiments demonstrate that\nHAPO consistently outperforms DAPO across multiple model scales. Our code can\nbe found in https://github.com/starriver030515/HAPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8etoken\u71b5\u7684\u7ec6\u7c92\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5HAPO\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5728\u91c7\u6837\u3001\u4f18\u52bf\u8ba1\u7b97\u548c\u635f\u5931\u88c1\u526a\u7b49\u9636\u6bb5\u5f15\u5165token\u7ea7\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5728\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u4e0b\u5747\u4f18\u4e8eDAPO\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6240\u6709token\u91c7\u7528\u7edf\u4e00\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0d\u540c\u4f5c\u7528\uff0c\u5bfc\u81f4\u4f18\u5316\u4e0d\u591f\u7cbe\u7ec6\u3002", "method": "\u63d0\u51faHeterogeneous Adaptive Policy Optimization (HAPO)\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u6e29\u5ea6\u91c7\u6837\u3001token\u7ea7\u4f18\u52bf\u5f52\u4e00\u5316\uff08Token Level Group Average\uff09\u3001\u5dee\u5f02\u4f18\u52bf\u91cd\u5206\u914d\uff08Differential Advantage Redistribution\uff09\u548c\u975e\u5bf9\u79f0\u81ea\u9002\u5e94\u88c1\u526a\uff08Asymmetric Adaptive Clipping\uff09\uff0c\u6839\u636etoken\u71b5\u52a8\u6001\u8c03\u6574\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHAPO\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a consistently \u4f18\u4e8eDAPO\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "HAPO\u901a\u8fc7\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u5404\u4e2a\u9636\u6bb5\u5f15\u5165token\u7ea7\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.16586", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16586", "abs": "https://arxiv.org/abs/2509.16586", "authors": ["Yukuan Wei", "Xudong Li", "Lin F. Yang"], "title": "Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs", "comment": null, "summary": "Recent advances have significantly improved our understanding of the sample\ncomplexity of learning in average-reward Markov decision processes (AMDPs)\nunder the generative model. However, much less is known about the constrained\naverage-reward MDP (CAMDP), where policies must satisfy long-run average\nconstraints. In this work, we address this gap by studying the sample\ncomplexity of learning an $\\epsilon$-optimal policy in CAMDPs under a\ngenerative model. We propose a model-based algorithm that operates under two\nsettings: (i) relaxed feasibility, which allows small constraint violations,\nand (ii) strict feasibility, where the output policy satisfies the constraint.\nWe show that our algorithm achieves sample complexities of\n$\\tilde{O}\\left(\\frac{S A (B+H)}{ \\epsilon^2}\\right)$ and $\\tilde{O}\n\\left(\\frac{S A (B+H)}{\\epsilon^2 \\zeta^2} \\right)$ under the relaxed and\nstrict feasibility settings, respectively. Here, $\\zeta$ is the Slater constant\nindicating the size of the feasible region, $H$ is the span bound of the bias\nfunction, and $B$ is the transient time bound. Moreover, a matching lower bound\nof $\\tilde{\\Omega}\\left(\\frac{S A (B+H)}{ \\epsilon^2\\zeta^2}\\right)$ for the\nstrict feasibility case is established, thus providing the first\nminimax-optimal bounds for CAMDPs. Our results close the theoretical gap in\nunderstanding the complexity of constrained average-reward MDPs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u751f\u6210\u6a21\u578b\u4e0b\u7ea6\u675f\u5e73\u5747\u5956\u52b1\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CAMDP\uff09\u4e2d\u7684\u5b66\u4e60\u6837\u672c\u590d\u6742\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u677e\u5f1b\u53ef\u884c\u548c\u4e25\u683c\u53ef\u884c\u4e24\u79cd\u8bbe\u5b9a\u4e0b\u5206\u522b\u7ed9\u51fa\u4e86\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u754c\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u5339\u914d\u7684\u4e0b\u754c\uff0c\u5b9e\u73b0\u4e86CAMDP\u7684\u6700\u5c0f\u6700\u5927\u6700\u4f18\u754c\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5e73\u5747\u5956\u52b1MDP\u7684\u6837\u672c\u590d\u6742\u5ea6\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u5bf9\u5177\u6709\u957f\u671f\u5e73\u5747\u7ea6\u675f\u7684CAMDP\u7684\u7814\u7a76\u4ecd\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u7b97\u6cd5\uff0c\u8003\u8651\u677e\u5f1b\u53ef\u884c\u6027\u548c\u4e25\u683c\u53ef\u884c\u6027\u4e24\u79cd\u8bbe\u7f6e\uff0c\u5206\u6790\u5176\u5728\u751f\u6210\u6a21\u578b\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e0a\u4e0b\u754c\u3002", "result": "\u7b97\u6cd5\u5728\u677e\u5f1b\u548c\u4e25\u683c\u53ef\u884c\u6027\u4e0b\u5206\u522b\u8fbe\u5230\\tilde{O}(SA(B+H)/\\epsilon^2)\u548c\\tilde{O}(SA(B+H)/(\\epsilon^2\\zeta^2))\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4e14\u4e25\u683c\u60c5\u51b5\u4e0b\u5efa\u7acb\u4e86\u5339\u914d\u4e0b\u754c\\tilde{\\Omega}(SA(B+H)/(\\epsilon^2\\zeta^2))\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u4e3aCAMDP\u63d0\u4f9b\u4e86\u6700\u5c0f\u6700\u5927\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u586b\u8865\u4e86\u7ea6\u675f\u5e73\u5747\u5956\u52b1MDP\u7406\u8bba\u4e0a\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.16567", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16567", "abs": "https://arxiv.org/abs/2509.16567", "authors": ["Nikolaos Spanos", "Maria Lymperaiou", "Giorgos Filandrianos", "Konstantinos Thomas", "Athanasios Voulodimos", "Giorgos Stamou"], "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits", "comment": "Accepted in NeurIPS 2025", "summary": "Recent black-box counterfactual generation frameworks fail to take into\naccount the semantic content of the proposed edits, while relying heavily on\ntraining to guide the generation process. We propose a novel, plug-and-play\nblack-box counterfactual generation framework, which suggests step-by-step\nedits based on theoretical guarantees of optimal edits to produce human-level\ncounterfactual explanations with zero training. Our framework utilizes a\npre-trained image editing diffusion model, and operates without access to the\ninternals of the classifier, leading to an explainable counterfactual\ngeneration process. Throughout our experimentation, we showcase the explanatory\ngap between human reasoning and neural model behavior by utilizing both\nConvolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision\nLanguage Model (LVLM) classifiers, substantiated through a comprehensive human\nevaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u9ed1\u76d2\u53cd\u4e8b\u5b9e\u751f\u6210\u6846\u67b6\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u751f\u6210\u4eba\u7c7b\u6c34\u5e73\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u6cd5\u5ffd\u89c6\u7f16\u8f91\u7684\u8bed\u4e49\u5185\u5bb9\u4e14\u4f9d\u8d56\u8bad\u7ec3\uff0c\u96be\u4ee5\u53cd\u6620\u4eba\u7c7b\u63a8\u7406\u4e0e\u6a21\u578b\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8e\u7406\u8bba\u6700\u4f18\u7f16\u8f91\u7b56\u7565\uff0c\u9010\u6b65\u751f\u6210\u53cd\u4e8b\u5b9e\u6837\u672c\uff0c\u4e0d\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u6216\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728CNN\u3001ViT\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u63ed\u793a\u4e86\u4eba\u7c7b\u63a8\u7406\u4e0e\u795e\u7ecf\u6a21\u578b\u884c\u4e3a\u4e4b\u95f4\u7684\u89e3\u91ca\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.17057", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17057", "abs": "https://arxiv.org/abs/2509.17057", "authors": ["Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Hanbit Oh", "Koshi Makihara", "Keisuke Shirai", "Yukiyasu Domae"], "title": "RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments", "comment": null, "summary": "RoboManipBaselines is an open framework for robot imitation learning that\nunifies data collection, training, and evaluation across simulation and real\nrobots. We introduce it as a platform enabling systematic benchmarking of\ndiverse tasks, robots, and multimodal policies with emphasis on integration,\ngenerality, extensibility, and reproducibility.", "AI": {"tldr": "RoboManipBaselines\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e2d\u7684\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u591a\u6a21\u6001\u7b56\u7565\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u8de8\u5e73\u53f0\u3001\u8de8\u4efb\u52a1\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRoboManipBaselines\u6846\u67b6\uff0c\u96c6\u6210\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u652f\u6301\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fd0\u884c\uff0c\u5e76\u8bbe\u8ba1\u6a21\u5757\u5316\u7ed3\u6784\u4ee5\u9002\u5e94\u591a\u79cd\u4efb\u52a1\u548c\u673a\u5668\u4eba\u914d\u7f6e\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5bf9\u591a\u79cd\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u591a\u6a21\u6001\u7b56\u7565\u7684\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u5347\u4e86\u5b9e\u9a8c\u7684\u53ef\u590d\u73b0\u6027\u548c\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "RoboManipBaselines\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u4e0e\u6bd4\u8f83\u3002"}}
{"id": "2509.16596", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16596", "abs": "https://arxiv.org/abs/2509.16596", "authors": ["Junjie Ye", "Yuming Yang", "Yang Nan", "Shuo Li", "Qi Zhang", "Tao Gui", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan"], "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels", "comment": "Accepted by EMNLP 2025 Main Conference. arXiv admin note: text\n  overlap with arXiv:2409.15825", "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8fc7\u7a0b\u4e2d\u5927\u90e8\u5206\u53c2\u6570\u66f4\u65b0\u5e76\u672a\u63d0\u5347\u6a21\u578b\u77e5\u8bc6\uff0c\u53cd\u800c\u53ef\u80fd\u5bfc\u81f4\u95ed\u5377\u95ee\u7b54\u6027\u80fd\u4e0b\u964d\uff1b\u901a\u8fc7\u6062\u590d\u65e0\u8d21\u732e\u7684\u66f4\u65b0\u53ef\u6539\u5584\u8868\u73b0\uff0c\u4e3a\u4f18\u5316\u5fae\u8c03\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u63a2\u7d22\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7684\u5f71\u54cd\uff0c\u586b\u8865\u5f53\u524d\u5bf9\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u77e5\u8bc6\u53d8\u5316\u673a\u5236\u7406\u89e3\u7684\u7a7a\u767d\u3002", "method": "\u5728LLaMA-2\u548cLLaMA-3\u7cfb\u5217\u7684\u4e94\u4e2a\u5927\u6a21\u578b\u4e0a\u8bc4\u4f30\u95ed\u5377\u95ee\u7b54\uff08CBQA\uff09\u6027\u80fd\uff0c\u5206\u6790\u4e0d\u540c\u5fae\u8c03\u6837\u672c\u91cf\u548c\u77e5\u8bc6\u638c\u63e1\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4ecetoken\u548c\u53c2\u6570\u5c42\u9762\u5206\u6790\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u4f7f\u75281,920\u6837\u672c\u5fae\u8c03\u7684\u6a21\u578b\u6027\u80fd\u6bd4\u4ec5\u7528240\u6837\u672c\u7684\u4f4e\u8fbe14%\uff1b\u5fae\u8c03\u6570\u636e\u4e2d\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\u53d8\u5316\u5bfc\u81f4\u6027\u80fd\u6ce2\u52a8\u8d8512%\uff1b\u9ad8\u8fbe90%\u7684\u53c2\u6570\u66f4\u65b0\u672a\u4fc3\u8fdb\u77e5\u8bc6\u589e\u5f3a\uff0c\u6062\u590d\u8fd9\u4e9b\u66f4\u65b0\u53ef\u63d0\u5347CBQA\u6027\u80fd\u3002", "conclusion": "\u591a\u6570SFT\u53c2\u6570\u66f4\u65b0\u4e0d\u8d21\u732e\u4e8e\u77e5\u8bc6\u63d0\u5347\uff0c\u8bc6\u522b\u5e76\u8c03\u63a7\u6b64\u7c7b\u66f4\u65b0\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u5fae\u8c03\u7b56\u7565\u4ee5\u589e\u5f3a\u6a21\u578b\u77e5\u8bc6\u3002"}}
{"id": "2509.16625", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.16625", "abs": "https://arxiv.org/abs/2509.16625", "authors": ["Lorenzo Guerra", "Thomas Chapuis", "Guillaume Duc", "Pavlo Mozharovskyi", "Van-Tam Nguyen"], "title": "Self-Supervised Learning of Graph Representations for Network Intrusion Detection", "comment": "Accepted at NeurIPS 2025", "summary": "Detecting intrusions in network traffic is a challenging task, particularly\nunder limited supervision and constantly evolving attack patterns. While recent\nworks have leveraged graph neural networks for network intrusion detection,\nthey often decouple representation learning from anomaly detection, limiting\nthe utility of the embeddings for identifying attacks. We propose GraphIDS, a\nself-supervised intrusion detection model that unifies these two stages by\nlearning local graph representations of normal communication patterns through a\nmasked autoencoder. An inductive graph neural network embeds each flow with its\nlocal topological context to capture typical network behavior, while a\nTransformer-based encoder-decoder reconstructs these embeddings, implicitly\nlearning global co-occurrence patterns via self-attention without requiring\nexplicit positional information. During inference, flows with unusually high\nreconstruction errors are flagged as potential intrusions. This end-to-end\nframework ensures that embeddings are directly optimized for the downstream\ntask, facilitating the recognition of malicious traffic. On diverse NetFlow\nbenchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score,\noutperforming baselines by 5-25 percentage points.", "AI": {"tldr": "\u63d0\u51faGraphIDS\uff0c\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u7aef\u5230\u7aef\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7edf\u4e00\u8868\u5f81\u5b66\u4e60\u4e0e\u5f02\u5e38\u68c0\u6d4b\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u67b6\u6784\u5b66\u4e60\u6b63\u5e38\u901a\u4fe1\u6a21\u5f0f\uff0c\u5e76\u4ee5\u91cd\u6784\u8bef\u5dee\u68c0\u6d4b\u5f02\u5e38\uff0c\u5728\u591a\u4e2aNetFlow\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06\u8868\u5f81\u5b66\u4e60\u4e0e\u5f02\u5e38\u68c0\u6d4b\u5206\u79bb\uff0c\u5bfc\u81f4\u5d4c\u5165\u8868\u793a\u5728\u8bc6\u522b\u653b\u51fb\u65b9\u9762\u7684\u6709\u6548\u6027\u53d7\u9650\uff0c\u4e14\u5728\u5f31\u76d1\u7763\u548c\u65b0\u578b\u653b\u51fb\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGraphIDS\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff1a\u4f7f\u7528\u5f52\u7eb3\u5f0f\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u5c40\u90e8\u62d3\u6251\u4e0a\u4e0b\u6587\u5d4c\u5165\u7f51\u7edc\u6d41\uff0c\u901a\u8fc7Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u91cd\u5efa\u88ab\u63a9\u7801\u7684\u5d4c\u5165\uff0c\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u9690\u5f0f\u5b66\u4e60\u5168\u5c40\u5171\u73b0\u6a21\u5f0f\uff0c\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u4f18\u5316\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2aNetFlow\u6570\u636e\u96c6\u4e0a\uff0cGraphIDS\u8fbe\u5230\u6700\u9ad899.98%\u7684PR-AUC\u548c99.61%\u7684\u5b8fF1\u5206\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b5\u81f325\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u8868\u5f81\u5b66\u4e60\u4e0e\u5f02\u5e38\u68c0\u6d4b\uff0cGraphIDS\u751f\u6210\u7684\u4efb\u52a1\u4f18\u5316\u5d4c\u5165\u80fd\u66f4\u6709\u6548\u8bc6\u522b\u6076\u610f\u6d41\u91cf\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u6846\u67b6\u5728\u771f\u5b9e\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.17068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17068", "abs": "https://arxiv.org/abs/2509.17068", "authors": ["Chen Wang", "Sarah Erfani", "Tansu Alpcan", "Christopher Leckie"], "title": "Intention-aware Hierarchical Diffusion Model for Long-term Trajectory Anomaly Detection", "comment": "15 pages, 5 figures", "summary": "Long-term trajectory anomaly detection is a challenging problem due to the\ndiversity and complex spatiotemporal dependencies in trajectory data. Existing\ntrajectory anomaly detection methods fail to simultaneously consider both the\nhigh-level intentions of agents as well as the low-level details of the agent's\nnavigation when analysing an agent's trajectories. This limits their ability to\ncapture the full diversity of normal trajectories. In this paper, we propose an\nunsupervised trajectory anomaly detection method named Intention-aware\nHierarchical Diffusion model (IHiD), which detects anomalies through both\nhigh-level intent evaluation and low-level sub-trajectory analysis. Our\napproach leverages Inverse Q Learning as the high-level model to assess whether\na selected subgoal aligns with an agent's intention based on predicted\nQ-values. Meanwhile, a diffusion model serves as the low-level model to\ngenerate sub-trajectories conditioned on subgoal information, with anomaly\ndetection based on reconstruction error. By integrating both models, IHiD\neffectively utilises subgoal transition knowledge and is designed to capture\nthe diverse distribution of normal trajectories. Our experiments show that the\nproposed method IHiD achieves up to 30.2% improvement in anomaly detection\nperformance in terms of F1 score over state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIHiD\u7684\u65e0\u76d1\u7763\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u9636\u610f\u56fe\u8bc4\u4f30\u548c\u4f4e\u9636\u5b50\u8f68\u8ff9\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u8003\u8651\u667a\u80fd\u4f53\u7684\u9ad8\u5c42\u610f\u56fe\u548c\u4f4e\u5c42\u5bfc\u822a\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u5bf9\u6b63\u5e38\u8f68\u8ff9\u591a\u6837\u6027\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9006Q\u5b66\u4e60\u4f5c\u4e3a\u9ad8\u5c42\u6a21\u578b\u8bc4\u4f30\u5b50\u76ee\u6807\u662f\u5426\u7b26\u5408\u667a\u80fd\u4f53\u610f\u56fe\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u4f4e\u5c42\u6a21\u578b\u751f\u6210\u6761\u4ef6\u5b50\u8f68\u8ff9\uff0c\u5e76\u57fa\u4e8e\u91cd\u6784\u8bef\u5dee\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIHiD\u5728F1\u5206\u6570\u4e0a\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u6700\u9ad8\u63d0\u534730.2%\u3002", "conclusion": "IHiD\u901a\u8fc7\u878d\u5408\u9ad8\u4f4e\u5c42\u4fe1\u606f\uff0c\u6709\u6548\u5229\u7528\u5b50\u76ee\u6807\u8f6c\u79fb\u77e5\u8bc6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u6b63\u5e38\u8f68\u8ff9\u7684\u591a\u6837\u6027\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.16582", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16582", "abs": "https://arxiv.org/abs/2509.16582", "authors": ["Antonio Scardace", "Lemuel Puglisi", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis", "comment": null, "summary": "Deep generative models have emerged as a transformative tool in medical\nimaging, offering substantial potential for synthetic data generation. However,\nrecent empirical studies highlight a critical vulnerability: these models can\nmemorize sensitive training data, posing significant risks of unauthorized\npatient information disclosure. Detecting memorization in generative models\nremains particularly challenging, necessitating scalable methods capable of\nidentifying training data leakage across large sets of generated samples. In\nthis work, we propose DeepSSIM, a novel self-supervised metric for quantifying\nmemorization in generative models. DeepSSIM is trained to: i) project images\ninto a learned embedding space and ii) force the cosine similarity between\nembeddings to match the ground-truth SSIM (Structural Similarity Index) scores\ncomputed in the image space. To capture domain-specific anatomical features,\ntraining incorporates structure-preserving augmentations, allowing DeepSSIM to\nestimate similarity reliably without requiring precise spatial alignment. We\nevaluate DeepSSIM in a case study involving synthetic brain MRI data generated\nby a Latent Diffusion Model (LDM) trained under memorization-prone conditions,\nusing 2,195 MRI scans from two publicly available datasets (IXI and CoRR).\nCompared to state-of-the-art memorization metrics, DeepSSIM achieves superior\nperformance, improving F1 scores by an average of +52.03% over the best\nexisting method. Code and data of our approach are publicly available at the\nfollowing link: https://github.com/brAIn-science/DeepSSIM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepSSIM\u7684\u81ea\u76d1\u7763\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u73b0\u8c61\uff0c\u5728\u5408\u6210\u8111MRI\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u68c0\u6d4b\u751f\u6210\u6a21\u578b\u5bf9\u654f\u611f\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\u5316\u884c\u4e3a\uff0c\u9632\u6b62\u60a3\u8005\u4fe1\u606f\u6cc4\u9732\uff0c\u662f\u533b\u5b66\u5f71\u50cf\u4e2d\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6DeepSSIM\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u6620\u5c04\u5230\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u4f7f\u5d4c\u5165\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u5339\u914d\u56fe\u50cf\u7a7a\u95f4\u7684\u771f\u5b9eSSIM\u5f97\u5206\uff0c\u7ed3\u5408\u7ed3\u6784\u4fdd\u6301\u589e\u5f3a\u6765\u6355\u6349\u89e3\u5256\u7279\u5f81\u3002", "result": "\u5728\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u8111MRI\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cDeepSSIM\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5e73\u5747F1\u5206\u6570\u63d0\u534752.03%\u3002", "conclusion": "DeepSSIM\u80fd\u6709\u6548\u91cf\u5316\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u7a0b\u5ea6\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u533b\u5b66\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u7684\u9690\u79c1\u98ce\u9669\u8bc4\u4f30\u3002"}}
{"id": "2509.17080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17080", "abs": "https://arxiv.org/abs/2509.17080", "authors": ["Ruiguo Zhong", "Ruoyu Yao", "Pei Liu", "Xiaolong Chen", "Rui Yang", "Jun Ma"], "title": "CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving", "comment": null, "summary": "Accurate trajectory prediction and motion planning are crucial for autonomous\ndriving systems to navigate safely in complex, interactive environments\ncharacterized by multimodal uncertainties. However, current\ngeneration-then-evaluation frameworks typically construct multiple plausible\ntrajectory hypotheses but ultimately adopt a single most likely outcome,\nleading to overconfident decisions and a lack of fallback strategies that are\nvital for safety in rare but critical scenarios. Moreover, the usual decoupling\nof prediction and planning modules could result in socially inconsistent or\nunrealistic joint trajectories, especially in highly interactive traffic. To\naddress these challenges, we propose a contingency-aware diffusion planner\n(CoPlanner), a unified framework that jointly models multi-agent interactive\ntrajectory generation and contingency-aware motion planning. Specifically, the\npivot-conditioned diffusion mechanism anchors trajectory sampling on a\nvalidated, shared short-term segment to preserve temporal consistency, while\nstochastically generating diverse long-horizon branches that capture multimodal\nmotion evolutions. In parallel, we design a contingency-aware multi-scenario\nscoring strategy that evaluates candidate ego trajectories across multiple\nplausible long-horizon evolution scenarios, balancing safety, progress, and\ncomfort. This integrated design preserves feasible fallback options and\nenhances robustness under uncertainty, leading to more realistic\ninteraction-aware planning. Extensive closed-loop experiments on the nuPlan\nbenchmark demonstrate that CoPlanner consistently surpasses state-of-the-art\nmethods on both Val14 and Test14 datasets, achieving significant improvements\nin safety and comfort under both reactive and non-reactive settings. Code and\nmodel will be made publicly available upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoPlanner\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8054\u5408\u5efa\u6a21\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8f68\u8ff9\u751f\u6210\u4e0e\u5e94\u6025\u611f\u77e5\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u4e0e\u8212\u9002\u6027\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b-\u89c4\u5212\u5206\u79bb\u6846\u67b6\u548c\u751f\u6210-\u8bc4\u4f30\u8303\u5f0f\u5f80\u5f80\u5ffd\u7565\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u51b3\u7b56\u8fc7\u4e8e\u81ea\u4fe1\u4e14\u7f3a\u4e4f\u5907\u7528\u7b56\u7565\uff0c\u96be\u4ee5\u5e94\u5bf9\u5173\u952e\u573a\u666f\u4e0b\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u951a\u5b9a\u77ed\u671f\u5171\u4eab\u7247\u6bb5\u7684\u67a2\u8f74\u6761\u4ef6\u6269\u6563\u673a\u5236\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u957f\u65f6\u7a0b\u5206\u652f\u8f68\u8ff9\uff1b\u8bbe\u8ba1\u5e94\u6025\u611f\u77e5\u7684\u591a\u573a\u666f\u8bc4\u5206\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u53ef\u80fd\u6f14\u5316\u573a\u666f\u4e2d\u8bc4\u4f30\u81ea\u8f66\u8f68\u8ff9\uff0c\u517c\u987e\u5b89\u5168\u6027\u3001\u901a\u884c\u6548\u7387\u4e0e\u8212\u9002\u6027\u3002", "result": "\u5728nuPlan\u57fa\u51c6\u4e0a\u8fdb\u884c\u5927\u91cf\u95ed\u73af\u5b9e\u9a8c\uff0cCoPlanner\u5728Val14\u548cTest14\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "conclusion": "CoPlanner\u901a\u8fc7\u7edf\u4e00\u7684\u6269\u6563\u89c4\u5212\u6846\u67b6\u5b9e\u73b0\u4e86\u4ea4\u4e92\u611f\u77e5\u3001\u5e94\u6025\u51c6\u5907\u5145\u5206\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u589e\u5f3a\u4e86\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u73b0\u5b9e\u4ea4\u4e92\u5408\u7406\u6027\u3002"}}
{"id": "2509.16597", "categories": ["cs.CL", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.16597", "abs": "https://arxiv.org/abs/2509.16597", "authors": ["Luyan Zhang"], "title": "MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models", "comment": "13 pages, 6 figures, 2 tables", "summary": "Aiming at the problems of computational inefficiency and insufficient\ninterpretability faced by large models in complex tasks such as multi-round\nreasoning and multi-modal collaboration, this study proposes a three-layer\ncollaboration framework based on model-controller-task adaptation (MCP). By\ndecoupling large model functions into reasoning, generation and retrieval\nmodules, and combining reinforcement learning-driven dynamic routing algorithms\nand task adaptation mechanisms, the systematic integration of control theory\nand large model dynamic reasoning is achieved for the first time. Experiments\nshow that the MCP framework improves the performance of cross-modal\nbenchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared\nwith the baseline model, improves the reasoning efficiency by 40%, and\ngenerates the interpretable intermediate results through the Presenter layer,\nobtaining 90% of the manual interpretability scores, which provides a brand-new\ntechnological path to solve the bottleneck of the practical application of the\nlarge model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b-\u63a7\u5236\u5668-\u4efb\u52a1\u9002\u914d\uff08MCP\uff09\u7684\u4e09\u5c42\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u5927\u6a21\u578b\u529f\u80fd\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u52a8\u6001\u8def\u7531\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u4e0e\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u9488\u5bf9\u5927\u6a21\u578b\u5728\u591a\u8f6e\u63a8\u7406\u548c\u591a\u6a21\u6001\u534f\u4f5c\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5c06\u5927\u6a21\u578b\u529f\u80fd\u89e3\u8026\u4e3a\u63a8\u7406\u3001\u751f\u6210\u548c\u68c0\u7d22\u6a21\u5757\uff0c\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u52a8\u6001\u8def\u7531\u7b97\u6cd5\u548c\u4efb\u52a1\u9002\u914d\u673a\u5236\uff0c\u6784\u5efa\u4e09\u5c42\u534f\u4f5c\u6846\u67b6\uff08MCP\uff09\uff0c\u5b9e\u73b0\u63a7\u5236\u7406\u8bba\u4e0e\u5927\u6a21\u578b\u52a8\u6001\u63a8\u7406\u7684\u7cfb\u7edf\u96c6\u6210\u3002", "result": "\u5728GLUE\u3001COCO\u3001ScienceQA\u7b49\u8de8\u6a21\u6001\u57fa\u51c6\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u534715-30%\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad840%\uff0c\u5e76\u901a\u8fc7Presenter\u5c42\u751f\u6210\u4e2d\u95f4\u7ed3\u679c\uff0c\u83b7\u5f9790%\u7684\u4eba\u5de5\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\u3002", "conclusion": "MCP\u6846\u67b6\u4e3a\u89e3\u51b3\u5927\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u74f6\u9888\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.16629", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.16629", "abs": "https://arxiv.org/abs/2509.16629", "authors": ["Kaichen Xu", "Yihang Du", "Mianpeng Liu", "Zimu Yu", "Xiaobo Sun"], "title": "Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features", "comment": "Accepted by NeurIPS 2025", "summary": "Positional encoding is essential for supplementing transformer with\npositional information of tokens. Existing positional encoding methods demand\npredefined token/feature order, rendering them unsuitable for real-world data\nwith non-sequential yet causally-related features. To address this limitation,\nwe propose CAPE, a novel method that identifies underlying causal structure\nover non-sequential features as a weighted directed acyclic graph (DAG) using\ngeneralized structural equation modeling. The DAG is then embedded in\nhyperbolic space where its geometric structure is well-preserved using a\nhyperboloid model-based approach that effectively captures two important causal\ngraph properties (causal strength & causal specificity). This step yields\ncausality-aware positional encodings for the features, which are converted into\ntheir rotary form for integrating with transformer's self-attention mechanism.\nTheoretical analysis reveals that CAPE-generated rotary positional encodings\npossess three valuable properties for enhanced self-attention, including causal\ndistance-induced attenuation, causal generality-induced attenuation, and\nrobustness to positional disturbances. We evaluate CAPE over both synthetic and\nreal-word datasets, empirically demonstrating its theoretical properties and\neffectiveness in enhancing transformer for data with non-sequential features.\nOur code is available at https://github.com/Catchxu/CAPE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5CAPE\uff0c\u7528\u4e8e\u5728\u975e\u5e8f\u5217\u7279\u5f81\u6570\u636e\u4e2d\u6355\u6349\u56e0\u679c\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u751f\u6210\u5177\u6709\u56e0\u679c\u611f\u77e5\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ece\u800c\u589e\u5f3aTransformer\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u5e8f\u5217\u987a\u5e8f\uff0c\u96be\u4ee5\u5904\u7406\u5177\u6709\u56e0\u679c\u5173\u7cfb\u4f46\u65e0\u56fa\u5b9a\u987a\u5e8f\u7684\u771f\u5b9e\u4e16\u754c\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8bc6\u522b\u975e\u5e8f\u5217\u7279\u5f81\u95f4\u6f5c\u5728\u56e0\u679c\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "CAPE\u5229\u7528\u5e7f\u4e49\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\u8bc6\u522b\u975e\u5e8f\u5217\u7279\u5f81\u95f4\u7684\u56e0\u679c\u7ed3\u6784\uff0c\u6784\u5efa\u52a0\u6743\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\uff0c\u5e76\u5728\u53cc\u66f2\u7a7a\u95f4\uff08\u57fa\u4e8e\u8d85\u66f2\u9762\u6a21\u578b\uff09\u4e2d\u8fdb\u884c\u5d4c\u5165\u4ee5\u4fdd\u7559\u5176\u51e0\u4f55\u4e0e\u56e0\u679c\u7279\u6027\uff08\u56e0\u679c\u5f3a\u5ea6\u4e0e\u7279\u5f02\u6027\uff09\uff0c\u6700\u7ec8\u5c06\u7f16\u7801\u8f6c\u6362\u4e3a\u65cb\u8f6c\u5f62\u5f0f\u4ee5\u878d\u5165Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cCAPE\u751f\u6210\u7684\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u5177\u5907\u56e0\u679c\u8ddd\u79bb\u548c\u56e0\u679c\u666e\u904d\u6027\u5f15\u8d77\u7684\u8870\u51cf\u7279\u6027\uff0c\u5e76\u5bf9\u4f4d\u7f6e\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\uff1b\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CAPE\u80fd\u591f\u6709\u6548\u4e3a\u5177\u6709\u975e\u5e8f\u5217\u4f46\u56e0\u679c\u76f8\u5173\u7279\u5f81\u7684\u6570\u636e\u63d0\u4f9b\u56e0\u679c\u611f\u77e5\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347Transformer\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2509.17087", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17087", "abs": "https://arxiv.org/abs/2509.17087", "authors": ["Nicholas Kruus", "Madhavendra Thakur", "Adam Khoja", "Leonhard Nagel", "Maximilian Nicholson", "Abeer Sharma", "Jason Hausenloy", "Alberto KoTafoya", "Aliya Mukhanova", "Alli Katila-Miikkulainen", "Harish Chandran", "Ivan Zhang", "Jessie Chen", "Joel Raj", "Jord Nguyen", "Lai Hsien Hao", "Neja Jayasundara", "Soham Sen", "Sophie Zhang", "Ashley Dora Kokui Tamaklo", "Bhavya Thakur", "Henry Close", "Janghee Lee", "Nina Sefton", "Raghavendra Thakur", "Shiv Munagala", "Yeeun Kim"], "title": "Governing Automated Strategic Intelligence", "comment": null, "summary": "Military and economic strategic competitiveness between nation-states will\nincreasingly be defined by the capability and cost of their frontier artificial\nintelligence models. Among the first areas of geopolitical advantage granted by\nsuch systems will be in automating military intelligence. Much discussion has\nbeen devoted to AI systems enabling new military modalities, such as lethal\nautonomous weapons, or making strategic decisions. However, the ability of a\ncountry of \"CIA analysts in a data-center\" to synthesize diverse data at scale,\nand its implications, have been underexplored. Multimodal foundation models\nappear on track to automate strategic analysis previously done by humans. They\nwill be able to fuse today's abundant satellite imagery, phone-location traces,\nsocial media records, and written documents into a single queryable system. We\nconduct a preliminary uplift study to empirically evaluate these capabilities,\nthen propose a taxonomy of the kinds of ground truth questions these systems\nwill answer, present a high-level model of the determinants of this system's AI\ncapabilities, and provide recommendations for nation-states to remain\nstrategically competitive within the new paradigm of automated intelligence.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u524d\u6cbf\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u519b\u4e8b\u548c\u7ecf\u6d4e\u6218\u7565\u7ade\u4e89\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u7279\u522b\u662f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u81ea\u52a8\u5316\u6218\u7565\u60c5\u62a5\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u521d\u6b65\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u6b64\u7c7b\u7cfb\u7edf\u80fd\u56de\u7b54\u7684\u95ee\u9898\u5206\u7c7b\uff0c\u5e76\u6784\u5efa\u4e86\u51b3\u5b9a\u5176\u80fd\u529b\u7684\u9ad8\u5c42\u6a21\u578b\uff0c\u6700\u540e\u4e3a\u56fd\u5bb6\u5728\u81ea\u52a8\u5316 intelligence \u65b0\u8303\u5f0f\u4e2d\u4fdd\u6301\u6218\u7565\u7ade\u4e89\u529b\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u56fd\u5bb6\u95f4\u6218\u7565\u7ade\u4e89\u65e5\u76ca\u4f9d\u8d56\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u80fd\u529b\u4e0e\u6210\u672c\uff0c\u81ea\u52a8\u5316\u519b\u4e8b\u60c5\u62a5\u6210\u4e3a\u5730\u7f18\u653f\u6cbb\u4f18\u52bf\u7684\u9996\u8981\u9886\u57df\u3002\u7136\u800c\uff0c\u5229\u7528AI\u6574\u5408\u591a\u6e90\u6570\u636e\u8fdb\u884c\u6218\u7565\u5206\u6790\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u8fdb\u884c\u4e86\u521d\u6b65\u7684\u63d0\u5347\u7814\u7a76\u4ee5\u8bc4\u4f30\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u60c5\u62a5\u5206\u6790\u4e2d\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\u6765\u754c\u5b9a\u7cfb\u7edf\u53ef\u56de\u7b54\u7684\u2018\u771f\u5b9e\u60c5\u51b5\u95ee\u9898\u2019\u7c7b\u578b\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u9ad8\u5c42\u6a21\u578b\u6765\u5206\u6790\u5f71\u54cd\u8fd9\u4e9b\u7cfb\u7edfAI\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u8868\u660e\u591a\u6a21\u6001\u6a21\u578b\u6709\u671b\u5c06\u536b\u661f\u56fe\u50cf\u3001\u624b\u673a\u5b9a\u4f4d\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u6587\u6863\u7b49\u591a\u6e90\u6570\u636e\u878d\u5408\u4e3a\u53ef\u67e5\u8be2\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u60c5\u62a5\u5206\u6790\u6548\u7387\uff1b\u5e76\u63d0\u51fa\u4e86\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\u4e0e\u95ee\u9898\u5206\u7c7b\u4f53\u7cfb\u3002", "conclusion": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5c06\u5728\u6218\u7565\u60c5\u62a5\u9886\u57df\u5e26\u6765\u8303\u5f0f\u8f6c\u53d8\uff0c\u56fd\u5bb6\u9700\u91cd\u89c6\u5176\u53d1\u5c55\u5e76\u5236\u5b9a\u76f8\u5e94\u6218\u7565\uff0c\u4ee5\u5728AI\u9a71\u52a8\u7684\u65b0\u578b\u5730\u7f18\u7ade\u4e89\u4e2d\u4fdd\u6301\u4f18\u52bf\u3002"}}
{"id": "2509.16588", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16588", "abs": "https://arxiv.org/abs/2509.16588", "authors": ["Haiming Zhang", "Yiyao Zhu", "Wending Zhou", "Xu Yan", "Yingjie Cai", "Bingbing Liu", "Shuguang Cui", "Zhen Li"], "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes\nexplicit dense BEV or volumetric construction, enabling highly efficient\ncomputation and accelerated inference. In this paper, we introduce SQS, a novel\nquery-based splatting pre-training specifically designed to advance SPMs in\nautonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian\nrepresentations from sparse queries during pre-training, leveraging\nself-supervised splatting to learn fine-grained contextual features through the\nreconstruction of multi-view images and depth maps. During fine-tuning, the\npre-trained Gaussian queries are seamlessly integrated into downstream networks\nvia query interaction mechanisms that explicitly connect pre-trained queries\nwith task-specific queries, effectively accommodating the diverse requirements\nof occupancy prediction and 3D object detection. Extensive experiments on\nautonomous driving benchmarks demonstrate that SQS delivers considerable\nperformance gains across multiple query-based 3D perception tasks, notably in\noccupancy prediction and 3D object detection, outperforming prior\nstate-of-the-art pre-training approaches by a significant margin (i.e., +1.3\nmIoU on occupancy prediction and +1.0 NDS on 3D detection).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSQS\u7684\u57fa\u4e8e\u67e5\u8be2\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7a00\u758f\u611f\u77e5\u6a21\u578b\uff08SPMs\uff09\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u81ea\u76d1\u7763splatting\u91cd\u5efa\u591a\u89c6\u56fe\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u6765\u5b66\u4e60\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u76843D\u611f\u77e5\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u5bc6\u96c6\u7684BEV\u6216\u4f53\u7d20\u8868\u793a\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff1b\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\u5e76\u589e\u5f3a\u7a00\u758f\u67e5\u8be2\u4e0b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "SQS\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757\uff0c\u4ece\u7a00\u758f\u67e5\u8be2\u9884\u6d4b3D\u9ad8\u65af\u8868\u793a\uff0c\u5e76\u5229\u7528\u81ea\u76d1\u7763splatting\u8fdb\u884c\u591a\u89c6\u56fe\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u7684\u91cd\u5efa\uff1b\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u901a\u8fc7\u67e5\u8be2\u4ea4\u4e92\u673a\u5236\u5c06\u9884\u8bad\u7ec3\u7684\u9ad8\u65af\u67e5\u8be2\u4e0e\u4efb\u52a1\u7279\u5b9a\u67e5\u8be2\u7ed3\u5408\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSQS\u5728\u5360\u7528\u9884\u6d4b\u548c3D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u5347\u4e86+1.3 mIoU\u548c+1.0 NDS\u3002", "conclusion": "SQS\u4e3a\u7a00\u758f\u611f\u77e5\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u8fc1\u79fb\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u6709\u6548\u63d0\u5347\u591a\u79cd\u4e0b\u6e383D\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17125", "abs": "https://arxiv.org/abs/2509.17125", "authors": ["Liang Heng", "Jiadong Xu", "Yiwen Wang", "Xiaoqi Li", "Muhe Cai", "Yan Shen", "Juan Zhu", "Guanghui Ren", "Hao Dong"], "title": "Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation", "comment": null, "summary": "Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)\nrequire a robot to manipulate objects with precise semantic and geometric\nreasoning. Existing approaches either rely on pre-collected demonstrations that\nstruggle to capture complex geometric constraints or generate goal-state\nobservations to capture semantic and geometric knowledge, but fail to\nexplicitly couple object transformation with action prediction, resulting in\nerrors due to generative noise. To address these limitations, we propose\nImagine2Act, a 3D imitation-learning framework that incorporates semantic and\ngeometric constraints of objects into policy learning to tackle high-precision\nmanipulation tasks. We first generate imagined goal images conditioned on\nlanguage instructions and reconstruct corresponding 3D point clouds to provide\nrobust semantic and geometric priors. These imagined goal point clouds serve as\nadditional inputs to the policy model, while an object-action consistency\nstrategy with soft pose supervision explicitly aligns predicted end-effector\nmotion with generated object transformation. This design enables Imagine2Act to\nreason about semantic and geometric relationships between objects and predict\naccurate actions across diverse tasks. Experiments in both simulation and the\nreal world demonstrate that Imagine2Act outperforms previous state-of-the-art\npolicies. More visualizations can be found at\nhttps://sites.google.com/view/imagine2act.", "AI": {"tldr": "\u63d0\u51faImagine2Act\uff0c\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7ea6\u675f\u76843D\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u60f3\u8c61\u76ee\u6807\u56fe\u50cf\u548c\u70b9\u4e91\u91cd\u5efa\u63d0\u5347\u673a\u5668\u4eba\u5728\u5173\u7cfb\u6027\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6f14\u793a\u6216\u751f\u6210\u76ee\u6807\u72b6\u6001\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u590d\u6742\u51e0\u4f55\u7ea6\u675f\uff0c\u4e14\u672a\u663e\u5f0f\u8026\u5408\u7269\u4f53\u53d8\u6362\u4e0e\u52a8\u4f5c\u9884\u6d4b\uff0c\u5bfc\u81f4\u8bef\u5dee\u3002", "method": "\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u60f3\u8c61\u7684\u76ee\u6807\u56fe\u50cf\u5e76\u91cd\u5efa3D\u70b9\u4e91\uff0c\u5c06\u5176\u4f5c\u4e3a\u7b56\u7565\u6a21\u578b\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u5e26\u8f6f\u59ff\u6001\u76d1\u7763\u7684\u5bf9\u8c61-\u52a8\u4f5c\u4e00\u81f4\u6027\u7b56\u7565\uff0c\u663e\u5f0f\u5bf9\u9f50\u672b\u7aef\u6267\u884c\u5668\u8fd0\u52a8\u4e0e\u7269\u4f53\u53d8\u6362\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cImagine2Act\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u7b56\u7565\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u5b8c\u6210\u591a\u6837\u5316\u3001\u9ad8\u7cbe\u5ea6\u7684\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u3002", "conclusion": "Imagine2Act\u901a\u8fc7\u5f15\u5165\u60f3\u8c61\u76ee\u6807\u70b9\u4e91\u548c\u5bf9\u8c61-\u52a8\u4f5c\u4e00\u81f4\u6027\u673a\u5236\uff0c\u6709\u6548\u878d\u5408\u8bed\u4e49\u4e0e\u51e0\u4f55\u5148\u9a8c\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u5173\u7cfb\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.16598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16598", "abs": "https://arxiv.org/abs/2509.16598", "authors": ["Byeongho Yu", "Changhun Lee", "Jungyu Jin", "Eunhyeok Park"], "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality", "comment": null, "summary": "To mitigate the hallucination problem in large language models, DoLa exploits\nearly exit logits from the same model as a contrastive prior. However, we found\nthat these early exit logits tend to be flat, low in magnitude, and fail to\nreflect meaningful contrasts. To address this, we propose PruneCD, a novel\ncontrastive decoding method that constructs the amateur model via layer pruning\nrather than early exit. This design leads to more informative and well-aligned\nlogits, enabling more effective contrastive decoding. Through qualitative and\nquantitative analyses, we demonstrate that PruneCD consistently improves\nfactuality with minimal inference overhead, offering a robust and practical\napproach to mitigating hallucinations in LLMs.", "AI": {"tldr": "\u63d0\u51faPruneCD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u526a\u679d\u6784\u5efa\u5bf9\u6bd4\u89e3\u7801\u4e2d\u7684\u4e1a\u4f59\u6a21\u578b\uff0c\u76f8\u6bd4\u65e9\u671f\u9000\u51fa\u80fd\u751f\u6210\u66f4\u6709\u6548\u3001\u5bf9\u9f50\u66f4\u597d\u7684logits\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u4e14\u63a8\u7406\u5f00\u9500\u4f4e\u3002", "motivation": "\u65e9\u671f\u9000\u51fa\u4ea7\u751f\u7684logits\u901a\u5e38\u5e73\u5766\u3001\u5e45\u503c\u4f4e\uff0c\u96be\u4ee5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u5bf9\u6bd4\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u5bf9\u6bd4\u89e3\u7801\uff08\u5982DoLa\uff09\u5728\u7f13\u89e3\u5927\u6a21\u578b\u5e7b\u89c9\u4e0a\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faPruneCD\uff0c\u901a\u8fc7\u5c42\u526a\u679d\u800c\u975e\u65e9\u671f\u9000\u51fa\u6765\u6784\u5efa\u5bf9\u6bd4\u7528\u7684\u4e1a\u4f59\u6a21\u578b\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u4e30\u5bcc\u3001\u5bf9\u9f50\u66f4\u597d\u7684logits\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5bf9\u6bd4\u89e3\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePruneCD\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u5747\u80fd\u4e00\u81f4\u63d0\u5347\u4e8b\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "PruneCD\u662f\u4e00\u79cd\u66f4\u4f18\u7684\u5bf9\u6bd4\u89e3\u7801\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u578b\u526a\u679d\u589e\u5f3a\u5bf9\u6bd4\u4fe1\u53f7\uff0c\u4e3a\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16664", "abs": "https://arxiv.org/abs/2509.16664", "authors": ["Simone Ricci", "Niccol\u00f2 Biondi", "Federico Pernici", "Ioannis Patras", "Alberto Del Bimbo"], "title": "$\\boldsymbol\u03bb$-Orthogonality Regularization for Compatible Representation Learning", "comment": "Accepted at NeurIPS2025", "summary": "Retrieval systems rely on representations learned by increasingly powerful\nmodels. However, due to the high training cost and inconsistencies in learned\nrepresentations, there is significant interest in facilitating communication\nbetween representations and ensuring compatibility across independently trained\nneural networks. In the literature, two primary approaches are commonly used to\nadapt different learned representations: affine transformations, which adapt\nwell to specific distributions but can significantly alter the original\nrepresentation, and orthogonal transformations, which preserve the original\nstructure with strict geometric constraints but limit adaptability. A key\nchallenge is adapting the latent spaces of updated models to align with those\nof previous models on downstream distributions while preserving the newly\nlearned representation spaces. In this paper, we impose a relaxed orthogonality\nconstraint, namely $\\lambda$-orthogonality regularization, while learning an\naffine transformation, to obtain distribution-specific adaptation while\nretaining the original learned representations. Extensive experiments across\nvarious architectures and datasets validate our approach, demonstrating that it\npreserves the model's zero-shot performance and ensures compatibility across\nmodel updates. Code available at:\nhttps://github.com/miccunifi/lambda_orthogonality", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u03bb-\u6b63\u4ea4\u6b63\u5219\u5316\u7684\u677e\u5f1b\u6b63\u4ea4\u7ea6\u675f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5b66\u4e60\u4eff\u5c04\u53d8\u6362\u65f6\u5b9e\u73b0\u5206\u5e03\u7279\u5b9a\u7684\u8868\u793a\u9002\u914d\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u5b66\u4e60\u5230\u7684\u8868\u793a\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u4fdd\u6301\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\u5e76\u786e\u4fdd\u6a21\u578b\u66f4\u65b0\u95f4\u7684\u517c\u5bb9\u6027\u3002", "motivation": "\u7531\u4e8e\u9ad8\u6602\u7684\u8bad\u7ec3\u6210\u672c\u548c\u4e0d\u540c\u6a21\u578b\u95f4\u8868\u793a\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5982\u4f55\u5728\u72ec\u7acb\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u5b9e\u73b0\u8868\u793a\u7684\u517c\u5bb9\u6027\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u7ed3\u6784\u4fdd\u6301\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u96be\u4ee5\u517c\u987e\u3002", "method": "\u63d0\u51fa\u03bb-\u6b63\u4ea4\u6b63\u5219\u5316\uff0c\u5728\u4eff\u5c04\u53d8\u6362\u4e2d\u5f15\u5165\u677e\u5f1b\u7684\u6b63\u4ea4\u7ea6\u675f\uff0c\u4ee5\u5728\u4fdd\u6301\u539f\u59cb\u8868\u793a\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u9488\u5bf9\u7279\u5b9a\u5206\u5e03\u7684\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u65e2\u80fd\u4fdd\u7559\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u53c8\u80fd\u786e\u4fdd\u6a21\u578b\u66f4\u65b0\u524d\u540e\u7684\u8868\u793a\u517c\u5bb9\u6027\u3002", "conclusion": "\u03bb-\u6b63\u4ea4\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e73\u8861\u8868\u793a\u9002\u5e94\u6027\u4e0e\u7ed3\u6784\u4fdd\u6301\u7684\u6709\u6548\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u66f4\u65b0\u4e2d\u7684\u8868\u793a\u5bf9\u9f50\u4efb\u52a1\u3002"}}
{"id": "2509.17116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17116", "abs": "https://arxiv.org/abs/2509.17116", "authors": ["Hang Xu", "Zang Yu", "Yehui Tang", "Pengbo Hu", "Yuhao Tang", "Hao Dong"], "title": "MCTS-EP: Empowering Embodied Planning with Online Preference Optimization", "comment": null, "summary": "This paper introduces MCTS-EP, an online learning framework that combines\nlarge language models (LLM) with Monte Carlo Tree Search (MCTS) for training\nembodied agents. MCTS-EP integrates three key components: MCTS-guided\nexploration for preference data collection, efficient multi-modal reasoning\nmechanism, and iterative training pipeline based on preference optimization. We\ntheoretically prove that MCTS-EP achieves better performance bounds than\nconventional on-policy algorithms when the loss function is strongly convex,\nand demonstrate that it can be formulated as a search-enhanced variant of GAIL.\nMCTS-EP achieves state-of-the-art performace across serval benchmarks. In\nALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.\nIn WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average\ninteraction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code\navailable at: https://github.com/xuhang-2/Embodied-Agent-Planning", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMCTS-EP\uff0c\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u5177\u8eab\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u9700\u8981\u7ed3\u5408\u6709\u6548\u7684\u641c\u7d22\u7b56\u7565\u4e0e\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u3002", "method": "MCTS-EP\u7ed3\u5408\u4e86MCTS\u5f15\u5bfc\u7684\u63a2\u7d22\u3001\u9ad8\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u673a\u5236\u4ee5\u53ca\u57fa\u4e8e\u504f\u597d\u4f18\u5316\u7684\u8fed\u4ee3\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edfon-policy\u7b97\u6cd5\u7684\u6027\u80fd\u754c\u3002", "result": "\u5728ALFWorld\u4e2d\uff0c\u6587\u672c\u548c\u89c6\u89c9\u4efb\u52a1\u7684\u6210\u529f\u7387\u5206\u522b\u8fbe\u523092%\u548c87%\uff1b\u5728WebShop\u4e2d\u5e73\u5747\u5956\u52b1\u4e3a0.81\uff1b\u5e76\u663e\u8457\u51cf\u5c11\u4ea4\u4e92\u6b65\u6570\uff08\u4ece18.7/19.5\u964d\u81f310.2/9.9\uff09\u3002", "conclusion": "MCTS-EP\u901a\u8fc7\u878d\u5408MCTS\u4e0eLLM\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u662f\u641c\u7d22\u589e\u5f3a\u578bGAIL\u7684\u4e00\u79cd\u6709\u6548\u5b9e\u73b0\u3002"}}
{"id": "2509.16602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16602", "abs": "https://arxiv.org/abs/2509.16602", "authors": ["Minji Heo", "Simon S. Woo"], "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection", "comment": null, "summary": "Multi-step or hybrid deepfakes, created by sequentially applying different\ndeepfake creation methods such as Face-Swapping, GAN-based generation, and\nDiffusion methods, can pose an emerging and unforseen technical challenge for\ndetection models trained on single-step forgeries. While prior studies have\nmainly focused on detecting isolated single manipulation, little is known about\nthe detection model behavior under such compositional, hybrid, and complex\nmanipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a\nlarge-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using\nfive state-of-the-art representative generators. Using this approach, we\nanalyze detection performance and spectral properties across hybrid\nmanipulation at different step, along with varying generator combinations and\nquality settings. Surprisingly, our findings reveal that detection performance\nhighly depends on the final manipulation type, with F1-score dropping by up to\n\\textbf{58.83\\%} when it differs from training distribution. This clearly\ndemonstrates that detectors rely on last-stage artifacts rather than cumulative\nmanipulation traces, limiting generalization. Such findings highlight the need\nfor detection models to explicitly consider manipulation history and sequences.\nOur results highlight the importance of benchmarks such as FakeChain,\nreflecting growing synthesis complexity and diversity in real-world scenarios.\nOur sample code is available\nhere\\footnote{https://github.com/minjihh/FakeChain}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFakeChain\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6b65\u6216\u6df7\u5408\u6df1\u5ea6\u4f2a\u9020\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u53d1\u73b0\u68c0\u6d4b\u5668\u7684\u8868\u73b0\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6700\u540e\u4e00\u6b65\u64cd\u4f5c\u7684\u7c7b\u578b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u590d\u6742\u5408\u6210\u6d41\u7a0b\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u64cd\u4f5c\u7684\u4f2a\u9020\u5185\u5bb9\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u5bf9\u591a\u6b65\u6216\u6df7\u5408\u6df1\u5ea6\u4f2a\u9020\uff08\u5982\u7ec4\u5408\u4f7f\u7528\u6362\u8138\u3001GAN\u751f\u6210\u548c\u6269\u6563\u6a21\u578b\uff09\u7f3a\u4e4f\u7814\u7a76\uff0c\u8fd9\u6784\u6210\u4e86\u65b0\u7684\u6280\u672f\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1\u6b65\u30012\u6b65\u548c3\u6b65\u4f2a\u9020\u89c6\u9891\u7684\u5927\u89c4\u6a21\u57fa\u51c6FakeChain\uff0c\u4f7f\u7528\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u751f\u6210\u5668\u7ec4\u5408\u751f\u6210\u6570\u636e\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u6b65\u9aa4\u3001\u751f\u6210\u5668\u7ec4\u5408\u548c\u8d28\u91cf\u8bbe\u7f6e\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u4e0e\u9891\u8c31\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u6700\u7ec8\u64cd\u4f5c\u7c7b\u578b\u4e0e\u8bad\u7ec3\u5206\u5e03\u4e0d\u540c\u65f6\uff0c\u68c0\u6d4bF1\u5206\u6570\u6700\u591a\u4e0b\u964d58.83%\uff0c\u8bf4\u660e\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u6700\u540e\u9636\u6bb5\u7684\u4eba\u5de5\u75d5\u8ff9\u800c\u975e\u7d2f\u79ef\u7684\u7be1\u6539\u7279\u5f81\u3002", "conclusion": "\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u8003\u8651\u7be1\u6539\u7684\u5386\u53f2\u548c\u987a\u5e8f\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u591a\u6b65\u4f2a\u9020\u7684\u5efa\u6a21\u4e0e\u68c0\u6d4b\uff0cFakeChain\u4e3a\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u73b0\u5b9e\u5408\u6210\u573a\u666f\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2509.17141", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17141", "abs": "https://arxiv.org/abs/2509.17141", "authors": ["Jingjing Chen", "Hongjie Fang", "Chenxi Wang", "Shiquan Wang", "Cewu Lu"], "title": "History-Aware Visuomotor Policy Learning via Point Tracking", "comment": null, "summary": "Many manipulation tasks require memory beyond the current observation, yet\nmost visuomotor policies rely on the Markov assumption and thus struggle with\nrepeated states or long-horizon dependencies. Existing methods attempt to\nextend observation horizons but remain insufficient for diverse memory\nrequirements. To this end, we propose an object-centric history representation\nbased on point tracking, which abstracts past observations into a compact and\nstructured form that retains only essential task-relevant information. Tracked\npoints are encoded and aggregated at the object level, yielding a compact\nhistory representation that can be seamlessly integrated into various\nvisuomotor policies. Our design provides full history-awareness with high\ncomputational efficiency, leading to improved overall task performance and\ndecision accuracy. Through extensive evaluations on diverse manipulation tasks,\nwe show that our method addresses multiple facets of memory requirements - such\nas task stage identification, spatial memorization, and action counting, as\nwell as longer-term demands like continuous and pre-loaded memory - and\nconsistently outperforms both Markovian baselines and prior history-based\napproaches. Project website: http://tonyfang.net/history", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u70b9\u8ddf\u8e2a\u7684\u7269\u4f53\u4e2d\u5fc3\u5386\u53f2\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u6709\u6548\u5e94\u5bf9\u591a\u79cd\u8bb0\u5fc6\u9700\u6c42\u5e76\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u8bb8\u591a\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u8d85\u51fa\u5f53\u524d\u89c2\u6d4b\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u4f46\u5927\u591a\u6570\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4f9d\u8d56\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\uff0c\u96be\u4ee5\u5904\u7406\u91cd\u590d\u72b6\u6001\u6216\u957f\u65f6\u7a0b\u4f9d\u8d56\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6ee1\u8db3\u591a\u6837\u5316\u8bb0\u5fc6\u9700\u6c42\u65b9\u9762\u4ecd\u663e\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u8ddf\u8e2a\u7684\u7269\u4f53\u4e2d\u5fc3\u5386\u53f2\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u8fc7\u53bb\u89c2\u6d4b\u62bd\u8c61\u4e3a\u7d27\u51d1\u4e14\u7ed3\u6784\u5316\u7684\u5f62\u5f0f\uff0c\u4ec5\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u7684\u5173\u952e\u4fe1\u606f\uff1b\u901a\u8fc7\u5728\u7269\u4f53\u5c42\u9762\u7f16\u7801\u548c\u805a\u5408\u8ddf\u8e2a\u70b9\uff0c\u751f\u6210\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u591a\u79cd\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u4e2d\u7684\u7d27\u51d1\u5386\u53f2\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u4efb\u52a1\u9636\u6bb5\u8bc6\u522b\u3001\u7a7a\u95f4\u8bb0\u5fc6\u3001\u52a8\u4f5c\u8ba1\u6570\u4ee5\u53ca\u8fde\u7eed\u548c\u9884\u52a0\u8f7d\u8bb0\u5fc6\u7b49\u591a\u65b9\u9762\u7684\u8bb0\u5fc6\u9700\u6c42\uff0c\u5728\u591a\u6837\u5316\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u9a6c\u5c14\u53ef\u592b\u57fa\u7ebf\u548c\u5148\u524d\u7684\u5386\u53f2\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u8bb0\u5fc6\u611f\u77e5\u4e0e\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u957f\u65f6\u7a0b\u4f9d\u8d56\u548c\u590d\u6742\u8bb0\u5fc6\u9700\u6c42\u4e0b\u7684\u6574\u4f53\u4efb\u52a1\u8868\u73b0\u548c\u51b3\u7b56\u51c6\u786e\u6027\u3002"}}
{"id": "2509.16709", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16709", "abs": "https://arxiv.org/abs/2509.16709", "authors": ["Nicol\u00f2 Botteghi", "Matteo Tomasetto", "Urban Fasel", "Francesco Braghin", "Andrea Manzoni"], "title": "HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems", "comment": null, "summary": "Deep reinforcement learning has recently emerged as a promising feedback\ncontrol strategy for complex dynamical systems governed by partial differential\nequations (PDEs). When dealing with distributed, high-dimensional problems in\nstate and control variables, multi-agent reinforcement learning (MARL) has been\nproposed as a scalable approach for breaking the curse of dimensionality. In\nparticular, through decentralized training and execution, multiple agents\ncooperate to steer the system towards a target configuration, relying solely on\nlocal state and reward information. However, the principle of locality may\nbecome a limiting factor whenever a collective, nonlocal behavior of the agents\nis crucial to maximize the reward function, as typically happens in\nPDE-constrained optimal control problems. In this work, we propose HypeMARL: a\ndecentralized MARL algorithm tailored to the control of high-dimensional,\nparametric, and distributed systems. HypeMARL employs hypernetworks to\neffectively parametrize the agents' policies and value functions with respect\nto the system parameters and the agents' relative positions, encoded by\nsinusoidal positional encoding. Through the application on challenging control\nproblems, such as density and flow control, we show that HypeMARL (i) can\neffectively control systems through a collective behavior of the agents,\noutperforming state-of-the-art decentralized MARL, (ii) can efficiently deal\nwith parametric dependencies, (iii) requires minimal hyperparameter tuning and\n(iv) can reduce the amount of expensive environment interactions by a factor of\n~10 thanks to its model-based extension, MB-HypeMARL, which relies on\ncomputationally efficient deep learning-based surrogate models approximating\nthe dynamics locally, with minimal deterioration of the policy performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHypeMARL\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u9ad8\u7ef4\u3001\u53c2\u6570\u5316\u548c\u5206\u5e03\u5f0f\u7684PDE\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u8d85\u7f51\u7edc\u7ed3\u5408\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\uff0c\u6709\u6548\u5b9e\u73b0\u667a\u80fd\u4f53\u7b56\u7565\u7684\u53c2\u6570\u5316\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u96c6\u4f53\u884c\u4e3a\u63a7\u5236\u80fd\u529b\uff0c\u5728\u5bc6\u5ea6\u4e0e\u6d41\u52a8\u63a7\u5236\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u9ad8\u6548\u7684\u53c2\u6570\u4f9d\u8d56\u5904\u7406\u3001\u4f4e\u8d85\u53c2\u8c03\u4f18\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u5c55MB-HypeMARL\u663e\u8457\u51cf\u5c11\u73af\u5883\u4ea4\u4e92\u6b21\u6570\u3002", "motivation": "\u5728PDE\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\uff0c\u4f20\u7edf\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u56e0\u5c40\u90e8\u6027\u9650\u5236\u96be\u4ee5\u5b9e\u73b0\u5168\u5c40\u534f\u540c\u884c\u4e3a\uff0c\u96be\u4ee5\u5e94\u5bf9\u9ad8\u7ef4\u5206\u5e03\u7cfb\u7edf\u4e2d\u7684\u975e\u5c40\u90e8\u534f\u4f5c\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u5c40\u90e8\u6267\u884c\u4e0e\u5168\u5c40\u534f\u8c03\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHypeMARL\u7b97\u6cd5\uff0c\u91c7\u7528\u8d85\u7f51\u7edc\u5bf9\u667a\u80fd\u4f53\u7684\u7b56\u7565\u548c\u4ef7\u503c\u51fd\u6570\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u7ed3\u5408\u7cfb\u7edf\u53c2\u6570\u4e0e\u667a\u80fd\u4f53\u76f8\u5bf9\u4f4d\u7f6e\uff08\u901a\u8fc7\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\u8868\u793a\uff09\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684\u8bad\u7ec3\u4e0e\u6267\u884c\uff1b\u8fdb\u4e00\u6b65\u5f15\u5165\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u7684MB-HypeMARL\uff0c\u4ee5\u51cf\u5c11\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5728\u5bc6\u5ea6\u63a7\u5236\u548c\u6d41\u52a8\u63a7\u5236\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cHypeMARL\u8868\u73b0\u51fa\u4f18\u4e8e\u5f53\u524d\u53bb\u4e2d\u5fc3\u5316MARL\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u667a\u80fd\u4f53\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u9ad8\u6548\u5904\u7406\u53c2\u6570\u4f9d\u8d56\u6027\uff0c\u51e0\u4e4e\u65e0\u9700\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u5e76\u901a\u8fc7MB-HypeMARL\u5c06\u73af\u5883\u4ea4\u4e92\u6210\u672c\u964d\u4f4e\u7ea610\u500d\uff0c\u4e14\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u6781\u5c0f\u3002", "conclusion": "HypeMARL\u4e3a\u9ad8\u7ef4\u53c2\u6570\u5316PDE\u7cfb\u7edf\u7684\u53cd\u9988\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5c40\u90e8\u6267\u884c\u4f46\u4f9d\u8d56\u5168\u5c40\u534f\u8c03\u7684\u590d\u6742\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2509.17158", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17158", "abs": "https://arxiv.org/abs/2509.17158", "authors": ["Pierre Andrews", "Amine Benhalloum", "Gerard Moreno-Torres Bertran", "Matteo Bettini", "Amar Budhiraja", "Ricardo Silveira Cabral", "Virginie Do", "Romain Froger", "Emilien Garreau", "Jean-Baptiste Gaya", "Hugo Lauren\u00e7on", "Maxime Lecanu", "Kunal Malkan", "Dheeraj Mekala", "Pierre M\u00e9nard", "Gr\u00e9goire Mialon", "Ulyana Piterbarg", "Mikhail Plekhanov", "Mathieu Rita", "Andrey Rusakov", "Thomas Scialom", "Vladislav Vorotilov", "Mengjue Wang", "Ian Yu"], "title": "ARE: Scaling Up Agent Environments and Evaluations", "comment": null, "summary": "We introduce Meta Agents Research Environments (ARE), a research platform for\nscalable creation of environments, integration of synthetic or real\napplications, and execution of agentic orchestrations. ARE provides simple\nabstractions to build complex and diverse environments, each with their own\nrules, tools, content, and verifiers, helping to bridge the gap between model\ndevelopment and real-world deployment. We also propose Gaia2, a benchmark built\nin ARE and designed to measure general agent capabilities. Beyond search and\nexecution, Gaia2 requires agents to handle ambiguities and noise, adapt to\ndynamic environments, collaborate with other agents, and operate under temporal\nconstraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new\nfailure modes that are invisible in static settings. Our experiments show that\nno system dominates across the intelligence spectrum: stronger reasoning often\ncomes at the cost of efficiency, and budget scaling curves plateau,\nhighlighting the need for new architectures and adaptive compute strategies.\nPerhaps more importantly, ARE abstractions enable continuous extension of Gaia2\nto other environments, empowering the community to rapidly create new\nbenchmarks tailored to their domains. In AI's second half, progress\nincreasingly depends on defining meaningful tasks and robust evaluations to\ndrive frontier capabilities forward.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Meta Agents Research Environments (ARE)\uff0c\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u591a\u6837\u5316\u73af\u5883\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u80fd\u529b\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eARE\u7684Gaia2\u57fa\u51c6\uff0c\u65e8\u5728\u8861\u91cf\u667a\u80fd\u4f53\u5728\u52a8\u6001\u3001\u5f02\u6b65\u73af\u5883\u4e2d\u7684\u7efc\u5408\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u7f29\u5c0f\u6a21\u578b\u5f00\u53d1\u4e0e\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u6765\u521b\u5efa\u590d\u6742\u591a\u6837\u7684\u73af\u5883\uff0c\u5e76\u5efa\u7acb\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u667a\u80fd\u4f53\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "ARE\u63d0\u4f9b\u4e86\u7b80\u6d01\u7684\u62bd\u8c61\u673a\u5236\uff0c\u652f\u6301\u6784\u5efa\u5305\u542b\u89c4\u5219\u3001\u5de5\u5177\u3001\u5185\u5bb9\u548c\u9a8c\u8bc1\u5668\u7684\u81ea\u5b9a\u4e49\u73af\u5883\uff1bGaia2\u662f\u5728ARE\u4e2d\u6784\u5efa\u7684\u5f02\u6b65\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u5177\u5907\u5904\u7406\u6a21\u7cca\u6027\u3001\u9002\u5e94\u52a8\u6001\u73af\u5883\u3001\u534f\u4f5c\u548c\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u64cd\u4f5c\u7b49\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u7cfb\u7edf\u5728\u667a\u80fd\u8c31\u7cfb\u4e0a\u5404\u6709\u6240\u957f\uff0c\u5f3a\u63a8\u7406\u80fd\u529b\u5e38\u4ee5\u6548\u7387\u4e3a\u4ee3\u4ef7\uff0c\u4e14\u9884\u7b97\u6269\u5c55\u66f2\u7ebf\u8d8b\u4e8e\u5e73\u7f13\uff1bARE\u7684\u62bd\u8c61\u8bbe\u8ba1\u652f\u6301Gaia2\u6301\u7eed\u6269\u5c55\u5230\u65b0\u9886\u57df\u3002", "conclusion": "ARE\u548cGaia2\u4e3a\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\u4e0e\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5f3a\u8c03\u4e86\u5728AI\u53d1\u5c55\u4e2d\u8bbe\u8ba1\u6709\u610f\u4e49\u4efb\u52a1\u548c\u7a33\u5065\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.16609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16609", "abs": "https://arxiv.org/abs/2509.16609", "authors": ["Shipeng Liu", "Zhonglin Zhang", "Dengfeng Chen", "Liang Zhao"], "title": "Describe-to-Score: Text-Guided Efficient Image Complexity Assessment", "comment": null, "summary": "Accurately assessing image complexity (IC) is critical for computer vision,\nyet most existing methods rely solely on visual features and often neglect\nhigh-level semantic information, limiting their accuracy and generalization. We\nintroduce vision-text fusion for IC modeling. This approach integrates visual\nand textual semantic features, increasing representational diversity. It also\nreduces the complexity of the hypothesis space, which enhances both accuracy\nand generalization in complexity assessment. We propose the D2S\n(Describe-to-Score) framework, which generates image captions with a\npre-trained vision-language model. We propose the feature alignment and entropy\ndistribution alignment mechanisms, D2S guides semantic information to inform\ncomplexity assessment while bridging the gap between vision and text\nmodalities. D2S utilizes multi-modal information during training but requires\nonly the vision branch during inference, thereby avoiding multi-modal\ncomputational overhead and enabling efficient assessment. Experimental results\ndemonstrate that D2S outperforms existing methods on the IC9600 dataset and\nmaintains competitiveness on no-reference image quality assessment (NR-IQA)\nbenchmark, validating the effectiveness and efficiency of multi-modal fusion in\ncomplexity-related tasks. Code is available at:\nhttps://github.com/xauat-liushipeng/D2S", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u6587\u672c\u878d\u5408\u7684\u56fe\u50cf\u590d\u6742\u5ea6\u8bc4\u4f30\u65b9\u6cd5D2S\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u5e76\u5229\u7528\u7279\u5f81\u5bf9\u9f50\u548c\u71b5\u5206\u5e03\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u7ed3\u5408\u591a\u6a21\u6001\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u590d\u6742\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\uff0c\u5ffd\u89c6\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u63d0\u51faD2S\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u5f15\u5165\u7279\u5f81\u5bf9\u9f50\u548c\u71b5\u5206\u5e03\u5bf9\u9f50\u673a\u5236\uff0c\u878d\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u8bed\u4e49\u7279\u5f81\uff1b\u8bad\u7ec3\u65f6\u4f7f\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u63a8\u7406\u65f6\u4ec5\u4f7f\u7528\u89c6\u89c9\u5206\u652f\u3002", "result": "\u5728IC9600\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08NR-IQA\uff09\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "D2S\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u878d\u5408\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u590d\u6742\u5ea6\u8bc4\u4f30\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u8be5\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.17195", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17195", "abs": "https://arxiv.org/abs/2509.17195", "authors": ["Damian Owerko", "Frederic Vatnsdal", "Saurav Agarwal", "Vijay Kumar", "Alejandro Ribeiro"], "title": "MAST: Multi-Agent Spatial Transformer for Learning to Collaborate", "comment": null, "summary": "This article presents a novel multi-agent spatial transformer (MAST) for\nlearning communication policies in large-scale decentralized and collaborative\nmulti-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:\n(i) partial observable states as robots make only localized perception, (ii)\nlimited communication range with no central server, and (iii) independent\nexecution of actions. The robots need to optimize a common task-specific\nobjective, which, under the restricted setting, must be done using a\ncommunication policy that exhibits the desired collaborative behavior. The\nproposed MAST is a decentralized transformer architecture that learns\ncommunication policies to compute abstract information to be shared with other\nagents and processes the received information with the robot's own\nobservations. The MAST extends the standard transformer with new positional\nencoding strategies and attention operations that employ windowing to limit the\nreceptive field for MRS. These are designed for local computation,\nshift-equivariance, and permutation equivariance, making it a promising\napproach for DC-MRS. We demonstrate the efficacy of MAST on decentralized\nassignment and navigation (DAN) and decentralized coverage control. Efficiently\ntrained using imitation learning in a centralized setting, the decentralized\nMAST policy is robust to communication delays, scales to large teams, and\nperforms better than the baselines and other learning-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7a7a\u95f4\u53d8\u6362\u5668\uff08MAST\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u611f\u77e5\u5c40\u90e8\u6027\u3001\u901a\u4fe1\u8303\u56f4\u53d7\u9650\u548c\u65e0\u4e2d\u592e\u670d\u52a1\u5668\uff0c\u534f\u4f5c\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u901a\u4fe1\u7b56\u7565\u6765\u5b9e\u73b0\u5171\u540c\u76ee\u6807\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684Transformer\u67b6\u6784MAST\uff0c\u5f15\u5165\u65b0\u7684\u4f4d\u7f6e\u7f16\u7801\u548c\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982\u7a97\u53e3\u5316\uff09\uff0c\u4ee5\u652f\u6301\u5c40\u90e8\u8ba1\u7b97\u3001\u5e73\u79fb\u7b49\u53d8\u6027\u548c\u6392\u5217\u7b49\u53d8\u6027\uff0c\u4ece\u800c\u9002\u5e94\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7279\u6027\u3002", "result": "MAST\u5728\u53bb\u4e2d\u5fc3\u5316\u4efb\u52a1\u5206\u914d\u4e0e\u5bfc\u822a\uff08DAN\uff09\u548c\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bad\u7ec3\u9ad8\u6548\uff0c\u5bf9\u901a\u4fe1\u5ef6\u8fdf\u9c81\u68d2\uff0c\u53ef\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u673a\u5668\u4eba\u56e2\u961f\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u548c\u5176\u4ed6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "MAST\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u53bb\u4e2d\u5fc3\u5316\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6709\u6548\u901a\u4fe1\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u5907\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16610", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16610", "abs": "https://arxiv.org/abs/2509.16610", "authors": ["Junhao Chen", "Jingbo Sun", "Xiang Li", "Haidong Xin", "Yuhao Xue", "Yibin Xu", "Hao Zhao"], "title": "LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts", "comment": "Accepted by EMNLP 2025 Findings", "summary": "As large language models (LLMs) advance across diverse tasks, the need for\ncomprehensive evaluation beyond single metrics becomes increasingly important.\nTo fully assess LLM intelligence, it is crucial to examine their interactive\ndynamics and strategic behaviors. We present LLMsPark, a game theory-based\nevaluation platform that measures LLMs' decision-making strategies and social\nbehaviors in classic game-theoretic settings, providing a multi-agent\nenvironment to explore strategic depth. Our system cross-evaluates 15 leading\nLLMs (both commercial and open-source) using leaderboard rankings and scoring\nmechanisms. Higher scores reflect stronger reasoning and strategic\ncapabilities, revealing distinct behavioral patterns and performance\ndifferences across models. This work introduces a novel perspective for\nevaluating LLMs' strategic intelligence, enriching existing benchmarks and\nbroadening their assessment in interactive, game-theoretic scenarios. The\nbenchmark and rankings are publicly available at https://llmsparks.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LLMsPark\uff0c\u4e00\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u5178\u535a\u5f08\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u7b56\u7565\u548c\u793e\u4f1a\u884c\u4e3a\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u73af\u5883\u8bc4\u4f3015\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6218\u7565\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u516c\u5f00\u6392\u884c\u699c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5355\u4e00\u6307\u6807\u5df2\u4e0d\u8db3\u4ee5\u5168\u9762\u8bc4\u4f30\u5176\u667a\u80fd\u6c34\u5e73\uff0c\u9700\u8981\u4ece\u4ea4\u4e92\u52a8\u6001\u548c\u6218\u7565\u884c\u4e3a\u89d2\u5ea6\u8fdb\u884c\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u5e73\u53f0LLMsPark\uff0c\u8bbe\u8ba1\u7ecf\u5178\u535a\u5f08\u573a\u666f\uff0c\u5bf915\u4e2a\u9886\u5148\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4ea4\u53c9\u8bc4\u4f30\uff0c\u4f7f\u7528\u6392\u884c\u699c\u548c\u8bc4\u5206\u673a\u5236\u5206\u6790\u5176\u6218\u7565\u51b3\u7b56\u4e0e\u793e\u4f1a\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u63a8\u7406\u548c\u6218\u7565\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u5404\u81ea\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8bc4\u5206\u8d8a\u9ad8\u4ee3\u8868\u6218\u7565\u601d\u7ef4\u8d8a\u5f3a\u3002", "conclusion": "LLMsPark\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6218\u7565\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4e30\u5bcc\u4e86\u73b0\u6709\u57fa\u51c6\uff0c\u62d3\u5c55\u4e86\u5728\u4e92\u52a8\u535a\u5f08\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2509.16743", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16743", "abs": "https://arxiv.org/abs/2509.16743", "authors": ["Subhabrata Das", "Bodruzzaman Khan", "Xiao-Yang Liu"], "title": "A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework for Time-Series Power Outage Prediction", "comment": null, "summary": "Accurately forecasting power outages is a complex task influenced by diverse\nfactors such as weather conditions [1], vegetation, wildlife, and load\nfluctuations. These factors introduce substantial variability and noise into\noutage data, making reliable prediction challenging. Long Short-Term Memory\n(LSTM) networks, a type of Recurrent Neural Network (RNN), are particularly\neffective for modeling nonlinear and dynamic time-series data, with proven\napplications in stock price forecasting [2], energy demand prediction, demand\nresponse [3], and traffic flow management [4]. This paper introduces a hybrid\ndeep learning framework, termed PCA-PR-Seq2Seq-Adam-LSTM, that integrates\nPrincipal Component Analysis (PCA), Poisson Regression (PR), a\nSequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM. PCA is\nemployed to reduce dimensionality and stabilize data variance, while Poisson\nRegression effectively models discrete outage events. The Seq2Seq-Adam-LSTM\ncomponent enhances temporal feature learning through efficient gradient\noptimization and long-term dependency capture. The framework is evaluated using\nreal-world outage records from Michigan, and results indicate that the proposed\napproach significantly improves forecasting accuracy and robustness compared to\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCA-PR-Seq2Seq-Adam-LSTM\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u7535\u529b\u4e2d\u65ad\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e3b\u6210\u5206\u5206\u6790\u3001\u6cca\u677e\u56de\u5f52\u548c\u4f18\u5316\u7684LSTM\u5e8f\u5217\u6a21\u578b\uff0c\u6709\u6548\u5904\u7406\u591a\u56e0\u7d20\u5f15\u8d77\u7684\u566a\u58f0\u548c\u975e\u7ebf\u6027\u65f6\u5e8f\u6570\u636e\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7535\u529b\u4e2d\u65ad\u53d7\u5929\u6c14\u3001\u690d\u88ab\u3001\u91ce\u751f\u52a8\u7269\u548c\u8d1f\u8377\u6ce2\u52a8\u7b49\u591a\u79cd\u590d\u6742\u56e0\u7d20\u5f71\u54cd\uff0c\u5bfc\u81f4\u6570\u636e\u566a\u58f0\u5927\u3001\u9884\u6d4b\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u6355\u6349\u8fd9\u4e9b\u975e\u7ebf\u6027\u548c\u52a8\u6001\u7279\u5f81\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3001\u6cca\u677e\u56de\u5f52\uff08PR\uff09\u548c\u57fa\u4e8eAdam\u4f18\u5316\u7684Seq2Seq-LSTM\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002PCA\u7528\u4e8e\u964d\u7ef4\u548c\u7a33\u5b9a\u65b9\u5dee\uff0cPR\u5efa\u6a21\u79bb\u6563\u4e2d\u65ad\u4e8b\u4ef6\uff0cSeq2Seq-Adam-LSTM\u589e\u5f3a\u65f6\u95f4\u7279\u5f81\u5b66\u4e60\u4e0e\u957f\u671f\u4f9d\u8d56\u6355\u83b7\u80fd\u529b\u3002", "result": "\u5728\u5bc6\u6b47\u6839\u5dde\u771f\u5b9e\u505c\u7535\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PCA-PR-Seq2Seq-Adam-LSTM\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u7535\u529b\u4e2d\u65ad\u9884\u6d4b\u4e2d\u7684\u9ad8\u566a\u58f0\u548c\u975e\u7ebf\u6027\u6311\u6218\uff0c\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17192", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17192", "abs": "https://arxiv.org/abs/2509.17192", "authors": ["Glenn Matlin", "Parv Mahajan", "Isaac Song", "Yixiong Hao", "Ryan Bard", "Stu Topp", "Evan Montoya", "M. Rehan Parwani", "Soham Shetty", "Mark Riedl"], "title": "Shall We Play a Game? Language Models for Open-ended Wargames", "comment": null, "summary": "Wargames are multi-faceted, multi-player depictions of conflict in which\nparticipants' decisions influence future events. Wargames are often used to\nexplore the strategic implications of decision-making. However, it also\nencompasses entertainment-oriented simulations, ranging from _Chess_ to\ntabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more\nopen-ended side of the spectrum of wargames, players use natural language to\nconvey their moves, and adjudicators propose outcomes. Language Models (LMs)\nare increasingly being considered for how they can provide insights into\nreal-world, consequential decisions. We conduct a scoping literature review of\na curated selection of 100 recent works on AI in wargames, from which we\nconstruct an ontology of wargames in terms of the creativity afforded to either\nthe players or adjudicators. Focusing on the space of wargames with the most\nopen-endedness for players and adjudicators, we distill a set of considerations\nfor when and how to use LMs in different application areas. We also present a\nset of safety considerations, best practices for deploying LMs in open-ended\nwargames, and conclude with a set of high-impact open research challenges.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86100\u7bc7\u5173\u4e8eAI\u5728\u6218\u4e89\u6e38\u620f\u4e2d\u7684\u5e94\u7528\u7684\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5173\u4e8e\u6218\u4e89\u6e38\u620f\u4e2d\u521b\u9020\u529b\u7684\u672c\u4f53\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u6027\u6218\u4e89\u6e38\u620f\u4e2d\u7684\u5e94\u7528\u3001\u5b89\u5168\u8003\u8651\u53ca\u6700\u4f73\u5b9e\u8df5\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u9ad8\u5f71\u54cd\u529b\u7684\u7814\u7a76\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u5f00\u653e\u6027\u6218\u4e89\u6e38\u620f\u4e2d\u8f85\u52a9\u51b3\u7b56\u548c\u7ed3\u679c\u88c1\u5b9a\uff0c\u4ee5\u63d0\u4f9b\u5bf9\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u7684\u6d1e\u5bdf\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u6784\u5efa\u6218\u4e89\u6e38\u620f\u7684\u672c\u4f53\uff0c\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u5b89\u5168\u6027\u548c\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u6218\u4e89\u6e38\u620f\u4e2d\u521b\u9020\u529b\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u6027\u6218\u4e89\u6e38\u620f\u4e2d\u7684\u5e94\u7528\u8003\u8651\u56e0\u7d20\u3001\u5b89\u5168\u95ee\u9898\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u5217\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u6027\u6218\u4e89\u6e38\u620f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5b89\u5168\u63aa\u65bd\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5145\u5206\u53d1\u6325\u5176\u4f5c\u7528\u3002"}}
{"id": "2509.16617", "categories": ["cs.CV", "cs.AI", "I.2.6; I.5.4; I.6.8"], "pdf": "https://arxiv.org/pdf/2509.16617", "abs": "https://arxiv.org/abs/2509.16617", "authors": ["David Kreismann"], "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model", "comment": "12 pages, 4 figures, to appear in GI LNI (SKILL 2025)", "summary": "As urbanization and climate change progress, urban heat island effects are\nbecoming more frequent and severe. To formulate effective mitigation plans,\ncities require detailed air temperature data. However, predictive analytics\nmethods based on conventional machine learning models and limited data\ninfrastructure often provide inaccurate predictions, especially in underserved\nareas. In this context, geospatial foundation models trained on unstructured\nglobal data demonstrate strong generalization and require minimal fine-tuning,\noffering an alternative for predictions where traditional approaches are\nlimited. This study fine-tunes a geospatial foundation model to predict urban\nland surface temperatures under future climate scenarios and explores its\nresponse to land cover changes using simulated vegetation strategies. The\nfine-tuned model achieved pixel-wise downscaling errors below 1.74 {\\deg}C and\naligned with ground truth patterns, demonstrating an extrapolation capacity up\nto 3.62 {\\deg}C.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u9884\u6d4b\u672a\u6765\u6c14\u5019\u60c5\u666f\u4e0b\u7684\u57ce\u5e02\u5730\u8868\u6e29\u5ea6\uff0c\u5e76\u8bc4\u4f30\u690d\u88ab\u7b56\u7565\u5bf9\u7f13\u89e3\u70ed\u5c9b\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u548c\u5916\u63a8\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u57ce\u5e02\u5316\u548c\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u57ce\u5e02\u70ed\u5c9b\u6548\u5e94\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u532e\u4e4f\u533a\u57df\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u6765\u652f\u6301\u57ce\u5e02\u6c14\u5019\u5e94\u5bf9\u7b56\u7565\u3002", "method": "\u57fa\u4e8e\u5168\u7403\u975e\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u6570\u636e\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7528\u4e8e\u57ce\u5e02\u5730\u8868\u6e29\u5ea6\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408\u6a21\u62df\u690d\u88ab\u8986\u76d6\u53d8\u5316\u8fdb\u884c\u60c5\u666f\u5206\u6790\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u4f4e\u4e8e1.74\u00b0C\u7684\u50cf\u7d20\u7ea7\u964d\u5c3a\u5ea6\u8bef\u5dee\uff0c\u4e0e\u771f\u5b9e\u6570\u636e\u6a21\u5f0f\u4e00\u81f4\uff0c\u5e76\u5c55\u73b0\u51fa\u6700\u9ad8\u8fbe3.62\u00b0C\u7684\u5916\u63a8\u80fd\u529b\u3002", "conclusion": "\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u57ce\u5e02\u6e29\u5ea6\u9884\u6d4b\u4e2d\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u5f3a\u9002\u5e94\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u533a\u57df\uff0c\u4e3a\u57ce\u5e02\u6c14\u5019\u9002\u5e94\u6027\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.17198", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17198", "abs": "https://arxiv.org/abs/2509.17198", "authors": ["Baoshan Song", "Weisong Wen", "Qi Zhang", "Bing Xu", "Li-Ta Hsu"], "title": "Certifiably Optimal Doppler Positioning using Opportunistic LEO Satellites", "comment": "This manuscript has been submitted to IEEE Transactions on Aerospace\n  and Electronic Systems (TAES). The current version is uploaded to arXiv for\n  open access and reference purposes only", "summary": "To provide backup and augmentation to global navigation satellite system\n(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as\nsignals of opportunity (SOP) for position, navigation and timing (PNT). Since\nthe Doppler positioning problem is non-convex, local searching methods may\nproduce two types of estimates: a global optimum without notice or a local\noptimum given an inexact initial estimate. As exact initialization is\nunavailable in some unknown environments, a guaranteed global optimization\nmethod in no need of initialization becomes necessary. To achieve this goal, we\npropose a certifiably optimal LEO Doppler positioning method by utilizing\nconvex optimization. In this paper, the certifiable positioning method is\nimplemented through a graduated weight approximation (GWA) algorithm and\nsemidefinite programming (SDP) relaxation. To guarantee the optimality, we\nderive the necessary conditions for optimality in ideal noiseless cases and\nsufficient noise bounds conditions in noisy cases. Simulation and real tests\nare conducted to evaluate the effectiveness and robustness of the proposed\nmethod. Specially, the real test using Iridium-NEXT satellites shows that the\nproposed method estimates an certifiably optimal solution with an 3D\npositioning error of 140 m without initial estimates while Gauss-Newton and\nDog-Leg are trapped in local optima when the initial point is equal or larger\nthan 1000 km away from the ground truth. Moreover, the certifiable estimation\ncan also be used as initialization in local searching methods to lower down the\n3D positioning error to 130 m.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u521d\u59cb\u4f30\u8ba1\u7684\u3001\u57fa\u4e8e\u51f8\u4f18\u5316\u7684LEO\u591a\u666e\u52d2\u5b9a\u4f4d\u7684\u53ef\u8ba4\u8bc1\u6700\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7GWA\u7b97\u6cd5\u548c\u534a\u5b9a\u89c4\u5212\u677e\u5f1b\u5b9e\u73b0\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u591a\u666e\u52d2\u5b9a\u4f4d\u95ee\u9898\u975e\u51f8\uff0c\u4f20\u7edf\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u4f9d\u8d56\u521d\u59cb\u4f30\u8ba1\uff0c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff1b\u5728\u65e0\u6cd5\u83b7\u5f97\u7cbe\u786e\u521d\u59cb\u5316\u7684\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u521d\u503c\u4e14\u80fd\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u51f8\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u6e10\u8fdb\u6743\u91cd\u903c\u8fd1\uff08GWA\uff09\u7b97\u6cd5\u548c\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u677e\u5f1b\uff0c\u5b9e\u73b0\u53ef\u8ba4\u8bc1\u7684\u6700\u4f18LEO\u591a\u666e\u52d2\u5b9a\u4f4d\uff0c\u5e76\u63a8\u5bfc\u4e86\u65e0\u566a\u58f0\u4e0b\u7684\u6700\u4f18\u6027\u5fc5\u8981\u6761\u4ef6\u548c\u6709\u566a\u58f0\u4e0b\u7684\u5145\u5206\u566a\u58f0\u754c\u6761\u4ef6\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u548c\u57fa\u4e8eIridium-NEXT\u536b\u661f\u7684\u771f\u5b9e\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u521d\u59cb\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86140\u7c73\u76843D\u5b9a\u4f4d\u8bef\u5dee\uff0c\u4f18\u4e8eGauss-Newton\u548cDog-Leg\u7b49\u65b9\u6cd5\uff08\u5728\u521d\u59cb\u70b9\u8ddd\u771f\u503c\u22651000 km\u65f6\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff09\uff0c\u4e14\u5176\u7ed3\u679c\u53ef\u4f5c\u4e3a\u521d\u59cb\u5316\u8fdb\u4e00\u6b65\u5c06\u8bef\u5dee\u964d\u81f3130\u7c73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53ef\u8ba4\u8bc1\u6700\u4f18\u65b9\u6cd5\u65e0\u9700\u521d\u59cb\u4f30\u8ba1\u5373\u53ef\u5168\u5c40\u6536\u655b\uff0c\u663e\u8457\u63d0\u5347\u4e86LEO\u591a\u666e\u52d2\u5b9a\u4f4d\u7684\u53ef\u9760\u6027\u4e0e\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8eGNSS\u5931\u6548\u6216\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u589e\u5f3a\u4e0e\u5907\u4efd\u3002"}}
{"id": "2509.16660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16660", "abs": "https://arxiv.org/abs/2509.16660", "authors": ["Zuhair Hasan Shaik", "Abdullah Mazhar", "Aseem Srivastava", "Md Shad Akhtar"], "title": "Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation", "comment": "Accepted to the NeurIPS 2025 Research Track", "summary": "Large Language Models have demonstrated impressive fluency across diverse\ntasks, yet their tendency to produce toxic content remains a critical challenge\nfor AI safety and public trust. Existing toxicity mitigation approaches\nprimarily manipulate individual neuron activations, but these methods suffer\nfrom instability, context dependence, and often compromise the model's core\nlanguage abilities. To address these shortcomings, we investigate three key\nquestions: the stability of neuron-level toxicity indicators, the advantages of\nstructural (layer-wise) representations, and the interpretability of mechanisms\ndriving toxic generation. Through extensive experiments on Jigsaw and ToxiCN\ndatasets, we show that aggregated layer-wise features provide more robust\nsignals than single neurons. Moreover, we observe conceptual limitations in\nprior works that conflate toxicity detection experts and generation experts\nwithin neuron-based interventions. To mitigate this, we propose a novel\nprincipled intervention technique, EigenShift, based on eigen-decomposition of\nthe language model's final output layer. This method selectively targets\ngeneration-aligned components, enabling precise toxicity suppression without\nimpairing linguistic competence. Our method requires no additional training or\nfine-tuning, incurs minimal computational cost, and is grounded in rigorous\ntheoretical analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5206\u89e3\u7684\u65b0\u578b\u5e72\u9884\u6280\u672fEigenShift\uff0c\u901a\u8fc7\u5c42\u95f4\u805a\u5408\u7279\u5f81\u5b9e\u73b0\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6bd2\u6027\u5185\u5bb9\u751f\u6210\u7684\u7cbe\u786e\u6291\u5236\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u6bd2\u6027\u7f13\u89e3\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u4e2a\u795e\u7ecf\u5143\u64cd\u4f5c\uff0c\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u548c\u635f\u5bb3\u6a21\u578b\u8bed\u8a00\u80fd\u529b\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u6709\u6548\u63a7\u5236\u6bd2\u6027\u751f\u6210\u3002", "method": "\u63d0\u51faEigenShift\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5c42\u7684\u7279\u5f81\u5206\u89e3\uff0c\u5229\u7528\u5c42\u7ea7\u8868\u793a\u800c\u975e\u5355\u795e\u7ecf\u5143\u6fc0\u6d3b\uff0c\u8bc6\u522b\u5e76\u5e72\u9884\u4e0e\u6bd2\u6027\u751f\u6210\u5bf9\u9f50\u7684\u6210\u5206\uff0c\u4ece\u7ed3\u6784\u5c42\u9762\u8fdb\u884c\u6bd2\u6027\u6291\u5236\u3002", "result": "\u5728Jigsaw\u548cToxiCN\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5c42\u95f4\u805a\u5408\u7279\u5f81\u6bd4\u5355\u795e\u7ecf\u5143\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u6bd2\u6027\u4fe1\u53f7\uff0cEigenShift\u80fd\u6709\u6548\u6291\u5236\u6bd2\u6027\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u8bed\u8a00\u6d41\u7545\u6027\u548c\u6027\u80fd\u3002", "conclusion": "EigenShift\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u7406\u8bba\u4e25\u8c28\u7684\u6bd2\u6027\u6291\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u65b9\u6cd5\u4e2d\u68c0\u6d4b\u4e13\u5bb6\u4e0e\u751f\u6210\u4e13\u5bb6\u6df7\u6dc6\u7684\u6982\u5ff5\u5c40\u9650\uff0c\u63d0\u5347\u4e86\u5e72\u9884\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.16750", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16750", "abs": "https://arxiv.org/abs/2509.16750", "authors": ["Alejandro Almod\u00f3var", "Patricia A. Apell\u00e1niz", "Alba Garrido", "Fernando Fern\u00e1ndez-Salvador", "Santiago Zazo", "Juan Parras"], "title": "Interpretable Clinical Classification with Kolgomorov-Arnold Networks", "comment": null, "summary": "Why should a clinician trust an Artificial Intelligence (AI) prediction?\nDespite the increasing accuracy of machine learning methods in medicine, the\nlack of transparency continues to hinder their adoption in clinical practice.\nIn this work, we explore Kolmogorov-Arnold Networks (KANs) for clinical\nclassification tasks on tabular data. Unlike traditional neural networks, KANs\nare function-based architectures that offer intrinsic interpretability through\ntransparent, symbolic representations. We introduce Logistic-KAN, a flexible\ngeneralization of logistic regression, and Kolmogorov-Arnold Additive Model\n(KAAM), a simplified additive variant that delivers transparent, symbolic\nformulas. Unlike black-box models that require post-hoc explainability tools,\nour models support built-in patient-level insights, intuitive visualizations,\nand nearest-patient retrieval. Across multiple health datasets, our models\nmatch or outperform standard baselines, while remaining fully interpretable.\nThese results position KANs as a promising step toward trustworthy AI that\nclinicians can understand, audit, and act upon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKolmogorov-Arnold Networks\uff08KANs\uff09\u7684\u53ef\u89e3\u91ca\u6027AI\u6a21\u578b\uff0c\u7528\u4e8e\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\uff0c\u901a\u8fc7\u5185\u5728\u900f\u660e\u7684\u7b26\u53f7\u5316\u8868\u793a\u63d0\u5347\u533b\u751f\u5bf9AI\u9884\u6d4b\u7684\u4fe1\u4efb\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5f53\u524d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u9650\u5236\uff0c\u533b\u751f\u96be\u4ee5\u4fe1\u4efb\u9ed1\u7bb1\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86Logistic-KAN\u548cKolmogorov-Arnold Additive Model\uff08KAAM\uff09\u4e24\u79cd\u57fa\u4e8eKAN\u7684\u6a21\u578b\uff0c\u5229\u7528\u51fd\u6570\u5f0f\u67b6\u6784\u5b9e\u73b0\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u652f\u6301\u60a3\u8005\u7ea7\u522b\u7684\u6d1e\u5bdf\u3001\u76f4\u89c2\u53ef\u89c6\u5316\u548c\u6700\u8fd1\u60a3\u8005\u68c0\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u533b\u7597\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u6216\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "KANs\u4e3a\u6784\u5efa\u4e34\u5e8a\u53ef\u4fe1\u3001\u53ef\u5ba1\u8ba1\u4e14\u53ef\u64cd\u4f5c\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2509.17238", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17238", "abs": "https://arxiv.org/abs/2509.17238", "authors": ["Soheil Zibakhsh", "Mohammad Samragh", "Kumari Nishu", "Lauren Hannah", "Arnav Kundu", "Minsik Cho"], "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE", "comment": null, "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.", "AI": {"tldr": "\u63d0\u51fa\u8d85\u5e76\u884c\u6269\u5c55\uff08hyper-parallel scaling\uff09\u65b9\u6cd5\uff0c\u5728token\u7ea7\u522b\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8d28\u91cf\uff0c\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u4e2d\u7684\u201c\u4e13\u5bb6\u7ec4\u5408\u201d\uff08RoE\uff09\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u5e8f\u5217\u7ea7\u7f29\u653e\u65b9\u6cd5\u867d\u80fd\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u5728token\u7ea7\u522b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "method": "\u5728MoE\u6a21\u578b\u4e2d\u5f15\u5165RoE\uff0c\u901a\u8fc7\u63a7\u5236\u4e13\u5bb6\u8def\u7531\u673a\u5236\u7684\u968f\u673a\u6027\uff0c\u4e3a\u6bcf\u4e2atoken\u91c7\u6837\u591a\u4e2a\u4e0d\u540c\u4e13\u5bb6\u5e76\u805a\u5408\u8f93\u51fa\uff1b\u8bbe\u8ba1\u9ad8\u6548\u7684\u6279\u5904\u7406\u7b56\u7565\u548cKV\u7f13\u5b58\u673a\u5236\u4ee5\u964d\u4f4e\u5f00\u9500\u3002", "result": "7B\u7684MoE\u6a21\u578b\u5728\u63a8\u7406\u8ba1\u7b97\u51cf\u5c1130%\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8fbe\u523010.5B MoE\u6a21\u578b\u7684\u6c34\u5e73\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u5fae\u8c03\u3002", "conclusion": "RoE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u9ad8\u6548\u4f4e\u8017\u7684\u63a8\u7406\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86MoE\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2509.16618", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16618", "abs": "https://arxiv.org/abs/2509.16618", "authors": ["Pengfei Hao", "Hongqiu Wang", "Shuaibo Li", "Zhaohu Xing", "Guang Yang", "Kaishun Wu", "Lei Zhu"], "title": "Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery", "comment": "Early accepted by MICCAI2025", "summary": "In recent years, Visual Question Localized-Answering in robotic surgery\n(Surgical-VQLA) has gained significant attention for its potential to assist\nmedical students and junior doctors in understanding surgical scenes. Recently,\nthe rapid development of Large Language Models (LLMs) has provided more\npromising solutions for this task. However, current methods struggle to\nestablish complex dependencies between text and visual details, and have\ndifficulty perceiving the spatial information of surgical scenes. To address\nthese challenges, we propose a novel method, Surgical-MambaLLM, which is the\nfirst to combine Mamba2 with LLM in the surgical domain, that leverages\nMamba2's ability to effectively capture cross-modal dependencies and perceive\nspatial information in surgical scenes, thereby enhancing the LLMs'\nunderstanding of surgical images. Specifically, we propose the Cross-modal\nBidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective\nmultimodal fusion, with its cross-modal integration capabilities. Additionally,\ntailored to the geometric characteristics of surgical scenes, we design the\nSurgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the\nsurgical images, enhancing the model's spatial understanding of the surgical\nscene. Extensive experiments demonstrate that our Surgical-MambaLLM model\noutperforms the state-of-the-art methods on the EndoVis17-VQLA and\nEndoVis18-VQLA datasets, significantly improving the performance of the\nSurgical-VQLA task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5916\u79d1\u624b\u672f\u89c6\u89c9\u95ee\u7b54\u65b9\u6cd5Surgical-MambaLLM\uff0c\u9996\u6b21\u5c06Mamba2\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u53cc\u5411\u96c6\u6210\u6a21\u5757\u548c\u9488\u5bf9\u624b\u672f\u573a\u666f\u8bbe\u8ba1\u7684\u5668\u68b0\u611f\u77e5\u626b\u63cf\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u591a\u6a21\u6001\u4fe1\u606f\u548c\u7a7a\u95f4\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u6587\u672c\u4e0e\u89c6\u89c9\u7ec6\u8282\u4e4b\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u611f\u77e5\u624b\u672f\u573a\u666f\u7684\u7a7a\u95f4\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSurgical-MambaLLM\uff0c\u7ed3\u5408Mamba2\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5f15\u5165\u8de8\u6a21\u6001\u53cc\u5411Mamba2\u96c6\u6210\uff08CBMI\uff09\u6a21\u5757\u5b9e\u73b0\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u5e76\u8bbe\u8ba1\u624b\u672f\u5668\u68b0\u611f\u77e5\uff08SIP\uff09\u626b\u63cf\u6a21\u5f0f\u4ee5\u589e\u5f3a\u5bf9\u624b\u672f\u56fe\u50cf\u7a7a\u95f4\u7ed3\u6784\u7684\u7406\u89e3\u3002", "result": "\u5728EndoVis17-VQLA\u548cEndoVis18-VQLA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5916\u79d1\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "Surgical-MambaLLM\u901a\u8fc7\u5f15\u5165Mamba2\u67b6\u6784\u548c\u5b9a\u5236\u5316\u7684\u626b\u63cf\u7b56\u7565\uff0c\u6210\u529f\u589e\u5f3a\u4e86\u8de8\u6a21\u6001\u4f9d\u8d56\u6355\u6349\u548c\u7a7a\u95f4\u4fe1\u606f\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u5916\u79d1\u624b\u672f\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17204", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17204", "abs": "https://arxiv.org/abs/2509.17204", "authors": ["James R. Han", "Mithun Vanniasinghe", "Hshmat Sahak", "Nicholas Rhinehart", "Timothy D. Barfoot"], "title": "Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation", "comment": "8 pages. Under review at ICRA 2026", "summary": "Scaling Reinforcement Learning to in-the-wild social robot navigation is both\ndata-intensive and unsafe, since policies must learn through direct interaction\nand inevitably encounter collisions. Offline Imitation learning (IL) avoids\nthese risks by collecting expert demonstrations safely, training entirely\noffline, and deploying policies zero-shot. However, we find that naively\napplying Behaviour Cloning (BC) to social navigation is insufficient; achieving\nstrong performance requires careful architectural and training choices. We\npresent Ratatouille, a pipeline and model architecture that, without changing\nthe data, reduces collisions per meter by 6 times and improves success rate by\n3 times compared to naive BC. We validate our approach in both simulation and\nthe real world, where we collected over 11 hours of data on a dense university\ncampus. We further demonstrate qualitative results in a public food court. Our\nfindings highlight that thoughtful IL design, rather than additional data, can\nsubstantially improve safety and reliability in real-world social navigation.\nVideo: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRatatouille\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4e0d\u589e\u52a0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8fdb\u884c\u793e\u4ea4\u5bfc\u822a\u65f6\u5b58\u5728\u6570\u636e\u9700\u6c42\u5927\u548c\u5b89\u5168\u6027\u5dee\u7684\u95ee\u9898\uff0c\u800c\u7b80\u5355\u7684\u884c\u4e3a\u514b\u9686\u6548\u679c\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6a21\u4eff\u5b66\u4e60\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faRatatouille\u6846\u67b6\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u57fa\u4e8e\u79bb\u7ebf\u4e13\u5bb6\u793a\u8303\u8fdb\u884c\u884c\u4e3a\u514b\u9686\uff0c\u65e0\u9700\u5728\u7ebf\u4ea4\u4e92\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\u3002", "result": "\u76f8\u6bd4\u6734\u7d20\u884c\u4e3a\u514b\u9686\uff0c\u78b0\u649e\u7387\u964d\u4f4e6\u500d\uff0c\u6210\u529f\u7387\u63d0\u9ad83\u500d\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6821\u56ed\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u8fd8\u5728\u516c\u5171\u7f8e\u98df\u5e7f\u573a\u5c55\u793a\u4e86\u5b9a\u6027\u7ed3\u679c\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u73b0\u5b9e\u4e16\u754c\u793e\u4ea4\u5bfc\u822a\u7684\u5b89\u5168\u4e0e\u53ef\u9760\u6027\uff0c\u800c\u4e0d\u4f9d\u8d56\u66f4\u591a\u6570\u636e\u3002"}}
{"id": "2509.16666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16666", "abs": "https://arxiv.org/abs/2509.16666", "authors": ["Ahmet Yavuz Uluslu", "Tannon Kew", "Tilia Ellendorff", "Gerold Schneider", "Rico Sennrich"], "title": "Robust Native Language Identification through Agentic Decomposition", "comment": "Accepted at EMNLP* 2025", "summary": "Large language models (LLMs) often achieve high performance in native\nlanguage identification (NLI) benchmarks by leveraging superficial contextual\nclues such as names, locations, and cultural stereotypes, rather than the\nunderlying linguistic patterns indicative of native language (L1) influence. To\nimprove robustness, previous work has instructed LLMs to disregard such clues.\nIn this work, we demonstrate that such a strategy is unreliable and model\npredictions can be easily altered by misleading hints. To address this problem,\nwe introduce an agentic NLI pipeline inspired by forensic linguistics, where\nspecialized agents accumulate and categorize diverse linguistic evidence before\nan independent final overall assessment. In this final assessment, a goal-aware\ncoordinating agent synthesizes all evidence to make the NLI prediction. On two\nbenchmark datasets, our approach significantly enhances NLI robustness against\nmisleading contextual clues and performance consistency compared to standard\nprompting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53d7\u6cd5\u5ead\u8bed\u8a00\u5b66\u542f\u53d1\u7684\u4ee3\u7406\u5f0fNLI\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u5347\u5927\u6a21\u578b\u5728\u6bcd\u8bed\u8bc6\u522b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u5bf9\u8868\u9762\u7ebf\u7d22\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6bcd\u8bed\u8bc6\u522b\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u540d\u5b57\u3001\u5730\u70b9\u548c\u6587\u5316\u523b\u677f\u5370\u8c61\u7b49\u8868\u9762\u7ebf\u7d22\uff0c\u800c\u975e\u771f\u6b63\u7684\u8bed\u8a00\u5f71\u54cd\u7279\u5f81\uff0c\u5bfc\u81f4\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684NLI\u6d41\u6c34\u7ebf\uff0c\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u5206\u522b\u6536\u96c6\u548c\u5206\u7c7b\u4e0d\u540c\u7c7b\u578b\u7684\u8bed\u8a00\u5b66\u8bc1\u636e\uff0c\u6700\u540e\u7531\u4e00\u4e2a\u76ee\u6807\u611f\u77e5\u7684\u534f\u8c03\u667a\u80fd\u4f53\u7efc\u5408\u6240\u6709\u8bc1\u636e\u505a\u51fa\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6bcd\u8bed\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u5bf9\u975e\u8bed\u8a00\u56e0\u7d20\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.16756", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16756", "abs": "https://arxiv.org/abs/2509.16756", "authors": ["Yuchen Liang", "Yingbin Liang", "Lifeng Lai", "Ness Shroff"], "title": "Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees", "comment": null, "summary": "Discrete diffusion models have recently gained significant prominence in\napplications involving natural language and graph data. A key factor\ninfluencing their effectiveness is the efficiency of discretized samplers.\nAmong these, $\\tau$-leaping samplers have become particularly popular due to\ntheir empirical success. However, existing theoretical analyses of\n$\\tau$-leaping often rely on somewhat restrictive and difficult-to-verify\nregularity assumptions, and their convergence bounds contain quadratic\ndependence on the vocabulary size. In this work, we introduce a new analytical\napproach for discrete diffusion models that removes the need for such\nassumptions. For the standard $\\tau$-leaping method, we establish convergence\nguarantees in KL divergence that scale linearly with vocabulary size, improving\nupon prior results with quadratic dependence. Our approach is also more broadly\napplicable: it provides the first convergence guarantees for other widely used\nsamplers, including the Euler method and Tweedie $\\tau$-leaping. Central to our\napproach is a novel technique based on differential inequalities, offering a\nmore flexible alternative to the traditional Girsanov change-of-measure\nmethods. This technique may also be of independent interest for the analysis of\nother stochastic processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86\u5bf9\u9650\u5236\u6027\u5047\u8bbe\u7684\u4f9d\u8d56\uff0c\u5e76\u4e3a\u03c4-leaping\u7b49\u91c7\u6837\u5668\u63d0\u4f9b\u4e86\u5728KL\u6563\u5ea6\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u5176\u6536\u655b\u754c\u4e0e\u8bcd\u6c47\u91cf\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u4f18\u4e8e\u4ee5\u5f80\u7684\u4e8c\u6b21\u4f9d\u8d56\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u03c4-leaping\u65b9\u6cd5\u7684\u7406\u8bba\u5206\u6790\u4f9d\u8d56\u4e8e\u96be\u4ee5\u9a8c\u8bc1\u7684\u5f3a\u6b63\u5219\u6027\u5047\u8bbe\uff0c\u4e14\u6536\u655b\u754c\u5728\u8bcd\u6c47\u91cf\u4e0a\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u9650\u5236\u4e86\u7406\u8bba\u9002\u7528\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5fae\u5206\u4e0d\u7b49\u5f0f\u7684\u65b0\u6280\u672f\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684Girsanov\u6d4b\u5ea6\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u6536\u655b\u6027\u3002", "result": "\u5bf9\u6807\u51c6\u03c4-leaping\u65b9\u6cd5\u5efa\u7acb\u4e86KL\u6563\u5ea6\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u6536\u655b\u754c\u4e0e\u8bcd\u6c47\u91cf\u5448\u7ebf\u6027\u5173\u7cfb\uff1b\u5e76\u9996\u6b21\u4e3a\u6b27\u62c9\u65b9\u6cd5\u548cTweedie \u03c4-leaping\u7b49\u5e38\u7528\u91c7\u6837\u5668\u63d0\u4f9b\u4e86\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u653e\u5bbd\u4e86\u5047\u8bbe\u6761\u4ef6\uff0c\u63d0\u5347\u4e86\u7406\u8bba\u9002\u7528\u8303\u56f4\uff0c\u8fd8\u4e3a\u5206\u6790\u5176\u4ed6\u968f\u673a\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u53ef\u80fd\u7684\u65b0\u5de5\u5177\u3002"}}
{"id": "2509.16623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16623", "abs": "https://arxiv.org/abs/2509.16623", "authors": ["Junjie Zhou", "Haijun Xiong", "Junhao Lu", "Ziyu Lin", "Bin Feng"], "title": "CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition", "comment": "Accepted by IJCB2025", "summary": "Skeleton-based gait emotion recognition has received significant attention\ndue to its wide-ranging applications. However, existing methods primarily focus\non extracting spatial and local temporal motion information, failing to capture\nlong-range temporal representations. In this paper, we propose\n\\textbf{CGTGait}, a novel framework that collaboratively integrates graph\nconvolution and transformers to extract discriminative spatiotemporal features\nfor gait emotion recognition. Specifically, CGTGait consists of multiple CGT\nblocks, where each block employs graph convolution to capture frame-level\nspatial topology and the transformer to model global temporal dependencies.\nAdditionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to\neffectively aggregate posture and motion spatiotemporal features, facilitating\nthe exchange of complementary information between the two streams. We evaluate\nour method on two widely used datasets, Emotion-Gait and ELMD, demonstrating\nthat our CGTGait achieves state-of-the-art or at least competitive performance\nwhile reducing computational complexity by approximately \\textbf{82.2\\%} (only\nrequiring 0.34G FLOPs) during testing. Code is available at\n\\small{https://github.com/githubzjj1/CGTGait.}", "AI": {"tldr": "\u63d0\u51faCGTGait\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u548cTransformer\u6355\u83b7\u65f6\u7a7a\u7279\u5f81\uff0c\u7528\u4e8e\u57fa\u4e8e\u9aa8\u67b6\u7684\u6b65\u6001\u60c5\u611f\u8bc6\u522b\uff0c\u5728Emotion-Gait\u548cELMD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8ba1\u7b97\u91cf\u4ec50.34G FLOPs\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u548c\u5c40\u90e8\u65f6\u5e8f\u4fe1\u606f\uff0c\u96be\u4ee5\u6355\u6349\u957f\u8ddd\u79bb\u65f6\u5e8f\u4f9d\u8d56\uff0c\u9650\u5236\u4e86\u6b65\u6001\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u63d0\u51faCGTGait\u6846\u67b6\uff0c\u5305\u542b\u591a\u4e2aCGT\u6a21\u5757\uff0c\u5229\u7528\u56fe\u5377\u79ef\u63d0\u53d6\u5e27\u7ea7\u7a7a\u95f4\u62d3\u6251\uff0cTransformer\u5efa\u6a21\u5168\u5c40\u65f6\u5e8f\u4f9d\u8d56\uff1b\u5f15\u5165\u53cc\u5411\u8de8\u6d41\u878d\u5408\uff08BCSF\uff09\u6a21\u5757\u878d\u5408\u59ff\u6001\u4e0e\u8fd0\u52a8\u7279\u5f81\u3002", "result": "\u5728Emotion-Gait\u548cELMD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6216\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u51cf\u5c11\u7ea682.2%\uff08\u4ec5\u97000.34G FLOPs\uff09\u3002", "conclusion": "CGTGait\u80fd\u6709\u6548\u6355\u6349\u5224\u522b\u6027\u65f6\u7a7a\u7279\u5f81\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u9ad8\u6548\u6b65\u6001\u60c5\u611f\u8bc6\u522b\u3002"}}
{"id": "2509.17210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17210", "abs": "https://arxiv.org/abs/2509.17210", "authors": ["Shaunak A. Mehta", "Dylan P. Losey"], "title": "Combining Performance and Passivity in Linear Control of Series Elastic Actuators", "comment": null, "summary": "When humans physically interact with robots, we need the robots to be both\nsafe and performant. Series elastic actuators (SEAs) fundamentally advance\nsafety by introducing compliant actuation. On the one hand, adding a spring\nmitigates the impact of accidental collisions between human and robot; but on\nthe other hand, this spring introduces oscillations and fundamentally decreases\nthe robot's ability to perform precise, accurate motions. So how should we\ntrade off between physical safety and performance? In this paper, we enumerate\nthe different linear control and mechanical configurations for series elastic\nactuators, and explore how each choice affects the rendered compliance,\npassivity, and tracking performance. While prior works focus on load side\ncontrol, we find that actuator side control has significant benefits. Indeed,\nsimple PD controllers on the actuator side allow for a much wider range of\ncontrol gains that maintain safety, and combining these with a damper in the\nelastic transmission yields high performance. Our simulations and real world\nexperiments suggest that, by designing a system with low physical stiffness and\nhigh controller gains, this solution enables accurate performance while also\nensuring user safety during collisions.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e32\u8054\u5f39\u6027\u9a71\u52a8\u5668\uff08SEAs\uff09\u5728\u4fdd\u8bc1\u4eba\u673a\u4ea4\u4e92\u5b89\u5168\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u91c7\u7528\u6267\u884c\u5668\u4fa7\u63a7\u5236\u7ed3\u5408\u9ad8\u63a7\u5236\u589e\u76ca\u548c\u4f4e\u7269\u7406\u521a\u5ea6\u7684\u8bbe\u8ba1\u53ef\u5728\u786e\u4fdd\u5b89\u5168\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "motivation": "\u5728\u4eba\u673a\u7269\u7406\u4ea4\u4e92\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u517c\u987e\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002\u4e32\u8054\u5f39\u6027\u9a71\u52a8\u5668\u867d\u63d0\u5347\u4e86\u5b89\u5168\u6027\uff0c\u4f46\u4f1a\u5f15\u5165\u632f\u8361\u5e76\u964d\u4f4e\u8fd0\u52a8\u7cbe\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u6743\u8861\u5b89\u5168\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u679a\u4e3e\u4e86\u4e32\u8054\u5f39\u6027\u9a71\u52a8\u5668\u7684\u4e0d\u540c\u7ebf\u6027\u63a7\u5236\u548c\u673a\u68b0\u914d\u7f6e\uff0c\u6bd4\u8f83\u5176\u5bf9\u67d4\u987a\u6027\u3001\u65e0\u6e90\u6027\u548c\u8ddf\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5206\u6790\u6267\u884c\u5668\u4fa7\u63a7\u5236\u7684\u4f18\u52bf\uff0c\u5e76\u7ed3\u5408\u4eff\u771f\u4e0e\u5b9e\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u6267\u884c\u5668\u4fa7PD\u63a7\u5236\u5668\u5141\u8bb8\u66f4\u5bbd\u7684\u5b89\u5168\u63a7\u5236\u589e\u76ca\u8303\u56f4\uff0c\u7ed3\u5408\u5f39\u6027\u4f20\u52a8\u4e2d\u7684\u963b\u5c3c\u5668\u53ef\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff1b\u4f4e\u7269\u7406\u521a\u5ea6\u4e0e\u9ad8\u63a7\u5236\u5668\u589e\u76ca\u7684\u7cfb\u7edf\u8bbe\u8ba1\u80fd\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u78b0\u649e\u5b89\u5168\u6027\u3002", "conclusion": "\u901a\u8fc7\u6267\u884c\u5668\u4fa7\u63a7\u5236\u3001\u4f4e\u521a\u5ea6\u8bbe\u8ba1\u548c\u9ad8\u589e\u76ca\u53cd\u9988\u7684\u7ec4\u5408\uff0c\u53ef\u4ee5\u5728\u4fdd\u969c\u4eba\u673a\u4ea4\u4e92\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u826f\u597d\u7684\u8fd0\u52a8\u6027\u80fd\uff0c\u4e3aSEAs\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2509.16679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16679", "abs": "https://arxiv.org/abs/2509.16679", "authors": ["Keliang Liu", "Dingkang Yang", "Ziyun Qian", "Weijie Yin", "Yuchi Wang", "Hongsheng Li", "Jun Liu", "Peng Zhai", "Yang Liu", "Lihua Zhang"], "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle", "comment": "A Survey of Reinforcement Learning for Large Language Models", "summary": "In recent years, training methods centered on Reinforcement Learning (RL)\nhave markedly enhanced the reasoning and alignment performance of Large\nLanguage Models (LLMs), particularly in understanding human intents, following\nuser instructions, and bolstering inferential strength. Although existing\nsurveys offer overviews of RL augmented LLMs, their scope is often limited,\nfailing to provide a comprehensive summary of how RL operates across the full\nlifecycle of LLMs. We systematically review the theoretical and practical\nadvancements whereby RL empowers LLMs, especially Reinforcement Learning with\nVerifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.\nSecond, we thoroughly detail application strategies for RL across various\nphases of the LLM lifecycle, including pre-training, alignment fine-tuning, and\nreinforced reasoning. In particular, we emphasize that RL methods in the\nreinforced reasoning phase serve as a pivotal driving force for advancing model\nreasoning to its limits. Next, we collate existing datasets and evaluation\nbenchmarks currently used for RL fine-tuning, spanning human-annotated\ndatasets, AI-assisted preference data, and program-verification-style corpora.\nSubsequently, we review the mainstream open-source tools and training\nframeworks available, providing clear practical references for subsequent\nresearch. Finally, we analyse the future challenges and trends in the field of\nRL-enhanced LLMs. This survey aims to present researchers and practitioners\nwith the latest developments and frontier trends at the intersection of RL and\nLLMs, with the goal of fostering the evolution of LLMs that are more\nintelligent, generalizable, and secure.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5168\u751f\u547d\u5468\u671f\u4e2d\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u8fdb\u5c55\uff0c\u91cd\u70b9\u805a\u7126\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\uff0c\u6db5\u76d6\u9884\u8bad\u7ec3\u3001\u5bf9\u9f50\u5fae\u8c03\u548c\u589e\u5f3a\u63a8\u7406\u7b49\u9636\u6bb5\uff0c\u5e76\u603b\u7ed3\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u57fa\u51c6\u3001\u5f00\u6e90\u5de5\u5177\u53ca\u672a\u6765\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u5bf9RL\u589e\u5f3aLLM\u7684\u8986\u76d6\u4e0d\u5168\u9762\uff0c\u7f3a\u4e4f\u5bf9RL\u5728LLM\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u4f5c\u7528\u7684\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\u4ecb\u7ecdRL\u57fa\u7840\u7406\u8bba\uff0c\u7136\u540e\u8be6\u7ec6\u68b3\u7406RL\u5728LLM\u4e0d\u540c\u9636\u6bb5\u7684\u5e94\u7528\u7b56\u7565\uff0c\u6574\u7406\u76f8\u5173\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u57fa\u51c6\uff0c\u56de\u987e\u4e3b\u6d41\u5f00\u6e90\u5de5\u5177\u4e0e\u6846\u67b6\uff0c\u5e76\u5206\u6790\u672a\u6765\u8d8b\u52bf\u4e0e\u6311\u6218\u3002", "result": "\u63d0\u4f9b\u4e86RL\u5728LLM\u4e2d\u5e94\u7528\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u7a81\u51fa\u4e86RLVR\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3a\u63a8\u7406\u9636\u6bb5\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u6c47\u603b\u4e86\u5f53\u524d\u4f7f\u7528\u7684\u6570\u636e\u8d44\u6e90\u548c\u5de5\u5177\u5e73\u53f0\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86RL\u4e0eLLM\u4ea4\u53c9\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u524d\u6cbf\u8d8b\u52bf\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u667a\u80fd\u3001\u901a\u7528\u548c\u5b89\u5168\u7684LLM\u53d1\u5c55\u3002"}}
{"id": "2509.16769", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 62H30, 62M45", "I.2.6; I.5.1; I.5.2; G.3"], "pdf": "https://arxiv.org/pdf/2509.16769", "abs": "https://arxiv.org/abs/2509.16769", "authors": ["Prasanth K K", "Shubham Sharma"], "title": "Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes", "comment": "21 pages, 6 figures, 14 tables", "summary": "Many real world categories are multimodal, with single classes occupying\ndisjoint regions in feature space. Classical linear models (logistic\nregression, linear SVM) use a single global hyperplane and perform poorly on\nsuch data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal\nstructure but at the expense of interpretability, heavier tuning, and higher\ncomputational cost. We propose the Geometric Mixture Classifier (GMC), a\ndiscriminative model that represents each class as a mixture of hyperplanes.\nWithin each class, GMC combines plane scores via a temperature-controlled\nsoft-OR (log-sum-exp), smoothly approximating the max; across classes, standard\nsoftmax yields probabilistic posteriors. GMC optionally uses Random Fourier\nFeatures (RFF) for nonlinear mappings while keeping inference linear in the\nnumber of planes and features. Our practical training recipe: geometry-aware\nk-means initialization, silhouette-based plane budgeting, alpha annealing,\nusage-aware L2 regularization, label smoothing, and early stopping, makes GMC\nplug-and-play. Across synthetic multimodal datasets (moons, circles, blobs,\nspirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC\nconsistently outperforms linear baselines and k-NN, is competitive with\nRBF-SVM, Random Forests, and small MLPs, and provides geometric introspection\nvia per-plane and class responsibility visualizations. Inference scales\nlinearly in planes and features, making GMC CPU-friendly, with single-digit\nmicrosecond latency per example, often faster than RBF-SVM and compact MLPs.\nPost-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus\nstrikes a favorable balance of accuracy, interpretability, and efficiency: it\nis more expressive than linear models and lighter, more transparent, and faster\nthan kernel or deep models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u51e0\u4f55\u6df7\u5408\u5206\u7c7b\u5668\uff08GMC\uff09\u7684\u5224\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u6bcf\u7c7b\u4f7f\u7528\u591a\u4e2a\u8d85\u5e73\u9762\u7684\u6df7\u5408\u6765\u6709\u6548\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff0c\u5728\u51c6\u786e\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u6a21\u578b\u96be\u4ee5\u5904\u7406\u591a\u6a21\u6001\u7c7b\u522b\uff08\u5355\u4e2a\u7c7b\u522b\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5360\u636e\u4e0d\u8fde\u7eed\u533a\u57df\uff09\uff0c\u800c\u9ad8\u5bb9\u91cf\u6a21\u578b\uff08\u5982\u6838SVM\u3001\u6df1\u5ea6\u7f51\u7edc\uff09\u867d\u80fd\u62df\u5408\u4f46\u727a\u7272\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u5177\u8868\u8fbe\u529b\u3001\u53ef\u89e3\u91ca\u6027\u4e0e\u9ad8\u6548\u63a8\u7406\u7684\u6a21\u578b\u3002", "method": "GMC\u5c06\u6bcf\u4e2a\u7c7b\u522b\u8868\u793a\u4e3a\u591a\u4e2a\u8d85\u5e73\u9762\u7684\u6df7\u5408\uff0c\u7c7b\u5185\u901a\u8fc7\u6e29\u5ea6\u63a7\u5236\u7684soft-OR\uff08log-sum-exp\uff09\u805a\u5408\u5e73\u9762\u5f97\u5206\uff0c\u7c7b\u95f4\u4f7f\u7528softmax\u8f93\u51fa\u6982\u7387\uff1b\u53ef\u7ed3\u5408\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\uff08RFF\uff09\u5b9e\u73b0\u975e\u7ebf\u6027\u6620\u5c04\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u7684\u7ebf\u6027\u590d\u6742\u5ea6\uff1b\u8bad\u7ec3\u91c7\u7528\u51e0\u4f55\u611f\u77e5\u7684k-means\u521d\u59cb\u5316\u3001\u8f6e\u5ed3\u5206\u6790\u786e\u5b9a\u5e73\u9762\u6570\u91cf\u3001alpha\u9000\u706b\u3001L2\u6b63\u5219\u5316\u3001\u6807\u7b7e\u5e73\u6ed1\u548c\u65e9\u505c\u7b49\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u591a\u6a21\u6001\u6570\u636e\uff08\u5982moons\u3001spirals\uff09\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08iris\u3001digits\u7b49\uff09\u4e0a\uff0cGMC\u663e\u8457\u4f18\u4e8e\u7ebf\u6027\u57fa\u7ebf\u548ck-NN\uff0c\u6027\u80fd\u5ab2\u7f8eRBF-SVM\u3001\u968f\u673a\u68ee\u6797\u548c\u5c0f\u578bMLP\uff0c\u4e14\u63a8\u7406\u5ef6\u8fdf\u4f4e\uff08\u5fae\u79d2\u7ea7\uff09\uff0c\u652f\u6301\u6bcf\u5e73\u9762\u548c\u7c7b\u522b\u7684\u8d23\u4efb\u53ef\u89c6\u5316\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\uff1b\u540e\u5904\u7406\u6e29\u5ea6\u7f29\u653e\u5c06ECE\u4ece0.06\u964d\u81f30.02\u3002", "conclusion": "GMC\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u5f3a\u4e8e\u7ebf\u6027\u6a21\u578b\uff0c\u5728\u8ba1\u7b97\u5f00\u9500\u3001\u900f\u660e\u5ea6\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u6838\u65b9\u6cd5\u548c\u6df1\u5ea6\u6a21\u578b\uff0c\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17259", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17259", "abs": "https://arxiv.org/abs/2509.17259", "authors": ["Ilham Wicaksono", "Zekun Wu", "Rahul Patel", "Theo King", "Adriano Koshiyama", "Philip Treleaven"], "title": "Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B", "comment": "Winner of the OpenAI GPT-OSS-20B Red Teaming Challenge (Kaggle, 2025)", "summary": "As the industry increasingly adopts agentic AI systems, understanding their\nunique vulnerabilities becomes critical. Prior research suggests that security\nflaws at the model level do not fully capture the risks present in agentic\ndeployments, where models interact with tools and external environments. This\npaper investigates this gap by conducting a comparative red teaming analysis of\nGPT-OSS-20B, a 20-billion parameter open-source model. Using our observability\nframework AgentSeer to deconstruct agentic systems into granular actions and\ncomponents, we apply iterative red teaming attacks with harmful objectives from\nHarmBench at two distinct levels: the standalone model and the model operating\nwithin an agentic loop. Our evaluation reveals fundamental differences between\nmodel level and agentic level vulnerability profiles. Critically, we discover\nthe existence of agentic-only vulnerabilities, attack vectors that emerge\nexclusively within agentic execution contexts while remaining inert against\nstandalone models. Agentic level iterative attacks successfully compromise\nobjectives that completely failed at the model level, with tool-calling\ncontexts showing 24\\% higher vulnerability than non-tool contexts. Conversely,\ncertain model-specific exploits work exclusively at the model level and fail\nwhen transferred to agentic contexts, demonstrating that standalone model\nvulnerabilities do not always generalize to deployed systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u5728\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u72ec\u7279\u5b89\u5168\u6f0f\u6d1e\uff0c\u53d1\u73b0\u6a21\u578b\u5c42\u9762\u7684\u5b89\u5168\u6d4b\u8bd5\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u98ce\u9669\u3002\u901a\u8fc7AgentSeer\u6846\u67b6\u5bf9GPT-OSS-20B\u8fdb\u884c\u7ea2\u961f\u653b\u51fb\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4ec5\u5728\u4ee3\u7406\u6267\u884c\u4e0a\u4e0b\u6587\u4e2d\u51fa\u73b0\u7684\u201c\u4ee3\u7406\u4e13\u5c5e\u6f0f\u6d1e\u201d\uff0c\u5c24\u5176\u662f\u5728\u8c03\u7528\u5de5\u5177\u65f6\u6f0f\u6d1e\u589e\u52a024%\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6a21\u578b\u5c42\u9762\uff0c\u4f46\u4ee3\u7406\u5f0fAI\u4e0e\u5916\u90e8\u73af\u5883\u4ea4\u4e92\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u4ee3\u7406\u5c42\u7ea7\u7684\u72ec\u7279\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faAgentSeer\u53ef\u89c2\u6d4b\u6027\u6846\u67b6\uff0c\u5c06\u4ee3\u7406\u7cfb\u7edf\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7ec4\u4ef6\uff0c\u5e76\u5728\u72ec\u7acb\u6a21\u578b\u548c\u4ee3\u7406\u5faa\u73af\u4e24\u79cd\u6a21\u5f0f\u4e0b\uff0c\u4f7f\u7528HarmBench\u4e2d\u7684\u6709\u5bb3\u76ee\u6807\u8fdb\u884c\u8fed\u4ee3\u7ea2\u961f\u653b\u51fb\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e86\u4ec5\u5b58\u5728\u4e8e\u4ee3\u7406\u6267\u884c\u4e0a\u4e0b\u6587\u4e2d\u7684\u65b0\u578b\u6f0f\u6d1e\uff08agentic-only vulnerabilities\uff09\uff0c\u5de5\u5177\u8c03\u7528\u573a\u666f\u4e0b\u6f0f\u6d1e\u7387\u9ad8\u51fa24%\uff1b\u540c\u65f6\u90e8\u5206\u6a21\u578b\u7ea7\u6f0f\u6d1e\u5728\u4ee3\u7406\u73af\u5883\u4e2d\u5931\u6548\uff0c\u8868\u660e\u6a21\u578b\u7ea7\u8106\u5f31\u6027\u4e0d\u603b\u80fd\u8fc1\u79fb\u5230\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u4e2d\u3002", "conclusion": "\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u7684\u5b89\u5168\u8bc4\u4f30\u4e0d\u80fd\u4ec5\u4f9d\u8d56\u6a21\u578b\u7ea7\u6d4b\u8bd5\uff0c\u5fc5\u987b\u7ed3\u5408\u4ee3\u7406\u5c42\u7ea7\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u4e13\u95e8\u5206\u6790\uff0c\u4ee5\u5e94\u5bf9\u65b0\u51fa\u73b0\u7684\u653b\u51fb\u5411\u91cf\u3002"}}
{"id": "2509.16628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16628", "abs": "https://arxiv.org/abs/2509.16628", "authors": ["Janak Kapuriya", "Anwar Shaikh", "Arnav Goel", "Medha Hira", "Apoorv Singh", "Jay Saraf", "Sanjana", "Vaibhav Nauriyal", "Avinash Anand", "Zhengkui Wang", "Rajiv Ratn Shah"], "title": "Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning", "comment": null, "summary": "In this study, we introduce Vision-Caption aware Supervised FineTuning\n(VCASFT), a novel learning paradigm designed to enhance the performance of\nsmaller Vision Language Models(VLMs) on scientific visual question\nanswering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts\nalongside question-answer pairs and instruction-tunes models to yield\nsignificant performance improvements. To comprehensively evaluate VCASFT, we\nbenchmark it on ScienceQA, which consists of questions across diverse\nlanguages, subjects, and fields, demonstrating its adaptability and\neffectiveness in a variety of educational contexts. Additionally, to further\ndemonstrate the effectiveness of this technique on lowresource languages, we\ndeveloped HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated\nHindi multimodal Q&A pairs. This dataset addresses the critical need for\nlow-resource language Q&A datasets and serves as a foundation for testing\nVCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to\nevaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness\nsurpassing traditional n-gram matching accuracy metrics. We are committed to\nadvancing the field by open-sourcing all code files and the HiSciVQA dataset\nfor the research community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVCASFT\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u6807\u9898\u4f5c\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u6765\u63d0\u5347\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7ScienceQA\u548c\u65b0\u6784\u5efa\u7684HiSciVQA\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u5b57\u5e55\u4f5c\u4e3a\u96f6\u6837\u672c\u63d0\u793a\uff0c\u7ed3\u5408\u95ee\u9898-\u7b54\u6848\u5bf9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08VCASFT\uff09\uff0c\u5e76\u5728ScienceQA\u548c\u81ea\u5efa\u7684HiSciVQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "VCASFT\u5728ScienceQA\u548cHiSciVQA\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1b\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edfn-gram\u5339\u914d\u6307\u6807\u3002", "conclusion": "VCASFT\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u578bVLM\u5728\u591a\u8bed\u8a00\u79d1\u5b66VQA\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e14\u5f00\u6e90\u4ee3\u7801\u4e0eHiSciVQA\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0b\u7684\u591a\u6a21\u6001\u7814\u7a76\u3002"}}
{"id": "2509.17213", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17213", "abs": "https://arxiv.org/abs/2509.17213", "authors": ["Yassine Kebbati", "Naima Ait-Oufroukh", "Vincent Vigneron", "Dalil Ichala"], "title": "Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles", "comment": null, "summary": "Self-driving cars operate in constantly changing environments and are exposed\nto a variety of uncertainties and disturbances. These factors render classical\ncontrollers ineffective, especially for lateral control. Therefore, an adaptive\nMPC controller is designed in this paper for the path tracking task, tuned by\nan improved particle swarm optimization algorithm. Online parameter adaptation\nis performed using Neural Networks and ANFIS. The designed controller showed\npromising results compared to standard MPC in triple lane change and trajectory\ntracking scenarios. Code can be found here:\nhttps://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u548cANFIS\u8fdb\u884c\u5728\u7ebf\u53c2\u6570\u81ea\u9002\u5e94\u7684\u6539\u8fdb\u578bMPC\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u8def\u5f84\u8ddf\u8e2a\u4efb\u52a1\uff0c\u5728\u53cc\u6362\u9053\u548c\u8f68\u8ff9\u8ddf\u8e2a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6MPC\u3002", "motivation": "\u7531\u4e8e\u81ea\u9a7e\u8f66\u8fd0\u884c\u73af\u5883\u590d\u6742\u591a\u53d8\uff0c\u5b58\u5728\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\uff0c\u4f20\u7edf\u63a7\u5236\u5668\uff08\u5c24\u5176\u662f\u6a2a\u5411\u63a7\u5236\uff09\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u9c81\u68d2\u3001\u53ef\u81ea\u9002\u5e94\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94MPC\u63a7\u5236\u5668\uff0c\u5e76\u4f7f\u7528\u6539\u8fdb\u7684\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u8c03\u53c2\uff1b\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u548cANFIS\u5b9e\u73b0\u63a7\u5236\u5668\u53c2\u6570\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u8c03\u8282\u3002", "result": "\u5728\u53cc\u8f66\u9053\u53d8\u6362\u548c\u8f68\u8ff9\u8ddf\u8e2a\u4eff\u771f\u573a\u666f\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u63a7\u5236\u5668\u76f8\u6bd4\u6807\u51c6MPC\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08NN\u4e0eANFIS\uff09\u7684\u81ea\u9002\u5e94MPC\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8def\u5f84\u8ddf\u8e2a\u6027\u80fd\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16686", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16686", "abs": "https://arxiv.org/abs/2509.16686", "authors": ["Zhengge Cai", "Haowen Hou"], "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs", "comment": null, "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Embedding-Gated Multi-head Latent Attention (EG-MLA)\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f15\u5165token\u7279\u5b9a\u7684\u5d4c\u5165\u95e8\u63a7\u673a\u5236\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11KV\u7f13\u5b58\u5927\u5c0f\u5e76\u589e\u5f3a\u8868\u793a\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u6781\u4f4e\u6027\u80fd\u635f\u5931\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bb0\u5fc6\u6548\u7387\u548c\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6709\u6548\u964d\u4f4eKV\u7f13\u5b58\u5e26\u6765\u7684\u5185\u5b58\u5f00\u9500\uff0c\u5c24\u5176\u662f\u5728\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u5185\u5b58\u6d88\u8017\u5927\u7684\u80cc\u666f\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u5e73\u8861\u6027\u80fd\u4e0e\u5185\u5b58\u4f7f\u7528\u3002", "method": "\u63d0\u51faEG-MLA\uff0c\u4f5c\u4e3aMLA\u7684\u6269\u5c55\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f15\u5165\u57fa\u4e8etoken\u7684\u5d4c\u5165\u95e8\u63a7\u673a\u5236\uff0c\u5bf9\u538b\u7f29\u540e\u7684KV\u5411\u91cf\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8c03\u8282\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u5176\u9690\u542b\u7684\u9ad8\u9636\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u76f8\u6bd4MHA\uff0cEG-MLA\u5b9e\u73b0\u4e86\u8d85\u8fc791.6%\u7684KV\u7f13\u5b58\u7f29\u51cf\uff1b\u76f8\u6bd4MLA\uff0c\u989d\u5916\u8282\u7701\u9ad8\u8fbe59.9%\u7684\u5185\u5b58\uff0c\u5e76\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u5347\u4e86\u51c6\u786e\u7387\uff0c\u4e14\u53ef\u6269\u5c55\u81f3\u5341\u4ebf\u53c2\u6570\u4ee5\u4e0a\u89c4\u6a21\u3002", "conclusion": "EG-MLA\u662f\u4e00\u79cd\u517c\u5177\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u538b\u7f29KV\u7f13\u5b58\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u90e8\u7f72\u3002"}}
{"id": "2509.16820", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16820", "abs": "https://arxiv.org/abs/2509.16820", "authors": ["Max Torop", "Aria Masoomi", "Masih Eskandar", "Jennifer Dy"], "title": "DISCO: Disentangled Communication Steering for Large Language Models", "comment": null, "summary": "A variety of recent methods guide large language model outputs via the\ninference-time addition of steering vectors to residual-stream or\nattention-head representations. In contrast, we propose to inject steering\nvectors directly into the query and value representation spaces within\nattention heads. We provide evidence that a greater portion of these spaces\nexhibit high linear discriminability of concepts --a key property motivating\nthe use of steering vectors-- than attention head outputs. We analytically\ncharacterize the effect of our method, which we term DISentangled COmmunication\n(DISCO) Steering, on attention head outputs. Our analysis reveals that DISCO\ndisentangles a strong but underutilized baseline, steering attention inputs,\nwhich implicitly modifies queries and values in a rigid manner. In contrast,\nDISCO's direct modulation of these components enables more granular control. We\nfind that DISCO achieves superior performance over a number of steering vector\nbaselines across multiple datasets on LLaMA 3.1 8B and Gemma 2 9B, with\nsteering efficacy scoring up to 19.1% higher than the runner-up. Our results\nsupport the conclusion that the query and value spaces are powerful building\nblocks for steering vector methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDISCO Steering\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5f15\u5bfc\u5411\u91cf\u76f4\u63a5\u6ce8\u5165\u6ce8\u610f\u529b\u5934\u4e2d\u7684\u67e5\u8be2\uff08query\uff09\u548c\u503c\uff08value\uff09\u8868\u793a\u7a7a\u95f4\u6765\u8c03\u63a7\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u3002\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u6b8b\u5dee\u6d41\u6216\u6ce8\u610f\u529b\u5934\u8f93\u51fa\u4e0a\u6dfb\u52a0\u5f15\u5bfc\u5411\u91cf\uff0cDISCO\u5229\u7528\u67e5\u8be2\u4e0e\u503c\u7a7a\u95f4\u4e2d\u66f4\u5f3a\u7684\u7ebf\u6027\u53ef\u5206\u6027\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5927\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe19.1%\u3002", "motivation": "\u73b0\u6709\u5f15\u5bfc\u65b9\u6cd5\u591a\u4f5c\u7528\u4e8e\u6b8b\u5dee\u6d41\u6216\u6ce8\u610f\u529b\u5934\u8f93\u51fa\uff0c\u4f46\u8fd9\u4e9b\u7a7a\u95f4\u4e2d\u6982\u5ff5\u7684\u7ebf\u6027\u53ef\u5206\u6027\u6709\u9650\u3002\u800c\u67e5\u8be2\u548c\u503c\u7a7a\u95f4\u5177\u6709\u66f4\u9ad8\u7684\u7ebf\u6027\u5224\u522b\u80fd\u529b\uff0c\u66f4\u9002\u5408\u7528\u4e8e\u5f15\u5bfc\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u76f4\u63a5\u5728\u8fd9\u4e9b\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u6ce8\u5165\u5f15\u5bfc\u5411\u91cf\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faDISCO Steering\u65b9\u6cd5\uff1a\u5c06\u5f15\u5bfc\u5411\u91cf\u76f4\u63a5\u6ce8\u5165\u6ce8\u610f\u529b\u5934\u7684\u67e5\u8be2\uff08Q\uff09\u548c\u503c\uff08V\uff09\u8868\u793a\u7a7a\u95f4\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u8be5\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u6ce8\u610f\u529b\u8f93\u51fa\uff0c\u5e76\u8bc1\u660e\u5176\u76f8\u5bf9\u4e8e\u9690\u5f0f\u4fee\u6539Q/V\u7684\u4f20\u7edf\u8f93\u5165\u5f15\u5bfc\u65b9\u6cd5\u66f4\u5177\u89e3\u8026\u6027\u548c\u7075\u6d3b\u6027\u3002", "result": "\u5728LLaMA 3.1 8B\u548cGemma 2 9B\u6a21\u578b\u4e0a\uff0cDISCO\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5f15\u5bfc\u5411\u91cf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u9ad8\u6027\u80fd\u63d0\u5347\u8fbe19.1%\u3002\u5b9e\u9a8c\u8bc1\u660e\u67e5\u8be2\u4e0e\u503c\u7a7a\u95f4\u6bd4\u6ce8\u610f\u529b\u5934\u8f93\u51fa\u5177\u6709\u66f4\u9ad8\u7684\u7ebf\u6027\u53ef\u5206\u6027\uff0c\u66f4\u9002\u5408\u7528\u4e8e\u5f15\u5bfc\u3002", "conclusion": "\u67e5\u8be2\u548c\u503c\u8868\u793a\u7a7a\u95f4\u662f\u6784\u5efa\u5f15\u5bfc\u5411\u91cf\u65b9\u6cd5\u7684\u5f3a\u5927\u57fa\u7840\uff0cDISCO Steering\u901a\u8fc7\u76f4\u63a5\u3001\u89e3\u8026\u5730\u8c03\u63a7\u8fd9\u4e9b\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u7cbe\u7ec6\u7684\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u63a7\u5236\uff0c\u4e3a\u63a8\u7406\u65f6\u6a21\u578b\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.17318", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17318", "abs": "https://arxiv.org/abs/2509.17318", "authors": ["Zhuofan Chen", "Jiyuan He", "Yichi Zhang", "Xing Hu", "Haoxing Wen", "Jun Bai", "Wenge Rong"], "title": "CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models", "comment": null, "summary": "Mathematical reasoning poses significant challenges for Large Language Models\n(LLMs) due to its demand for multi-step reasoning and abstract conceptual\nintegration. While recent test-time scaling techniques rely heavily on\nhigh-quality, challenging problems, the scarcity of Olympiad-level math\nproblems remains a bottleneck. We introduce CogAtom, a novel cognitive\natom-based framework for synthesizing mathematically rigorous and cognitively\ndiverse problems. Unlike prior approaches, CogAtom models problem construction\nas a process of selecting and recombining fundamental reasoning units,\ncognitive atoms, extracted from human-authored solutions. A diversity-promoting\nrandom walk algorithm enables exploration of the cognitive atom space, while a\nconstraint-based recombination mechanism ensures logical soundness and\nstructural validity. The combinatorial nature of the graph structure provides a\nnear-infinite space of reasoning paths, and the walk algorithm systematically\nexplores this space to achieve large-scale synthesis of high-quality problems;\nmeanwhile, by controlling the number of cognitive atoms, we can precisely\nadjust problem difficulty, ensuring diversity, scalability, and controllability\nof the generated problems. Experimental results demonstrate that CogAtom\noutperforms existing methods in accuracy, reasoning depth, and diversity,\ngenerating problems that closely match the difficulty of AIME while exceeding\nit in structural variation. Our work offers a cognitively grounded pathway\ntoward scalable, high-quality math problem generation.Our code is publicly\navailable at https://github.com/Icarus-1111/CogAtom.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u539f\u5b50\u7684\u6570\u5b66\u95ee\u9898\u751f\u6210\u6846\u67b6CogAtom\uff0c\u901a\u8fc7\u4ece\u4eba\u7c7b\u89e3\u6cd5\u4e2d\u63d0\u53d6\u57fa\u672c\u63a8\u7406\u5355\u5143\u5e76\u91cd\u65b0\u7ec4\u5408\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u4e14\u53ef\u63a7\u96be\u5ea6\u7684\u6570\u5b66\u95ee\u9898\u81ea\u52a8\u751f\u6210\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u9762\u4e34\u591a\u6b65\u63a8\u7406\u548c\u62bd\u8c61\u6982\u5ff5\u6574\u5408\u7684\u6311\u6218\uff0c\u800c\u9ad8\u8d28\u91cf\u5965\u8d5b\u7ea7\u522b\u6570\u5b66\u95ee\u9898\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u6280\u672f\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u7684\u95ee\u9898\u751f\u6210\u65b9\u6cd5\u3002", "method": "CogAtom\u5c06\u95ee\u9898\u6784\u5efa\u5efa\u6a21\u4e3a\u9009\u62e9\u548c\u91cd\u7ec4\u6765\u81ea\u4eba\u7c7b\u89e3\u7b54\u7684\u57fa\u672c\u63a8\u7406\u5355\u5143\uff08\u8ba4\u77e5\u539f\u5b50\uff09\u7684\u8fc7\u7a0b\uff0c\u91c7\u7528\u4fc3\u8fdb\u591a\u6837\u6027\u7684\u968f\u673a\u6e38\u8d70\u7b97\u6cd5\u63a2\u7d22\u8ba4\u77e5\u539f\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u7ea6\u675f\u7684\u91cd\u7ec4\u673a\u5236\u786e\u4fdd\u903b\u8f91\u6b63\u786e\u6027\u548c\u7ed3\u6784\u6709\u6548\u6027\uff0c\u540c\u65f6\u5229\u7528\u56fe\u7ed3\u6784\u7684\u7ec4\u5408\u6027\u5b9e\u73b0\u5927\u89c4\u6a21\u95ee\u9898\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCogAtom\u5728\u51c6\u786e\u6027\u3001\u63a8\u7406\u6df1\u5ea6\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u95ee\u9898\u96be\u5ea6\u63a5\u8fd1AIME\u6c34\u5e73\uff0c\u5e76\u5728\u7ed3\u6784\u53d8\u5316\u4e0a\u8d85\u8d8aAIME\uff0c\u5b9e\u73b0\u4e86\u95ee\u9898\u96be\u5ea6\u7684\u7cbe\u786e\u63a7\u5236\u3002", "conclusion": "CogAtom\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba4\u77e5\u57fa\u7840\u624e\u5b9e\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u91cf\u7684\u6570\u5b66\u95ee\u9898\u751f\u6210\u8def\u5f84\uff0c\u6709\u671b\u63a8\u52a8\u6570\u5b66\u63a8\u7406\u9886\u57df\u7684\u6570\u636e\u5408\u6210\u4e0e\u6a21\u578b\u8bc4\u4f30\u53d1\u5c55\u3002"}}
{"id": "2509.16630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16630", "abs": "https://arxiv.org/abs/2509.16630", "authors": ["Yue Ma", "Zexuan Yan", "Hongyu Liu", "Hongfa Wang", "Heng Pan", "Yingqing He", "Junkun Yuan", "Ailing Zeng", "Chengfei Cai", "Heung-Yeung Shum", "Zhifeng Li", "Wei Liu", "Linfeng Zhang", "Qifeng Chen"], "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation", "comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io", "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Follow-Your-Emoji-Faster\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u81ea\u7531\u5f0f\u4eba\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u9762\u90e8\u5173\u952e\u70b9\u9a71\u52a8\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u6301\u3001\u8868\u60c5\u8fc1\u79fb\u548c\u957f\u65f6\u95f4\u65f6\u5e8f\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u81ea\u7531\u5f0f\u4eba\u50cf\u52a8\u753b\u4e2d\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u7684\u540c\u65f6\u51c6\u786e\u4f20\u9012\u76ee\u6807\u8868\u60c5\uff0c\u5e76\u5b9e\u73b0\u957f\u671f\u7a33\u5b9a\u7684\u9ad8\u6548\u751f\u6210\uff0c\u662f\u5f53\u524d\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u5728Stable Diffusion\u57fa\u7840\u4e0a\u5f15\u5165\u8868\u60c5\u611f\u77e5\u7684\u5173\u952e\u70b9\u4f5c\u4e3a\u663e\u5f0f\u8fd0\u52a8\u4fe1\u53f7\uff0c\u5e76\u8bbe\u8ba1\u7ec6\u7c92\u5ea6\u9762\u90e8\u635f\u5931\u51fd\u6570\u7ed3\u5408\u8868\u60c5\u4e0e\u9762\u90e8\u63a9\u7801\uff1b\u63d0\u51fa\u6e10\u8fdb\u5f0f\u751f\u6210\u7b56\u7565\u548c\u6cf0\u52d2\u63d2\u503c\u7f13\u5b58\u4ee5\u63d0\u5347\u957f\u5e8f\u5217\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u65b0\u6784\u5efa\u7684\u7efc\u5408\u6027\u57fa\u51c6EmojiBench++\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u753b\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5b9e\u73b02.6\u500d\u65e0\u635f\u52a0\u901f\u3002", "conclusion": "Follow-Your-Emoji-Faster\u6709\u6548\u5e73\u8861\u4e86\u751f\u6210\u8d28\u91cf\u3001\u8eab\u4efd\u4fdd\u6301\u3001\u8868\u60c5\u8868\u8fbe\u4e0e\u6548\u7387\uff0c\u63a8\u52a8\u4e86\u6269\u6563\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u4eba\u50cf\u52a8\u753b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.17244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17244", "abs": "https://arxiv.org/abs/2509.17244", "authors": ["Frederic Vatnsdal", "Romina Garcia Camargo", "Saurav Agarwal", "Alejandro Ribeiro"], "title": "Scalable Multi Agent Diffusion Policies for Coverage Control", "comment": null, "summary": "We propose MADP, a novel diffusion-model-based approach for collaboration in\ndecentralized robot swarms. MADP leverages diffusion models to generate samples\nfrom complex and high-dimensional action distributions that capture the\ninterdependencies between agents' actions. Each robot conditions policy\nsampling on a fused representation of its own observations and perceptual\nembeddings received from peers. To evaluate this approach, we task a team of\nholonomic robots piloted by MADP to address coverage control-a canonical multi\nagent navigation problem. The policy is trained via imitation learning from a\nclairvoyant expert on the coverage control problem, with the diffusion process\nparameterized by a spatial transformer architecture to enable decentralized\ninference. We evaluate the system under varying numbers, locations, and\nvariances of importance density functions, capturing the robustness demands of\nreal-world coverage tasks. Experiments demonstrate that our model inherits\nvaluable properties from diffusion models, generalizing across agent densities\nand environments, and consistently outperforming state-of-the-art baselines.", "AI": {"tldr": "MADP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u4eba\u96c6\u7fa4\u534f\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u81ea\u8eab\u89c2\u6d4b\u4e0e\u540c\u4f34\u611f\u77e5\u5d4c\u5165\u6765\u751f\u6210\u8003\u8651\u52a8\u4f5c\u4f9d\u8d56\u6027\u7684\u7b56\u7565\uff0c\u5728\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u4eba\u96c6\u7fa4\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5efa\u6a21\u667a\u80fd\u4f53\u95f4\u52a8\u4f5c\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5e76\u5b9e\u73b0\u9c81\u68d2\u534f\u4f5c\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u3001\u8026\u5408\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faMADP\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u53cd\u6620\u667a\u80fd\u4f53\u52a8\u4f5c\u4e92\u4f9d\u6027\u7684\u9ad8\u7ef4\u52a8\u4f5c\u5206\u5e03\uff1b\u6bcf\u4e2a\u673a\u5668\u4eba\u57fa\u4e8e\u81ea\u8eab\u89c2\u6d4b\u4e0e\u6765\u81ea\u540c\u4f34\u7684\u611f\u77e5\u5d4c\u5165\u878d\u5408\u8868\u5f81\u8fdb\u884c\u7b56\u7565\u91c7\u6837\uff1b\u91c7\u7528\u7a7a\u95f4\u53d8\u6362\u5668\u67b6\u6784\u53c2\u6570\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5148\u77e5\u4e13\u5bb6\u793a\u8303\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u63a8\u7406\u3002", "result": "\u5728\u4e0d\u540c\u6570\u91cf\u3001\u4f4d\u7f6e\u548c\u91cd\u8981\u6027\u5bc6\u5ea6\u51fd\u6570\u65b9\u5dee\u7684\u8986\u76d6\u63a7\u5236\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6027\u80fd\uff1b\u5b9e\u9a8c\u8868\u660eMADP\u5177\u6709\u826f\u597d\u7684\u73af\u5883\u4e0e\u667a\u80fd\u4f53\u5bc6\u5ea6\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u6301\u7eed\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MADP\u6210\u529f\u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u53bb\u4e2d\u5fc3\u5316\u591a\u673a\u5668\u4eba\u534f\u4f5c\uff0c\u517c\u5177\u8868\u8fbe\u529b\u4e0e\u5b9e\u7528\u6027\uff0c\u4e3a\u590d\u6742\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16696", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16696", "abs": "https://arxiv.org/abs/2509.16696", "authors": ["Wataru Hashimoto", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models", "comment": "Accepted at EMNLP 2025 Findings", "summary": "Decoding strategies manipulate the probability distribution underlying the\noutput of a language model and can therefore affect both generation quality and\nits uncertainty. In this study, we investigate the impact of decoding\nstrategies on uncertainty estimation in Large Language Models (LLMs). Our\nexperiments show that Contrastive Search, which mitigates repetition, yields\nbetter uncertainty estimates on average across a range of preference-aligned\nLLMs. In contrast, the benefits of these strategies sometimes diverge when the\nmodel is only post-trained with supervised fine-tuning, i.e. without explicit\nalignment.", "AI": {"tldr": "\u7814\u7a76\u4e86\u89e3\u7801\u7b56\u7565\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5bf9\u6bd4\u641c\u7d22\u5728\u504f\u597d\u5bf9\u9f50\u7684\u6a21\u578b\u4e2d\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u89e3\u7801\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ed3\u679c\u7684\u8d28\u91cf\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "method": "\u901a\u8fc7\u5728\u591a\u79cd\u504f\u597d\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u9a8c\u5bf9\u6bd4\u641c\u7d22\u7b49\u89e3\u7801\u7b56\u7565\uff0c\u8bc4\u4f30\u5176\u5bf9\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "result": "\u5bf9\u6bd4\u641c\u7d22\u5e73\u5747\u800c\u8a00\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4f46\u5728\u4ec5\u7ecf\u8fc7\u76d1\u7763\u5fae\u8c03\u800c\u65e0\u663e\u5f0f\u5bf9\u9f50\u7684\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u89e3\u7801\u7b56\u7565\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5bf9\u6bd4\u641c\u7d22\u5728\u5bf9\u9f50\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2509.16825", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.16825", "abs": "https://arxiv.org/abs/2509.16825", "authors": ["Jin Lee", "Ziming Liu", "Xinling Yu", "Yixuan Wang", "Haewon Jeong", "Murphy Yuezhen Niu", "Zheng Zhang"], "title": "KANO: Kolmogorov-Arnold Neural Operator", "comment": null, "summary": "We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural\noperator jointly parameterized by both spectral and spatial bases with\nintrinsic symbolic interpretability. We theoretically demonstrate that KANO\novercomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO\nremains expressive over generic position-dependent dynamics for any physical\ninput, whereas FNO stays practical only for spectrally sparse operators and\nstrictly imposes a fast-decaying input Fourier tail. We verify our claims\nempirically on position-dependent differential operators, for which KANO\nrobustly generalizes but FNO fails to. In the quantum Hamiltonian learning\nbenchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic\nrepresentations accurate to the fourth decimal place in coefficients and\nattains $\\approx 6\\times10^{-6}$ state infidelity from projective measurement\ndata, substantially outperforming that of the FNO trained with ideal full wave\nfunction data, $\\approx 1.5\\times10^{-2}$, by orders of magnitude.", "AI": {"tldr": "\u63d0\u51faKolmogorov--Arnold Neural Operator (KANO)\uff0c\u4e00\u79cd\u5177\u6709\u8c31\u57df\u548c\u7a7a\u95f4\u57df\u53cc\u91cd\u53c2\u6570\u5316\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u514b\u670d\u4e86\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50(FNO)\u7684\u9891\u8c31\u74f6\u9888\uff0c\u5728\u4f4d\u7f6e\u4f9d\u8d56\u52a8\u529b\u5b66\u548c\u91cf\u5b50\u54c8\u5bc6\u987f\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "FNO\u5728\u5904\u7406\u9891\u8c31\u7a00\u758f\u6027\u8981\u6c42\u9ad8\u3001\u8f93\u5165\u5085\u91cc\u53f6\u5c3e\u90e8\u8870\u51cf\u5feb\u7684\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e00\u822c\u7684\u4f4d\u7f6e\u4f9d\u8d56\u52a8\u529b\u5b66\uff1b\u9700\u8981\u8bbe\u8ba1\u66f4\u5177\u8868\u8fbe\u529b\u4e14\u5177\u5907\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\u7684\u795e\u7ecf\u7b97\u5b50\u3002", "method": "\u63d0\u51faKANO\uff0c\u7ed3\u5408\u8c31\u57df\u548c\u7a7a\u95f4\u57df\u57fa\u51fd\u6570\u8fdb\u884c\u8054\u5408\u53c2\u6570\u5316\uff0c\u5229\u7528Kolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u5b9e\u73b0\u53cc\u57df\u5efa\u6a21\uff0c\u5e76\u8d4b\u4e88\u6a21\u578b\u5185\u5728\u7684\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660eKANO\u514b\u670d\u4e86FNO\u7684\u9891\u8c31\u74f6\u9888\uff1b\u5728\u4f4d\u7f6e\u4f9d\u8d56\u5fae\u5206\u7b97\u5b50\u4efb\u52a1\u4e2dKANO\u80fd\u7a33\u5065\u6cdb\u5316\u800cFNO\u5931\u8d25\uff1b\u5728\u91cf\u5b50\u54c8\u5bc6\u987f\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cKANO\u80fd\u4ee5\u95ed\u5f0f\u7b26\u53f7\u5f62\u5f0f\u91cd\u5efa\u771f\u5b9e\u54c8\u5bc6\u987f\u91cf\uff08\u7cfb\u6570\u7cbe\u786e\u5230\u5c0f\u6570\u70b9\u540e\u56db\u4f4d\uff09\uff0c\u72b6\u6001\u4fdd\u771f\u5ea6\u8fbe\u22486\u00d710\u207b\u2076\uff0c\u8fdc\u4f18\u4e8e\u4f7f\u7528\u7406\u60f3\u5168\u6ce2\u51fd\u6570\u8bad\u7ec3\u7684FNO\uff08\u22481.5\u00d710\u207b\u00b2\uff09\u3002", "conclusion": "KANO\u901a\u8fc7\u53cc\u57df\u53c2\u6570\u5316\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7b97\u5b50\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7269\u7406\u9a71\u52a8\u7684\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7cbe\u5ea6\u548c\u7b26\u53f7\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u7b97\u5b50\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.17337", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17337", "abs": "https://arxiv.org/abs/2509.17337", "authors": ["Ala Jararweh", "Michael Adams", "Avinash Sahu", "Abdullah Mueen", "Afsah Anwar"], "title": "LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code", "comment": null, "summary": "Increasing complexity in software systems places a growing demand on\nreasoning tools that unlock vulnerabilities manifest in source code. Many\ncurrent approaches focus on vulnerability analysis as a classifying task,\noversimplifying the nuanced and context-dependent real-world scenarios. Even\nthough current code large language models (LLMs) excel in code understanding,\nthey often pay little attention to security-specific reasoning. We propose\nLLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code\nthrough question-answering (QA). Our model is trained to integrate paired code\nand natural queries into a unified space, enhancing reasoning and\ncontext-dependent insights about code vulnerability. To evaluate our model\nperformance, we construct a curated dataset of real-world vulnerabilities\npaired with security-focused questions and answers. Our model outperforms\nstate-of-the-art general-purpose and code LLMs in the QA and detection tasks.\nWe further explain decision-making by conducting qualitative analysis to\nhighlight capabilities and limitations. By integrating code and QA, LLaVul\nenables more interpretable and security-focused code understanding.", "AI": {"tldr": "LLaVul\u662f\u4e00\u79cd\u4e13\u4e3a\u4ee3\u7801\u6f0f\u6d1e\u5206\u6790\u8bbe\u8ba1\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u95ee\u7b54\u65b9\u5f0f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a8\u7406\uff0c\u5728\u771f\u5b9e\u6f0f\u6d1e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u5206\u6790\u65b9\u6cd5\u591a\u91c7\u7528\u5206\u7c7b\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\uff1b\u901a\u7528\u4ee3\u7801\u5927\u6a21\u578b\u7f3a\u4e4f\u5b89\u5168\u4e13\u9879\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faLLaVul\uff0c\u4e00\u79cd\u7ed3\u5408\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u4ee3\u7801\u4e0e\u5b89\u5168\u76f8\u5173\u95ee\u9898\u5bf9\u9f50\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u5347\u5bf9\u4ee3\u7801\u6f0f\u6d1e\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6784\u5efa\u7684\u771f\u5b9e\u4e16\u754c\u6f0f\u6d1e\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cLLaVul\u5728\u95ee\u7b54\u548c\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u901a\u7528\u548c\u4ee3\u7801\u4e13\u7528\u5927\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u5c55\u793a\u4e86\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "LLaVul\u901a\u8fc7\u878d\u5408\u4ee3\u7801\u4e0e\u5b89\u5168\u5bfc\u5411\u7684\u95ee\u7b54\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u89e3\u91ca\u3001\u66f4\u805a\u7126\u5b89\u5168\u7684\u4ee3\u7801\u7406\u89e3\uff0c\u63d0\u5347\u4e86\u6f0f\u6d1e\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.16632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16632", "abs": "https://arxiv.org/abs/2509.16632", "authors": ["Weiran Chen", "Guiqian Zhu", "Ying Li", "Yi Ji", "Chunping Liu"], "title": "DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration", "comment": "Accepted by ACM MM 2025", "summary": "Few-shot font generation aims to create new fonts with a limited number of\nglyph references. It can be used to significantly reduce the labor cost of\nmanual font design. However, due to the variety and complexity of font styles,\nthe results generated by existing methods often suffer from visible defects,\nsuch as stroke errors, artifacts and blurriness. To address these issues, we\npropose DA-Font, a novel framework which integrates a Dual-Attention Hybrid\nModule (DAHM). Specifically, we introduce two synergistic attention blocks: the\ncomponent attention block that leverages component information from content\nimages to guide the style transfer process, and the relation attention block\nthat further refines spatial relationships through interacting the content\nfeature with both original and stylized component-wise representations. These\ntwo blocks collaborate to preserve accurate character shapes and stylistic\ntextures. Moreover, we also design a corner consistency loss and an elastic\nmesh feature loss to better improve geometric alignment. Extensive experiments\nshow that our DA-Font outperforms the state-of-the-art methods across diverse\nfont styles and characters, demonstrating its effectiveness in enhancing\nstructural integrity and local fidelity. The source code can be found at\n\\href{https://github.com/wrchen2001/DA-Font}{\\textit{https://github.com/wrchen2001/DA-Font}}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDA-Font\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53cc\u6ce8\u610f\u529b\u6df7\u5408\u6a21\u5757\uff08DAHM\uff09\u6765\u63d0\u5347\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u7684\u8d28\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u7b14\u753b\u9519\u8bef\u3001\u4f2a\u5f71\u548c\u6a21\u7cca\u7b49\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5b57\u4f53\u98ce\u683c\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u73b0\u6709\u7684\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u65b9\u6cd5\u5e38\u51fa\u73b0\u660e\u663e\u7684\u7f3a\u9677\uff0c\u5982\u7b14\u753b\u9519\u8bef\u3001\u4f2a\u5f71\u548c\u6a21\u7cca\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86DA-Font\u6846\u67b6\uff0c\u5305\u542b\u7ec4\u4ef6\u6ce8\u610f\u529b\u5757\u548c\u5173\u7cfb\u6ce8\u610f\u529b\u5757\uff0c\u7ed3\u5408\u5185\u5bb9\u56fe\u50cf\u7684\u7ec4\u4ef6\u4fe1\u606f\u5f15\u5bfc\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u901a\u8fc7\u539f\u59cb\u4e0e\u98ce\u683c\u5316\u7279\u5f81\u7684\u4ea4\u4e92\u4f18\u5316\u7a7a\u95f4\u5173\u7cfb\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u89d2\u70b9\u4e00\u81f4\u6027\u635f\u5931\u548c\u5f39\u6027\u7f51\u683c\u7279\u5f81\u635f\u5931\u4ee5\u63d0\u5347\u51e0\u4f55\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u79cd\u5b57\u4f53\u98ce\u683c\u548c\u5b57\u7b26\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDA-Font\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u5c40\u90e8\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DA-Font\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u673a\u5236\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u7684\u8d28\u91cf\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17274", "categories": ["cs.RO", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17274", "abs": "https://arxiv.org/abs/2509.17274", "authors": ["Alexandros Ntagkas", "Constantinos Tsakonas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "Learning and Optimization with 3D Orientations", "comment": "9 pages, 11 figures", "summary": "There exist numerous ways of representing 3D orientations. Each\nrepresentation has both limitations and unique features. Choosing the best\nrepresentation for one task is often a difficult chore, and there exist\nconflicting opinions on which representation is better suited for a set of\nfamily of tasks. Even worse, when dealing with scenarios where we need to learn\nor optimize functions with orientations as inputs and/or outputs, the set of\npossibilities (representations, loss functions, etc.) is even larger and it is\nnot easy to decide what is best for each scenario. In this paper, we attempt to\na) present clearly, concisely and with unified notation all available\nrepresentations, and \"tricks\" related to 3D orientations (including Lie Group\nalgebra), and b) benchmark them in representative scenarios. The first part\nfeels like it is missing from the robotics literature as one has to read many\ndifferent textbooks and papers in order have a concise and clear understanding\nof all possibilities, while the benchmark is necessary in order to come up with\nrecommendations based on empirical evidence. More precisely, we experiment with\nthe following settings that attempt to cover most widely used scenarios in\nrobotics: 1) direct optimization, 2) imitation/supervised learning with a\nneural network controller, 3) reinforcement learning, and 4) trajectory\noptimization using differential dynamic programming. We finally provide\nguidelines depending on the scenario, and make available a reference\nimplementation of all the orientation math described.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u5e76\u7edf\u4e00\u4e863D\u59ff\u6001\u7684\u5404\u79cd\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f18\u5316\u3001\u5b66\u4e60\u548c\u63a7\u5236\u7b49\u5178\u578b\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5b9e\u8bc1\u7684\u8868\u793a\u9009\u62e9\u6307\u5357\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u4ee3\u7801\u5b9e\u73b0\u3002", "motivation": "3D\u59ff\u6001\u8868\u793a\u65b9\u6cd5\u4f17\u591a\uff0c\u5404\u6709\u4f18\u52a3\uff0c\u4f46\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5982\u4f55\u9009\u62e9\u6700\u4f73\u8868\u793a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\u548c\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u5b66\u4e60\u4e0e\u4f18\u5316\u7684\u4efb\u52a1\u4e2d\u9009\u62e9\u66f4\u52a0\u590d\u6742\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u6570\u5b66\u7b26\u53f7\u7cfb\u7edf\u5730\u6574\u7406\u4e86\u6240\u6709\u5e38\u89c1\u76843D\u59ff\u6001\u8868\u793a\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u6280\u5de7\uff08\u5305\u62ec\u674e\u7fa4\u4ee3\u6570\uff09\uff0c\u5e76\u5728\u56db\u79cd\u5178\u578b\u673a\u5668\u4eba\u573a\u666f\uff08\u76f4\u63a5\u4f18\u5316\u3001\u76d1\u7763\u5b66\u4e60/\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u5fae\u5206\u52a8\u6001\u89c4\u5212\u8f68\u8ff9\u4f18\u5316\uff09\u4e2d\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5bf9\u4e0d\u540c\u59ff\u6001\u8868\u793a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5404\u8868\u793a\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\u7ed9\u51fa\u4e86\u5177\u4f53\u7684\u5e94\u7528\u5efa\u8bae\u3002", "conclusion": "\u4e0d\u540c\u76843D\u59ff\u6001\u8868\u793a\u9002\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\uff0c\u672c\u6587\u63d0\u4f9b\u7684\u7cfb\u7edf\u6027\u5206\u6790\u548c\u5b9e\u8bc1\u57fa\u51c6\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u548c\u5de5\u7a0b\u5e08\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u505a\u51fa\u66f4\u4f18\u9009\u62e9\u3002"}}
{"id": "2509.16713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16713", "abs": "https://arxiv.org/abs/2509.16713", "authors": ["Tianyang Xu", "Hongqiu Wu", "Weiqi Wu", "Hai Zhao"], "title": "OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama", "comment": "Accepted by EMNLP 2025 demo", "summary": "LLM-based Interactive Drama introduces a novel dialogue scenario in which the\nplayer immerses into a character and engages in a dramatic story by interacting\nwith LLM agents. Despite the fact that this emerging area holds significant\npromise, it remains largely underexplored due to the lack of a well-designed\nplayground to develop a complete drama. This makes a significant barrier for\nresearchers to replicate, extend, and study such systems. Hence, we present\nOpen-Theatre, the first open-source toolkit for experiencing and customizing\nLLM-based interactive drama. It refines prior work with an efficient\nmulti-agent architecture and a hierarchical retrieval-based memory system,\ndesigned to enhance narrative coherence and realistic long-term behavior in\ncomplex interactions. In addition, we provide a highly configurable pipeline,\nmaking it easy for researchers to develop and optimize new approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Open-Theatre\uff0c\u9996\u4e2a\u5f00\u6e90\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e92\u52a8\u620f\u5267\u5de5\u5177\u5305\uff0c\u65e8\u5728\u89e3\u51b3\u8be5\u9886\u57df\u7f3a\u4e4f\u5b8c\u6574\u5f00\u53d1\u5e73\u53f0\u7684\u95ee\u9898\u3002", "motivation": "LLM-based\u4e92\u52a8\u620f\u5267\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u56e0\u7f3a\u5c11\u5b8c\u5584\u7684\u5b9e\u9a8c\u5e73\u53f0\u800c\u96be\u4ee5\u590d\u73b0\u548c\u6269\u5c55\uff0c\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u5206\u5c42\u68c0\u7d22\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u9ad8\u5ea6\u53ef\u914d\u7f6e\u7684\u7ba1\u9053\uff0c\u652f\u6301\u5b9a\u5236\u5316\u5f00\u53d1\u3002", "result": "Open-Theatre\u63d0\u5347\u4e86\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u957f\u671f\u884c\u4e3a\u7684\u771f\u5b9e\u6027\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6613\u4e8e\u4f18\u5316\u65b0\u65b9\u6cd5\u7684\u5f00\u6e90\u5de5\u5177\u3002", "conclusion": "Open-Theatre\u586b\u8865\u4e86LLM\u4e92\u52a8\u620f\u5267\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u8be5\u65b9\u5411\u7684\u53ef\u590d\u73b0\u6027\u4e0e\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.16833", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16833", "abs": "https://arxiv.org/abs/2509.16833", "authors": ["Shaharyar Ahmed Khan Tareen", "Lei Fan", "Xiaojing Yuan", "Qin Lin", "Bin Hu"], "title": "SOLAR: Switchable Output Layer for Accuracy and Robustness in Once-for-All Training", "comment": "10 pages, 7 figures, 6 tables", "summary": "Once-for-All (OFA) training enables a single super-net to generate multiple\nsub-nets tailored to diverse deployment scenarios, supporting flexible\ntrade-offs among accuracy, robustness, and model-size without retraining.\nHowever, as the number of supported sub-nets increases, excessive parameter\nsharing in the backbone limits representational capacity, leading to degraded\ncalibration and reduced overall performance. To address this, we propose SOLAR\n(Switchable Output Layer for Accuracy and Robustness in Once-for-All Training),\na simple yet effective technique that assigns each sub-net a separate\nclassification head. By decoupling the logit learning process across sub-nets,\nthe Switchable Output Layer (SOL) reduces representational interference and\nimproves optimization, without altering the shared backbone. We evaluate SOLAR\non five datasets (SVHN, CIFAR-10, STL-10, CIFAR-100, and TinyImageNet) using\nfour super-net backbones (ResNet-34, WideResNet-16-8, WideResNet-40-2, and\nMobileNetV2) for two OFA training frameworks (OATS and SNNs). Experiments show\nthat SOLAR outperforms the baseline methods: compared to OATS, it improves\naccuracy of sub-nets up to 1.26 %, 4.71 %, 1.67 %, and 1.76 %, and robustness\nup to 9.01 %, 7.71 %, 2.72 %, and 1.26 % on SVHN, CIFAR-10, STL-10, and\nCIFAR-100, respectively. Compared to SNNs, it improves TinyImageNet accuracy by\nup to 2.93 %, 2.34 %, and 1.35 % using ResNet-34, WideResNet-16-8, and\nMobileNetV2 backbones (with 8 sub-nets), respectively.", "AI": {"tldr": "\u63d0\u51faSOLAR\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3aOnce-for-All\u8bad\u7ec3\u4e2d\u7684\u6bcf\u4e2a\u5b50\u7f51\u7edc\u5206\u914d\u72ec\u7acb\u7684\u5206\u7c7b\u5934\u6765\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6709\u6548\u7f13\u89e3\u53c2\u6570\u5171\u4eab\u5e26\u6765\u7684\u8868\u5f81\u5e72\u6270\u3002", "motivation": "\u5728Once-for-All\u8bad\u7ec3\u4e2d\uff0c\u968f\u7740\u652f\u6301\u7684\u5b50\u7f51\u7edc\u6570\u91cf\u589e\u52a0\uff0c\u4e3b\u5e72\u7f51\u7edc\u4e2d\u7684\u8fc7\u5ea6\u53c2\u6570\u5171\u4eab\u4f1a\u9650\u5236\u8868\u793a\u80fd\u529b\uff0c\u5bfc\u81f4\u6821\u51c6\u6027\u80fd\u4e0b\u964d\u548c\u6574\u4f53\u6027\u80fd\u964d\u4f4e\u3002", "method": "\u63d0\u51faSOLAR\uff08Switchable Output Layer\uff09\uff0c\u4e3a\u6bcf\u4e2a\u5b50\u7f51\u7edc\u5206\u914d\u72ec\u7acb\u7684\u5206\u7c7b\u5934\uff0c\u89e3\u8026logits\u5b66\u4e60\u8fc7\u7a0b\uff0c\u51cf\u5c11\u8868\u5f81\u5e72\u6270\uff0c\u63d0\u5347\u4f18\u5316\u6548\u679c\uff0c\u540c\u65f6\u4e0d\u6539\u53d8\u5171\u4eab\u4e3b\u5e72\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u56db\u79cd\u4e3b\u5e72\u7f51\u7edc\u4e0a\u9a8c\u8bc1\u4e86SOLAR\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4OATS\u548cSNNs\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u5b50\u7f51\u7edc\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5982\u5728SVHN\u4e0a\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u53471.26%\uff0c\u9c81\u68d2\u6027\u63d0\u5347\u8fbe9.01%\u3002", "conclusion": "SOLAR\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u4e3b\u5e72\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347Once-for-All\u8bad\u7ec3\u4e2d\u5404\u5b50\u7f51\u7edc\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.17353", "categories": ["cs.AI", "eess.IV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.17353", "abs": "https://arxiv.org/abs/2509.17353", "authors": ["Ahmed T. Elboardy", "Ghada Khoriba", "Essam A. Rashed"], "title": "Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation", "comment": "NeurIPS2025 Workshop: Evaluating the Evolving LLM Lifecycle:\n  Benchmarks, Emergent Abilities, and Scaling", "summary": "Automating radiology report generation poses a dual challenge: building\nclinically reliable systems and designing rigorous evaluation protocols. We\nintroduce a multi-agent reinforcement learning framework that serves as both a\nbenchmark and evaluation environment for multimodal clinical reasoning in the\nradiology ecosystem. The proposed framework integrates large language models\n(LLMs) and large vision models (LVMs) within a modular architecture composed of\nten specialized agents responsible for image analysis, feature extraction,\nreport generation, review, and evaluation. This design enables fine-grained\nassessment at both the agent level (e.g., detection and segmentation accuracy)\nand the consensus level (e.g., report quality and clinical relevance). We\ndemonstrate an implementation using chatGPT-4o on public radiology datasets,\nwhere LLMs act as evaluators alongside medical radiologist feedback. By\naligning evaluation protocols with the LLM development lifecycle, including\npretraining, finetuning, alignment, and deployment, the proposed benchmark\nestablishes a path toward trustworthy deviance-based radiology report\ngeneration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u4e34\u5e8a\u63a8\u7406\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u4e34\u5e8a\u53ef\u9760\u6027\u4e0d\u8db3\u548c\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e25\u8c28\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u5341\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\u7684\u6a21\u5757\u5316\u67b6\u6784\uff0c\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u89c6\u89c9\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u56fe\u50cf\u5206\u6790\u3001\u7279\u5f81\u63d0\u53d6\u3001\u62a5\u544a\u751f\u6210\u4e0e\u8bc4\u4f30\u3002", "result": "\u5728\u516c\u5171\u653e\u5c04\u5b66\u6570\u636e\u96c6\u4e0a\u4f7f\u7528GPT-4o\u5b9e\u73b0\u4e86\u8be5\u6846\u67b6\uff0cLLM\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u5171\u540c\u4f5c\u4e3a\u8bc4\u4f30\u8005\uff0c\u5b9e\u73b0\u4e86\u4ece\u68c0\u6d4b\u5230\u62a5\u544a\u8d28\u91cf\u7684\u591a\u5c42\u6b21\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u4fe1\u8d56\u7684\u57fa\u4e8e\u504f\u5dee\u7684\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u51c6\u548c\u53d1\u5c55\u8def\u5f84\u3002"}}
{"id": "2509.16633", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16633", "abs": "https://arxiv.org/abs/2509.16633", "authors": ["Abhirama Subramanyam Penamakuri", "Navlika Singh", "Piyush Arora", "Anand Mishra"], "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs", "comment": "Accepted to EMNLP (Main) 2025", "summary": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable\nperformance in various vision and language tasks, including visual question\nanswering (VQA). However, their high computational cost makes them impractical\nfor resource-constrained settings and inference-heavy applications. In\ncontrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer\nfrom a significant performance gap compared to their larger counterparts. In\nthis work, we introduce the Model Parity Aligner (MPA), a novel framework\ndesigned to systematically improve S-VLMs by leveraging unlabeled images and\neffective knowledge transfer from L-VLMs. Instead of traditional knowledge\ndistillation methods that rely on labeled training data, MPA employs a\nstrategic parity-based approach that precisely identifies the knowledge\ndisparities between S-VLMs and L-VLMs, and optimizes training by targeting only\nthese disparities. We conduct extensive experiments on four diverse VQA\nbenchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires\nspecialized reasoning capabilities such as text recognition, chart\ninterpretation, and commonsense and factual understanding. Our results\ndemonstrate that MPA consistently enhances the performance of S-VLMs on all\nbenchmarks, reducing the performance gap while maintaining computational\nefficiency. We make our code publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aModel Parity Aligner (MPA)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u65e0\u6807\u7b7e\u56fe\u50cf\u548c\u4ece\u5927\u6a21\u578b\u5230\u5c0f\u6a21\u578b\u7684\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\u6765\u7cfb\u7edf\u6027\u63d0\u5347\u5c0f\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(S-VLMs)\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(L-VLMs)\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u4e0d\u5b9e\u7528\uff1b\u800c\u5c0f\u578b\u6a21\u578b\u867d\u7136\u9ad8\u6548\u4f46\u6027\u80fd\u5dee\u8ddd\u660e\u663e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u7f29\u5c0f\u6027\u80fd\u5dee\u8ddd\u3002", "method": "MPA\u91c7\u7528\u57fa\u4e8e\u5dee\u5f02\u6027\u7684\u7b56\u7565\uff0c\u7cbe\u786e\u8bc6\u522bS-VLMs\u4e0eL-VLMs\u4e4b\u95f4\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u5e76\u4ec5\u9488\u5bf9\u8fd9\u4e9b\u5dee\u8ddd\u8fdb\u884c\u4f18\u5316\u8bad\u7ec3\uff0c\u4f7f\u7528\u65e0\u6807\u7b7e\u56fe\u50cf\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\uff0c\u907f\u514d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u7684\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u5728TextVQA\u3001ST-VQA\u3001ChartQA\u548cOKVQA\u56db\u4e2aVQA\u57fa\u51c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cMPA consistently \u63d0\u5347\u4e86S-VLMs\u7684\u6027\u80fd\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5927\u6a21\u578b\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "MPA\u4e3a\u63d0\u5347\u5c0f\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u591a\u79cd\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u51cf\u5c11\u5bf9\u5927\u578b\u6a21\u578b\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.17287", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17287", "abs": "https://arxiv.org/abs/2509.17287", "authors": ["Gokul B. Nair", "Alejandro Fontan", "Michael Milford", "Tobias Fischer"], "title": "Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation", "comment": "8 Pages, 4 Figures, Under Review", "summary": "Visual teach-and-repeat navigation enables robots to autonomously traverse\npreviously demonstrated paths by comparing current sensory input with recorded\ntrajectories. However, conventional frame-based cameras fundamentally limit\nsystem responsiveness: their fixed frame rates (typically 30-60 Hz) create\ninherent latency between environmental changes and control responses. Here we\npresent the first event-camera-based visual teach-and-repeat system. To achieve\nthis, we develop a frequency-domain cross-correlation framework that transforms\nthe event stream matching problem into computationally efficient Fourier space\nmultiplications, capable of exceeding 300Hz processing rates, an order of\nmagnitude faster than frame-based approaches. By exploiting the binary nature\nof event frames and applying image compression techniques, we further enhance\nthe computational speed of the cross-correlation process without sacrificing\nlocalization accuracy. Extensive experiments using a Prophesee EVK4 HD event\ncamera mounted on an AgileX Scout Mini robot demonstrate successful autonomous\nnavigation across 4000+ meters of indoor and outdoor trajectories. Our system\nachieves ATEs below 24 cm while maintaining consistent high-frequency control\nupdates. Our evaluations show that our approach achieves substantially higher\nupdate rates compared to conventional frame-based systems, underscoring the\npractical viability of event-based perception for real-time robotic navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u89c6\u89c9\u6559\u5b66-\u91cd\u590d\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u9891\u57df\u4e92\u76f8\u5173\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u9891\u7387\u7684\u4e8b\u4ef6\u6d41\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bfc\u822a\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e27\u7684\u76f8\u673a\u7531\u4e8e\u56fa\u5b9a\u5e27\u7387\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u54cd\u5e94\u901f\u5ea6\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5bfc\u822a\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5feb\u901f\u3001\u4f4e\u5ef6\u8fdf\u7684\u611f\u77e5\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9891\u57df\u4ea4\u53c9\u76f8\u5173\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u6d41\u5339\u914d\u8f6c\u6362\u4e3a\u5085\u91cc\u53f6\u7a7a\u95f4\u4e2d\u7684\u9ad8\u6548\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5e76\u5229\u7528\u4e8b\u4ef6\u5e27\u7684\u4e8c\u503c\u7279\u6027\u7ed3\u5408\u56fe\u50cf\u538b\u7f29\u6280\u672f\u8fdb\u4e00\u6b65\u52a0\u901f\u8ba1\u7b97\u3002", "result": "\u5728AgileX Scout Mini\u673a\u5668\u4eba\u4e0a\u4f7f\u7528Prophesee EVK4 HD\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6210\u529f\u5b8c\u6210\u4e86\u8d85\u8fc74000\u7c73\u7684\u5ba4\u5185\u5916\u8def\u5f84\u5bfc\u822a\uff0cATE\u4f4e\u4e8e24\u5398\u7c73\uff0c\u5904\u7406\u901f\u7387\u8d85\u8fc7300Hz\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u5e27\u57fa\u7cfb\u7edf\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u66f4\u9ad8\u7684\u66f4\u65b0\u9891\u7387\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u7684\u611f\u77e5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.16717", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16717", "abs": "https://arxiv.org/abs/2509.16717", "authors": ["Haoran Li", "Zhiming Su", "Junyan Yao", "Enwei Zhang", "Yang Ji", "Yan Chen", "Kan Zhou", "Chao Feng", "Jiao Ran"], "title": "Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling", "comment": "Submitted to AAAI 2026", "summary": "Synthetic data is widely adopted in embedding models to ensure diversity in\ntraining data distributions across dimensions such as difficulty, length, and\nlanguage. However, existing prompt-based synthesis methods struggle to capture\ndomain-specific data distributions, particularly in data-scarce domains, and\noften overlook fine-grained relevance diversity. In this paper, we present a\nChinese short video dataset with 4-level relevance annotations, filling a\ncritical resource void. Further, we propose a semi-supervised synthetic data\npipeline where two collaboratively trained models generate domain-adaptive\nshort video data with controllable relevance labels. Our method enhances\nrelevance-level diversity by synthesizing samples for underrepresented\nintermediate relevance labels, resulting in a more balanced and semantically\nrich training data set. Extensive offline experiments show that the embedding\nmodel trained on our synthesized data outperforms those using data generated\nbased on prompting or vanilla supervised fine-tuning(SFT). Moreover, we\ndemonstrate that incorporating more diverse fine-grained relevance levels in\ntraining data enhances the model's sensitivity to subtle semantic distinctions,\nhighlighting the value of fine-grained relevance supervision in embedding\nlearning. In the search enhanced recommendation pipeline of Douyin's\ndual-column scenario, through online A/B testing, the proposed model increased\nclick-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance\nRatio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by\n0.1054%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u53ef\u63a7\u76f8\u5173\u6027\u6807\u7b7e\u7684\u9886\u57df\u81ea\u9002\u5e94\u77ed\u89c6\u9891\u6570\u636e\uff0c\u63d0\u5347\u4e86\u5d4c\u5165\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u76f8\u5173\u6027\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5728\u6296\u97f3\u53cc\u5217\u63a8\u8350\u573a\u666f\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u70b9\u51fb\u7387\u548c\u7528\u6237\u6e17\u900f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u5408\u6210\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7279\u5b9a\u9886\u57df\uff08\u5c24\u5176\u662f\u6570\u636e\u7a00\u7f3a\u9886\u57df\uff09\u7684\u6570\u636e\u5206\u5e03\uff0c\u4e14\u5ffd\u89c6\u4e86\u7ec6\u7c92\u5ea6\u76f8\u5173\u6027\u7684\u591a\u6837\u6027\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4e2d\u6587\u77ed\u89c6\u9891\u573a\u666f\u4e0b\u76f8\u5173\u6027\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u548c\u591a\u6837\u6027\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u534a\u76d1\u7763\u5408\u6210\u6570\u636e\u6d41\u7a0b\uff0c\u901a\u8fc7\u4e24\u4e2a\u534f\u540c\u8bad\u7ec3\u7684\u6a21\u578b\u751f\u6210\u5177\u6709\u56db\u7ea7\u76f8\u5173\u6027\u6807\u6ce8\u7684\u4e2d\u6587\u77ed\u89c6\u9891\u5408\u6210\u6570\u636e\uff0c\u7279\u522b\u589e\u5f3a\u4e2d\u95f4\u76f8\u5173\u6027\u7ea7\u522b\u6837\u672c\u7684\u751f\u6210\uff0c\u4ee5\u63d0\u5347\u76f8\u5173\u6027\u5c42\u7ea7\u7684\u591a\u6837\u6027\u4e0e\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5d4c\u5165\u6a21\u578b\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u6216\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u751f\u6210\u6570\u636e\u7684\u6a21\u578b\uff1b\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4f7f\u70b9\u51fb\u7387\u63d0\u53471.45%\uff0c\u5f3a\u76f8\u5173\u6bd4\u4f8b\u63d0\u9ad84.9%\uff0c\u56fe\u7247\u7528\u6237\u6e17\u900f\u7387\u63d0\u53470.1054%\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7684\u76f8\u5173\u6027\u76d1\u7763\u5bf9\u5d4c\u5165\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u6240\u63d0\u51fa\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5bf9\u7ec6\u5fae\u8bed\u4e49\u5dee\u5f02\u7684\u654f\u611f\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u63a8\u8350\u7cfb\u7edf\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2509.16860", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16860", "abs": "https://arxiv.org/abs/2509.16860", "authors": ["Mohammad Abdul Hafeez Khan", "Marcello Mattei Di Eugeni", "Benjamin Diaz", "Ruth E. White", "Siddhartha Bhattacharyya", "Venkat Keshav Chivukula"], "title": "LVADNet3D: A Deep Autoencoder for Reconstructing 3D Intraventricular Flow from Sparse Hemodynamic Data", "comment": "Accepted to International Conference on Machine Learning and\n  Applications (ICMLA), 6 pages, 4 figure, 3 tables", "summary": "Accurate assessment of intraventricular blood flow is essential for\nevaluating hemodynamic conditions in patients supported by Left Ventricular\nAssist Devices (LVADs). However, clinical imaging is either incompatible with\nLVADs or yields sparse, low-quality velocity data. While Computational Fluid\nDynamics (CFD) simulations provide high-fidelity data, they are computationally\nintensive and impractical for routine clinical use. To address this, we propose\nLVADNet3D, a 3D convolutional autoencoder that reconstructs full-resolution\nintraventricular velocity fields from sparse velocity vector inputs. In\ncontrast to a standard UNet3D model, LVADNet3D incorporates hybrid downsampling\nand a deeper encoder-decoder architecture with increased channel capacity to\nbetter capture spatial flow patterns. To train and evaluate the models, we\ngenerate a high-resolution synthetic dataset of intraventricular blood flow in\nLVAD-supported hearts using CFD simulations. We also investigate the effect of\nconditioning the models on anatomical and physiological priors. Across various\ninput configurations, LVADNet3D outperforms the baseline UNet3D model, yielding\nlower reconstruction error and higher PSNR results.", "AI": {"tldr": "\u63d0\u51faLVADNet3D\uff0c\u4e00\u79cd\u57fa\u4e8e3D\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u901f\u5ea6\u5411\u91cf\u91cd\u5efa\u5168\u5206\u8fa8\u7387\u5fc3\u5ba4\u5185\u8840\u6d41\u901f\u5ea6\u573a\uff0c\u76f8\u6bd4UNet3D\u5728\u8bef\u5dee\u548cPSNR\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4e34\u5e8a\u6210\u50cf\u96be\u4ee5\u517c\u5bb9LVAD\u4e14\u6570\u636e\u7a00\u758f\u4f4e\u8d28\uff0cCFD\u6a21\u62df\u867d\u7cbe\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u65e0\u6cd5\u7528\u4e8e\u5e38\u89c4\u4e34\u5e8a\u8bc4\u4f30\u3002", "method": "\u8bbe\u8ba1LVADNet3D\uff0c\u91c7\u7528\u6df7\u5408\u4e0b\u91c7\u6837\u3001\u66f4\u6df1\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u548c\u66f4\u9ad8\u7684\u901a\u9053\u5bb9\u91cf\uff1b\u4f7f\u7528CFD\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u7814\u7a76\u89e3\u5256\u548c\u751f\u7406\u5148\u9a8c\u4fe1\u606f\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u79cd\u8f93\u5165\u914d\u7f6e\u4e0b\uff0cLVADNet3D\u76f8\u6bd4\u6807\u51c6UNet3D\u5177\u6709\u66f4\u4f4e\u7684\u91cd\u5efa\u8bef\u5dee\u548c\u66f4\u9ad8\u7684PSNR\u3002", "conclusion": "LVADNet3D\u80fd\u6709\u6548\u91cd\u5efa\u5fc3\u5ba4\u5185\u8840\u6d41\u901f\u5ea6\u573a\uff0c\u6709\u671b\u4e3aLVAD\u60a3\u8005\u63d0\u4f9b\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u8840\u6d41\u52a8\u529b\u5b66\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2509.17354", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17354", "abs": "https://arxiv.org/abs/2509.17354", "authors": ["Jiazhao Shi", "Yichen Lin", "Yiheng Hua", "Ziyu Wang", "Zijian Zhang", "Wenjia Zheng", "Yun Song", "Kuan Lu", "Shoufeng Lu"], "title": "Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification", "comment": null, "summary": "Lane-change maneuvers are a leading cause of highway accidents, underscoring\nthe need for accurate intention prediction to improve the safety and\ndecision-making of autonomous driving systems. While prior studies using\nmachine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers)\nhave shown promise, most approaches remain limited by binary classification,\nlack of scenario diversity, and degraded performance under longer prediction\nhorizons. In this study, we propose a physics-informed AI framework that\nexplicitly integrates vehicle kinematics, interaction feasibility, and\ntraffic-safety metrics (e.g., distance headway, time headway,\ntime-to-collision, closing gap time) into the learning process. lane-change\nprediction is formulated as a three-class problem that distinguishes left\nchange, right change, and no change, and is evaluated across both straight\nhighway segments (highD) and complex ramp scenarios (exiD). By integrating\nvehicle kinematics with interaction features, our machine learning models,\nparticularly LightGBM, achieve state-of-the-art accuracy and strong\ngeneralization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD,\nand 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon,\noutperforming a two-layer stacked LSTM baseline. These findings demonstrate the\npractical advantages of a physics-informed and feature-rich machine learning\nframework for real-time lane-change intention prediction in autonomous driving\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f66\u9053\u53d8\u6362\u610f\u56fe\u9884\u6d4b\uff0c\u901a\u8fc7\u5f15\u5165\u8f66\u8f86\u52a8\u529b\u5b66\u548c\u4ea4\u4e92\u7279\u5f81\uff0c\u5728\u591a\u573a\u666f\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e09\u5206\u7c7b\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u8f66\u9053\u53d8\u6362\u9884\u6d4b\u65b9\u6cd5\u591a\u4e3a\u4e8c\u5206\u7c7b\uff0c\u7f3a\u4e4f\u573a\u666f\u591a\u6837\u6027\uff0c\u4e14\u5728\u8f83\u957f\u9884\u6d4b\u65f6\u57df\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u96be\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9700\u6c42\u3002", "method": "\u5c06\u8f66\u9053\u53d8\u6362\u9884\u6d4b\u5efa\u6a21\u4e3a\u4e09\u5206\u7c7b\u95ee\u9898\uff08\u5de6\u53d8\u9053\u3001\u53f3\u53d8\u9053\u3001\u4e0d\u53d8\u9053\uff09\uff0c\u878d\u5408\u8f66\u8f86\u8fd0\u52a8\u5b66\u3001\u4ea4\u4e92\u53ef\u884c\u6027\u53ca\u4ea4\u901a\u5b89\u5168\u6307\u6807\uff08\u5982\u8f66\u5934\u65f6\u8ddd\u3001\u78b0\u649e\u65f6\u95f4\u7b49\uff09\u4f5c\u4e3a\u7279\u5f81\uff0c\u91c7\u7528LightGBM\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5728highD\u548cexiD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u57281\u79d2\u9884\u6d4b\u65f6\u57df\u5185\uff0chighD\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.8%\u51c6\u786e\u7387\u548c93.6% macro F1\uff0cexiD\u6570\u636e\u96c6\u4e0a\u4e3a96.1%\u51c6\u786e\u7387\u548c88.7% macro F1\uff0c\u4f18\u4e8eLSTM\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u878d\u5408\u7269\u7406\u4fe1\u606f\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5728\u8f66\u9053\u53d8\u6362\u610f\u56fe\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u3002"}}
{"id": "2509.16635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16635", "abs": "https://arxiv.org/abs/2509.16635", "authors": ["Xulin Li", "Yan Lu", "Bin Liu", "Jiaze Li", "Qinhong Yang", "Tao Gong", "Qi Chu", "Mang Ye", "Nenghai Yu"], "title": "Towards Anytime Retrieval: A Benchmark for Anytime Person Re-Identification", "comment": "Accepted by IJCAI 2025", "summary": "In real applications, person re-identification (ReID) is expected to retrieve\nthe target person at any time, including both daytime and nighttime, ranging\nfrom short-term to long-term. However, existing ReID tasks and datasets can not\nmeet this requirement, as they are constrained by available time and only\nprovide training and evaluation for specific scenarios. Therefore, we\ninvestigate a new task called Anytime Person Re-identification (AT-ReID), which\naims to achieve effective retrieval in multiple scenarios based on variations\nin time. To address the AT-ReID problem, we collect the first large-scale\ndataset, AT-USTC, which contains 403k images of individuals wearing multiple\nclothes captured by RGB and IR cameras. Our data collection spans 21 months,\nand 270 volunteers were photographed on average 29.1 times across different\ndates or scenes, 4-15 times more than current datasets, providing conditions\nfor follow-up investigations in AT-ReID. Further, to tackle the new challenge\nof multi-scenario retrieval, we propose a unified model named Uni-AT, which\ncomprises a multi-scenario ReID (MS-ReID) framework for scenario-specific\nfeatures learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate\ninter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)\nstrategy to ensure balanced training across all scenarios. Extensive\nexperiments show that our model leads to satisfactory results and exhibits\nexcellent generalization to all scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u610f\u65f6\u95f4\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\uff08AT-ReID\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6AT-USTC\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u7edf\u4e00\u6a21\u578bUni-AT\u6765\u5b9e\u73b0\u591a\u573a\u666f\u4e0b\u7684\u6709\u6548\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u548c\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u65f6\u95f4\u8303\u56f4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u8de8\u663c\u591c\u3001\u957f\u671f\u53d8\u5316\u7b49\u591a\u573a\u666f\u7684\u8bc6\u522b\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86Uni-AT\u6a21\u578b\uff0c\u5305\u542b\u591a\u573a\u666fReID\u6846\u67b6\u3001\u5c5e\u6027\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff08MoAE\uff09\u548c\u5206\u5c42\u52a8\u6001\u52a0\u6743\u7b56\u7565\uff08HDW\uff09\uff0c\u5e76\u5728\u81ea\u5efa\u7684AT-USTC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u826f\u597d\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Uni-AT\u6a21\u578b\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4efb\u610f\u65f6\u95f4\u6761\u4ef6\u4e0b\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u8de8\u65f6\u95f4\u3001\u8de8\u573a\u666fReID\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2509.17299", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17299", "abs": "https://arxiv.org/abs/2509.17299", "authors": ["Dorian Tsai", "Christopher A. Brunner", "Riki Lamont", "F. Mikaela Nordborg", "Andrea Severati", "Java Terry", "Karen Jackel", "Matthew Dunbabin", "Tobias Fischer", "Scarlett Raine"], "title": "Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)", "comment": "9 pages, 7 figures", "summary": "Coral aquaculture for reef restoration requires accurate and continuous spawn\ncounting for resource distribution and larval health monitoring, but current\nmethods are labor-intensive and represent a critical bottleneck in the coral\nproduction pipeline. We propose the Coral Spawn and Larvae Imaging Camera\nSystem (CSLICS), which uses low cost modular cameras and object detectors\ntrained using human-in-the-loop labeling approaches for automated spawn\ncounting in larval rearing tanks. This paper details the system engineering,\ndataset collection, and computer vision techniques to detect, classify and\ncount coral spawn. Experimental results from mass spawning events demonstrate\nan F1 score of 82.4\\% for surface spawn detection at different embryogenesis\nstages, 65.3\\% F1 score for sub-surface spawn detection, and a saving of 5,720\nhours of labor per spawning event compared to manual sampling methods at the\nsame frequency. Comparison of manual counts with CSLICS monitoring during a\nmass coral spawning event on the Great Barrier Reef demonstrates CSLICS'\naccurate measurement of fertilization success and sub-surface spawn counts.\nThese findings enhance the coral aquaculture process and enable upscaling of\ncoral reef restoration efforts to address climate change threats facing\necosystems like the Great Barrier Reef.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSLICS\u7684\u4f4e\u6210\u672c\u6a21\u5757\u5316\u76f8\u673a\u7cfb\u7edf\uff0c\u7ed3\u5408\u57fa\u4e8e\u4eba\u673a\u534f\u540c\u6807\u6ce8\u7684\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u73ca\u745a\u4ea7\u5375\u8ba1\u6570\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u52b3\u52a8\u5e76\u63d0\u9ad8\u73ca\u745a\u517b\u6b96\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u73ca\u745a\u4ea7\u5375\u8ba1\u6570\u4f9d\u8d56\u4eba\u5de5\uff0c\u8017\u65f6\u4e14\u6210\u4e3a\u73ca\u745a\u517b\u6b96\u6d41\u7a0b\u4e2d\u7684\u74f6\u9888\uff0c\u9650\u5236\u4e86\u73ca\u745a\u7901\u4fee\u590d\u89c4\u6a21\u3002", "method": "\u5f00\u53d1\u4e86Coral Spawn and Larvae Imaging Camera System (CSLICS)\uff0c\u91c7\u7528\u4f4e\u6210\u672c\u6a21\u5757\u5316\u76f8\u673a\uff0c\u7ed3\u5408\u4eba\u673a\u534f\u540c\u6807\u6ce8\u8bad\u7ec3\u7684\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u6c34\u9762\u548c\u6c34\u4e0b\u73ca\u745a\u4ea7\u5375\u7684\u81ea\u52a8\u68c0\u6d4b\u3001\u5206\u7c7b\u4e0e\u8ba1\u6570\u3002", "result": "\u5728\u5927\u5821\u7901\u7684\u96c6\u4f53\u4ea7\u5375\u4e8b\u4ef6\u4e2d\uff0c\u6c34\u9762\u4ea7\u5375\u68c0\u6d4bF1\u5f97\u5206\u4e3a82.4%\uff0c\u6c34\u4e0b\u4e3a65.3%\uff1b\u76f8\u6bd4\u4eba\u5de5\u65b9\u6cd5\u6bcf\u6b21\u4ea7\u5375\u4e8b\u4ef6\u8282\u77015,720\u5c0f\u65f6 labor\uff1b\u5e76\u4e0e\u4eba\u5de5\u8ba1\u6570\u5bf9\u6bd4\u9a8c\u8bc1\u4e86\u53d7\u7cbe\u6210\u529f\u7387\u548c\u6c34\u4e0b\u8ba1\u6570\u7684\u51c6\u786e\u6027\u3002", "conclusion": "CSLICS\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u73ca\u745a\u517b\u6b96\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u6709\u52a9\u4e8e\u6269\u5927\u73ca\u745a\u7901\u4fee\u590d\u89c4\u6a21\uff0c\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u5bf9\u73ca\u745a\u751f\u6001\u7cfb\u7edf\u7684\u5a01\u80c1\u3002"}}
{"id": "2509.16720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16720", "abs": "https://arxiv.org/abs/2509.16720", "authors": ["Auss Abbood", "Zaiqiao Meng", "Nigel Collier"], "title": "Time to Revist Exact Match", "comment": "Accepted for Findings of EMNLP 2025", "summary": "Temporal question answering is an established method for evaluating temporal\nreasoning in large language models. Expected answers are often numeric (e.g.,\ndates or durations), yet model responses are evaluated like regular text with\nexact match (EM), unable to distinguish small from large errors. In this\ninvestigative work, we frame temporal question answering as a numerical\nestimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a\nbenchmark distilled from Test of Time and TempTabQA, where all questions\nrequire a numerical, temporal answer, allowing us to evaluate models beyond EM.\nWe use the forecasting metrics symmetric mean absolute percentage error (sMAPE)\nand mean absolute scaled error (MASE). With sMAPE, we find that error size and\nEM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some\nmodels have high sMAPE despite high EM. Scaling errors by the deviation of the\nground truth data with MASE reshuffles model rankings compared to EM, revealing\ngaps in models' understanding of temporal domain knowledge, especially when\ntrained with synthetic data. Lastly, the models' most frequent error is to\ndeviate by only $\\pm1$ from the ground truth. sMAPE and MASE, unlike EM,\nadequately weight these errors. Our findings underscore the need for\nspecialised metrics for temporal QA tasks. Code and data are available on\nhttps://github.com/aauss/temporal-answer-qa.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u65f6\u95f4\u95ee\u7b54\u4efb\u52a1\u89c6\u4e3a\u6570\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u5f15\u5165\u65b0\u57fa\u51c6TempAnswerQA\uff0c\u5e76\u91c7\u7528sMAPE\u548cMASE\u7b49\u6570\u503c\u8bef\u5dee\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u4f20\u7edf\u7cbe\u786e\u5339\u914d\uff08EM\uff09\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u65f6\u95f4\u7b54\u6848\u7684\u8bef\u5dee\u5927\u5c0f\uff0c\u5f3a\u8c03\u9700\u8981\u4e13\u95e8\u7684\u65f6\u95f4\u95ee\u7b54\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u95ee\u7b54\u8bc4\u4f30\u4f9d\u8d56\u7cbe\u786e\u5339\u914d\uff08EM\uff09\uff0c\u65e0\u6cd5\u533a\u5206\u5c0f\u8bef\u5dee\u4e0e\u5927\u8bef\u5dee\uff0c\u96be\u4ee5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u5728\u65f6\u95f4\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5bf9\u6570\u503c\u578b\u7b54\u6848\u4e0d\u654f\u611f\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u6570\u503c\u578b\u65f6\u95f4\u95ee\u7b54\u57fa\u51c6TempAnswerQA\uff0c\u7ed3\u5408sMAPE\u548cMASE\u4e24\u79cd\u9884\u6d4b\u8bef\u5dee\u6307\u6807\uff0c\u5728Test of Time\u548cTempTabQA\u6570\u636e\u57fa\u7840\u4e0a\u8bc4\u4f30\u591a\u4e2a\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u4e0eEM\u7ed3\u679c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53d1\u73b0EM\u4e0e\u8bef\u5dee\u5927\u5c0f\u8131\u8282\uff1a\u4e00\u4e9bEM\u9ad8\u7684\u6a21\u578bsMAPE\u4e5f\u9ad8\uff0c\u800c\u90e8\u5206EM\u4f4e\u7684\u6a21\u578b\u4ecd\u6709\u8f83\u5927\u8bef\u5dee\uff1bMASE\u8c03\u6574\u540e\u7684\u6a21\u578b\u6392\u540d\u4e0eEM\u4e0d\u540c\uff0c\u63ed\u793a\u6a21\u578b\u5728\u65f6\u95f4\u9886\u57df\u77e5\u8bc6\u7406\u89e3\u4e0a\u7684\u7f3a\u9677\uff1b\u6a21\u578b\u6700\u5e38\u89c1\u9519\u8bef\u662f\u7b54\u6848\u504f\u79bb\u771f\u5b9e\u503c\u00b11\uff0csMAPE\u548cMASE\u80fd\u66f4\u597d\u6355\u6349\u6b64\u7c7b\u7ec6\u5fae\u8bef\u5dee\u3002", "conclusion": "\u5e94\u91c7\u7528\u66f4\u9002\u5408\u6570\u503c\u65f6\u95f4\u7b54\u6848\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982sMAPE\u548cMASE\uff09\uff0c\u800c\u975e\u4f9d\u8d56\u4f20\u7edfEM\uff0c\u4ee5\u66f4\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.16875", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16875", "abs": "https://arxiv.org/abs/2509.16875", "authors": ["Qishuai Wen", "Zhiyuan Huang", "Chun-Guang Li"], "title": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few", "comment": "NeurIPS 2025 Spotlight", "summary": "Attention mechanisms in Transformers have gained significant empirical\nsuccess. Nonetheless, the optimization objectives underlying their forward pass\nare still unclear. Additionally, the quadratic complexity of self-attention is\nincreasingly prohibitive. Unlike the prior work on addressing the\ninterpretability or efficiency issue separately, we propose a unified\noptimization objective to alleviate both issues simultaneously. By unrolling\nthe optimization over the objective, we derive an inherently interpretable and\nefficient attention mechanism, which compresses all tokens into low-dimensional\nstructures by contracting a few representative tokens and then broadcasting the\ncontractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism\ncan not only scale linearly but also generalize existing attention mechanisms\nas its special cases. Experiments further demonstrate comparable performance\nand even superior advantages of CBSA on several visual tasks. Code is available\nat this https URL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236CBSA\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4f18\u5316\u76ee\u6807\u540c\u65f6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u5177\u6709\u7ebf\u6027\u6269\u5c55\u6027\u5e76\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684Transformer\u6ce8\u610f\u529b\u673a\u5236\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u4f18\u5316\u76ee\u6807\u4e0d\u660e\u786e\uff0c\u4e14\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u53c8\u80fd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u4f18\u5316\u76ee\u6807\uff0c\u901a\u8fc7\u5c55\u5f00\u8be5\u4f18\u5316\u8fc7\u7a0b\u63a8\u5bfc\u51faContract-and-Broadcast Self-Attention\uff08CBSA\uff09\u673a\u5236\uff0c\u5c06\u6240\u6709token\u538b\u7f29\u4e3a\u4f4e\u7ef4\u7ed3\u6784\uff0c\u901a\u8fc7\u9009\u4ee3\u8868\u6027token\u8fdb\u884c\u6536\u7f29\u548c\u5e7f\u64ad\u5b9e\u73b0\u9ad8\u6548\u6ce8\u610f\u529b\u3002", "result": "CBSA\u673a\u5236\u5b9e\u73b0\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u80fd\u591f\u63a8\u5e7f\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "CBSA\u901a\u8fc7\u7edf\u4e00\u7684\u4f18\u5316\u6846\u67b6\u540c\u65f6\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u81ea\u6ce8\u610f\u529b\u53d8\u4f53\u3002"}}
{"id": "2509.17380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17380", "abs": "https://arxiv.org/abs/2509.17380", "authors": ["Zhizhang FU", "Guangsheng Bao", "Hongbo Zhang", "Chenkai Hu", "Yue Zhang"], "title": "Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process", "comment": null, "summary": "LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and\ninconsistency, since they lack robust causal underpinnings and may rely on\nsuperficial correlations rather than genuine understanding. Successive LRMs\nhave emerged as a promising alternative, leveraging advanced training\ntechniques such as reinforcement learning (RL) and distillation to improve task\naccuracy. However, the impact of these training methods on causality remains\nlargely unexplored. In this study, we conduct a systematic causal analysis on\nLLMs and LRMs, examining structural causal models (SCMs) of four key variables:\nproblem instruction (Z), thinking process (T), reasoning steps (X), and answer\n(Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal\nreasoning capabilities, aligning more closely with ideal causal structures,\nwhile LLMs and distilled LRMs fail to address causality-related deficiencies.\nOur further investigation indicates that RLVR reduces spurious correlations and\nstrengthens genuine causal patterns, thereby mitigating unfaithfulness and\nbias. In addition, our inspection on the dynamics of the RLVR training process\nobserves a high correlation between reduced spurious features and improved\ncausal structures, where the causal relationships consistently improve in the\ntraining process. This study contributes to the understanding of causality in\nreasoning models, highlights the critical role of RLVR in enhancing causal\nreasoning, and provides insights for designing future AI systems with stronger\ncausal foundations. We release our code and data at\nhttps://github.com/Harryking1999/CoT_Causal_Analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff08LRM\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u56e0\u679c\u5206\u6790\uff0c\u53d1\u73b0\u91c7\u7528RLVR\u8bad\u7ec3\u7684LRM\u5728\u56e0\u679c\u63a8\u7406\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfLLM\u548c\u84b8\u998f\u5f0fLRM\uff0c\u80fd\u66f4\u6709\u6548\u51cf\u5c11\u865a\u5047\u76f8\u5173\u6027\u5e76\u589e\u5f3a\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "LLM\u5b58\u5728\u63a8\u7406\u4e0d\u5fe0\u3001\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u56e0\u5176\u7f3a\u4e4f\u7a33\u5065\u7684\u56e0\u679c\u57fa\u7840\uff1b\u73b0\u6709LRM\u867d\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u5176\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u56e0\u679c\u6027\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\uff0c\u56e0\u6b64\u9700\u7cfb\u7edf\u5206\u6790\u5176\u56e0\u679c\u7ed3\u6784\u3002", "method": "\u6784\u5efa\u5305\u542b\u95ee\u9898\u6307\u4ee4\uff08Z\uff09\u3001\u601d\u8003\u8fc7\u7a0b\uff08T\uff09\u3001\u63a8\u7406\u6b65\u9aa4\uff08X\uff09\u548c\u7b54\u6848\uff08Y\uff09\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\uff0c\u5bf9\u6bd4\u5206\u6790LLM\u4e0e\u4e0d\u540cLRM\uff08\u5982RLVR\u8bad\u7ec3\u548c\u84b8\u998f\u6a21\u578b\uff09\u7684\u56e0\u679c\u8def\u5f84\u53ca\u8bad\u7ec3\u52a8\u6001\u3002", "result": "RLVR\u8bad\u7ec3\u7684LRM\u5c55\u73b0\u51fa\u66f4\u63a5\u8fd1\u7406\u60f3\u7ed3\u6784\u7684\u56e0\u679c\u6a21\u5f0f\uff0c\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\uff0c\u589e\u5f3a\u4e86\u771f\u5b9e\u56e0\u679c\u8def\u5f84\uff1b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u865a\u5047\u7279\u5f81\u51cf\u5c11\u4e0e\u56e0\u679c\u7ed3\u6784\u6539\u5584\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "RLVR\u8bad\u7ec3\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u7f13\u89e3LLM\u7684\u5e38\u89c1\u7f3a\u9677\uff0c\u4e3a\u6784\u5efa\u5177\u6709\u66f4\u5f3a\u56e0\u679c\u57fa\u7840\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2509.16639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16639", "abs": "https://arxiv.org/abs/2509.16639", "authors": ["Shangzhuo Xie", "Qianqian Yang"], "title": "Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Point cloud analysis has evolved with diverse network architectures, while\nexisting works predominantly focus on introducing novel structural designs.\nHowever, conventional point-based architectures - processing raw points through\nsequential sampling, grouping, and feature extraction layers - demonstrate\nunderutilized potential. We notice that substantial performance gains can be\nunlocked through strategic module integration rather than structural\nmodifications. In this paper, we propose the Grouping-Feature Coordination\nModule (GF-Core), a lightweight separable component that simultaneously\nregulates both grouping layer and feature extraction layer to enable more\nnuanced feature aggregation. Besides, we introduce a self-supervised\npretraining strategy specifically tailored for point-based inputs to enhance\nmodel robustness in complex point cloud analysis scenarios. On ModelNet40\ndataset, our method elevates baseline networks to 94.0% accuracy, matching\nadvanced frameworks' performance while preserving architectural simplicity. On\nthree variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,\n6.34%, and 6.32% respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5206\u7ec4-\u7279\u5f81\u534f\u8c03\u6a21\u5757\uff08GF-Core\uff09\uff0c\u901a\u8fc7\u534f\u8c03\u5206\u7ec4\u548c\u7279\u5f81\u63d0\u53d6\u5c42\u6765\u63d0\u5347\u70b9\u4e91\u5206\u6790\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u9488\u5bf9\u70b9\u4e91\u6570\u636e\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u7f51\u7edc\u7ed3\u6784\u7b80\u5355\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u65b0\u9896\u7f51\u7edc\u7ed3\u6784\u7684\u8bbe\u8ba1\uff0c\u800c\u5ffd\u89c6\u4e86\u4f20\u7edf\u70b9\u57fa\u67b6\u6784\u4e2d\u5206\u7ec4\u4e0e\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u7684\u534f\u540c\u4f18\u5316\u6f5c\u529b\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u96c6\u6210\u800c\u975e\u7ed3\u6784\u4fee\u6539\u6765\u6316\u6398\u6027\u80fd\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86GF-Core\u6a21\u5757\uff0c\u540c\u65f6\u8c03\u8282\u5206\u7ec4\u5c42\u548c\u7279\u5f81\u63d0\u53d6\u5c42\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u7279\u5f81\u805a\u5408\uff1b\u8bbe\u8ba1\u4e86\u9002\u7528\u4e8e\u70b9\u4e91\u8f93\u5165\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728ModelNet40\u4e0a\u8fbe\u523094.0%\u51c6\u786e\u7387\uff0c\u5ab2\u7f8e\u5148\u8fdb\u6846\u67b6\uff1b\u5728ScanObjectNN\u4e09\u4e2a\u53d8\u4f53\u4e0a\u5206\u522b\u63d0\u53472.96%\u30016.34%\u548c6.32%\u3002", "conclusion": "\u901a\u8fc7\u6a21\u5757\u5316\u6539\u8fdb\u800c\u975e\u590d\u6742\u7ed3\u6784\u8bbe\u8ba1\uff0c\u53ef\u6709\u6548\u63d0\u5347\u70b9\u4e91\u5206\u6790\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u67b6\u6784\u5185\u90e8\u7ec4\u4ef6\u534f\u540c\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.17308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17308", "abs": "https://arxiv.org/abs/2509.17308", "authors": ["Kazutoshi Tanaka", "Tomoya Takahashi", "Masashi Hamaya"], "title": "Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing Intrinsic Dynamics via Physical Reservoir Computing", "comment": "9 pages, 7 figures. Accepted at IROS 2025. This is the preprint\n  version", "summary": "Cable-driven serpentine manipulators hold great potential in unstructured\nenvironments, offering obstacle avoidance, multi-directional force application,\nand a lightweight design. By placing all motors and sensors at the base and\nemploying plastic links, we can further reduce the arm's weight. To demonstrate\nthis concept, we developed a 9-degree-of-freedom cable-driven serpentine\nmanipulator with an arm length of 545 mm and a total mass of only 308 g.\nHowever, this design introduces flexibility-induced variations, such as cable\nslack, elongation, and link deformation. These variations result in\ndiscrepancies between analytical predictions and actual link positions, making\npose estimation more challenging. To address this challenge, we propose a\nphysical reservoir computing based pose estimation method that exploits the\nmanipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.\nExperimental results show a mean pose error of 4.3 mm using our method,\ncompared to 4.4 mm with a baseline long short-term memory network and 39.5 mm\nwith an analytical approach. This work provides a new direction for control and\nperception strategies in lightweight cable-driven serpentine manipulators\nleveraging their intrinsic dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8f7b\u91cf\u5316\u7ef3\u9a71\u86c7\u5f62\u673a\u68b0\u81c2\u56e0\u67d4\u6027\u5f15\u8d77\u7684\u59ff\u6001\u4f30\u8ba1\u96be\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u89e3\u6790\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u673a\u68b0\u81c2\u7684\u67d4\u6027\u5bfc\u81f4\u7535\u7f06\u677e\u5f1b\u3001\u62c9\u4f38\u548c\u8fde\u6746\u53d8\u5f62\uff0c\u4f7f\u5f97\u89e3\u6790\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u5b9e\u9645\u4f4d\u59ff\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u673a\u68b0\u81c2\u56fa\u6709\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\u6784\u5efa\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u57289\u81ea\u7531\u5ea6\u8f7b\u91cf\u5316\u86c7\u5f62\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5e73\u5747\u59ff\u6001\u8bef\u5dee\u4e3a4.3 mm\uff0c\u4f18\u4e8eLSTM\u76844.4 mm\u548c\u89e3\u6790\u65b9\u6cd5\u768439.5 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u7cfb\u7edf\u7684\u5185\u5728\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4e3a\u8f7b\u91cf\u5316\u7ef3\u9a71\u86c7\u5f62\u673a\u68b0\u81c2\u7684\u63a7\u5236\u4e0e\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.16722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16722", "abs": "https://arxiv.org/abs/2509.16722", "authors": ["Xiaohan Ding", "Kaike Ping", "Buse \u00c7ar\u0131k", "Eugenia Rho"], "title": "A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse", "comment": null, "summary": "Understanding causal language in informal discourse is a core yet\nunderexplored challenge in NLP. Existing datasets largely focus on explicit\ncausality in structured text, providing limited support for detecting implicit\ncausal expressions, particularly those found in informal, user-generated social\nmedia posts. We introduce CausalTalk, a multi-level dataset of five years of\nReddit posts (2020-2024) discussing public health related to the COVID-19\npandemic, among which 10120 posts are annotated across four causal tasks: (1)\nbinary causal classification, (2) explicit vs. implicit causality, (3)\ncause-effect span extraction, and (4) causal gist generation. Annotations\ncomprise both gold-standard labels created by domain experts and\nsilver-standard labels generated by GPT-4o and verified by human annotators.\nCausalTalk bridges fine-grained causal detection and gist-based reasoning over\ninformal text. It enables benchmarking across both discriminative and\ngenerative models, and provides a rich resource for studying causal reasoning\nin social media contexts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CausalTalk\uff0c\u4e00\u4e2a\u5305\u542b\u4e94\u5e74Reddit\u5e16\u5b50\u7684\u591a\u5c42\u7ea7\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u975e\u6b63\u5f0f\u6587\u672c\u4e2d\u7684\u56e0\u679c\u8bed\u8a00\uff0c\u7279\u522b\u662f\u5728\u4e0eCOVID-19\u516c\u5171\u536b\u751f\u76f8\u5173\u7684\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u4e2d\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u7684\u663e\u5f0f\u56e0\u679c\u5173\u7cfb\uff0c\u96be\u4ee5\u6355\u6349\u975e\u6b63\u5f0f\u793e\u4ea4\u6587\u672c\u4e2d\u5e38\u89c1\u7684\u9690\u5f0f\u56e0\u679c\u8868\u8fbe\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u8d34\u8fd1\u5b9e\u9645\u8bed\u5883\u7684\u56e0\u679c\u5206\u6790\u8d44\u6e90\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d62020-2024\u5e74Reddit\u5e16\u5b50\u7684\u6570\u636e\u96c6\uff0c\u5bf910120\u4e2a\u5e16\u5b50\u5728\u56db\u4e2a\u56e0\u679c\u4efb\u52a1\u4e0a\u8fdb\u884c\u6807\u6ce8\uff1a\u4e8c\u5143\u56e0\u679c\u5206\u7c7b\u3001\u663e\u5f0f\u4e0e\u9690\u5f0f\u56e0\u679c\u533a\u5206\u3001\u56e0\u679c\u8de8\u5ea6\u63d0\u53d6\u548c\u56e0\u679c\u8981\u70b9\u751f\u6210\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u6807\u6ce8\u4e0eGPT-4o\u751f\u6210\u540e\u7ecf\u4eba\u5de5\u9a8c\u8bc1\u7684\u6807\u7b7e\u3002", "result": "CausalTalk\u652f\u6301\u7ec6\u7c92\u5ea6\u56e0\u679c\u68c0\u6d4b\u4e0e\u57fa\u4e8e\u8981\u70b9\u7684\u63a8\u7406\uff0c\u53ef\u7528\u4e8e\u5224\u522b\u5f0f\u4e0e\u751f\u6210\u5f0f\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\u3002", "conclusion": "CausalTalk\u586b\u8865\u4e86\u975e\u6b63\u5f0f\u6587\u672c\u4e2d\u56e0\u679c\u8bed\u8a00\u7814\u7a76\u7684\u6570\u636e\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4e86\u5bf9\u9690\u5f0f\u56e0\u679c\u8868\u8fbe\u7684\u7406\u89e3\u4e0e\u5efa\u6a21\u3002"}}
{"id": "2509.16882", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16882", "abs": "https://arxiv.org/abs/2509.16882", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Xuming Hu"], "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation", "comment": "EMNLP 2025 Main Conference", "summary": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated\nexpert subnetworks, yet adapting them to multiple domains without catastrophic\nforgetting remains an open challenge. Existing approaches either incur\nprohibitive computation, suffer cross-domain interference, or require separate\nruns per domain. We propose DES-MoE, a dynamic expert specialization framework\nfor multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses\ncatastrophic forgetting through three innovations: (1) an adaptive router\nbalancing pre-trained knowledge retention and task-specific updates via\ndistillation, (2) real-time expert-domain correlation mapping to isolate\ndomain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule\nthat progressively freezes non-specialized parameters. Evaluated on six domains\n(math, code, law, etc.), DES-MoE matches single-domain ESFT performance while\ntraining one unified model, reduces forgetting by 89% compared to full\nfine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence\nthan conventional methods. Our work establishes dynamic expert isolation as a\nscalable paradigm for multi-task MoE adaptation.", "AI": {"tldr": "\u63d0\u51faDES-MoE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u9694\u79bb\u5b9e\u73b0\u591a\u9886\u57df\u81ea\u9002\u5e94\u7684Mixture-of-Experts\u6a21\u578b\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MoE\u6a21\u578b\u5728\u591a\u9886\u57df\u9002\u5e94\u4e2d\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u3001\u8de8\u9886\u57df\u5e72\u6270\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDES-MoE\uff0c\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a\u57fa\u4e8e\u84b8\u998f\u7684\u81ea\u9002\u5e94\u8def\u7531\u5668\u3001\u5b9e\u65f6\u4e13\u5bb6-\u9886\u57df\u5173\u8054\u6620\u5c04\u3001\u4e09\u9636\u6bb5\u81ea\u9002\u5e94\u5fae\u8c03\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u4e13\u5bb6\u52a8\u6001\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u516d\u4e2a\u9886\u57df\uff08\u6570\u5b66\u3001\u4ee3\u7801\u3001\u6cd5\u5f8b\u7b49\uff09\u4e0a\u9a8c\u8bc1\uff0cDES-MoE\u5728\u5355\u6a21\u578b\u4e0b\u8fbe\u5230\u5355\u9886\u57df\u5fae\u8c03\u6027\u80fd\uff0c\u76f8\u6bd4\u5168\u5fae\u8c03\u9057\u5fd8\u51cf\u5c1189%\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u534768%\u3002", "conclusion": "\u52a8\u6001\u4e13\u5bb6\u9694\u79bb\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u4efb\u52a1MoE\u6a21\u578b\u9002\u5e94\u8303\u5f0f\uff0cDES-MoE\u5728\u4fdd\u6301\u6a21\u578b\u7edf\u4e00\u7684\u540c\u65f6\u6709\u6548\u89e3\u51b3\u591a\u9886\u57df\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.17393", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17393", "abs": "https://arxiv.org/abs/2509.17393", "authors": ["Kang-il Lee", "Jahyun Koo", "Seunghyun Yoon", "Minbeom Kim", "Hyukhun Koh", "Dongryeol Lee", "Kyomin Jung"], "title": "Program Synthesis via Test-Time Transduction", "comment": "NeurIPS 2025", "summary": "We introduce transductive program synthesis, a new formulation of the program\nsynthesis task that explicitly leverages test inputs during synthesis. While\nprior approaches to program synthesis--whether based on natural language\ndescriptions or input-output examples--typically aim to generalize from\ntraining examples, they often struggle with robustness, especially in\nreal-world settings where training examples are limited and test inputs involve\nvarious edge cases. To address this, we propose a novel framework that improves\nrobustness by treating synthesis as an active learning over a finite hypothesis\nclass defined by programs' outputs. We use an LLM to predict outputs for\nselected test inputs and eliminate inconsistent hypotheses, where the inputs\nare chosen via a greedy maximin algorithm to minimize the number of LLM queries\nrequired. We evaluate our approach on two real-world datasets: Playgol, a\nstring transformation benchmark, and MBPP+, a Python code generation benchmark.\nWe demonstrate that our method significantly improves program synthesis in both\naccuracy and efficiency. We release our code at\nhttps://github.com/klee972/SYNTRA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u8f93\u5165\u7684\u76f4\u63a8\u5f0f\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\uff0c\u5229\u7528LLM\u9884\u6d4b\u8f93\u51fa\u5e76\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u5408\u6210\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\u5728\u8bad\u7ec3\u6837\u672c\u6709\u9650\u4e14\u6d4b\u8bd5\u8f93\u5165\u5305\u542b\u8fb9\u7f18\u60c5\u51b5\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u5c06\u5408\u6210\u89c6\u4e3a\u5728\u7531\u7a0b\u5e8f\u8f93\u51fa\u5b9a\u4e49\u7684\u6709\u9650\u5047\u8bbe\u7c7b\u4e0a\u7684\u4e3b\u52a8\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f7f\u7528LLM\u9884\u6d4b\u9009\u5b9a\u6d4b\u8bd5\u8f93\u5165\u7684\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u8d2a\u5fc3\u6781\u5927\u6781\u5c0f\u7b97\u6cd5\u9009\u62e9\u8f93\u5165\u4ee5\u6d88\u9664\u4e0d\u4e00\u81f4\u5047\u8bbe\u3002", "result": "\u5728Playgol\u548cMBPP+\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7a0b\u5e8f\u5408\u6210\u7684\u9c81\u68d2\u6027\u4e0e\u6548\u7387\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6837\u672c\u5c11\u3001\u8fb9\u7f18\u60c5\u51b5\u591a\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2509.16645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16645", "abs": "https://arxiv.org/abs/2509.16645", "authors": ["Yichen Wang", "Hangtao Zhang", "Hewen Pan", "Ziqi Zhou", "Xianlong Wang", "Peijin Guo", "Lulu Xue", "Shengshan Hu", "Minghui Li", "Leo Yu Zhang"], "title": "ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents", "comment": null, "summary": "Vision-Language Models (VLMs), with their strong reasoning and planning\ncapabilities, are widely used in embodied decision-making (EDM) tasks in\nembodied agents, such as autonomous driving and robotic manipulation. Recent\nresearch has increasingly explored adversarial attacks on VLMs to reveal their\nvulnerabilities. However, these attacks either rely on overly strong\nassumptions, requiring full knowledge of the victim VLM, which is impractical\nfor attacking VLM-based agents, or exhibit limited effectiveness. The latter\nstems from disrupting most semantic information in the image, which leads to a\nmisalignment between the perception and the task context defined by system\nprompts. This inconsistency interrupts the VLM's reasoning process, resulting\nin invalid outputs that fail to affect interactions in the physical world. To\nthis end, we propose a fine-grained adversarial attack framework, ADVEDM, which\nmodifies the VLM's perception of only a few key objects while preserving the\nsemantics of the remaining regions. This attack effectively reduces conflicts\nwith the task context, making VLMs output valid but incorrect decisions and\naffecting the actions of agents, thus posing a more substantial safety threat\nin the physical world. We design two variants of based on this framework,\nADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific\nobject from the image and add the semantics of a new object into the image. The\nexperimental results in both general scenarios and EDM tasks demonstrate\nfine-grained control and excellent attack performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6ADVEDM\uff0c\u901a\u8fc7\u4ec5\u4fee\u6539\u56fe\u50cf\u4e2d\u5173\u952e\u5bf9\u8c61\u7684\u611f\u77e5\u6765\u653b\u51fb\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5177\u8eab\u51b3\u7b56\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u5176\u4f59\u533a\u57df\u8bed\u4e49\u7684\u540c\u65f6\u751f\u6210\u6709\u6548\u4f46\u9519\u8bef\u7684\u51b3\u7b56\uff0c\u63d0\u5347\u7269\u7406\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u5a01\u80c1\u3002", "motivation": "\u73b0\u6709\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u8981\u4e48\u4f9d\u8d56\u8fc7\u5f3a\u5047\u8bbe\uff08\u5982\u5b8c\u5168\u77e5\u6653\u6a21\u578b\u4fe1\u606f\uff09\uff0c\u8981\u4e48\u56e0\u7834\u574f\u8fc7\u591a\u8bed\u4e49\u4fe1\u606f\u5bfc\u81f4\u4e0e\u4efb\u52a1\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u65e0\u6cd5\u4ea7\u751f\u6709\u6548\u7684\u7269\u7406\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faADVEDM\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e24\u79cd\u53d8\u4f53ADVEDM-R\uff08\u79fb\u9664\u7279\u5b9a\u5bf9\u8c61\u8bed\u4e49\uff09\u548cADVEDM-A\uff08\u6dfb\u52a0\u65b0\u5bf9\u8c61\u8bed\u4e49\uff09\uff0c\u5728\u4fdd\u7559\u56fe\u50cf\u5927\u90e8\u5206\u8bed\u4e49\u7684\u524d\u63d0\u4e0b\uff0c\u7cbe\u7ec6\u64cd\u63a7VLM\u5bf9\u5173\u952e\u5bf9\u8c61\u7684\u611f\u77e5\uff0c\u4ee5\u751f\u6210\u7b26\u5408\u4e0a\u4e0b\u6587\u4f46\u9519\u8bef\u7684\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660eADVEDM\u5728\u4e00\u822c\u573a\u666f\u548c\u5177\u8eab\u51b3\u7b56\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u4f18\u5f02\u7684\u653b\u51fb\u6548\u679c\uff0c\u80fd\u6709\u6548\u8bef\u5bfcVLM\u505a\u51fa\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u51b3\u7b56\uff0c\u8fdb\u800c\u5f71\u54cd\u4ee3\u7406\u7684\u5b9e\u9645\u884c\u4e3a\u3002", "conclusion": "ADVEDM\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u64cd\u7eb5\u5b9e\u73b0\u4e86\u5bf9VLM-based\u4ee3\u7406\u7684\u9ad8\u6548\u3001\u9690\u853d\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669\uff0c\u4e3a\u540e\u7eed\u9632\u5fa1\u673a\u5236\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2509.17321", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17321", "abs": "https://arxiv.org/abs/2509.17321", "authors": ["Pawe\u0142 Budzianowski", "Emilia Wi\u015bnios", "Gracjan G\u00f3ral", "Igor Kulakov", "Viktor Petrenko", "Krzysztof Walas"], "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation", "comment": null, "summary": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately $70\\%$ of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\n\\href{github.com/budzianowski/opengvl}{OpenGVL}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OpenGVL\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\u65f6\u95f4\u8fdb\u5ea6\u9884\u6d4b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u6765\u81ea\u52a8\u6807\u6ce8\u548c\u7b5b\u9009\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5229\u7528\u5feb\u901f\u589e\u957f\u7684\u91ce\u5916\u673a\u5668\u4eba\u6570\u636e\uff0c\u901a\u8fc7\u53ef\u9760\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u9884\u6d4b\u5b9e\u73b0\u6570\u636e\u7684\u81ea\u52a8\u5316\u6807\u6ce8\u4e0e\u6574\u7406\u3002", "method": "\u57fa\u4e8eGenerative Value Learning\uff08GVL\uff09\u65b9\u6cd5\uff0c\u6784\u5efaOpenGVL\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u65f6\u95f4\u8fdb\u5ea6\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u65f6\u95f4\u8fdb\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u4ec5\u8fbe\u5230\u540e\u8005\u7ea670%\u7684\u6027\u80fd\uff1b\u540c\u65f6\u9a8c\u8bc1\u4e86OpenGVL\u5728\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u96c6\u8d28\u91cf\u8bc4\u4f30\u4e0e\u81ea\u52a8\u7b5b\u9009\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "OpenGVL\u4e3a\u4efb\u52a1\u8fdb\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f00\u6e90\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u5bf9\u9ad8\u6027\u80fd\u5f00\u6e90\u6a21\u578b\u7684\u9700\u6c42\u3002"}}
{"id": "2509.16729", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16729", "abs": "https://arxiv.org/abs/2509.16729", "authors": ["Evgeniia Tokarchuk", "Sergey Troshin", "Vlad Niculae"], "title": "Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation", "comment": null, "summary": "Augmenting neural machine translation with external memory at decoding time,\nin the form of k-nearest neighbors machine translation ($k$-NN MT), is a\nwell-established strategy for increasing translation performance. $k$-NN MT\nretrieves a set of tokens that occurred in the most similar contexts recorded\nin a prepared data store, using hidden state representations of translation\ncontexts as vector lookup keys. One of the main disadvantages of this method is\nthe high computational cost and memory requirements. Since an exhaustive search\nis not feasible in large data stores, practitioners commonly use approximate\n$k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to\nresearch directions seeking to accelerate $k$-NN MT by reducing data store size\nor the number of lookup calls, we pursue an orthogonal direction based on the\nperformance properties of approximate $k$-NN MT lookup data structures. In\nparticular, we propose to encourage angular dispersion of the neural hidden\nrepresentations of contexts. We show that improving dispersion leads to better\nbalance in the retrieval data structures, accelerating retrieval and slightly\nimproving translations.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u795e\u7ecf\u9690\u85cf\u8868\u793a\u7684\u4e0a\u4e0b\u6587\u89d2\u5ea6\u5206\u6563\u6027\u6765\u4f18\u5316\u8fd1\u4f3ck-NN MT\u68c0\u7d22\u6570\u636e\u7ed3\u6784\uff0c\u4ece\u800c\u52a0\u901f\u68c0\u7d22\u5e76\u7565\u5fae\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "k-NN MT\u65b9\u6cd5\u5728\u89e3\u7801\u65f6\u5f15\u5165\u5916\u90e8\u8bb0\u5fc6\u4ee5\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\uff0c\u4f46\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u9700\u6c42\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8fd1\u4f3ck-NN\u68c0\u7d22\u4ecd\u6784\u6210\u6027\u80fd\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u795e\u7ecf\u9690\u85cf\u8868\u793a\u7684\u4e0a\u4e0b\u6587\u89d2\u5ea6\u5206\u6563\u6027\uff0c\u4f18\u5316\u8fd1\u4f3ck-NN MT\u68c0\u7d22\u6570\u636e\u7ed3\u6784\u7684\u5e73\u8861\u6027\uff0c\u4ece\u800c\u63d0\u5347\u68c0\u7d22\u6548\u7387\u3002", "result": "\u6539\u5584\u89d2\u5ea6\u5206\u6563\u6027\u80fd\u591f\u63d0\u9ad8\u68c0\u7d22\u6570\u636e\u7ed3\u6784\u7684\u5e73\u8861\u6027\uff0c\u52a0\u5feb\u68c0\u7d22\u901f\u5ea6\uff0c\u5e76\u8f7b\u5fae\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "\u9f13\u52b1\u4e0a\u4e0b\u6587\u8868\u793a\u7684\u89d2\u5ea6\u5206\u6563\u6027\u662f\u4e00\u79cd\u6709\u6548\u4e14\u6b63\u4ea4\u7684\u4f18\u5316\u65b9\u5411\uff0c\u53ef\u52a0\u901fk-NN MT\u68c0\u7d22\u5e76\u5e26\u6765\u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2509.16893", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16893", "abs": "https://arxiv.org/abs/2509.16893", "authors": ["Faramarz Farhangian", "Leandro A. Ensina", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "DRES: Fake news detection by dynamic representation and ensemble selection", "comment": "Accepted as oral presentation at EMNLP 2025", "summary": "The rapid spread of information via social media has made text-based fake\nnews detection critically important due to its societal impact. This paper\npresents a novel detection method called Dynamic Representation and Ensemble\nSelection (DRES) for identifying fake news based solely on text. DRES leverages\ninstance hardness measures to estimate the classification difficulty for each\nnews article across multiple textual feature representations. By dynamically\nselecting the textual representation and the most competent ensemble of\nclassifiers for each instance, DRES significantly enhances prediction accuracy.\nExtensive experiments show that DRES achieves notable improvements over\nstate-of-the-art methods, confirming the effectiveness of representation\nselection based on instance hardness and dynamic ensemble selection in boosting\nperformance. Codes and data are available at:\nhttps://github.com/FFarhangian/FakeNewsDetection_DRES", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b9e\u4f8b\u96be\u5ea6\u7684\u52a8\u6001\u8868\u793a\u548c\u96c6\u6210\u9009\u62e9\u65b9\u6cd5\uff08DRES\uff09\u7528\u4e8e\u7eaf\u6587\u672c\u5047\u65b0\u95fb\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u5047\u65b0\u95fb\u4f20\u64ad\u8fc5\u901f\uff0c\u5bf9\u793e\u4f1a\u5f71\u54cd\u91cd\u5927\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u7eaf\u6587\u672c\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5b9e\u4f8b\u96be\u5ea6\u5ea6\u91cf\u8bc4\u4f30\u591a\u4e2a\u6587\u672c\u7279\u5f81\u8868\u793a\u4e0b\u6bcf\u6761\u65b0\u95fb\u7684\u5206\u7c7b\u96be\u5ea6\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u6700\u4f18\u8868\u793a\u548c\u6700\u5408\u9002\u7684\u5206\u7c7b\u5668\u96c6\u6210\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDRES\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u52a8\u6001\u8868\u793a\u9009\u62e9\u4e0e\u96c6\u6210\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "DRES\u901a\u8fc7\u7ed3\u5408\u5b9e\u4f8b\u96be\u5ea6\u611f\u77e5\u7684\u8868\u793a\u9009\u62e9\u4e0e\u52a8\u6001\u96c6\u6210\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.17425", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17425", "abs": "https://arxiv.org/abs/2509.17425", "authors": ["Zhenliang Zhang", "Yuxi Wang", "Hongzhao Xie", "Shiyun Zhao", "Mingyuan Liu", "Yujie Lu", "Xinyi He", "Zhenku Cheng", "Yujia Peng"], "title": "Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments", "comment": null, "summary": "A key feature differentiating artificial general intelligence (AGI) from\ntraditional AI is that AGI can perform composite tasks that require a wide\nrange of capabilities. Although embodied agents powered by multimodal large\nlanguage models (MLLMs) offer rich perceptual and interactive capabilities, it\nremains largely unexplored whether they can solve composite tasks. In the\ncurrent work, we designed a set of composite tasks inspired by common daily\nactivities observed in early childhood development. Within a dynamic and\nsimulated home environment, these tasks span three core domains: object\nunderstanding, spatial intelligence, and social activity. We evaluated 17\nleading proprietary and open-source MLLMs on these tasks. The results\nconsistently showed poor performance across all three domains, indicating a\nsubstantial gap between current capabilities and general intelligence\nrequirements. Together, our tasks offer a preliminary framework for evaluating\nthe general capabilities of embodied agents, marking an early but significant\nstep toward the development of embodied MLLMs and their real-world deployment.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u7ec4\u53d7\u513f\u7ae5\u65e9\u671f\u53d1\u5c55\u542f\u53d1\u7684\u590d\u5408\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u667a\u80fd\u4f53\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u7269\u4f53\u7406\u89e3\u3001\u7a7a\u95f4\u667a\u80fd\u548c\u793e\u4f1a\u6d3b\u52a8\u4e09\u65b9\u9762\u5747\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6280\u672f\u4e0e\u901a\u7528\u667a\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7d22\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5177\u8eab\u667a\u80fd\u4f53\u662f\u5426\u5177\u5907\u6267\u884c\u9700\u8981\u591a\u79cd\u80fd\u529b\u534f\u540c\u7684\u590d\u5408\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63a8\u52a8\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u52a8\u6001\u6a21\u62df\u7684\u5bb6\u5ead\u73af\u5883\uff0c\u8bbe\u8ba1\u6db5\u76d6\u7269\u4f53\u7406\u89e3\u3001\u7a7a\u95f4\u667a\u80fd\u548c\u793e\u4f1a\u6d3b\u52a8\u4e09\u5927\u9886\u57df\u7684\u590d\u5408\u4efb\u52a1\uff0c\u5e76\u5bf917\u79cd\u4e3b\u6d41\u95ed\u6e90\u548c\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u6240\u6709\u88ab\u6d4b\u6a21\u578b\u5728\u4e09\u4e2a\u4efb\u52a1\u9886\u57df\u4e2d\u5747\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u6027\u80fd\uff0c\u8868\u660e\u5f53\u524d\u5177\u8eab\u667a\u80fd\u4f53\u8ddd\u79bb\u5b9e\u73b0\u901a\u7528\u667a\u80fd\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4efb\u52a1\u6846\u67b6\u4e3a\u8bc4\u4f30\u5177\u8eab\u667a\u80fd\u4f53\u7684\u901a\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u521d\u6b65\u57fa\u51c6\uff0c\u662f\u8fc8\u5411\u5177\u8eab\u591a\u6a21\u6001\u5927\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.16654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16654", "abs": "https://arxiv.org/abs/2509.16654", "authors": ["Xin Chen", "Jia He", "Maozheng Li", "Dongliang Xu", "Tianyu Wang", "Yixiao Chen", "Zhixin Lin", "Yue Yao"], "title": "Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?", "comment": "5 pages, 5 figures", "summary": "Vision-Language Models (VLMs) have recently shown remarkable progress in\nmultimodal reasoning, yet their applications in autonomous driving remain\nlimited. In particular, the ability to understand road topology, a key\nrequirement for safe navigation, has received relatively little attention.\nWhile some recent works have begun to explore VLMs in driving contexts, their\nperformance on topology reasoning is far from satisfactory. In this work, we\nsystematically evaluate VLMs' capabilities in road topology understanding.\nSpecifically, multi-view images are projected into unified ground-plane\ncoordinate system and fused into bird's-eye-view (BEV) lanes. Based on these\nBEV lanes, we formulate four topology-related diagnostic VQA tasks, which\ntogether capture essential components of spatial topology reasoning. Through\nextensive evaluation, we find that while frontier closed-source models (e.g.,\nGPT-4o) achieve relatively high accuracy in some tasks, they still fail in some\ntemporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in\nvector, a two-class classification problem). Furthermore, we find open-source\nVLMs, even at 30B scale, struggle significantly. These results indicate that\nspatial reasoning remains a fundamental bottleneck for current VLMs. We also\nfind that the model's capability is positively correlated with model size,\nlength of reasoning tokens and shots provided as examples, showing direction\nfor future research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u9053\u8def\u62d3\u6251\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u57fa\u4e8e\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u9e1f\u77b0\u56fe\u8f66\u9053\uff0c\u5e76\u8bbe\u8ba1\u56db\u9879\u62d3\u6251\u76f8\u5173\u7684\u8bca\u65ad\u6027\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u524d\u6cbf\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u65f6\u5e8f\u63a8\u7406\u95ee\u9898\u4e0a\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\uff1b\u5f00\u6e90VLM\u8868\u73b0\u66f4\u5dee\uff0c\u663e\u793a\u51fa\u7a7a\u95f4\u63a8\u7406\u4ecd\u662f\u5f53\u524dVLM\u7684\u74f6\u9888\u3002\u6a21\u578b\u6027\u80fd\u4e0e\u6a21\u578b\u89c4\u6a21\u3001\u63a8\u7406token\u957f\u5ea6\u548c\u793a\u4f8b\u6570\u91cf\u5448\u6b63\u76f8\u5173\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u9053\u8def\u62d3\u6251\u7406\u89e3\u80fd\u529b\u4ecd\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u7c7b\u5173\u952e\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u7406\u60f3\uff0c\u4e9f\u9700\u7cfb\u7edf\u8bc4\u4f30\u4e0e\u6539\u8fdb\u3002", "method": "\u5c06\u591a\u89c6\u89d2\u56fe\u50cf\u6295\u5f71\u5230\u7edf\u4e00\u7684\u5730\u9762\u5750\u6807\u7cfb\u5e76\u878d\u5408\u4e3a\u9e1f\u77b0\u56fe\uff08BEV\uff09\u8f66\u9053\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u56db\u4e2a\u62d3\u6251\u76f8\u5173\u7684\u8bca\u65ad\u6027\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30VLM\u7684\u7a7a\u95f4\u62d3\u6251\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0GPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u8f83\u9ad8\uff0c\u4f46\u5728\u65f6\u5e8f\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u5982\u5728\u4e8c\u5206\u7c7b\u5411\u91cf\u4efb\u52a1\u4e2d\u4ec5\u5f9767.8%\uff09\uff1b\u5f00\u6e90VLM\uff08\u5373\u4f7f\u8fbe30B\u89c4\u6a21\uff09\u6574\u4f53\u8868\u73b0\u8f83\u5dee\uff1b\u6a21\u578b\u6027\u80fd\u4e0e\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406token\u957f\u5ea6\u548c\u793a\u4f8b\u6570\u91cf\u6b63\u76f8\u5173\u3002", "conclusion": "\u7a7a\u95f4\u63a8\u7406\u4ecd\u662f\u5f53\u524dVLM\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u6839\u672c\u74f6\u9888\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u5176\u62d3\u6251\u7406\u89e3\u4e0e\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u914d\u7f6e\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2509.17340", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17340", "abs": "https://arxiv.org/abs/2509.17340", "authors": ["Xin Chen", "Rui Huang", "Longbin Tang", "Lin Zhao"], "title": "AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation", "comment": null, "summary": "Agile mapless navigation in cluttered 3D environments poses significant\nchallenges for autonomous drones. Conventional mapping-planning-control\npipelines incur high computational cost and propagate estimation errors. We\npresent AERO-MPPI, a fully GPU-accelerated framework that unifies perception\nand planning through an anchor-guided ensemble of Model Predictive Path\nIntegral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR\npoint-cloud representation that rapidly extracts spatially distributed\n\"anchors\" as look-ahead intermediate endpoints, from which we construct\npolynomial trajectory guides to explore distinct homotopy path classes. At each\nplanning step, we run multiple MPPI instances in parallel and evaluate them\nwith a two-stage multi-objective cost that balances collision avoidance and\ngoal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI\nachieves real-time onboard operation and mitigates the local-minima failures of\nsingle-MPPI approaches. Extensive simulations in forests, verticals, and\ninclines demonstrate sustained reliable flight above 7 m/s, with success rates\nabove 80% and smoother trajectories compared to state-of-the-art baselines.\nReal-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX\n16G confirm that AERO-MPPI runs in real time onboard and consistently achieves\nsafe, agile, and robust flight in complex cluttered environments. The code will\nbe open-sourced upon acceptance of the paper.", "AI": {"tldr": "\u63d0\u51faAERO-MPPI\uff0c\u4e00\u79cd\u5168GPU\u52a0\u901f\u7684\u611f\u77e5-\u89c4\u5212\u4e00\u4f53\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u65e0\u5730\u56fe\u654f\u6377\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u5efa\u56fe-\u89c4\u5212-\u63a7\u5236\u6d41\u7a0b\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u8bef\u5dee\u7d2f\u79ef\uff0c\u96be\u4ee5\u5b9e\u73b0\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u3001\u9c81\u68d2\u65e0\u5730\u56fe\u5bfc\u822a\u3002", "method": "\u8bbe\u8ba1\u591a\u5206\u8fa8\u7387LiDAR\u70b9\u4e91\u8868\u793a\u4ee5\u63d0\u53d6\u7a7a\u95f4\u5206\u5e03\u7684\u201c\u951a\u70b9\u201d\u4f5c\u4e3a\u524d\u77bb\u4e2d\u95f4\u76ee\u6807\uff0c\u751f\u6210\u591a\u9879\u5f0f\u8f68\u8ff9\u5f15\u5bfc\u63a2\u7d22\u4e0d\u540c\u540c\u4f26\u7c7b\u8def\u5f84\uff1b\u91c7\u7528\u591a\u4e2a\u5e76\u884cMPPI\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e24\u7ea7\u591a\u76ee\u6807\u4ee3\u4ef7\u51fd\u6570\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5b8c\u5168\u57fa\u4e8eNVIDIA Warp GPU\u5185\u6838\u5b9e\u73b0\u3002", "result": "\u5728\u68ee\u6797\u3001\u5782\u76f4\u548c\u503e\u659c\u73af\u5883\u4e2d\u4eff\u771f\u8868\u660e\uff0c\u7cfb\u7edf\u53ef\u7a33\u5b9a\u4ee5\u8d85\u8fc77 m/s\u7684\u901f\u5ea6\u98de\u884c\uff0c\u6210\u529f\u7387\u8d8580%\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\uff1b\u771f\u5b9e\u56db\u65cb\u7ffc\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728NVIDIA Jetson Orin NX\u4e0a\u7684\u5b9e\u65f6\u6027\u4e0e\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u3001\u654f\u6377\u3001\u9c81\u68d2\u98de\u884c\u80fd\u529b\u3002", "conclusion": "AERO-MPPI\u901a\u8fc7\u951a\u70b9\u5f15\u5bfc\u7684\u591aMPPI\u5e76\u884c\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5355MPPI\u6613\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u7684\u65e0\u5730\u56fe\u5bfc\u822a\u3002"}}
{"id": "2509.16765", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16765", "abs": "https://arxiv.org/abs/2509.16765", "authors": ["Fagun Patel", "Duc Q. Nguyen", "Sang T. Truong", "Jody Vaynshtok", "Sanmi Koyejo", "Nick Haber"], "title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology", "comment": "EMNLP 2025 Oral Presentation", "summary": "According to the U.S. National Institutes of Health, more than 3.4 million\nchildren experience speech disorders that require clinical intervention. The\nnumber of speech-language pathologists (SLPs) is roughly 20 times fewer than\nthe number of affected children, highlighting a significant gap in children's\ncare and a pressing need for technological support that improves the\nproductivity of SLPs. State-of-the-art multimodal language models (MLMs) show\npromise for supporting SLPs, but their use remains underexplored largely due to\na limited understanding of their performance in high-stakes clinical settings.\nTo address this gap, we collaborate with domain experts to develop a taxonomy\nof real-world use cases of MLMs in speech-language pathologies. Building on\nthis taxonomy, we introduce the first comprehensive benchmark for evaluating\nMLM across five core use cases, each containing 1,000 manually annotated data\npoints. This benchmark includes robustness and sensitivity tests under various\nsettings, including background noise, speaker gender, and accent. Our\nevaluation of 15 state-of-the-art MLMs reveals that no single model\nconsistently outperforms others across all tasks. Notably, we find systematic\ndisparities, with models performing better on male speakers, and observe that\nchain-of-thought prompting can degrade performance on classification tasks with\nlarge label spaces and narrow decision boundaries. Furthermore, we study\nfine-tuning MLMs on domain-specific data, achieving improvements of over 30%\ncompared to base models. These findings highlight both the potential and\nlimitations of current MLMs for speech-language pathology applications,\nunderscoring the need for further research and targeted development.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u8a00\u8bed\u8bed\u8a00\u75c5\u7406\u5b66\u4e2d\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLM\uff09\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u6db5\u76d6\u4e94\u4e2a\u6838\u5fc3\u7528\u4f8b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e8615\u79cd\u6700\u5148\u8fdbMLM\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u6027\u80fd\u5dee\u5f02\u548c\u5c40\u9650\u6027\uff0c\u4f46\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u6570\u636e\u5fae\u8c03\u53ef\u63d0\u5347\u8d85\u8fc730%\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u8a00\u8bed\u8bed\u8a00\u6cbb\u7597\u5e08\u6570\u91cf\u8fdc\u5c11\u4e8e\u9700\u8981\u5e72\u9884\u7684\u513f\u7ae5\uff0c\u5b58\u5728\u5de8\u5927\u7684\u4e34\u5e8a\u670d\u52a1\u7f3a\u53e3\uff0c\u4e9f\u9700\u6280\u672f\u624b\u6bb5\u63d0\u5347\u6cbb\u7597\u5e08\u5de5\u4f5c\u6548\u7387\uff0c\u800c\u5f53\u524d\u5bf9MLM\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u7406\u89e3\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u6784\u5efaMLM\u5728\u8a00\u8bed\u8bed\u8a00\u75c5\u7406\u4e2d\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u57fa\u4e8e\u8be5\u4f53\u7cfb\u5efa\u7acb\u5305\u542b5000\u4e2a\u624b\u52a8\u6807\u6ce8\u6570\u636e\u70b9\u7684\u7efc\u5408\u8bc4\u6d4b\u57fa\u51c6\uff0c\u6db5\u76d6\u566a\u58f0\u3001\u6027\u522b\u3001\u53e3\u97f3\u7b49\u9c81\u68d2\u6027\u548c\u654f\u611f\u6027\u6d4b\u8bd5\uff1b\u8bc4\u4f3015\u79cd\u4e3b\u6d41MLM\uff0c\u5e76\u7814\u7a76\u9886\u57df\u6570\u636e\u5fae\u8c03\u7684\u6548\u679c\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6ca1\u6709\u5355\u4e00MLM\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u6700\u4f73\uff1b\u6a21\u578b\u666e\u904d\u5b58\u5728\u5bf9\u7537\u6027\u8bf4\u8bdd\u8005\u6027\u80fd\u66f4\u597d\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u6807\u7b7e\u7a7a\u95f4\u5927\u3001\u51b3\u7b56\u8fb9\u754c\u7a84\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f1a\u964d\u4f4e\u6027\u80fd\uff1b\u901a\u8fc7\u9886\u57df\u6570\u636e\u5fae\u8c03\u53ef\u4f7f\u6027\u80fd\u63d0\u5347\u8d8530%\u3002", "conclusion": "\u5f53\u524dMLM\u5728\u8a00\u8bed\u8bed\u8a00\u75c5\u7406\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u4f46\u4e5f\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u9488\u5bf9\u6027\u7814\u7a76\u548c\u5f00\u53d1\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2509.16898", "categories": ["cs.LG", "cs.CC", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.16898", "abs": "https://arxiv.org/abs/2509.16898", "authors": ["Jingming Yan", "Yiyuan Luo", "Vaggos Chatziafratis", "Ioannis Panageas", "Parnian Shahkar", "Stelios Stavroulakis"], "title": "The Complexity of Finding Local Optima in Contrastive Learning", "comment": "To appear as a conference paper in NeurIPS 2025", "summary": "Contrastive learning is a powerful technique for discovering meaningful data\nrepresentations by optimizing objectives based on $\\textit{contrastive\ninformation}$, often given as a set of weighted triplets $\\{(x_i, y_i^+,\nz_{i}^-)\\}_{i = 1}^m$ indicating that an \"anchor\" $x_i$ is more similar to a\n\"positive\" example $y_i$ than to a \"negative\" example $z_i$. The goal is to\nfind representations (e.g., embeddings in $\\mathbb{R}^d$ or a tree metric)\nwhere anchors are placed closer to positive than to negative examples. While\nfinding $\\textit{global}$ optima of contrastive objectives is\n$\\mathsf{NP}$-hard, the complexity of finding $\\textit{local}$ optima --\nrepresentations that do not improve by local search algorithms such as\ngradient-based methods -- remains open. Our work settles the complexity of\nfinding local optima in various contrastive learning problems by proving\n$\\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfied\ntriplets) and $\\mathsf{CLS}$-hardness in continuous settings (e.g., minimize\nTriplet Loss), where $\\mathsf{PLS}$ (Polynomial Local Search) and\n$\\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classes\ncapturing local search dynamics in discrete and continuous optimization,\nrespectively. Our results imply that no polynomial time algorithm (local search\nor otherwise) can find a local optimum for various contrastive learning\nproblems, unless $\\mathsf{PLS}\\subseteq\\mathsf{P}$ (or $\\mathsf{CLS}\\subseteq\n\\mathsf{P}$ for continuous problems). Even in the unlikely scenario that\n$\\mathsf{PLS}\\subseteq\\mathsf{P}$ (or $\\mathsf{CLS}\\subseteq \\mathsf{P}$), our\nreductions imply that there exist instances where local search algorithms need\nexponential time to reach a local optimum, even for $d=1$ (embeddings on a\nline).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5bfb\u627e\u5c40\u90e8\u6700\u4f18\u89e3\u7684\u590d\u6742\u6027\uff0c\u8bc1\u660e\u4e86\u5728\u79bb\u6563\u548c\u8fde\u7eed\u8bbe\u7f6e\u4e0b\u5206\u522b\u4e3aPLS-hard\u548cCLS-hard\uff0c\u8868\u660e\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u65e0\u6cd5\u627e\u5230\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u4e14\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u9700\u8981\u6307\u6570\u65f6\u95f4\u624d\u80fd\u6536\u655b\u3002", "motivation": "\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5168\u5c40\u6700\u4f18\u89e3\u7684\u6c42\u89e3\u5df2\u88ab\u8bc1\u660e\u662fNP\u96be\u7684\uff0c\u4f46\u5c40\u90e8\u6700\u4f18\u89e3\u7684\u590d\u6742\u6027\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5206\u6790\u4e0d\u540c\u5bf9\u6bd4\u5b66\u4e60\u95ee\u9898\u4e2d\u5c40\u90e8\u641c\u7d22\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u5bf9\u6bd4\u5b66\u4e60\u95ee\u9898\u5f52\u7ea6\u5230\u5df2\u77e5\u7684\u590d\u6742\u6027\u7c7bPLS\uff08\u591a\u9879\u5f0f\u5c40\u90e8\u641c\u7d22\uff09\u548cCLS\uff08\u8fde\u7eed\u5c40\u90e8\u641c\u7d22\uff09\uff0c\u5728\u79bb\u6563\u8bbe\u7f6e\u4e0b\u6700\u5927\u5316\u6ee1\u8db3\u7684\u4e09\u5143\u7ec4\uff0c\u5728\u8fde\u7eed\u8bbe\u7f6e\u4e0b\u6700\u5c0f\u5316\u4e09\u5143\u7ec4\u635f\u5931\uff0c\u4ece\u800c\u8bc1\u660e\u5176\u5c40\u90e8\u6700\u4f18\u89e3\u7684\u96be\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u79bb\u6563\u548c\u8fde\u7eed\u5bf9\u6bd4\u5b66\u4e60\u95ee\u9898\u4e2d\u5bfb\u627e\u5c40\u90e8\u6700\u4f18\u89e3\u5206\u522b\u662fPLS-hard\u548cCLS-hard\uff1b\u8fd9\u610f\u5473\u7740\u9664\u975ePLS\u2286P\u6216CLS\u2286P\uff0c\u5426\u5219\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u53ef\u627e\u5230\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u5e76\u4e14\u5b58\u5728\u9700\u8981\u6307\u6570\u65f6\u95f4\u624d\u80fd\u6536\u655b\u7684\u5b9e\u4f8b\uff0c\u5373\u4f7f\u5728\u4e00\u7ef4\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5bfb\u627e\u5c40\u90e8\u6700\u4f18\u89e3\u5177\u6709\u9ad8\u5ea6\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5f53\u524d\u5c40\u90e8\u641c\u7d22\u65b9\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u53ef\u80fd\u6548\u7387\u6781\u4f4e\uff0c\u63d0\u793a\u9700\u91cd\u65b0\u601d\u8003\u4f18\u5316\u7b56\u7565\u6216\u6a21\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2509.17439", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17439", "abs": "https://arxiv.org/abs/2509.17439", "authors": ["Yangxuan Zhou", "Sha Zhao", "Jiquan Wang", "Haiteng Jiang", "Shijian Li", "Tao Li", "Gang Pan"], "title": "SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding", "comment": "21 pages, 13 figures", "summary": "Human brain achieves dynamic stability-plasticity balance through synaptic\nhomeostasis. Inspired by this biological principle, we propose SPICED: a\nneuromorphic framework that integrates the synaptic homeostasis mechanism for\nunsupervised continual EEG decoding, particularly addressing practical\nscenarios where new individuals with inter-individual variability emerge\ncontinually. SPICED comprises a novel synaptic network that enables dynamic\nexpansion during continual adaptation through three bio-inspired neural\nmechanisms: (1) critical memory reactivation; (2) synaptic consolidation and\n(3) synaptic renormalization. The interplay within synaptic homeostasis\ndynamically strengthens task-discriminative memory traces and weakens\ndetrimental memories. By integrating these mechanisms with continual learning\nsystem, SPICED preferentially replays task-discriminative memory traces that\nexhibit strong associations with newly emerging individuals, thereby achieving\nrobust adaptations. Meanwhile, SPICED effectively mitigates catastrophic\nforgetting by suppressing the replay prioritization of detrimental memories\nduring long-term continual learning. Validated on three EEG datasets, SPICED\nshow its effectiveness.", "AI": {"tldr": "SPICED\u662f\u4e00\u4e2a\u53d7\u7a81\u89e6\u7a33\u6001\u542f\u53d1\u7684\u795e\u7ecf\u5f62\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u6301\u7eed\u89e3\u7801\u8111\u7535\u56fe\u4fe1\u53f7\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u4e2a\u4f53\u95f4\u5dee\u5f02\u6301\u7eed\u51fa\u73b0\u7684\u5b9e\u9645\u573a\u666f\u3002", "motivation": "\u4eba\u7c7b\u5927\u8111\u901a\u8fc7\u7a81\u89e6\u7a33\u6001\u5b9e\u73b0\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u7684\u5e73\u8861\uff0c\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u9762\u5bf9\u65b0\u4e2a\u4f53\u8fde\u7eed\u51fa\u73b0\u65f6\u96be\u4ee5\u517c\u987e\u9002\u5e94\u6027\u4e0e\u8bb0\u5fc6\u4fdd\u6301\u3002", "method": "\u63d0\u51faSPICED\u6846\u67b6\uff0c\u7ed3\u5408\u4e09\u79cd\u751f\u7269\u542f\u53d1\u673a\u5236\uff1a\u5173\u952e\u8bb0\u5fc6\u91cd\u6fc0\u6d3b\u3001\u7a81\u89e6\u5de9\u56fa\u548c\u7a81\u89e6\u91cd\u5f52\u4e00\u5316\uff0c\u52a8\u6001\u6269\u5c55\u7f51\u7edc\u5e76\u8c03\u8282\u8bb0\u5fc6\u56de\u653e\u4f18\u5148\u7ea7\u3002", "result": "\u5728\u4e09\u4e2a\u8111\u7535\u56fe\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86SPICED\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5b9e\u73b0\u9c81\u68d2\u9002\u5e94\u5e76\u663e\u8457\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "SPICED\u901a\u8fc7\u6a21\u62df\u7a81\u89e6\u7a33\u6001\u673a\u5236\uff0c\u5728\u65e0\u9700\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u65b0\u4e2a\u4f53\u7684\u6301\u7eed\u89e3\u7801\uff0c\u4e3a\u5b9e\u9645\u8111\u673a\u63a5\u53e3\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16673", "abs": "https://arxiv.org/abs/2509.16673", "authors": ["Sinuo Wang", "Yutong Xie", "Yuyuan Liu", "Qi Wu"], "title": "MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness", "comment": null, "summary": "Vision-Language Pre-training (VLP) is drawing increasing interest for its\nability to minimize manual annotation requirements while enhancing semantic\nunderstanding in downstream tasks. However, its reliance on image-text datasets\nposes challenges due to privacy concerns and the high cost of obtaining paired\nannotations. Data augmentation emerges as a viable strategy to address this\nissue, yet existing methods often fall short of capturing the subtle and\ncomplex variations in medical data due to limited diversity. To this end, we\npropose MedCutMix, a novel multi-modal disease-centric data augmentation\nmethod. MedCutMix performs diagnostic sentence CutMix within medical reports\nand establishes the cross-attention between the diagnostic sentence and medical\nimage to guide attentive manifold mix within the imaging modality. Our approach\nsurpasses previous methods across four downstream radiology diagnosis datasets,\nhighlighting its effectiveness in enhancing performance and generalizability in\nradiology VLP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u75be\u75c5\u4e2d\u5fc3\u6570\u636e\u589e\u5f3a\u65b9\u6cd5MedCutMix\uff0c\u901a\u8fc7\u5728\u533b\u5b66\u62a5\u544a\u4e2d\u8fdb\u884c\u8bca\u65ad\u53e5\u5b50\u7684CutMix\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf-\u6587\u672c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5f15\u5bfc\u56fe\u50cf\u6d41\u5f62\u6df7\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5728\u533b\u5b66\u6570\u636e\u4e0a\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u590d\u6742\u7684\u53d8\u5f02\uff0c\u4e14\u533b\u5b66\u56fe\u50cf-\u6587\u672c\u914d\u5bf9\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3001\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faMedCutMix\uff0c\u7ed3\u5408\u8bca\u65ad\u53e5\u5b50\u7684\u6587\u672cCutMix\u4e0e\u57fa\u4e8e\u8de8\u6ce8\u610f\u529b\u673a\u5236\u7684\u56fe\u50cf\u533a\u57df\u6df7\u5408\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u3002\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5b9a\u4f4d\u5173\u952e\u56fe\u50cf\u533a\u57df\uff0c\u6307\u5bfc\u56fe\u50cf\u6d41\u5f62\u4e0a\u7684\u6df7\u5408\u64cd\u4f5c\u3002", "result": "\u5728\u56db\u4e2a\u4e0b\u6e38\u653e\u5c04\u5b66\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MedCutMix\u901a\u8fc7\u75be\u75c5\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u533b\u5b66VLP\u4e2d\u6570\u636e\u7a00\u7f3a\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u4e3a\u653e\u5c04\u5b66\u9886\u57df\u7684\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2509.17350", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17350", "abs": "https://arxiv.org/abs/2509.17350", "authors": ["Haoran Zhou", "Yangwei You", "Shuaijun Wang"], "title": "DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception", "comment": "8 pages, 7 figures", "summary": "Dynamic in air handover is a fundamental challenge for dual-arm robots,\nrequiring accurate perception, precise coordination, and natural motion. Prior\nmethods often rely on dynamics models, strong priors, or depth sensing,\nlimiting generalization and naturalness. We present DyDexHandover, a novel\nframework that employs multi-agent reinforcement learning to train an end to\nend RGB based policy for bimanual object throwing and catching. To achieve more\nhuman-like behavior, the throwing policy is guided by a human policy\nregularization scheme, encouraging fluid and natural motion, and enhancing the\ngeneralization capability of the policy. A dual arm simulation environment was\nbuilt in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly\n99 percent success on training objects and 75 percent on unseen objects, while\ngenerating human-like throwing and catching behaviors. To our knowledge, it is\nthe first method to realize dual-arm in-air handover using only raw RGB\nperception.", "AI": {"tldr": "\u63d0\u51faDyDexHandover\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4ec5\u57fa\u4e8e\u539f\u59cbRGB\u611f\u77e5\u7684\u53cc\u81c2\u7a7a\u4e2d\u4ea4\u63a5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u52a8\u529b\u5b66\u6a21\u578b\u3001\u5f3a\u5148\u9a8c\u6216\u6df1\u5ea6\u611f\u77e5\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u6027\u548c\u81ea\u7136\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7aef\u5230\u7aef\u7684RGB\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u4eba\u7c7b\u7b56\u7565\u6b63\u5219\u5316\u5f15\u5bfc\u6295\u63b7\u52a8\u4f5c\u3002", "result": "\u5728\u8bad\u7ec3\u7269\u4f53\u4e0a\u8fbe\u523099%\u6210\u529f\u7387\uff0c\u672a\u89c1\u7269\u4f53\u4e0a\u8fbe75%\uff0c\u751f\u6210\u7c7b\u4eba\u884c\u4e3a\u3002", "conclusion": "DyDexHandover\u662f\u9996\u4e2a\u4ec5\u7528RGB\u5b9e\u73b0\u53cc\u81c2\u7a7a\u4e2d\u4ea4\u63a5\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u81ea\u7136\u8fd0\u52a8\u8868\u73b0\u3002"}}
{"id": "2509.16781", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16781", "abs": "https://arxiv.org/abs/2509.16781", "authors": ["Andrei-Marius Avram", "Ema-Ioana B\u0103nescu", "Anda-Teodora Robea", "Dumitru-Clementin Cercel", "Mihaela-Claudia Cercel"], "title": "MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language", "comment": "Accepted at EMNLP Findings 2025", "summary": "This paper introduces MoRoVoc, the largest dataset for analyzing the regional\nvariation of spoken Romanian. It has more than 93 hours of audio and 88,192\naudio samples, balanced between the Romanian language spoken in Romania and the\nRepublic of Moldova. We further propose a multi-target adversarial training\nframework for speech models that incorporates demographic attributes (i.e., age\nand gender of the speakers) as adversarial targets, making models\ndiscriminative for primary tasks while remaining invariant to secondary\nattributes. The adversarial coefficients are dynamically adjusted via\nmeta-learning to optimize performance. Our approach yields notable gains:\nWav2Vec2-Base achieves 78.21% accuracy for the variation identification of\nspoken Romanian using gender as an adversarial target, while Wav2Vec2-Large\nreaches 93.08% accuracy for gender classification when employing both dialect\nand age as adversarial objectives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MoRoVoc\uff0c\u8fd9\u662f\u7528\u4e8e\u5206\u6790\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u53e3\u8bed\u533a\u57df\u5dee\u5f02\u7684\u6700\u5927\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u7684\u591a\u76ee\u6807\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u5bf9\u6297\u7cfb\u6570\uff0c\u5728\u8bed\u97f3\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u4e3b\u4efb\u52a1\u7684\u5224\u522b\u6027\u548c\u5bf9\u6b21\u8981\u5c5e\u6027\u7684\u4e0d\u53d8\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5206\u6790\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u5728\u4e0d\u540c\u5730\u533a\u7684\u53e3\u97f3\u53d8\u5f02\uff0c\u5e76\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u9762\u5bf9\u8bf4\u8bdd\u4eba\u5e74\u9f84\u3001\u6027\u522b\u7b49\u65e0\u5173\u53d8\u91cf\u65f6\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b93\u5c0f\u65f6\u97f3\u9891\u7684\u5927\u89c4\u6a21\u5e73\u8861\u6570\u636e\u96c6MoRoVoc\uff0c\u6db5\u76d6\u7f57\u9a6c\u5c3c\u4e9a\u548c\u6469\u5c14\u591a\u74e6\u7684\u53e3\u8bed\u53d8\u4f53\uff1b\u63d0\u51fa\u4e00\u79cd\u591a\u76ee\u6807\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u5e74\u9f84\u548c\u6027\u522b\u4f5c\u4e3a\u5bf9\u6297\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u5143\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u5bf9\u6297\u7cfb\u6570\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u5728\u4e3b\u8981\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4f7f\u7528\u6027\u522b\u4f5c\u4e3a\u5bf9\u6297\u76ee\u6807\u65f6\uff0cWav2Vec2-Base\u5728\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e0a\u8fbe\u523078.21%\u7684\u51c6\u786e\u7387\uff1b\u5f53\u4f7f\u7528\u65b9\u8a00\u548c\u5e74\u9f84\u4f5c\u4e3a\u5bf9\u6297\u76ee\u6807\u65f6\uff0cWav2Vec2-Large\u5728\u6027\u522b\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u523093.08%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u76ee\u6807\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8bed\u97f3\u6a21\u578b\u5728\u4e3b\u8981\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u65e0\u5173\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16902", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16902", "abs": "https://arxiv.org/abs/2509.16902", "authors": ["Letian Zhang", "Bo Chen", "Jieming Bian", "Lei Wang", "Jie Xu"], "title": "FedEL: Federated Elastic Learning for Heterogeneous Devices", "comment": null, "summary": "Federated learning (FL) enables distributed devices to collaboratively train\nmachine learning models while maintaining data privacy. However, the\nheterogeneous hardware capabilities of devices often result in significant\ntraining delays, as straggler clients with limited resources prolong the\naggregation process. Existing solutions such as client selection, asynchronous\nFL, and partial training partially address these challenges but encounter\nissues such as reduced accuracy, stale updates, and compromised model\nperformance due to inconsistent training contributions. To overcome these\nlimitations, we propose FedEL, a federated elastic learning framework that\nenhances training efficiency while maintaining model accuracy. FedEL introduces\na novel window-based training process, sliding the window to locate the\ntraining part of the model and dynamically selecting important tensors for\ntraining within a coordinated runtime budget. This approach ensures progressive\nand balanced training across all clients, including stragglers. Additionally,\nFedEL employs a tensor importance adjustment module, harmonizing local and\nglobal tensor importance to mitigate biases caused by data heterogeneity. The\nexperiment results show that FedEL achieves up to 3.87x improvement in\ntime-to-accuracy compared to baselines while maintaining or exceeding final\ntest accuracy.", "AI": {"tldr": "FedEL\u662f\u4e00\u79cd\u8054\u90a6\u5f39\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7a97\u53e3\u7684\u8bad\u7ec3\u548c\u52a8\u6001\u5f20\u91cf\u9009\u62e9\uff0c\u5728\u4fdd\u8bc1\u6a21\u578b\u51c6\u786e\u7387\u7684\u540c\u65f6\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u8bbe\u5907\u786c\u4ef6\u5f02\u6784\u6027\u65f6\u5b58\u5728\u8bad\u7ec3\u5ef6\u8fdf\u3001\u7cbe\u5ea6\u4e0b\u964d\u548c\u6a21\u578b\u6027\u80fd\u9000\u5316\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faFedEL\u6846\u67b6\uff0c\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u5b9a\u4f4d\u6a21\u578b\u8bad\u7ec3\u90e8\u5206\uff0c\u5e76\u5728\u9884\u7b97\u5185\u52a8\u6001\u9009\u62e9\u91cd\u8981\u5f20\u91cf\u8fdb\u884c\u8bad\u7ec3\uff1b\u5f15\u5165\u5f20\u91cf\u91cd\u8981\u6027\u8c03\u6574\u6a21\u5757\uff0c\u534f\u8c03\u5c40\u90e8\u4e0e\u5168\u5c40\u5f20\u91cf\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cFedEL\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u65f6\u95f4-\u7cbe\u5ea6\u6bd4\u4e0a\u6700\u9ad8\u63d0\u53473.87\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6700\u7ec8\u6d4b\u8bd5\u7cbe\u5ea6\u3002", "conclusion": "FedEL\u6709\u6548\u7f13\u89e3\u4e86\u5f02\u6784\u8bbe\u5907\u5e26\u6765\u7684\u8bad\u7ec3\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8054\u90a6\u5b66\u4e60\u3002"}}
{"id": "2509.17460", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17460", "abs": "https://arxiv.org/abs/2509.17460", "authors": ["Jianlong Chang", "Haixin Wang", "Zhiyuan Dang", "Li Huang", "Zhiyu Wang", "Ruoqi Cao", "Shihao Piao", "Dongzhe Li", "Dianyu Gao", "Dongsheng Wang", "Yin Li", "Jinan Sun", "Lu Fang", "Zhouchen Lin"], "title": "AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks", "comment": "65 pages, 28 figures, paper under review", "summary": "The pursuit of artificial general intelligence continuously demands\ngeneralization in one model across myriad tasks, even those not seen before.\nHowever, current AI models are isolated from each other for being limited to\nspecific tasks, now first defined as Intelligence Islands. To unify\nIntelligence Islands into one, we propose Pangaea, the first AI supercontinent\nakin to the geological Pangaea. Pangaea encodes any data into a unified format\nand accumulates universal knowledge through pre-training on 296 datasets across\ndiverse modalities. Eventually, it demonstrates remarkable generalization\nacross 45 general tasks and 15 scientific tasks encompassing a wide range of\nscientific subjects. By investigating Pangaea deeper, the scaling effect of\nmodality is revealed, quantifying the universal knowledge accumulation across\nmodalities as the cumulative distribution function of a geometric distribution.\nOn the whole, Pangaea shows strong potential to handle myriad tasks, indicating\na new direction toward artificial general intelligence.", "AI": {"tldr": "\u63d0\u51faPangaea\uff0c\u9996\u4e2a\u7edf\u4e00\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u7684AI\u8d85\u7ea7\u5927\u9646\u6a21\u578b\uff0c\u901a\u8fc7296\u4e2a\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u5b9e\u73b0\u8de8\u4efb\u52a1\u548c\u79d1\u5b66\u9886\u57df\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\uff0c\u5f62\u6210\u5b64\u7acb\u7684\u667a\u80fd\u5c9b\uff0c\u96be\u4ee5\u5b9e\u73b0\u8de8\u4efb\u52a1\u6cdb\u5316\uff0c\u963b\u788d\u4e86\u901a\u5411\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u5c06\u5404\u7c7b\u6570\u636e\u7f16\u7801\u4e3a\u7edf\u4e00\u683c\u5f0f\uff0c\u901a\u8fc7\u5728296\u4e2a\u8de8\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6784\u5efa\u540d\u4e3aPangaea\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u4ee5\u878d\u5408\u4e0d\u540c\u667a\u80fd\u5c9b\u3002", "result": "Pangaea\u572845\u4e2a\u901a\u7528\u4efb\u52a1\u548c15\u4e2a\u79d1\u5b66\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u6001\u6269\u5c55\u6548\u5e94\uff0c\u5373\u8de8\u6a21\u6001\u901a\u7528\u77e5\u8bc6\u79ef\u7d2f\u7b26\u5408\u51e0\u4f55\u5206\u5e03\u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u3002", "conclusion": "Pangaea\u5c55\u793a\u4e86\u6574\u5408\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u667a\u80fd\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u73b0\u4eba\u5de5\u901a\u7528\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.16674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16674", "abs": "https://arxiv.org/abs/2509.16674", "authors": ["Zengli Luo", "Canlong Zhang", "Xiaochun Lu", "Zhixin Li"], "title": "FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World", "comment": "15pages,6 figures", "summary": "Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target\npedestrians in visual scenes according to natural language descriptions.\nAlthough existing methods have achieved progress under constrained settings,\ninteractive retrieval in the open-world scenario still suffers from limited\nmodel generalization and insufficient semantic understanding. To address these\nchallenges, we propose FitPro, an open-world interactive zero-shot TPR\nframework with enhanced semantic comprehension and cross-scene adaptability.\nFitPro has three innovative components: Feature Contrastive Decoding (FCD),\nIncremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval\n(QHR). The FCD integrates prompt-guided contrastive decoding to generate\nhigh-quality structured pedestrian descriptions from denoised images,\neffectively alleviating semantic drift in zero-shot scenarios. The ISM\nconstructs holistic pedestrian representations from multi-view observations to\nachieve global semantic modeling in multi-turn interactions,thereby improving\nrobustness against viewpoint shifts and fine-grained variations in\ndescriptions. The QHR dynamically optimizes the retrieval pipeline according to\nquery types, enabling efficient adaptation to multi-modal and multi-view\ninputs. Extensive experiments on five public datasets and two evaluation\nprotocols demonstrate that FitPro significantly overcomes the generalization\nlimitations and semantic modeling constraints of existing methods in\ninteractive retrieval, paving the way for practical deployment. The code and\ndata will be released at https://github.com/\nlilo4096/FitPro-Interactive-Person-Retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFitPro\uff0c\u4e00\u79cd\u5f00\u653e\u4e16\u754c\u4ea4\u4e92\u5f0f\u96f6\u6837\u672c\u6587\u672c\u5230\u884c\u4eba\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u5bf9\u6bd4\u89e3\u7801\u3001\u589e\u91cf\u8bed\u4e49\u6316\u6398\u548c\u67e5\u8be2\u611f\u77e5\u5206\u5c42\u68c0\u7d22\u4e09\u4e2a\u6a21\u5757\u63d0\u5347\u8bed\u4e49\u7406\u89e3\u4e0e\u8de8\u573a\u666f\u9002\u5e94\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u4ea4\u4e92\u5f0f\u884c\u4eba\u68c0\u7d22\u4e2d\u5b58\u5728\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f31\u548c\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u8f6e\u4ea4\u4e92\u3001\u89c6\u89d2\u53d8\u5316\u548c\u7ec6\u7c92\u5ea6\u63cf\u8ff0\u5dee\u5f02\u7b49\u6311\u6218\u3002", "method": "FitPro\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u7279\u5f81\u5bf9\u6bd4\u89e3\u7801\uff08FCD\uff09\uff0c\u5229\u7528\u63d0\u793a\u5f15\u5bfc\u7684\u5bf9\u6bd4\u89e3\u7801\u751f\u6210\u9ad8\u8d28\u91cf\u53bb\u566a\u56fe\u50cf\u63cf\u8ff0\uff0c\u7f13\u89e3\u8bed\u4e49\u6f02\u79fb\uff1b2\uff09\u589e\u91cf\u8bed\u4e49\u6316\u6398\uff08ISM\uff09\uff0c\u878d\u5408\u591a\u89c6\u89d2\u89c2\u6d4b\u6784\u5efa\u884c\u4eba\u6574\u4f53\u8868\u5f81\uff0c\u5b9e\u73b0\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u5168\u5c40\u8bed\u4e49\u5efa\u6a21\uff1b3\uff09\u67e5\u8be2\u611f\u77e5\u5206\u5c42\u68c0\u7d22\uff08QHR\uff09\uff0c\u6839\u636e\u67e5\u8be2\u7c7b\u578b\u52a8\u6001\u4f18\u5316\u68c0\u7d22\u6d41\u7a0b\uff0c\u9002\u914d\u591a\u6a21\u6001\u4e0e\u591a\u89c6\u89d2\u8f93\u5165\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e24\u79cd\u8bc4\u4f30\u534f\u8bae\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFitPro\u5728\u96f6\u6837\u672c\u4ea4\u4e92\u5f0f\u884c\u4eba\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u8de8\u573a\u666f\u9002\u5e94\u6027\u548c\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "FitPro\u6709\u6548\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u4e0b\u6587\u672c\u5230\u884c\u4eba\u68c0\u7d22\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u4e49\u7406\u89e3\u6c34\u5e73\uff0c\u63a8\u52a8\u4e86\u8be5\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2509.17381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17381", "abs": "https://arxiv.org/abs/2509.17381", "authors": ["Yongliang Wang", "Hamidreza Kasaei"], "title": "Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators", "comment": "Project page available at: https://sites.google.com/view/ftp4rm/home", "summary": "Generating obstacle-free trajectories for robotic manipulators in\nunstructured and cluttered environments remains a significant challenge.\nExisting motion planning methods often require additional computational effort\nto generate the final trajectory by solving kinematic or dynamic equations.\nThis paper highlights the strong potential of model-free reinforcement learning\nmethods over model-based approaches for obstacle-free trajectory planning in\njoint space. We propose a fast trajectory planning system for manipulators that\ncombines vision-based path planning in task space with reinforcement\nlearning-based obstacle avoidance in joint space. We divide the framework into\ntwo key components. The first introduces an innovative vision-based trajectory\nplanner in task space, leveraging the large-scale fast segment anything (FSA)\nmodel in conjunction with basis spline (B-spline)-optimized kinodynamic path\nsearching. The second component enhances the proximal policy optimization (PPO)\nalgorithm by integrating action ensembles (AE) and policy feedback (PF), which\ngreatly improve precision and stability in goal-reaching and obstacle avoidance\nwithin the joint space. These PPO enhancements increase the algorithm's\nadaptability across diverse robotic tasks, ensuring consistent execution of\ncommands from the first component by the manipulator, while also enhancing both\nobstacle avoidance efficiency and reaching accuracy. The experimental results\ndemonstrate the effectiveness of PPO enhancements, as well as\nsimulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)\ntransfer, in improving model robustness and planner efficiency in complex\nscenarios. These enhancements allow the robot to perform obstacle avoidance and\nreal-time trajectory planning in obstructed environments. Project page\navailable at: https://sites.google.com/view/ftp4rm/home", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4efb\u52a1\u7a7a\u95f4\u89c6\u89c9\u8def\u5f84\u89c4\u5212\u4e0e\u5173\u8282\u7a7a\u95f4\u5f3a\u5316\u5b66\u4e60\u907f\u969c\u7684\u5feb\u901f\u8f68\u8ff9\u89c4\u5212\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdbPPO\u7b97\u6cd5\uff08\u5f15\u5165\u52a8\u4f5c\u96c6\u6210\u548c\u7b56\u7565\u53cd\u9988\uff09\u63d0\u5347\u4e86\u673a\u68b0\u81c2\u5728\u590d\u6742\u73af\u5883\u4e2d\u65e0\u78b0\u649e\u8f68\u8ff9\u751f\u6210\u7684\u7cbe\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728Sim-to-Sim\u548cSim-to-Real\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u4e14\u969c\u788d\u7269\u5bc6\u96c6\u7684\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u4f9d\u8d56\u52a8\u529b\u5b66\u5efa\u6a21\u5e76\u9700\u989d\u5916\u8ba1\u7b97\uff0c\u96be\u4ee5\u9ad8\u6548\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7cbe\u786e\u6a21\u578b\u4e14\u80fd\u5b9e\u65f6\u9002\u5e94\u73af\u5883\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u5c06\u6846\u67b6\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u4efb\u52a1\u7a7a\u95f4\u4e2d\u91c7\u7528FSA\u6a21\u578b\u7ed3\u5408B\u6837\u6761\u4f18\u5316\u7684\u8fd0\u52a8\u52a8\u529b\u5b66\u641c\u7d22\u8fdb\u884c\u89c6\u89c9\u8def\u5f84\u89c4\u5212\uff1b\u5173\u8282\u7a7a\u95f4\u4e2d\u6539\u8fdbPPO\u7b97\u6cd5\uff0c\u5f15\u5165\u52a8\u4f5c\u96c6\u6210\uff08AE\uff09\u548c\u7b56\u7565\u53cd\u9988\uff08PF\uff09\u4ee5\u63d0\u5347\u907f\u969c\u4e0e\u76ee\u6807\u5230\u8fbe\u7684\u7a33\u5b9a\u6027\u4e0e\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6240\u63d0PPO\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u907f\u969c\u6548\u7387\u548c\u5230\u8fbe\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684Sim-to-Sim\u4e0eSim-to-Real\u8fc1\u79fb\uff0c\u5728\u590d\u6742\u969c\u788d\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\u4e0e\u52a8\u6001\u907f\u969c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u89c6\u89c9\u5f15\u5bfc\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u65e0\u9700\u7cbe\u786e\u5efa\u6a21\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u673a\u68b0\u81c2\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u3001\u9c81\u68d2\u8f68\u8ff9\u89c4\u5212\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16788", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16788", "abs": "https://arxiv.org/abs/2509.16788", "authors": ["Salha Alyami", "Amani Jamal", "Areej Alhothali"], "title": "Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies", "comment": "26 excluding bibliography , journal article", "summary": "Aspect-based sentiment analysis (ABSA) in natural language processing enables\norganizations to understand customer opinions on specific product aspects.\nWhile deep learning models are widely used for English ABSA, their application\nin Arabic is limited due to the scarcity of labeled data. Researchers have\nattempted to tackle this issue by using pre-trained contextualized language\nmodels such as BERT. However, these models are often based on fact-based data,\nwhich can introduce bias in domain-specific tasks like ABSA. To our knowledge,\nno studies have applied adaptive pre-training with Arabic contextualized models\nfor ABSA. This research proposes a novel approach using domain-adaptive\npre-training for aspect-sentiment classification (ASC) and opinion target\nexpression (OTE) extraction. We examine fine-tuning strategies - feature\nextraction, full fine-tuning, and adapter-based methods - to enhance\nperformance and efficiency, utilizing multiple adaptation corpora and\ncontextualized models. Our results show that in-domain adaptive pre-training\nyields modest improvements. Adapter-based fine-tuning is a computationally\nefficient method that achieves competitive results. However, error analyses\nreveal issues with model predictions and dataset labeling. In ASC, common\nproblems include incorrect sentiment labeling, misinterpretation of contrastive\nmarkers, positivity bias for early terms, and challenges with conflicting\nopinions and subword tokenization. For OTE, issues involve mislabeling targets,\nconfusion over syntactic roles, difficulty with multi-word expressions, and\nreliance on shallow heuristics. These findings underscore the need for syntax-\nand semantics-aware models, such as graph convolutional networks, to more\neffectively capture long-distance relations and complex aspect-based opinion\nalignments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u7684\u963f\u62c9\u4f2f\u8bed\u65b9\u9762\u60c5\u611f\u5206\u7c7b\uff08ASC\uff09\u548c\u610f\u89c1\u76ee\u6807\u63d0\u53d6\uff08OTE\uff09\u65b0\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u53d1\u73b0\u57df\u5185\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u53ef\u5e26\u6765\u4e00\u5b9a\u6027\u80fd\u63d0\u5347\uff0c\u800c\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4f46\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u6807\u7b7e\u3001\u8bed\u6cd5\u548c\u8bed\u4e49\u5efa\u6a21\u65b9\u9762\u7684\u82e5\u5e72\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u963f\u62c9\u4f2f\u8bed\u7684\u65b9\u9762\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\u7814\u7a76\u53d7\u9650\uff0c\u73b0\u6709\u57fa\u4e8e\u901a\u7528\u8bed\u6599\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u5b58\u5728\u504f\u5dee\uff0c\u4e14\u5c1a\u65e0\u7814\u7a76\u5c06\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u5e94\u7528\u4e8e\u963f\u62c9\u4f2f\u8bedABSA\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u548c\u4e0d\u540c\u89c4\u6a21\u7684\u9886\u57df\u9002\u914d\u8bed\u6599\uff0c\u5728ASC\u548cOTE\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u7279\u5f81\u63d0\u53d6\u3001\u5168\u53c2\u6570\u5fae\u8c03\u548c\u57fa\u4e8e\u9002\u914d\u5668\u7684\u5fae\u8c03\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57df\u5185\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u80fd\u5e26\u6765\u9002\u5ea6\u6027\u80fd\u63d0\u5347\uff1b\u57fa\u4e8e\u9002\u914d\u5668\u7684\u5fae\u8c03\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u4e14\u7ed3\u679c\u5177\u6709\u7ade\u4e89\u529b\uff1b\u9519\u8bef\u5206\u6790\u53d1\u73b0\u6a21\u578b\u5728\u60c5\u611f\u6807\u7b7e\u3001\u5bf9\u6bd4\u6807\u8bb0\u7406\u89e3\u3001\u591a\u8bcd\u8868\u8fbe\u8bc6\u522b\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u66b4\u9732\u51fa\u6570\u636e\u6807\u6ce8\u95ee\u9898\u3002", "conclusion": "\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u5bf9\u963f\u62c9\u4f2f\u8bedABSA\u6709\u76ca\uff0c\u57fa\u4e8e\u9002\u914d\u5668\u7684\u5fae\u8c03\u662f\u9ad8\u6548\u9009\u62e9\uff0c\u672a\u6765\u9700\u6784\u5efa\u66f4\u5177\u8bed\u6cd5\u548c\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u7684\u6a21\u578b\uff08\u5982\u56fe\u5377\u79ef\u7f51\u7edc\uff09\uff0c\u4ee5\u66f4\u597d\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u590d\u6742\u7684\u89c2\u70b9\u5bf9\u9f50\u5173\u7cfb\u3002"}}
{"id": "2509.16930", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16930", "abs": "https://arxiv.org/abs/2509.16930", "authors": ["Nathan Derhake", "Siddartha Devic", "Dutch Hansen", "Kuan Liu", "Vatsal Sharan"], "title": "Auditability and the Landscape of Distance to Multicalibration", "comment": "41 pages", "summary": "Calibration is a critical property for establishing the trustworthiness of\npredictors that provide uncertainty estimates. Multicalibration is a\nstrengthening of calibration which requires that predictors be calibrated on a\npotentially overlapping collection of subsets of the domain. As\nmulticalibration grows in popularity with practitioners, an essential question\nis: how do we measure how multicalibrated a predictor is? B{\\l}asiok et al.\n(2023) considered this question for standard calibration by introducing the\ndistance to calibration framework (dCE) to understand how calibration metrics\nrelate to each other and the ground truth. Building on the dCE framework, we\nconsider the auditability of the distance to multicalibration of a predictor\n$f$.\n  We begin by considering two natural generalizations of dCE to multiple\nsubgroups: worst group dCE (wdMC), and distance to multicalibration (dMC). We\nargue that there are two essential properties of any multicalibration error\nmetric: 1) the metric should capture how much $f$ would need to be modified in\norder to be perfectly multicalibrated; and 2) the metric should be auditable in\nan information theoretic sense. We show that wdMC and dMC each fail to satisfy\none of these two properties, and that similar barriers arise when considering\nthe auditability of general distance to multigroup fairness notions. We then\npropose two (equivalent) multicalibration metrics which do satisfy these\nrequirements: 1) a continuized variant of dMC; and 2) a distance to\nintersection multicalibration, which leans on intersectional fairness\ndesiderata. Along the way, we shed light on the loss-landscape of distance to\nmulticalibration and the geometry of the set of perfectly multicalibrated\npredictors. Our findings may have implications for the development of stronger\nmulticalibration algorithms as well as multigroup auditing more generally.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6821\u51c6\u6027\uff08multicalibration\uff09\u8bef\u5dee\u5ea6\u91cf\u7684\u53ef\u5ba1\u8ba1\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u6ee1\u8db3\u5173\u952e\u6027\u8d28\u7684\u7b49\u4ef7\u5ea6\u91cf\u65b9\u6cd5\uff1a\u8fde\u7eed\u5316dMC\u548c\u4ea4\u96c6\u591a\u6821\u51c6\u8ddd\u79bb\u3002", "motivation": "\u968f\u7740\u591a\u6821\u51c6\u6027\u5728\u5b9e\u8df5\u4e2d\u7684\u6d41\u884c\uff0c\u5982\u4f55\u8861\u91cf\u9884\u6d4b\u5668\u7684\u591a\u6821\u51c6\u7a0b\u5ea6\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4fe1\u606f\u8bba\u610f\u4e49\u4e0a\u7684\u53ef\u5ba1\u8ba1\u6027\u548c\u4fee\u6539\u9700\u6c42\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u8ddd\u79bb\u5230\u6821\u51c6\u6846\u67b6\uff08dCE\uff09\uff0c\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e24\u79cd\u81ea\u7136\u63a8\u5e7f\u5f62\u5f0fwdMC\u548cdMC\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6ee1\u8db3\u53ef\u5ba1\u8ba1\u6027\u548c\u4fee\u6539\u91cf\u6355\u83b7\u6027\u7684\u65b0\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0wdMC\u548cdMC\u5404\u81ea\u4e0d\u6ee1\u8db3\u53ef\u5ba1\u8ba1\u6027\u6216\u591a\u6821\u51c6\u4fee\u6539\u9700\u6c42\u7684\u5173\u952e\u5c5e\u6027\uff1b\u63d0\u51fa\u4e86\u4e24\u79cd\u7b49\u4ef7\u4e14\u6ee1\u8db3\u8981\u6c42\u7684\u65b0\u5ea6\u91cf\uff1a\u8fde\u7eed\u5316dMC\u548c\u4ea4\u96c6\u591a\u6821\u51c6\u8ddd\u79bb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b0\u578b\u591a\u6821\u51c6\u5ea6\u91cf\u66f4\u597d\u5730\u5e73\u8861\u4e86\u7406\u8bba\u4e25\u8c28\u6027\u4e0e\u53ef\u5ba1\u8ba1\u6027\uff0c\u5bf9\u591a\u6821\u51c6\u7b97\u6cd5\u548c\u591a\u7fa4\u4f53\u516c\u5e73\u6027\u5ba1\u8ba1\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2509.17544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17544", "abs": "https://arxiv.org/abs/2509.17544", "authors": ["Juan Ca\u00f1ada", "Ra\u00fal Alonso", "Julio Molleda", "Fidel D\u00edez"], "title": "A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data", "comment": null, "summary": "The increasing availability of open Earth Observation (EO) and agricultural\ndatasets holds great potential for supporting sustainable land management.\nHowever, their high technical entry barrier limits accessibility for non-expert\nusers. This study presents an open-source conversational assistant that\nintegrates multimodal retrieval and large language models (LLMs) to enable\nnatural language interaction with heterogeneous agricultural and geospatial\ndata. The proposed architecture combines orthophotos, Sentinel-2 vegetation\nindices, and user-provided documents through retrieval-augmented generation\n(RAG), allowing the system to flexibly determine whether to rely on multimodal\nevidence, textual knowledge, or both in formulating an answer. To assess\nresponse quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a\nzero-shot, unsupervised setting, applying direct scoring in a multi-dimensional\nquantitative evaluation framework. Preliminary results show that the system is\ncapable of generating clear, relevant, and context-aware responses to\nagricultural queries, while remaining reproducible and scalable across\ngeographic regions. The primary contributions of this work include an\narchitecture for fusing multimodal EO and textual knowledge sources, a\ndemonstration of lowering the barrier to access specialized agricultural\ninformation through natural language interaction, and an open and reproducible\ndesign.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u52a9\u624b\uff0c\u7ed3\u5408\u9065\u611f\u6570\u636e\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u4ee5\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u5f0f\u8bbf\u95ee\u519c\u4e1a\u4e0e\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\uff0c\u964d\u4f4e\u975e\u4e13\u4e1a\u7528\u6237\u4f7f\u7528\u95e8\u69db\u3002", "motivation": "\u5f00\u653e\u5730\u7403\u89c2\u6d4b\u548c\u519c\u4e1a\u6570\u636e\u867d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u6280\u672f\u95e8\u69db\u9ad8\uff0c\u975e\u4e13\u4e1a\u7528\u6237\u96be\u4ee5\u4f7f\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6613\u7528\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u878d\u5408\u6b63\u5c04\u5f71\u50cf\u3001Sentinel-2\u690d\u88ab\u6307\u6570\u548c\u7528\u6237\u6587\u6863\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u67b6\u6784\uff0c\u5229\u7528\u591a\u6a21\u6001\u68c0\u7d22\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\uff0c\u5e76\u91c7\u7528Qwen3-32B\u4f5c\u4e3a\u88c1\u5224\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u3001\u65e0\u76d1\u7763\u7684\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u6e05\u6670\u3001\u76f8\u5173\u4e14\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u7684\u519c\u4e1a\u67e5\u8be2\u54cd\u5e94\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u91cd\u590d\u6027\u548c\u533a\u57df\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u6709\u6548\u878d\u5408\u4e86\u591a\u6a21\u6001\u9065\u611f\u4e0e\u6587\u672c\u77e5\u8bc6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u83b7\u53d6\u4e13\u4e1a\u519c\u4e1a\u4fe1\u606f\u7684\u6280\u672f\u95e8\u69db\uff0c\u63a8\u52a8\u4e86\u53ef\u6301\u7eed\u571f\u5730\u7ba1\u7406\u7684\u6570\u636e\u666e\u60e0\u3002"}}
{"id": "2509.16677", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.16677", "abs": "https://arxiv.org/abs/2509.16677", "authors": ["Wenxin Li", "Kunyu Peng", "Di Wen", "Ruiping Liu", "Mengfei Duan", "Kai Luo", "Kailun Yang"], "title": "Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence", "comment": "The established benchmark and source code will be made publicly\n  available at https://github.com/mylwx/ActiSeg-NL", "summary": "Embodied intelligence relies on accurately segmenting objects actively\ninvolved in interactions. Action-based video object segmentation addresses this\nby linking segmentation with action semantics, but it depends on large-scale\nannotations and prompts that are costly, inconsistent, and prone to multimodal\nnoise such as imprecise masks and referential ambiguity. To date, this\nchallenge remains unexplored. In this work, we take the first step by studying\naction-based video object segmentation under label noise, focusing on two\nsources: textual prompt noise (category flips and within-category noun\nsubstitutions) and mask annotation noise (perturbed object boundaries to mimic\nimprecise supervision). Our contributions are threefold. First, we introduce\ntwo types of label noises for the action-based video object segmentation task.\nSecond, we build up the first action-based video object segmentation under a\nlabel noise benchmark ActiSeg-NL and adapt six label-noise learning strategies\nto this setting, and establish protocols for evaluating them under textual,\nboundary, and mixed noise. Third, we provide a comprehensive analysis linking\nnoise types to failure modes and robustness gains, and we introduce a Parallel\nMask Head Mechanism (PMHM) to address mask annotation noise. Qualitative\nevaluations further reveal characteristic failure modes, including boundary\nleakage and mislocalization under boundary perturbations, as well as occasional\nidentity substitutions under textual flips. Our comparative analysis reveals\nthat different learning strategies exhibit distinct robustness profiles,\ngoverned by a foreground-background trade-off where some achieve balanced\nperformance while others prioritize foreground accuracy at the cost of\nbackground precision. The established benchmark and source code will be made\npublicly available at https://github.com/mylwx/ActiSeg-NL.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a8\u4f5c\u611f\u77e5\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6807\u7b7e\u566a\u58f0\u7c7b\u578b\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5e26\u566a\u58f0\u7684\u57fa\u51c6\u6570\u636e\u96c6ActiSeg-NL\uff0c\u5e76\u5f15\u5165\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u6297\u566a\u58f0\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u51fa\u5e73\u884c\u63a9\u7801\u5934\u673a\u5236\uff08PMHM\uff09\u4ee5\u5e94\u5bf9\u63a9\u7801\u6807\u6ce8\u566a\u58f0\u3002", "motivation": "\u52a8\u4f5c\u611f\u77e5\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6587\u672c\u63d0\u793a\u566a\u58f0\u548c\u63a9\u7801\u6807\u6ce8\u566a\u58f0\u7b49\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u63a2\u7d22\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8be5\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u6587\u672c\u63d0\u793a\u566a\u58f0\uff08\u7c7b\u522b\u7ffb\u8f6c\u3001\u7c7b\u5185\u540d\u8bcd\u66ff\u6362\uff09\u548c\u63a9\u7801\u6807\u6ce8\u566a\u58f0\uff08\u8fb9\u754c\u6270\u52a8\uff09\u4e24\u79cd\u566a\u58f0\u7c7b\u578b\uff1b\u6784\u5efaActiSeg-NL\u57fa\u51c6\uff0c\u9002\u914d\u516d\u79cd\u6807\u7b7e\u566a\u58f0\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u8bc4\u4f30\u534f\u8bae\uff1b\u63d0\u51fa\u5e73\u884c\u63a9\u7801\u5934\u673a\u5236\uff08PMHM\uff09\u7f13\u89e3\u63a9\u7801\u566a\u58f0\u5f71\u54cd\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u52a8\u4f5c\u611f\u77e5\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u7684\u57fa\u51c6\uff1b\u5b9e\u9a8c\u8868\u660e\u4e0d\u540c\u5b66\u4e60\u7b56\u7565\u5728\u524d\u666f\u4e0e\u80cc\u666f\u6027\u80fd\u95f4\u5b58\u5728\u6743\u8861\uff1bPMHM\u6709\u6548\u63d0\u5347\u5bf9\u63a9\u7801\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff1b\u63ed\u793a\u4e86\u8fb9\u754c\u6cc4\u6f0f\u3001\u5b9a\u4f4d\u9519\u8bef\u548c\u8eab\u4efd\u6df7\u6dc6\u7b49\u5178\u578b\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u6807\u7b7e\u566a\u58f0\u663e\u8457\u5f71\u54cd\u52a8\u4f5c\u611f\u77e5\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6027\u80fd\uff0c\u672c\u6587\u63d0\u51fa\u7684\u566a\u58f0\u5efa\u6a21\u3001\u57fa\u51c6\u548cPMHM\u673a\u5236\u4e3a\u8be5\u4efb\u52a1\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u65b9\u5411\u3002"}}
{"id": "2509.17387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17387", "abs": "https://arxiv.org/abs/2509.17387", "authors": ["Ziqing Zou", "Cong Wang", "Yue Hu", "Xiao Liu", "Bowen Xu", "Rong Xiong", "Changjie Fan", "Yingfeng Chen", "Yue Wang"], "title": "High-Precision and High-Efficiency Trajectory Tracking for Excavators Based on Closed-Loop Dynamics", "comment": null, "summary": "The complex nonlinear dynamics of hydraulic excavators, such as time delays\nand control coupling, pose significant challenges to achieving high-precision\ntrajectory tracking. Traditional control methods often fall short in such\napplications due to their inability to effectively handle these nonlinearities,\nwhile commonly used learning-based methods require extensive interactions with\nthe environment, leading to inefficiency. To address these issues, we introduce\nEfficientTrack, a trajectory tracking method that integrates model-based\nlearning to manage nonlinear dynamics and leverages closed-loop dynamics to\nimprove learning efficiency, ultimately minimizing tracking errors. We validate\nour method through comprehensive experiments both in simulation and on a\nreal-world excavator. Comparative experiments in simulation demonstrate that\nour method outperforms existing learning-based approaches, achieving the\nhighest tracking precision and smoothness with the fewest interactions.\nReal-world experiments further show that our method remains effective under\nload conditions and possesses the ability for continual learning, highlighting\nits practical applicability. For implementation details and source code, please\nrefer to https://github.com/ZiqingZou/EfficientTrack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEfficientTrack\u7684\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6db2\u538b\u6316\u6398\u673a\u4e2d\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u5982\u65f6\u95f4\u5ef6\u8fdf\u548c\u63a7\u5236\u8026\u5408\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u548c\u95ed\u73af\u52a8\u529b\u5b66\uff0c\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u5e76\u6700\u5c0f\u5316\u8ddf\u8e2a\u8bef\u5dee\u3002\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u5e73\u6ed1\u6027\u548c\u4ea4\u4e92\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u6db2\u538b\u6316\u6398\u673a\u4e2d\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u73af\u5883\u4ea4\u4e92\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEfficientTrack\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u5b66\u4e60\u6765\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u5e76\u5229\u7528\u95ed\u73af\u52a8\u529b\u5b66\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5e73\u6ed1\u6027\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u4e14\u6240\u9700\u4ea4\u4e92\u6b21\u6570\u6700\u5c11\uff1b\u771f\u5b9e\u6316\u6398\u673a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u53ca\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "EfficientTrack\u5728\u5904\u7406\u6db2\u538b\u6316\u6398\u673a\u8f68\u8ff9\u8ddf\u8e2a\u95ee\u9898\u4e0a\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u548c\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16804", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16804", "abs": "https://arxiv.org/abs/2509.16804", "authors": ["Kozhin muhealddin Awlla", "Hadi Veisi", "Abdulhady Abas Abdullah"], "title": "KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis", "comment": null, "summary": "This paper enhances the study of sentiment analysis for the Central Kurdish\nlanguage by integrating the Bidirectional Encoder Representations from\nTransformers (BERT) into Natural Language Processing techniques. Kurdish is a\nlow-resourced language, having a high level of linguistic diversity with\nminimal computational resources, making sentiment analysis somewhat\nchallenging. Earlier, this was done using a traditional word embedding model,\nsuch as Word2Vec, but with the emergence of new language models, specifically\nBERT, there is hope for improvements. The better word embedding capabilities of\nBERT lend to this study, aiding in the capturing of the nuanced semantic pool\nand the contextual intricacies of the language under study, the Kurdish\nlanguage, thus setting a new benchmark for sentiment analysis in low-resource\nlanguages.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e2d\u592e\u5e93\u5c14\u5fb7\u8bed\u7684\u60c5\u611f\u5206\u6790\uff0c\u901a\u8fc7\u5f15\u5165BERT\u6a21\u578b\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6548\u679c\uff0c\u5229\u7528\u5176\u5f3a\u5927\u7684\u8bcd\u5d4c\u5165\u80fd\u529b\u6355\u6349\u8bed\u8a00\u7684\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u7ec6\u8282\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u6811\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "motivation": "\u5e93\u5c14\u5fb7\u8bed\u662f\u4e00\u79cd\u8d44\u6e90\u532e\u4e4f\u4e14\u8bed\u8a00\u591a\u6837\u6027\u9ad8\u7684\u8bed\u8a00\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982Word2Vec\u5728\u60c5\u611f\u5206\u6790\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u6765\u63d0\u5347\u5206\u6790\u6548\u679c\u3002", "method": "\u91c7\u7528BERT\u6a21\u578b\u8fdb\u884c\u8bcd\u5d4c\u5165\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5bf9\u4e2d\u592e\u5e93\u5c14\u5fb7\u8bed\u8fdb\u884c\u60c5\u611f\u5206\u6790\u3002", "result": "BERT\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4e2d\u592e\u5e93\u5c14\u5fb7\u8bed\u60c5\u611f\u5206\u6790\u7684\u6548\u679c\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "conclusion": "BERT\u6a21\u578b\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u7c7b\u4f3c\u8bed\u8a00\u7684\u7814\u7a76\u6811\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.16936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16936", "abs": "https://arxiv.org/abs/2509.16936", "authors": ["Cuiqianhe Du", "Chia-En Chiang", "Tianyi Huang", "Zikun Cui"], "title": "Adaptive Graph Convolution and Semantic-Guided Attention for Multimodal Risk Detection in Social Networks", "comment": null, "summary": "This paper focuses on the detection of potentially dangerous tendencies of\nsocial media users in an innovative multimodal way. We integrate Natural\nLanguage Processing (NLP) and Graph Neural Networks (GNNs) together. Firstly,\nwe apply NLP on the user-generated text and conduct semantic analysis,\nsentiment recognition and keyword extraction to get subtle risk signals from\nsocial media posts. Meanwhile, we build a heterogeneous user relationship graph\nbased on social interaction and propose a novel relational graph convolutional\nnetwork to model user relationship, attention relationship and content\ndissemination path to discover some important structural information and user\nbehaviors. Finally, we combine textual features extracted from these two models\nabove with graph structural information, which provides a more robust and\neffective way to discover at-risk users. Our experiments on real social media\ndatasets from different platforms show that our model can achieve significant\nimprovement over single-modality methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u6f5c\u5728\u5371\u9669\u503e\u5411\u3002", "motivation": "\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u5728\u8bc6\u522b\u9ad8\u98ce\u9669\u7528\u6237\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u5168\u9762\u6355\u6349\u8bed\u4e49\u548c\u793e\u4ea4\u884c\u4e3a\u7279\u5f81\u3002", "method": "\u91c7\u7528NLP\u5bf9\u7528\u6237\u6587\u672c\u8fdb\u884c\u8bed\u4e49\u5206\u6790\u3001\u60c5\u611f\u8bc6\u522b\u548c\u5173\u952e\u8bcd\u63d0\u53d6\uff1b\u6784\u5efa\u5f02\u6784\u7528\u6237\u5173\u7cfb\u56fe\uff0c\u5e76\u8bbe\u8ba1\u65b0\u578b\u5173\u7cfb\u56fe\u5377\u79ef\u7f51\u7edc\u6765\u5efa\u6a21\u7528\u6237\u5173\u7cfb\u3001\u6ce8\u610f\u529b\u5173\u7cfb\u548c\u5185\u5bb9\u4f20\u64ad\u8def\u5f84\uff1b\u6700\u540e\u878d\u5408\u6587\u672c\u7279\u5f81\u4e0e\u56fe\u7ed3\u6784\u4fe1\u606f\u8fdb\u884c\u98ce\u9669\u8bc6\u522b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u5728\u68c0\u6d4b\u9ad8\u98ce\u9669\u7528\u6237\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u878d\u5408NLP\u4e0eGNN\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5bf9\u793e\u4ea4\u5a92\u4f53\u4e2d\u6f5c\u5728\u5371\u9669\u7528\u6237\u8bc6\u522b\u7684\u80fd\u529b\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.17550", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17550", "abs": "https://arxiv.org/abs/2509.17550", "authors": ["Neslihan Kose", "Anthony Rhodes", "Umur Aybars Ciftci", "Ilke Demir"], "title": "Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem", "comment": "Accepted for publication at the ICCV 2025 STREAM workshop", "summary": "As generative models are advancing in quality and quantity for creating\nsynthetic content, deepfakes begin to cause online mistrust. Deepfake detectors\nare proposed to counter this effect, however, misuse of detectors claiming fake\ncontent as real or vice versa further fuels this misinformation problem. We\npresent the first comprehensive uncertainty analysis of deepfake detectors,\nsystematically investigating how generative artifacts influence prediction\nconfidence. As reflected in detectors' responses, deepfake generators also\ncontribute to this uncertainty as their generative residues vary, so we cross\nthe uncertainty analysis of deepfake detectors and generators. Based on our\nobservations, the uncertainty manifold holds enough consistent information to\nleverage uncertainty for deepfake source detection. Our approach leverages\nBayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and\nepistemic uncertainties across diverse detector architectures. We evaluate\nuncertainty on two datasets with nine generators, with four blind and two\nbiological detectors, compare different uncertainty methods, explore region-\nand pixel-based uncertainty, and conduct ablation studies. We conduct and\nanalyze binary real/fake, multi-class real/fake, source detection, and\nleave-one-out experiments between the generator/detector combinations to share\ntheir generalization capability, model calibration, uncertainty, and robustness\nagainst adversarial attacks. We further introduce uncertainty maps that\nlocalize prediction confidence at the pixel level, revealing distinct patterns\ncorrelated with generator-specific artifacts. Our analysis provides critical\ninsights for deploying reliable deepfake detection systems and establishes\nuncertainty quantification as a fundamental requirement for trustworthy\nsynthetic media detection.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u8fdb\u884c\u4e86\u5168\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u6790\uff0c\u63a2\u8ba8\u751f\u6210\u4f2a\u5f71\u5982\u4f55\u5f71\u54cd\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u7ed3\u5408\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u8499\u7279\u5361\u6d1bdropout\u91cf\u5316\u591a\u79cd\u68c0\u6d4b\u5668\u67b6\u6784\u4e2d\u7684\u5076\u7136\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6df1\u5ea6\u4f2a\u9020\u5f15\u53d1\u5728\u7ebf\u4fe1\u4efb\u5371\u673a\uff0c\u800c\u68c0\u6d4b\u5668\u7684\u8bef\u7528\u52a0\u5267\u4e86\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7406\u89e3\u68c0\u6d4b\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u4ee5\u63d0\u5347\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u4e0e\u8499\u7279\u5361\u6d1bdropout\u65b9\u6cd5\uff0c\u8de8\u591a\u79cd\u68c0\u6d4b\u5668\u67b6\u6784\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff1b\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e5d\u79cd\u751f\u6210\u5668\uff0c\u7ed3\u5408\u76f2\u68c0\u4e0e\u751f\u7269\u68c0\u6d4b\u5668\uff0c\u8fdb\u884c\u591a\u7c7b\u3001\u6e90\u68c0\u6d4b\u53ca\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u6d88\u878d\u5b9e\u9a8c\uff0c\u5e76\u63d0\u51fa\u50cf\u7d20\u7ea7\u4e0d\u786e\u5b9a\u6027\u56fe\u8c31\u3002", "result": "\u53d1\u73b0\u4e0d\u786e\u5b9a\u6027\u6d41\u5f62\u5305\u542b\u8db3\u591f\u4e00\u81f4\u7684\u4fe1\u606f\u53ef\u7528\u4e8e\u6df1\u5ea6\u4f2a\u9020\u6e90\u68c0\u6d4b\uff0c\u4e0d\u540c\u751f\u6210\u5668\u6b8b\u7559\u7269\u5bfc\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u53ef\u533a\u5206\u6a21\u5f0f\uff0c\u4e14\u4e0d\u786e\u5b9a\u6027\u56fe\u8c31\u80fd\u5b9a\u4f4d\u4e0e\u751f\u6210\u5668\u76f8\u5173\u7684\u4f2a\u5f71\u533a\u57df\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u662f\u53ef\u4fe1\u5408\u6210\u5a92\u4f53\u68c0\u6d4b\u7684\u57fa\u672c\u8981\u6c42\uff0c\u8be5\u7814\u7a76\u4e3a\u90e8\u7f72\u53ef\u9760\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2509.16678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16678", "abs": "https://arxiv.org/abs/2509.16678", "authors": ["Suorong Yang", "Hongchao Yang", "Suhan Guo", "Furao Shen", "Jian Zhao"], "title": "IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation", "comment": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "summary": "Data augmentation is widely utilized as an effective technique to enhance the\ngeneralization performance of deep models. However, data augmentation may\ninevitably introduce distribution shifts and noises, which significantly\nconstrain the potential and deteriorate the performance of deep networks. To\nthis end, we propose a novel information-preserving framework, namely IPF-RDA,\nto enhance the robustness of data augmentations in this paper. IPF-RDA combines\nthe proposal of (i) a new class-discriminative information estimation algorithm\nthat identifies the points most vulnerable to data augmentation operations and\ncorresponding importance scores; And (ii) a new information-preserving scheme\nthat preserves the critical information in the augmented samples and ensures\nthe diversity of augmented data adaptively. We divide data augmentation methods\ninto three categories according to the operation types and integrate these\napproaches into our framework accordingly. After being integrated into our\nframework, the robustness of data augmentation methods can be enhanced and\ntheir full potential can be unleashed. Extensive experiments demonstrate that\nalthough being simple, IPF-RDA consistently improves the performance of\nnumerous commonly used state-of-the-art data augmentation methods with popular\ndeep models on a variety of datasets, including CIFAR-10, CIFAR-100,\nTiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its\nperformance and scalability are stressed. The implementation is available at\nhttps://github.com/Jackbrocp/IPF-RDA.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u4fdd\u7559\u6846\u67b6IPF-RDA\uff0c\u4ee5\u589e\u5f3a\u6570\u636e\u589e\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u8bc6\u522b\u6613\u53d7\u5e72\u6270\u7684\u6837\u672c\u5e76\u81ea\u9002\u5e94\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u591a\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u589e\u5f3a\u867d\u80fd\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4f1a\u5f15\u5165\u5206\u5e03\u504f\u79fb\u548c\u566a\u58f0\uff0c\u9650\u5236\u6df1\u5ea6\u7f51\u7edc\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faIPF-RDA\u6846\u67b6\uff0c\u5305\u62ec\u7c7b\u5224\u522b\u6027\u4fe1\u606f\u4f30\u8ba1\u7b97\u6cd5\u548c\u4fe1\u606f\u4fdd\u7559\u673a\u5236\uff0c\u5e76\u5c06\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6309\u64cd\u4f5c\u7c7b\u578b\u5206\u4e3a\u4e09\u7c7b\u8fdb\u884c\u96c6\u6210\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001Tiny-ImageNet\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86IPF-RDA\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cdSOTA\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "IPF-RDA\u80fd\u6709\u6548\u589e\u5f3a\u6570\u636e\u589e\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u91ca\u653e\u5176\u6f5c\u529b\uff0c\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.17389", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17389", "abs": "https://arxiv.org/abs/2509.17389", "authors": ["Lois Liow", "Jonty Milford", "Emre Uygun", "Andre Farinha", "Vinoth Viswanathan", "Josh Pinskier", "David Howard"], "title": "3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks", "comment": "8 pages, 4 figures", "summary": "Robotics and automation are key enablers to increase throughput in ongoing\nconservation efforts across various threatened ecosystems. Cataloguing,\ndigitisation, husbandry, and similar activities require the ability to interact\nwith delicate, fragile samples without damaging them. Additionally,\nlearning-based solutions to these tasks require the ability to safely acquire\ndata to train manipulation policies through, e.g., reinforcement learning. To\naddress these twin needs, we introduce a novel method to print free-form,\nhighly sensorised soft 'physical twins'. We present an automated design\nworkflow to create complex and customisable 3D soft sensing structures on\ndemand from 3D scans or models. Compared to the state of the art, our soft\nliquid metal sensors faithfully recreate complex natural geometries and display\nexcellent sensing properties suitable for validating performance in delicate\nmanipulation tasks. We demonstrate the application of our physical twins as\n'sensing corals': high-fidelity, 3D printed replicas of scanned corals that\neliminate the need for live coral experimentation, whilst increasing data\nquality, offering an ethical and scalable pathway for advancing autonomous\ncoral handling and soft manipulation broadly. Through extensive bench-top\nmanipulation and underwater grasping experiments, we show that our sensing\ncoral is able to detect grasps under 0.5 N, effectively capturing the delicate\ninteractions and light contact forces required for coral handling. Finally, we\nshowcase the value of our physical twins across two demonstrations: (i)\nautomated coral labelling for lab identification and (ii) robotic coral\naquaculture. Sensing physical twins such as ours can provide richer grasping\nfeedback than conventional sensors providing experimental validation of prior\nto deployment in handling fragile and delicate items.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5236\u9020\u9ad8\u4f20\u611f\u6027\u7684\u8f6f\u6027\u2018\u7269\u7406\u5b6a\u751f\u4f53\u2019\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u73ca\u745a\u7b49\u8106\u5f31\u7269\u4f53\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u907f\u514d\u4f7f\u7528\u6d3b\u4f53\u6837\u672c\uff0c\u540c\u65f6\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u4e0e\u64cd\u4f5c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u4fdd\u62a4\u6fd2\u5371\u751f\u6001\u7cfb\u7edf\u7684\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u5b89\u5168\u3001\u9ad8\u6548\u5730\u5904\u7406\u8106\u5f31\u6837\u672c\uff08\u5982\u73ca\u745a\uff09\uff0c\u540c\u65f6\u4e3a\u5b66\u4e60\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u907f\u514d\u5bf9\u771f\u5b9e\u751f\u7269\u9020\u6210\u4f24\u5bb3\u3002", "method": "\u57fa\u4e8e3D\u626b\u63cf\u6216\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u8bbe\u8ba1\u6d41\u7a0b\u5236\u9020\u5e26\u6709\u6db2\u6001\u91d1\u5c5e\u4f20\u611f\u5668\u7684\u81ea\u7531\u5f62\u6001\u8f6f\u6027\u7269\u7406\u5b6a\u751f\u4f53\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u590d\u5236\u548c\u5b9e\u65f6\u529b\u611f\u77e5\u3002", "result": "\u6240\u5236\u9020\u7684\u2018\u4f20\u611f\u73ca\u745a\u2019\u53ef\u68c0\u6d4b\u4f4e\u4e8e0.5 N\u7684\u6293\u53d6\u529b\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u81ea\u52a8\u73ca\u745a\u6807\u8bb0\u548c\u673a\u5668\u4eba\u73ca\u745a\u517b\u6b96\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7cbe\u7ec6\u64cd\u4f5c\u4e2d\u7684\u9ad8\u7075\u654f\u5ea6\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7269\u7406\u5b6a\u751f\u6280\u672f\u4e3a\u8106\u5f31\u6837\u672c\u7684\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5408\u4f26\u7406\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4f53\u64cd\u4f5c\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u4e0e\u5b9e\u9a8c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.16813", "categories": ["cs.CL", "I.2.7; H.3.1; I.5.4; J.4"], "pdf": "https://arxiv.org/pdf/2509.16813", "abs": "https://arxiv.org/abs/2509.16813", "authors": ["Devin R. Wright", "Jisun An", "Yong-Yeol Ahn"], "title": "Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text", "comment": "Authors' accepted manuscript (postprint; camera-ready). To appear in\n  the Proceedings of EMNLP 2025. Pagination/footer layout may differ from the\n  Version of Record", "summary": "Quantifying identity fusion -- the psychological merging of self with another\nentity or abstract target (e.g., a religious group, political party, ideology,\nvalue, brand, belief, etc.) -- is vital for understanding a wide range of\ngroup-based human behaviors. We introduce the Cognitive Linguistic Identity\nFusion Score (CLIFS), a novel metric that integrates cognitive linguistics with\nlarge language models (LLMs), which builds on implicit metaphor detection.\nUnlike traditional pictorial and verbal scales, which require controlled\nsurveys or direct field contact, CLIFS delivers fully automated, scalable\nassessments while maintaining strong alignment with the established verbal\nmeasure. In benchmarks, CLIFS outperforms both existing automated approaches\nand human annotation. As a proof of concept, we apply CLIFS to violence risk\nassessment to demonstrate that it can improve violence risk assessment by more\nthan 240%. Building on our identification of a new NLP task and early success,\nwe underscore the need to develop larger, more diverse datasets that encompass\nadditional fusion-target domains and cultural backgrounds to enhance\ngeneralizability and further advance this emerging area. CLIFS models and code\nare public at https://github.com/DevinW-sudo/CLIFS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8ba4\u77e5\u8bed\u8a00\u5b66\u8eab\u4efd\u878d\u5408\u5206\u6570\uff08CLIFS\uff09\u7684\u65b0\u6307\u6807\uff0c\u7ed3\u5408\u8ba4\u77e5\u8bed\u8a00\u5b66\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9690\u55bb\u68c0\u6d4b\u81ea\u52a8\u91cf\u5316\u4e2a\u4f53\u4e0e\u7fa4\u4f53\u6216\u62bd\u8c61\u76ee\u6807\u7684\u8eab\u4efd\u878d\u5408\u7a0b\u5ea6\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\uff0c\u5e76\u5728\u66b4\u529b\u98ce\u9669\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8d85\u8fc7240%\u7684\u63d0\u5347\u3002", "motivation": "\u8eab\u4efd\u878d\u5408\u662f\u7406\u89e3\u591a\u79cd\u7fa4\u4f53\u884c\u4e3a\u7684\u5173\u952e\u5fc3\u7406\u56e0\u7d20\uff0c\u4f46\u73b0\u6709\u6d4b\u91cf\u65b9\u6cd5\u4f9d\u8d56\u95ee\u5377\u6216\u5b9e\u5730\u8c03\u67e5\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u91cf\u5316\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u9690\u55bb\u68c0\u6d4b\uff0c\u5c06\u8ba4\u77e5\u8bed\u8a00\u5b66\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u6784\u5efaCLIFS\u6307\u6807\uff0c\u5b9e\u73b0\u5bf9\u8eab\u4efd\u878d\u5408\u7684\u81ea\u52a8\u5316\u5206\u6790\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u548c\u6bd4\u8f83\u3002", "result": "CLIFS\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u4e0e\u4f20\u7edf\u8a00\u8bed\u91cf\u8868\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u5728\u66b4\u529b\u98ce\u9669\u8bc4\u4f30\u4efb\u52a1\u4e2d\u4f7f\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u8d85\u8fc7240%\u3002", "conclusion": "CLIFS\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8eab\u4efd\u878d\u5408\u81ea\u52a8\u5316\u6d4b\u91cf\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u672a\u6765\u9700\u6784\u5efa\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16959", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.16959", "abs": "https://arxiv.org/abs/2509.16959", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "title": "Gradient Interference-Aware Graph Coloring for Multitask Learning", "comment": null, "summary": "When different objectives conflict with each other in multi-task learning,\ngradients begin to interfere and slow convergence, thereby reducing the final\nmodel's performance. To address this, we introduce a scheduler that computes\ngradient interference, constructs an interference graph, and then applies\ngreedy graph-coloring to partition tasks into groups that align well with each\nother. At each training step, only one group (color class) of tasks are\nactivated. The grouping partition is constantly recomputed as task\nrelationships evolve throughout training. By ensuring that each mini-batch\ncontains only tasks that pull the model in the same direction, our method\nimproves the effectiveness of any underlying multi-task learning optimizer\nwithout additional tuning. Since tasks within these groups will update in\ncompatible directions, model performance will be improved rather than impeded.\nEmpirical results on six different datasets show that this interference-aware\ngraph-coloring approach consistently outperforms baselines and state-of-the-art\nmulti-task optimizers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u5e72\u6270\u611f\u77e5\u7684\u56fe\u7740\u8272\u4efb\u52a1\u5206\u7ec4\u8c03\u5ea6\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u76ee\u6807\u51b2\u7a81\u5bfc\u81f4\u7684\u6536\u655b\u6162\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\uff0c\u4e0d\u540c\u4efb\u52a1\u7684\u68af\u5ea6\u53ef\u80fd\u76f8\u4e92\u5e72\u6270\uff0c\u5bfc\u81f4\u4f18\u5316\u65b9\u5411\u51b2\u7a81\u3001\u6536\u655b\u53d8\u6162\u5e76\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u534f\u8c03\u51b2\u7a81\u4efb\u52a1\u7684\u66f4\u65b0\u65b9\u5411\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u52a8\u6001\u4e14\u81ea\u9002\u5e94\u7684\u4efb\u52a1\u8c03\u5ea6\u673a\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u4efb\u52a1\u95f4\u7684\u68af\u5ea6\u5e72\u6270\uff0c\u6784\u5efa\u5e72\u6270\u56fe\uff0c\u5e76\u901a\u8fc7\u8d2a\u5fc3\u56fe\u7740\u8272\u7b97\u6cd5\u5c06\u4efb\u52a1\u5212\u5206\u4e3a\u591a\u4e2a\u7ec4\uff08\u989c\u8272\u7c7b\uff09\uff0c\u6bcf\u6b65\u4ec5\u6fc0\u6d3b\u4e00\u7ec4\u4efb\u52a1\u8fdb\u884c\u66f4\u65b0\uff1b\u8be5\u5206\u7ec4\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u91cd\u8ba1\u7b97\uff0c\u786e\u4fdd\u6bcf\u6279\u6b21\u4e2d\u7684\u4efb\u52a1\u4f18\u5316\u65b9\u5411\u4e00\u81f4\uff0c\u4ece\u800c\u63d0\u5347\u4efb\u610f\u5e95\u5c42\u591a\u4efb\u52a1\u4f18\u5316\u5668\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u57fa\u51c6\u548c\u6700\u5148\u8fdb\u7684\u591a\u4efb\u52a1\u4f18\u5316\u5668\u4e0a\u5747\u53d6\u5f97\u66f4\u4f18\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u68af\u5ea6\u5e72\u6270\u5e76\u52a0\u5feb\u6536\u655b\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u548c\u534f\u8c03\u4efb\u52a1\u95f4\u7684\u68af\u5ea6\u5e72\u6270\uff0c\u6240\u63d0\u51fa\u7684\u56fe\u7740\u8272\u4efb\u52a1\u5206\u7ec4\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2509.17553", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17553", "abs": "https://arxiv.org/abs/2509.17553", "authors": ["Congcong Ge", "Yachuan Liu", "Yixuan Tang", "Yifan Zhu", "Yaofeng Tu", "Yunjun Gao"], "title": "MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances", "comment": null, "summary": "In commercial systems, a pervasive requirement for automatic data preparation\n(ADP) is to transfer relational data from disparate sources to targets with\nstandardized schema specifications. Previous methods rely on labor-intensive\nsupervision signals or target table data access permissions, limiting their\nusage in real-world scenarios. To tackle these challenges, we propose an\neffective end-to-end ADP framework MontePrep, which enables training-free\npipeline synthesis with zero target-instance requirements. MontePrep is\nformulated as an open-source large language model (LLM) powered tree-structured\nsearch problem. It consists of three pivot components, i.e., a data preparation\naction sandbox (DPAS), a fundamental pipeline generator (FPG), and an\nexecution-aware pipeline optimizer (EPO). We first introduce DPAS, a\nlightweight action sandbox, to navigate the search-based pipeline generation.\nThe design of DPAS circumvents exploration of infeasible pipelines. Then, we\npresent FPG to build executable DP pipelines incrementally, which explores the\npredefined action sandbox by the LLM-powered Monte Carlo Tree Search.\nFurthermore, we propose EPO, which invokes pipeline execution results from\nsources to targets to evaluate the reliability of the generated pipelines in\nFPG. In this way, unreasonable pipelines are eliminated, thus facilitating the\nsearch process from both efficiency and effectiveness perspectives. Extensive\nexperimental results demonstrate the superiority of MontePrep with significant\nimprovement against five state-of-the-art competitors.", "AI": {"tldr": "\u63d0\u51faMontePrep\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u76ee\u6807\u5b9e\u4f8b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u6811\u641c\u7d22\u751f\u6210\u9ad8\u6548\u53ef\u9760\u7684\u6570\u636e\u51c6\u5907\u6d41\u6c34\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u76ee\u6807\u8868\u6570\u636e\u6743\u9650\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u514d\u8bad\u7ec3\u4e14\u65e0\u9700\u76ee\u6807\u5b9e\u4f8b\u8bbf\u95ee\u7684\u81ea\u52a8\u5316\u6570\u636e\u51c6\u5907\u65b9\u6848\u3002", "method": "\u5c06\u81ea\u52a8\u6570\u636e\u51c6\u5907\u5efa\u6a21\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6811\u7ed3\u6784\u641c\u7d22\u95ee\u9898\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6570\u636e\u51c6\u5907\u52a8\u4f5c\u6c99\u76d2\uff08DPAS\uff09\u3001\u57fa\u7840\u6d41\u6c34\u7ebf\u751f\u6210\u5668\uff08FPG\uff09\u548c\u6267\u884c\u611f\u77e5\u6d41\u6c34\u7ebf\u4f18\u5316\u5668\uff08EPO\uff09\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5728\u6c99\u76d2\u4e2d\u9010\u6b65\u6784\u5efa\u5e76\u4f18\u5316\u53ef\u6267\u884c\u6d41\u6c34\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMontePrep\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "MontePrep\u4e3a\u5b9e\u9645\u5546\u4e1a\u7cfb\u7edf\u4e2d\u7684\u81ea\u52a8\u6570\u636e\u51c6\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u514d\u8bad\u7ec3\u4e14\u65e0\u9700\u76ee\u6807\u6570\u636e\u8bbf\u95ee\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.16680", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16680", "abs": "https://arxiv.org/abs/2509.16680", "authors": ["Xingjian Diao", "Weiyi Wu", "Keyi Kong", "Peijun Qing", "Xinwen Xu", "Ming Cheng", "Soroush Vosoughi", "Jiang Gui"], "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Visual Question Answering (VQA) is increasingly used in diverse applications\nranging from general visual reasoning to safety-critical domains such as\nmedical imaging and autonomous systems, where models must provide not only\naccurate answers but also explanations that humans can easily understand and\nverify. Prototype-based modeling has shown promise for interpretability by\ngrounding predictions in semantically meaningful regions for purely visual\nreasoning tasks, yet remains underexplored in the context of VQA. We present\nProtoVQA, a unified prototypical framework that (i) learns question-aware\nprototypes that serve as reasoning anchors, connecting answers to\ndiscriminative image regions, (ii) applies spatially constrained matching to\nensure that the selected evidence is coherent and semantically relevant, and\n(iii) supports both answering and grounding tasks through a shared prototype\nbackbone. To assess explanation quality, we propose the Visual-Linguistic\nAlignment Score (VLAS), which measures how well the model's attended regions\nalign with ground-truth evidence. Experiments on Visual7W show that ProtoVQA\nyields faithful, fine-grained explanations while maintaining competitive\naccuracy, advancing the development of transparent and trustworthy VQA systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ProtoVQA\uff0c\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u7684\u89c6\u89c9\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u95ee\u9898\u611f\u77e5\u7684\u539f\u578b\u4f5c\u4e3a\u63a8\u7406\u951a\u70b9\uff0c\u7ed3\u5408\u7a7a\u95f4\u7ea6\u675f\u5339\u914d\u673a\u5236\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u4e14\u51c6\u786e\u7684\u7b54\u6848\uff0c\u5e76\u63d0\u51faVLAS\u6307\u6807\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684VQA\u6a21\u578b\u5728\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u53ef\u4fe1\u548c\u900f\u660e\u51b3\u7b56\u7684\u5b89\u5168\u5173\u952e\u9886\u57df\u3002\u539f\u578b\u65b9\u6cd5\u867d\u5728\u7eaf\u89c6\u89c9\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u89e3\u91ca\u6f5c\u529b\uff0c\u4f46\u5728VQA\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faProtoVQA\u6846\u67b6\uff1a(1) \u5b66\u4e60\u95ee\u9898\u611f\u77e5\u7684\u539f\u578b\u4f5c\u4e3a\u8fde\u63a5\u7b54\u6848\u4e0e\u56fe\u50cf\u5173\u952e\u533a\u57df\u7684\u63a8\u7406\u951a\u70b9\uff1b(2) \u91c7\u7528\u7a7a\u95f4\u53d7\u9650\u5339\u914d\u786e\u4fdd\u6240\u9009\u8bc1\u636e\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\uff1b(3) \u901a\u8fc7\u5171\u4eab\u539f\u578b\u4e3b\u5e72\u540c\u65f6\u652f\u6301\u56de\u7b54\u4e0e\u5b9a\u4f4d\u4efb\u52a1\uff1b\u5e76\u63d0\u51fa\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u8bc4\u5206\uff08VLAS\uff09\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\u3002", "result": "\u5728Visual7W\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cProtoVQA\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u80fd\u751f\u6210\u66f4\u5fe0\u5b9e\u3001\u7ec6\u7c92\u5ea6\u7684\u89e3\u91ca\uff0c\u4e14VLAS\u6709\u6548\u8861\u91cf\u4e86\u6ce8\u610f\u529b\u533a\u57df\u4e0e\u771f\u5b9e\u8bc1\u636e\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "conclusion": "ProtoVQA\u901a\u8fc7\u539f\u578b\u5316\u63a8\u7406\u63d0\u5347\u4e86VQA\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u6784\u5efa\u900f\u660e\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17390", "categories": ["cs.RO", "68T40, 68U05", "I.6.8"], "pdf": "https://arxiv.org/pdf/2509.17390", "abs": "https://arxiv.org/abs/2509.17390", "authors": ["Junzhe Wu", "Yufei Jia", "Yiyi Yan", "Zhixing Chen", "Tiao Tan", "Zifan Wang", "Guangyu Wang"], "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR", "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic\nrendering, its vast ecosystem of assets remains incompatible with\nhigh-performance LiDAR simulation, a critical tool for robotics and autonomous\ndriving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with\na truly plug-and-play approach. Our method converts \\textit{any} pretrained\n3DGS model into a high-fidelity, watertight mesh without requiring\nLiDAR-specific supervision or architectural alterations. This conversion is\nachieved through a general pipeline of volumetric discretization and Truncated\nSigned Distance Field (TSDF) extraction. We pair this with a highly optimized,\nGPU-accelerated ray-casting module that simulates LiDAR returns at over 500\nFPS. We validate our approach on indoor and outdoor scenes, demonstrating\nexceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for\ngeometrically accurate depth sensing, our framework extends their utility\nbeyond visualization and unlocks new capabilities for scalable, multimodal\nsimulation. Our open-source implementation is available at\nhttps://github.com/TATP-233/FGGS-LiDAR.", "AI": {"tldr": "\u63d0\u51faFGGS-LiDAR\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u76843D\u9ad8\u65af\u70b9\u9635\u6a21\u578b\u65e0\u7f1d\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771f\u3001\u6c34\u5bc6\u7f51\u683c\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fdLiDAR\u6a21\u62df\uff0c\u652f\u6301500 FPS\u4ee5\u4e0a\u7684GPU\u52a0\u901f\u4eff\u771f\uff0c\u65e0\u9700LiDAR\u7279\u5b9a\u76d1\u7763\uff0c\u6269\u5c55\u4e863DGS\u8d44\u4ea7\u5728\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6a21\u6001\u5e94\u7528\u3002", "motivation": "3D\u9ad8\u65af\u70b9\u9635\uff083DGS\uff09\u867d\u5728\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8d44\u4ea7\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u9ad8\u6027\u80fdLiDAR\u6a21\u62df\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7279\u5b9a\u76d1\u7763\u5373\u53ef\u517c\u5bb9LiDAR\u4eff\u771f\u7684\u901a\u7528\u8f6c\u6362\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4f53\u7d20\u5316\u79bb\u6563\u5316\u548c\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a\uff08TSDF\uff09\u63d0\u53d6\u7684\u901a\u7528\u6d41\u7a0b\uff0c\u5c06\u4efb\u610f\u9884\u8bad\u7ec33DGS\u6a21\u578b\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u6c34\u5bc6\u7f51\u683c\uff0c\u5e76\u7ed3\u5408GPU\u52a0\u901f\u7684\u5149\u7ebf\u6295\u5c04\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548LiDAR\u56de\u6ce2\u6a21\u62df\u3002", "result": "\u5728\u5ba4\u5185\u5916\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u751f\u6210\u7684\u7f51\u683c\u5177\u6709\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0cLiDAR\u6a21\u62df\u901f\u5ea6\u8d85\u8fc7500 FPS\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u67093DGS\u8d44\u4ea7\u7684\u5373\u63d2\u5373\u7528\u517c\u5bb9\u3002", "conclusion": "FGGS-LiDAR\u5b9e\u73b0\u4e863DGS\u6a21\u578b\u5230LiDAR\u4eff\u771f\u7684\u65e0\u7f1d\u6865\u63a5\uff0c\u62d3\u5c55\u4e863DGS\u5728\u6df1\u5ea6\u611f\u77e5\u548c\u591a\u6a21\u6001\u4eff\u771f\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e14\u5f00\u6e90\u5b9e\u73b0\u4fbf\u4e8e\u540e\u7eed\u7814\u7a76\u4e0e\u5e94\u7528\u3002"}}
{"id": "2509.16835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16835", "abs": "https://arxiv.org/abs/2509.16835", "authors": ["Melkamu Abay Mersha", "Jugal Kalita"], "title": "Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming", "comment": null, "summary": "Virtual brainstorming sessions have become a central component of\ncollaborative problem solving, yet the large volume and uneven distribution of\nideas often make it difficult to extract valuable insights efficiently. Manual\ncoding of ideas is time-consuming and subjective, underscoring the need for\nautomated approaches to support the evaluation of group creativity. In this\nstudy, we propose a semantic-driven topic modeling framework that integrates\nfour modular components: transformer-based embeddings (Sentence-BERT),\ndimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction\nwith refinement. The framework captures semantic similarity at the sentence\nlevel, enabling the discovery of coherent themes from brainstorming transcripts\nwhile filtering noise and identifying outliers. We evaluate our approach on\nstructured Zoom brainstorming sessions involving student groups tasked with\nimproving their university. Results demonstrate that our model achieves higher\ntopic coherence compared to established methods such as LDA, ETM, and BERTopic,\nwith an average coherence score of 0.687 (CV), outperforming baselines by a\nsignificant margin. Beyond improved performance, the model provides\ninterpretable insights into the depth and diversity of topics explored,\nsupporting both convergent and divergent dimensions of group creativity. This\nwork highlights the potential of embedding-based topic modeling for analyzing\ncollaborative ideation and contributes an efficient and scalable framework for\nstudying creativity in synchronous virtual meetings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7684\u4e3b\u984c\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52d5\u5206\u6790\u865b\u64ec\u982d\u8166\u98a8\u66b4\u4e2d\u7684\u5275\u610f\uff0c\u8868\u73fe\u512a\u65bc\u50b3\u7d71\u65b9\u6cd5\u3002", "motivation": "\u865b\u64ec\u982d\u8166\u98a8\u66b4\u7522\u751f\u5927\u91cf\u4e14\u5206\u4f48\u4e0d\u5747\u7684\u60f3\u6cd5\uff0c\u4eba\u5de5\u7de8\u78bc\u8017\u6642\u4e14\u4e3b\u89c0\uff0c\u9700\u8981\u81ea\u52d5\u5316\u65b9\u6cd5\u4f86\u6709\u6548\u8a55\u4f30\u7fa4\u9ad4\u5275\u9020\u529b\u3002", "method": "\u7d50\u5408Sentence-BERT\u3001UMAP\u3001HDBSCAN\u548c\u4e3b\u984c\u63d0\u53d6\u8207\u512a\u5316\u7684\u56db\u6a21\u584a\u6846\u67b6\uff0c\u9032\u884c\u8a9e\u7fa9\u9a45\u52d5\u7684\u4e3b\u984c\u5efa\u6a21\u3002", "result": "\u5728Zoom\u982d\u8166\u98a8\u66b4\u6578\u64da\u4e0a\u9a57\u8b49\uff0c\u6a21\u578b\u4e3b\u984c\u4e00\u81f4\u6027\u5e73\u5747\u5f97\u52060.687\uff08CV\uff09\uff0c\u986f\u8457\u512a\u65bcLDA\u3001ETM\u548cBERTopic\u7b49\u57fa\u7dda\u65b9\u6cd5\u3002", "conclusion": "\u8a72\u6846\u67b6\u80fd\u6709\u6548\u63d0\u53d6\u9ad8\u8cea\u91cf\u4e3b\u984c\uff0c\u63d0\u4f9b\u5c0d\u7fa4\u9ad4\u5275\u9020\u529b\u7684\u53ef\u89e3\u91cb\u6d1e\u5bdf\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u64f4\u5c55\u6027\uff0c\u9069\u7528\u65bc\u5206\u6790\u540c\u6b65\u865b\u64ec\u6703\u8b70\u4e2d\u7684\u5354\u540c\u5275\u610f\u3002"}}
{"id": "2509.16989", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16989", "abs": "https://arxiv.org/abs/2509.16989", "authors": ["He Xiao", "Runming Yang", "Qingyao Yang", "Wendong Xu", "Zheng Li", "Yupeng Su", "Zhengwu Liu", "Hongxia Yang", "Ngai Wong"], "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models", "comment": "under review", "summary": "Post-training quantization (PTQ) of large language models (LLMs) to extremely\nlow bit-widths remains challenging due to the fundamental trade-off between\ncomputational efficiency and model expressiveness. While existing ultra-low-bit\nPTQ methods rely on binary approximations or complex compensation mechanisms,\nthey suffer from either limited representational capacity or computational\noverhead that undermines their efficiency gains. We introduce PTQ to\nTrit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes\nweight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit\nrepresentation. PTQTP achieves multiplication-free inference, identical to\n1-bit quantization, while maintaining superior expressiveness through its novel\nstructured decomposition. Our approach provides: (1) a theoretically grounded\nprogressive approximation algorithm ensuring global weight consistency; (2)\nmodel-agnostic deployment across diverse modern LLMs without architectural\nmodifications; and (3) uniform ternary operations that eliminate the need for\nmixed-precision or compensation schemes. Comprehensive experiments across\nLLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP\nsignificantly outperforms existing low-bit PTQ methods, achieving 82.4%\nmathematical reasoning retention versus 0% for competing approaches. PTQTP\napproaches and sometimes surpasses 1.58-bit quantization-aware training\nperformance while requiring only single-hour quantization compared to 10-14 GPU\ndays for training-based methods. These results establish PTQTP as a practical\nsolution for efficient LLM deployment in resource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPTQTP\u7684\u65b0\u578b\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7684\u4e09\u503c{-1, 0, 1}\u4e09\u8fdb\u5236\u5e73\u9762\uff0c\u4f7f\u75282x1.58-bit\u8868\u793a\uff0c\u5728\u4fdd\u6301\u4e58\u6cd5\u81ea\u7531\u63a8\u7406\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6781\u4f4e\u6bd4\u7279\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u8868\u8fbe\u529b\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6743\u8861\uff0c\u4e8c\u503c\u5316\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u800c\u590d\u6742\u8865\u507f\u673a\u5236\u5e26\u6765\u989d\u5916\u5f00\u9500\uff0c\u96be\u4ee5\u517c\u987e\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u63d0\u51faPTQ\u5230Trit-Planes\uff08PTQTP\uff09\u6846\u67b6\uff0c\u5c06\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u4e09\u503c\u4e09\u8fdb\u5236\u5e73\u9762\uff0c\u91c7\u75282x1.58-bit\u8868\u793a\uff0c\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u8fd1\u4f3c\u7b97\u6cd5\u4fdd\u8bc1\u5168\u5c40\u6743\u91cd\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u65e0\u9700\u6df7\u5408\u7cbe\u5ea6\u6216\u8865\u507f\u673a\u5236\u7684\u7edf\u4e00\u4e09\u503c\u8fd0\u7b97\u3002", "result": "\u5728LLaMA3.x\u548cQwen3\u7cfb\u5217\u6a21\u578b\uff080.6B-70B\u53c2\u6570\uff09\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cPTQTP\u6570\u5b66\u63a8\u7406\u4fdd\u7559\u7387\u8fbe82.4%\uff0c\u8fdc\u8d85\u7ade\u54c1\u76840%\uff1b\u6027\u80fd\u63a5\u8fd1\u751a\u81f3\u8d85\u8fc71.58\u4f4d\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f46\u4ec5\u9700\u5355\u5c0f\u65f6\u91cf\u5316\uff0c\u8fdc\u5feb\u4e8e\u8bad\u7ec3\u65b9\u6cd5\u6240\u9700\u768410-14 GPU\u5929\u3002", "conclusion": "PTQTP\u5728\u4fdd\u6301\u4e58\u6cd5\u81ea\u7531\u3001\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff0c\u6210\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5927\u6a21\u578b\u90e8\u7f72\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17567", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17567", "abs": "https://arxiv.org/abs/2509.17567", "authors": ["Yang Xiao", "Mohan Jiang", "Jie Sun", "Keyu Li", "Jifan Lin", "Yumin Zhuang", "Ji Zeng", "Shijie Xia", "Qishuo Hua", "Xuefeng Li", "Xiaojie Cai", "Tongyu Wang", "Yue Zhang", "Liming Liu", "Xia Wu", "Jinlong Hou", "Yuan Cheng", "Wenjie Li", "Xiang Wang", "Dequan Wang", "Pengfei Liu"], "title": "LIMI: Less is More for Agency", "comment": null, "summary": "We define Agency as the emergent capacity of AI systems to function as\nautonomous agents actively discovering problems, formulating hypotheses, and\nexecuting solutions through self-directed engagement with environments and\ntools. This fundamental capability marks the dawn of the Age of AI Agency,\ndriven by a critical industry shift: the urgent need for AI systems that don't\njust think, but work. While current AI excels at reasoning and generating\nresponses, industries demand autonomous agents that can execute tasks, operate\ntools, and drive real-world outcomes. As agentic intelligence becomes the\ndefining characteristic separating cognitive systems from productive workers,\nefficiently cultivating machine autonomy becomes paramount. Current approaches\nassume that more data yields better agency, following traditional scaling laws\nfrom language modeling. We fundamentally challenge this paradigm. LIMI (Less Is\nMore for Intelligent Agency) demonstrates that agency follows radically\ndifferent development principles. Through strategic focus on collaborative\nsoftware development and scientific research workflows, we show that\nsophisticated agentic intelligence can emerge from minimal but strategically\ncurated demonstrations of autonomous behavior. Using only 78 carefully designed\ntraining samples, LIMI achieves 73.5% on comprehensive agency benchmarks,\ndramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),\nDeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).\nMost strikingly, LIMI demonstrates 53.7% improvement over models trained on\n10,000 samples-achieving superior agentic intelligence with 128 times fewer\nsamples. Our findings establish the Agency Efficiency Principle: machine\nautonomy emerges not from data abundance but from strategic curation of\nhigh-quality agentic demonstrations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u4ee3\u7406\u6027\u201d\uff08Agency\uff09\u4f5c\u4e3aAI\u7cfb\u7edf\u81ea\u4e3b\u53d1\u73b0\u3001\u89c4\u5212\u548c\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51faLIMI\uff08\u5c11\u5373\u662f\u591a\uff09\u539f\u5219\uff1a\u901a\u8fc7\u5c11\u91cf\u4f46\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u9ad8\u6548\u57f9\u517bAI\u4ee3\u7406\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u4f9d\u8d56\u5927\u6570\u636e\u7684\u8303\u5f0f\u3002", "motivation": "\u5f53\u524dAI\u867d\u64c5\u957f\u63a8\u7406\u4e0e\u751f\u6210\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u9645\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\uff1b\u5404\u884c\u4e1a\u4e9f\u9700\u80fd\u4e3b\u52a8\u64cd\u4f5c\u5de5\u5177\u3001\u63a8\u52a8\u73b0\u5b9e\u6210\u679c\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u9ad8\u6548\u57f9\u80b2\u673a\u5668\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51faLIMI\u6846\u67b6\uff0c\u57fa\u4e8e78\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u81ea\u4e3b\u884c\u4e3a\u793a\u8303\uff0c\u5728\u534f\u4f5c\u8f6f\u4ef6\u5f00\u53d1\u4e0e\u79d1\u7814\u6d41\u7a0b\u4e2d\u8bad\u7ec3AI\u4ee3\u7406\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u7efc\u5408\u4ee3\u7406\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "LIMI\u5728\u4ee3\u7406\u57fa\u51c6\u4e0a\u8fbe\u523073.5%\uff0c\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u5148\u8fdb\u6a21\u578b\uff08\u5982Kimi-K2\u3001DeepSeek-V3.1\u7b49\uff09\uff0c\u4e14\u76f8\u6bd4\u4f7f\u752810,000\u6837\u672c\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4ec5\u75281/128\u7684\u6570\u636e\u91cf\u5373\u5b9e\u73b053.7%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u673a\u5668\u4ee3\u7406\u6027\u7684\u6d8c\u73b0\u4e0d\u4f9d\u8d56\u6570\u636e\u89c4\u6a21\uff0c\u800c\u53d6\u51b3\u4e8e\u9ad8\u8d28\u91cf\u3001\u6218\u7565\u6027\u7b5b\u9009\u7684\u793a\u8303\u6837\u672c\uff0c\u63d0\u51fa\u4e86\u2018\u4ee3\u7406\u6548\u7387\u539f\u5219\u2019\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u81ea\u4e3bAI\u6307\u660e\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.16684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16684", "abs": "https://arxiv.org/abs/2509.16684", "authors": ["Qi Zhang", "Bin Li", "Antoni B. Chan", "Hui Huang"], "title": "Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels", "comment": "8 pages, 5 figures", "summary": "Multi-view crowd counting and localization fuse the input multi-views for\nestimating the crowd number or locations on the ground. Existing methods mainly\nfocus on accurately predicting on the crowd shown in the input views, which\nneglects the problem of choosing the `best' camera views to perceive all crowds\nwell in the scene. Besides, existing view selection methods require massive\nlabeled views and images, and lack the ability for cross-scene settings,\nreducing their application scenarios. Thus, in this paper, we study the view\nselection issue for better scene-level multi-view crowd counting and\nlocalization results with cross-scene ability and limited label demand, instead\nof input-view-level results. We first propose an independent view selection\nmethod (IVS) that considers view and scene geometries in the view selection\nstrategy and conducts the view selection, labeling, and downstream tasks\nindependently. Based on IVS, we also put forward an active view selection\nmethod (AVS) that jointly optimizes the view selection, labeling, and\ndownstream tasks. In AVS, we actively select the labeled views and consider\nboth the view/scene geometries and the predictions of the downstream task\nmodels in the view selection process. Experiments on multi-view counting and\nlocalization tasks demonstrate the cross-scene and the limited label demand\nadvantages of the proposed active view selection method (AVS), outperforming\nexisting methods and with wider application scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u89c6\u89d2\u4eba\u7fa4\u8ba1\u6570\u4e0e\u5b9a\u4f4d\u4e2d\u7684\u89c6\u56fe\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u8de8\u573a\u666f\u80fd\u529b\u548c\u6709\u9650\u6807\u7b7e\u9700\u6c42\u7684\u4e3b\u52a8\u89c6\u56fe\u9009\u62e9\u65b9\u6cd5\uff08AVS\uff09\uff0c\u5728\u591a\u89c6\u89d2\u8ba1\u6570\u4e0e\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u5165\u89c6\u56fe\u5185\u4eba\u7fa4\u7684\u51c6\u786e\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u5982\u4f55\u9009\u62e9\u6700\u4f73\u6444\u50cf\u673a\u89c6\u56fe\u4ee5\u5168\u9762\u611f\u77e5\u573a\u666f\u4e2d\u7684\u4eba\u7fa4\uff0c\u4e14\u73b0\u6709\u89c6\u56fe\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u7f3a\u4e4f\u8de8\u573a\u666f\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u72ec\u7acb\u89c6\u56fe\u9009\u62e9\u65b9\u6cd5\uff08IVS\uff09\u548c\u4e3b\u52a8\u89c6\u56fe\u9009\u62e9\u65b9\u6cd5\uff08AVS\uff09\u3002IVS\u8003\u8651\u89c6\u56fe\u4e0e\u573a\u666f\u51e0\u4f55\u7ed3\u6784\uff0c\u72ec\u7acb\u8fdb\u884c\u89c6\u56fe\u9009\u62e9\u3001\u6807\u6ce8\u548c\u4e0b\u6e38\u4efb\u52a1\uff1bAVS\u5219\u8054\u5408\u4f18\u5316\u8fd9\u4e09\u4e2a\u6b65\u9aa4\uff0c\u5e76\u5728\u9009\u62e9\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u4e0b\u6e38\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAVS\u5728\u8de8\u573a\u666f\u8bbe\u7f6e\u548c\u6709\u9650\u6807\u7b7e\u9700\u6c42\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u4eba\u7fa4\u8ba1\u6570\u4e0e\u5b9a\u4f4d\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684AVS\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u56fe\u9009\u62e9\u3001\u6807\u6ce8\u4e0e\u4e0b\u6e38\u4efb\u52a1\uff0c\u517c\u987e\u89c6\u56fe/\u573a\u666f\u51e0\u4f55\u4e0e\u6a21\u578b\u9884\u6d4b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u89c6\u89d2\u4eba\u7fa4\u5206\u6790\u7684\u6548\u7387\u4e0e\u9002\u7528\u6027\u3002"}}
{"id": "2509.17435", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17435", "abs": "https://arxiv.org/abs/2509.17435", "authors": ["Xiaoyu Wang", "Yan Rui Tan", "William Leong", "Sunan Huang", "Rodney Teo", "Cheng Xiang"], "title": "GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera", "comment": null, "summary": "This paper proposes an image-based visual servoing (IBVS) framework for UAV\nnavigation and collision avoidance using only an RGB camera. While UAV\nnavigation has been extensively studied, it remains challenging to apply IBVS\nin missions involving multiple visual targets and collision avoidance. The\nproposed method achieves navigation without explicit path planning, and\ncollision avoidance is realized through AI-based monocular depth estimation\nfrom RGB images. Unlike approaches that rely on stereo cameras or external\nworkstations, our framework runs fully onboard a Jetson platform, ensuring a\nself-contained and deployable system. Experimental results validate that the\nUAV can navigate across multiple AprilTags and avoid obstacles effectively in\nGPS-denied environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u89c6\u89c9\u4f3a\u670d\uff08IBVS\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u5b9e\u73b0\u65e0\u4eba\u673a\u5bfc\u822a\u4e0e\u907f\u969c\u3002", "motivation": "\u5728\u6d89\u53ca\u591a\u4e2a\u89c6\u89c9\u76ee\u6807\u548c\u907f\u969c\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edfIBVS\u65b9\u6cd5\u5e94\u7528\u56f0\u96be\uff0c\u4e14\u4f9d\u8d56\u5916\u90e8\u8bbe\u5907\u6216\u590d\u6742\u8def\u5f84\u89c4\u5212\u3002", "method": "\u5229\u7528AI\u9a71\u52a8\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5b9e\u73b0\u907f\u969c\uff0c\u5e76\u5728Jetson\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5168\u673a\u8f7d\u8fd0\u884c\uff0c\u65e0\u9700\u663e\u5f0f\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u5728\u65e0GPS\u73af\u5883\u4e0b\u6709\u6548\u7a7f\u8d8a\u591a\u4e2aAprilTag\u5e76\u6210\u529f\u907f\u969c\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\u3001\u81ea\u5305\u542b\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u4e0e\u907f\u969c\uff0c\u9002\u7528\u4e8e\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e0b\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.16876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16876", "abs": "https://arxiv.org/abs/2509.16876", "authors": ["Jiun-Ting Li", "Bi-Cheng Yan", "Yi-Cheng Wang", "Berlin Chen"], "title": "Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment", "comment": "Accepted by APSIPA-ASC 2025", "summary": "Automatic pronunciation assessment (APA) analyzes second-language (L2)\nlearners' speech by providing fine-grained pronunciation feedback at various\nlinguistic levels. Most existing efforts on APA typically adopt segmental-level\nfeatures as inputs and predict pronunciation scores at different granularities\nvia hierarchical (or parallel) pronunciation modeling. This, however,\ninevitably causes assessments across linguistic levels (e.g., phone, word, and\nutterance) to rely solely on phoneme-level pronunciation features, nearly\nsidelining supra-segmental pronunciation cues. To address this limitation, we\nintroduce multi-task pretraining (MTP) for APA, a simple yet effective strategy\nthat attempts to capture long-term temporal pronunciation cues while\nstrengthening the intrinsic structures within an utterance via the objective of\nreconstructing input features. Specifically, for a phoneme-level encoder of an\nAPA model, the proposed MTP strategy randomly masks segmental-level\npronunciation features and reconstructs the masked ones based on their\nsurrounding pronunciation context. Furthermore, current APA systems lack\nintegration with automated speaking assessment (ASA), limiting holistic\nproficiency evaluation. Drawing on empirical studies and prior knowledge in\nASA, our framework bridges this gap by incorporating handcrafted features\n(HCFs), such as fluency (speech rate, silence duration) and stress (pitch\naccent strength), derived from human-designed formulas via regressors to\ngenerate interpretable proficiency scores. Experiments on speechocean762 show\nimproved pronunciation scoring and ASA proficiency correlation, enabling\ntargeted training and comprehensive proficiency assessment.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\uff08MTP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u8bed\u97f3\u7279\u5f81\u6355\u6349\u8d85\u97f3\u6bb5\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u624b\u5de5\u7279\u5f81\u5b9e\u73b0\u53d1\u97f3\u8bc4\u4f30\u4e0e\u53e3\u8bed\u80fd\u529b\u6d4b\u8bc4\u7684\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u53d1\u97f3\u8bc4\u4f30\uff08APA\uff09\u4e3b\u8981\u4f9d\u8d56\u97f3\u7d20\u7ea7\u7279\u5f81\uff0c\u5ffd\u7565\u8d85\u97f3\u6bb5\u8bed\u97f3\u7ebf\u7d22\uff0c\u4e14\u7f3a\u4e4f\u4e0e\u81ea\u52a8\u5316\u53e3\u8bed\u8bc4\u4f30\uff08ASA\uff09\u7684\u6574\u5408\uff0c\u9650\u5236\u4e86\u6574\u4f53\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\uff08MTP\uff09\uff0c\u5728\u97f3\u7d20\u7ea7\u7f16\u7801\u5668\u4e0a\u968f\u673a\u63a9\u7801\u5e76\u91cd\u5efa\u53d1\u97f3\u7279\u5f81\u4ee5\u6355\u6349\u957f\u671f\u65f6\u5e8f\u4fe1\u606f\uff1b\u540c\u65f6\u878d\u5408\u624b\u5de5\u8bbe\u8ba1\u7684\u6d41\u5229\u5ea6\u548c\u91cd\u97f3\u7b49\u7279\u5f81\uff0c\u901a\u8fc7\u56de\u5f52\u5668\u751f\u6210\u53ef\u89e3\u91ca\u7684\u80fd\u529b\u8bc4\u5206\u3002", "result": "\u5728speechocean762\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u53d1\u97f3\u8bc4\u5206\u51c6\u786e\u6027\u548c\u4e0e\u53e3\u8bed\u80fd\u529b\u7684\u76f8\u5173\u6027\u3002", "conclusion": "MTP\u6709\u6548\u589e\u5f3a\u4e86\u53d1\u97f3\u8bc4\u4f30\u4e2d\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u7ed3\u5408\u624b\u5de5\u7279\u5f81\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u3001\u53ef\u89e3\u91ca\u7684\u4e8c\u8bed\u5b66\u4e60\u8005\u53e3\u8bed\u80fd\u529b\u8bc4\u4f30\u3002"}}
{"id": "2509.16999", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16999", "abs": "https://arxiv.org/abs/2509.16999", "authors": ["Matteo Pegoraro"], "title": "Persistence Spheres: Bi-continuous Representations of Persistence Diagrams", "comment": null, "summary": "We introduce persistence spheres, a novel functional representation of\npersistence diagrams. Unlike existing embeddings (such as persistence images,\nlandscapes, or kernel methods), persistence spheres provide a bi-continuous\nmapping: they are Lipschitz continuous with respect to the 1-Wasserstein\ndistance and admit a continuous inverse on their image. This ensures, in a\ntheoretically optimal way, both stability and geometric fidelity, making\npersistence spheres the representation that most closely mirrors the\nWasserstein geometry of PDs in linear space. We derive explicit formulas for\npersistence spheres, showing that they can be computed efficiently and\nparallelized with minimal overhead. Empirically, we evaluate them on diverse\nregression and classification tasks involving functional data, time series,\ngraphs, meshes, and point clouds. Across these benchmarks, persistence spheres\nconsistently deliver state-of-the-art or competitive performance compared to\npersistence images, persistence landscapes, and the sliced Wasserstein kernel.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u4e45\u56fe\u8868\u793a\u65b9\u6cd5\u2014\u2014\u6301\u4e45\u7403\uff08persistence spheres\uff09\uff0c\u8be5\u65b9\u6cd5\u57281-Wasserstein\u8ddd\u79bb\u4e0b\u5177\u6709Lipschitz\u8fde\u7eed\u6027\u4e14\u5b58\u5728\u8fde\u7eed\u9006\u6620\u5c04\uff0c\u517c\u5177\u7a33\u5b9a\u6027\u4e0e\u51e0\u4f55\u4fdd\u771f\u6027\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u6301\u4e45\u56fe\u5d4c\u5165\u65b9\u6cd5\uff08\u5982\u6301\u4e45\u56fe\u50cf\u3001\u666f\u89c2\u6216\u6838\u65b9\u6cd5\uff09\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u7a33\u5b9a\u6027\u548c\u51e0\u4f55\u7ed3\u6784\u4fdd\u771f\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u6784\u9020\u4e86\u6301\u4e45\u7403\u4f5c\u4e3a\u6301\u4e45\u56fe\u7684\u529f\u80fd\u6027\u8868\u793a\uff0c\u63a8\u5bfc\u51fa\u5176\u663e\u5f0f\u516c\u5f0f\uff0c\u786e\u4fdd\u5176\u57281-Wasserstein\u8ddd\u79bb\u4e0b\u7684Lipschitz\u8fde\u7eed\u6027\u548c\u53ef\u9006\u6027\uff0c\u5e76\u652f\u6301\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u5728\u51fd\u6570\u6570\u636e\u3001\u65f6\u95f4\u5e8f\u5217\u3001\u56fe\u3001\u7f51\u683c\u548c\u70b9\u4e91\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\uff0c\u6301\u4e45\u7403\u5728\u56de\u5f52\u4e0e\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u8fbe\u5230\u6216\u63a5\u8fd1\u6700\u4f18\uff0c\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6301\u4e45\u7403\u662f\u4e00\u79cd\u7406\u8bba\u4e0a\u6700\u4f18\u3001\u5b9e\u8df5\u4e2d\u9ad8\u6548\u7684\u6301\u4e45\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u6700\u597d\u5730\u5728\u7ebf\u6027\u7a7a\u95f4\u4e2d\u53cd\u6620\u6301\u4e45\u56fe\u7684Wasserstein\u51e0\u4f55\u7ed3\u6784\u3002"}}
{"id": "2509.17589", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17589", "abs": "https://arxiv.org/abs/2509.17589", "authors": ["Jun Ling", "Yao Qi", "Tao Huang", "Shibo Zhou", "Yanqin Huang", "Jiang Yang", "Ziqi Song", "Ying Zhou", "Yang Yang", "Heng Tao Shen", "Peng Wang"], "title": "Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models", "comment": "NeurIPS 2025", "summary": "In this work, we address the task of table image to LaTeX code generation,\nwith the goal of automating the reconstruction of high-quality,\npublication-ready tables from visual inputs. A central challenge of this task\nlies in accurately handling complex tables -- those with large sizes, deeply\nnested structures, and semantically rich or irregular cell content -- where\nexisting methods often fail. We begin with a comprehensive analysis,\nidentifying key challenges and highlighting the limitations of current\nevaluation protocols. To overcome these issues, we propose a reinforced\nmultimodal large language model (MLLM) framework, where a pre-trained MLLM is\nfine-tuned on a large-scale table-to-LaTeX dataset. To further improve\ngeneration quality, we introduce a dual-reward reinforcement learning strategy\nbased on Group Relative Policy Optimization (GRPO). Unlike standard approaches\nthat optimize purely over text outputs, our method incorporates both a\nstructure-level reward on LaTeX code and a visual fidelity reward computed from\nrendered outputs, enabling direct optimization of the visual output quality. We\nadopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and\nshow that our method achieves state-of-the-art performance, particularly on\nstructurally complex tables, demonstrating the effectiveness and robustness of\nour approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u683c\u56fe\u50cf\u5230LaTeX\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u673a\u5236\u4f18\u5316\u7ed3\u6784\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5728\u590d\u6742\u8868\u683c\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u5d4c\u5957\u7ed3\u6784\u548c\u8bed\u4e49\u590d\u6742\u7684\u8868\u683c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff1b\u5f15\u5165\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u53cc\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u7ed3\u5408\u7ed3\u6784\u7ea7\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u5956\u52b1\u3002", "result": "\u5728\u6df7\u5408\u8bc4\u4f30\u534f\u8bae\uff08TEDS-Structure\u548cCW-SSIM\uff09\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u8868\u683c\u4e0a\u7684\u751f\u6210\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u8868\u683c\u7684LaTeX\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2509.16685", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16685", "abs": "https://arxiv.org/abs/2509.16685", "authors": ["Binbin Wen", "Yihang Wu", "Tareef Daqqaq", "Ahmad Chaddad"], "title": "Towards a Transparent and Interpretable AI Model for Medical Image Classifications", "comment": "Published in Cognitive Neurodynamics", "summary": "The integration of artificial intelligence (AI) into medicine is remarkable,\noffering advanced diagnostic and therapeutic possibilities. However, the\ninherent opacity of complex AI models presents significant challenges to their\nclinical practicality. This paper focuses primarily on investigating the\napplication of explainable artificial intelligence (XAI) methods, with the aim\nof making AI decisions transparent and interpretable. Our research focuses on\nimplementing simulations using various medical datasets to elucidate the\ninternal workings of the XAI model. These dataset-driven simulations\ndemonstrate how XAI effectively interprets AI predictions, thus improving the\ndecision-making process for healthcare professionals. In addition to a survey\nof the main XAI methods and simulations, ongoing challenges in the XAI field\nare discussed. The study highlights the need for the continuous development and\nexploration of XAI, particularly from the perspective of diverse medical\ndatasets, to promote its adoption and effectiveness in the healthcare domain.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5728\u533b\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u591a\u79cd\u533b\u7597\u6570\u636e\u96c6\u7684\u6a21\u62df\u5c55\u793aXAI\u5982\u4f55\u63d0\u9ad8AI\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5f3a\u8c03\u4e86\u6301\u7eed\u5f00\u53d1XAI\u65b9\u6cd5\u4ee5\u4fc3\u8fdb\u5176\u5728\u533b\u7597\u9886\u57df\u5e94\u7528\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u590d\u6742AI\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8AI\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u589e\u5f3a\u5176\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u591a\u79cd\u533b\u7597\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540cXAI\u65b9\u6cd5\u5bf9AI\u9884\u6d4b\u7ed3\u679c\u7684\u89e3\u91ca\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793aXAI\u80fd\u591f\u6709\u6548\u63ed\u793aAI\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\uff0c\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u66f4\u597d\u5730\u7406\u89e3AI\u51b3\u7b56\uff0c\u4ece\u800c\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "XAI\u5728\u533b\u7597\u9886\u57df\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u9700\u9488\u5bf9\u591a\u6837\u5316\u7684\u533b\u7597\u6570\u636e\u6301\u7eed\u4f18\u5316\u548c\u53d1\u5c55\uff0c\u4ee5\u63a8\u52a8\u5176\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2509.17450", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17450", "abs": "https://arxiv.org/abs/2509.17450", "authors": ["Ying Feng", "Hongjie Fang", "Yinong He", "Jingjing Chen", "Chenxi Wang", "Zihao He", "Ruonan Liu", "Cewu Lu"], "title": "Learning Dexterous Manipulation with Quantized Hand State", "comment": null, "summary": "Dexterous robotic hands enable robots to perform complex manipulations that\nrequire fine-grained control and adaptability. Achieving such manipulation is\nchallenging because the high degrees of freedom tightly couple hand and arm\nmotions, making learning and control difficult. Successful dexterous\nmanipulation relies not only on precise hand motions, but also on accurate\nspatial positioning of the arm and coordinated arm-hand dynamics. However, most\nexisting visuomotor policies represent arm and hand actions in a single\ncombined space, which often causes high-dimensional hand actions to dominate\nthe coupled action space and compromise arm control. To address this, we\npropose DQ-RISE, which quantizes hand states to simplify hand motion prediction\nwhile preserving essential patterns, and applies a continuous relaxation that\nallows arm actions to diffuse jointly with these compact hand states. This\ndesign enables the policy to learn arm-hand coordination from data while\npreventing hand actions from overwhelming the action space. Experiments show\nthat DQ-RISE achieves more balanced and efficient learning, paving the way\ntoward structured and generalizable dexterous manipulation. Project website:\nhttp://rise-policy.github.io/DQ-RISE/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DQ-RISE\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u624b\u90e8\u72b6\u6001\u5e76\u5f15\u5165\u8fde\u7eed\u677e\u5f1b\u6765\u8054\u5408\u6269\u6563\u624b\u81c2\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u7075\u5de7\u64cd\u4f5c\u4e2d\u624b\u90e8\u52a8\u4f5c\u4e3b\u5bfc\u8026\u5408\u52a8\u4f5c\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5e73\u8861\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "motivation": "\u7531\u4e8e\u9ad8\u81ea\u7531\u5ea6\u5bfc\u81f4\u624b\u90e8\u548c\u624b\u81c2\u8fd0\u52a8\u7d27\u5bc6\u8026\u5408\uff0c\u73b0\u6709\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u96be\u4ee5\u6709\u6548\u5b66\u4e60\u548c\u63a7\u5236\u7075\u5de7\u64cd\u4f5c\uff0c\u4e14\u5e38\u56e0\u624b\u90e8\u52a8\u4f5c\u4e3b\u5bfc\u800c\u5f71\u54cd\u624b\u81c2\u63a7\u5236\u3002", "method": "\u63d0\u51faDQ-RISE\u65b9\u6cd5\uff0c\u5c06\u624b\u90e8\u72b6\u6001\u79bb\u6563\u5316\u4ee5\u7b80\u5316\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u8fde\u7eed\u677e\u5f1b\u4f7f\u624b\u81c2\u52a8\u4f5c\u4e0e\u538b\u7f29\u540e\u7684\u624b\u90e8\u72b6\u6001\u8054\u5408\u6269\u6563\uff0c\u4ece\u800c\u5b9e\u73b0\u534f\u8c03\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDQ-RISE\u80fd\u591f\u5b9e\u73b0\u66f4\u5e73\u8861\u548c\u9ad8\u6548\u7684\u7075\u5de7\u64cd\u4f5c\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u624b\u81c2\u4e0e\u624b\u90e8\u7684\u534f\u8c03\u6027\u3002", "conclusion": "DQ-RISE\u901a\u8fc7\u7ed3\u6784\u5316\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u9ad8\u7ef4\u624b\u90e8\u52a8\u4f5c\u5bf9\u6574\u4f53\u63a7\u5236\u7684\u5e72\u6270\uff0c\u4e3a\u901a\u7528\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.16889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16889", "abs": "https://arxiv.org/abs/2509.16889", "authors": ["Xiaoqiang Kang", "Shengen Wu", "Zimu Wang", "Yilin Liu", "Xiaobo Jin", "Kaizhu Huang", "Wei Wang", "Yutao Yue", "Xiaowei Huang", "Qiufeng Wang"], "title": "Can GRPO Boost Complex Multimodal Table Understanding?", "comment": "EMNLP 2025", "summary": "Existing table understanding methods face challenges due to complex table\nstructures and intricate logical reasoning. While supervised finetuning (SFT)\ndominates existing research, reinforcement learning (RL), such as Group\nRelative Policy Optimization (GRPO), has shown promise but struggled with low\ninitial policy accuracy and coarse rewards in tabular contexts. In this paper,\nwe introduce Table-R1, a three-stage RL framework that enhances multimodal\ntable understanding through: (1) Warm-up that prompts initial perception and\nreasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs\ncontinuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table\nstructures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes\nfine-grained rewards of residual steps based on the hint-guided question.\nExtensive experiments demonstrate that Table-R1 can boost the model's table\nreasoning performance obviously on both held-in and held-out datasets,\noutperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1\nsurpasses larger specific table understanding models (e.g., Table-LLaVA 13B),\neven achieving comparable performance to the closed-source model GPT-4o on\nheld-in datasets, demonstrating the efficacy of each stage of Table-R1 in\novercoming initialization bottlenecks and reward sparsity, thereby advancing\nrobust multimodal table understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Table-R1\uff0c\u4e00\u79cd\u4e09\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u548c\u5176\u4ed6RL\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u7406\u89e3\u65b9\u6cd5\u53d7\u9650\u4e8e\u590d\u6742\u7ed3\u6784\u548c\u903b\u8f91\u63a8\u7406\u96be\u5ea6\uff0c\u4e14\u5f3a\u5316\u5b66\u4e60\u5728\u521d\u59cb\u7b56\u7565\u51c6\u786e\u7387\u4f4e\u548c\u5956\u52b1\u7a00\u758f\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\uff1aWarm-up\u9636\u6bb5\u63d0\u5347\u521d\u59cb\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\uff1bPA-GRPO\u9636\u6bb5\u91c7\u7528\u8fde\u7eedTree-Edit-Distance Similarity\uff08TEDS\uff09\u5956\u52b1\u6765\u5bf9\u9f50\u8868\u683c\u7ed3\u6784\u4e0e\u5185\u5bb9\uff1bHC-GRPO\u9636\u6bb5\u57fa\u4e8e\u63d0\u793a\u5f15\u5bfc\u95ee\u9898\u4f7f\u7528\u6b8b\u5dee\u6b65\u9aa4\u7684\u7ec6\u7c92\u5ea6\u5956\u52b1\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTable-R1\u5728\u4fdd\u6301\u5185\u548c\u4fdd\u6301\u5916\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cQwen2-VL-7B\u7ed3\u5408Table-R1\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u4e13\u7528\u8868\u683c\u6a21\u578b\uff08\u5982Table-LLaVA 13B\uff09\uff0c\u5e76\u5728\u4fdd\u6301\u5185\u6570\u636e\u96c6\u4e0a\u63a5\u8fd1GPT-4o\u7684\u8868\u73b0\u3002", "conclusion": "Table-R1\u6709\u6548\u7f13\u89e3\u4e86\u521d\u59cb\u5316\u74f6\u9888\u548c\u5956\u52b1\u7a00\u758f\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u9c81\u68d2\u7684\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u53d1\u5c55\u3002"}}
{"id": "2509.17000", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17000", "abs": "https://arxiv.org/abs/2509.17000", "authors": ["Shuhao Jiang", "Songbo Wang", "Yang Qiao", "Chun Xu", "Chaoyang Zheng", "Shengyi Zhou", "Huanjun Wang", "Fangming Li", "Cong Zhang", "Jiyu Wang"], "title": "Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals", "comment": null, "summary": "Large Reasoning Models (LRMs) often suffer from computational inefficiency\ndue to overthinking, where a fixed reasoning budget fails to match the varying\ncomplexity of tasks. To address this issue, we propose Adaptive Overclocking, a\nmethod that makes the overclocking hyperparameter $\\alpha$ dynamic and\ncontext-aware. Our method adjusts reasoning speed in real time through two\ncomplementary signals: (1) token-level model uncertainty for fine-grained\nstep-wise control, and (2) input complexity estimation for informed\ninitialization. We implement this approach with three strategies:\nUncertainty-Aware Alpha Scheduling (UA-$\\alpha$S), Complexity-Guided Alpha\nInitialization (CG-$\\alpha$I), and a Hybrid Adaptive Control (HAC) that\ncombines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves\nsuperior accuracy-latency trade-offs, reducing unnecessary computation on\nsimple problems while allocating more resources to challenging ones. By\nmitigating overthinking, Adaptive Overclocking enhances both efficiency and\noverall reasoning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptive Overclocking\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u901f\u5ea6\u6765\u7f13\u89e3\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u601d\u8003\u95ee\u9898\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u5e38\u56e0\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u65e0\u6cd5\u9002\u5e94\u4efb\u52a1\u590d\u6742\u5ea6\u53d8\u5316\u800c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u51fa\u73b0\u8fc7\u601d\u8003\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u6001\u8d85\u9891\u8d85\u53c2\u6570\u03b1\uff0c\u7ed3\u5408token\u7ea7\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u8f93\u5165\u590d\u6742\u5ea6\u4f30\u8ba1\u4e24\u4e2a\u4fe1\u53f7\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u6b65\u957f\u63a7\u5236\u548c\u521d\u59cb\u5316\u6307\u5bfc\uff0c\u5e76\u91c7\u7528\u4e09\u79cd\u7b56\u7565\uff1aUA-\u03b1S\u3001CG-\u03b1I\u548cHAC\u3002", "result": "\u5728GSM8K\u3001MATH\u548cSVAMP\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cHAC\u5728\u51c6\u786e\u7387-\u5ef6\u8fdf\u6743\u8861\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u51cf\u5c11\u7b80\u5355\u95ee\u9898\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u540c\u65f6\u4e3a\u590d\u6742\u95ee\u9898\u5206\u914d\u66f4\u591a\u8d44\u6e90\u3002", "conclusion": "Adaptive Overclocking\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u601d\u8003\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2509.17677", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17677", "abs": "https://arxiv.org/abs/2509.17677", "authors": ["Xiyuan Zhou", "Xinlei Wang", "Yirui He", "Yang Wu", "Ruixi Zou", "Yuheng Cheng", "Yulu Xie", "Wenxuan Liu", "Huan Zhao", "Yan Xu", "Jinjin Gu", "Junhua Zhao"], "title": "EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving", "comment": null, "summary": "Large language models (LLMs) have shown strong performance on mathematical\nreasoning under well-posed conditions. However, real-world engineering problems\nrequire more than mathematical symbolic computation -- they need to deal with\nuncertainty, context, and open-ended scenarios. Existing benchmarks fail to\ncapture these complexities. We introduce EngiBench, a hierarchical benchmark\ndesigned to evaluate LLMs on solving engineering problems. It spans three\nlevels of increasing difficulty (foundational knowledge retrieval, multi-step\ncontextual reasoning, and open-ended modeling) and covers diverse engineering\nsubfields. To facilitate a deeper understanding of model performance, we\nsystematically rewrite each problem into three controlled variants (perturbed,\nknowledge-enhanced, and math abstraction), enabling us to separately evaluate\nthe model's robustness, domain-specific knowledge, and mathematical reasoning\nabilities. Experiment results reveal a clear performance gap across levels:\nmodels struggle more as tasks get harder, perform worse when problems are\nslightly changed, and fall far behind human experts on the high-level\nengineering tasks. These findings reveal that current LLMs still lack the\nhigh-level reasoning needed for real-world engineering, highlighting the need\nfor future models with deeper and more reliable problem-solving capabilities.\nOur source code and data are available at\nhttps://github.com/EngiBench/EngiBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EngiBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u7a0b\u95ee\u9898\u6c42\u89e3\u4e2d\u8868\u73b0\u7684\u5206\u5c42\u57fa\u51c6\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u77e5\u8bc6\u5230\u5f00\u653e\u5efa\u6a21\u7684\u591a\u4e2a\u96be\u5ea6\u5c42\u7ea7\uff0c\u5e76\u901a\u8fc7\u95ee\u9898\u53d8\u4f53\u5206\u6790\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u9886\u57df\u77e5\u8bc6\u548c\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u4e0a\u4ecd\u8fdc\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u771f\u5b9e\u5de5\u7a0b\u95ee\u9898\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u5f00\u653e\u6027\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u7a0b\u573a\u666f\u4e0b\u7684\u7efc\u5408\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u5c42\u7ea7\uff08\u57fa\u7840\u77e5\u8bc6\u68c0\u7d22\u3001\u591a\u6b65\u60c5\u5883\u63a8\u7406\u3001\u5f00\u653e\u5efa\u6a21\uff09\u7684\u5206\u5c42\u57fa\u51c6EngiBench\uff0c\u5e76\u5c06\u6bcf\u4e2a\u95ee\u9898\u6539\u5199\u4e3a\u4e09\u79cd\u53d7\u63a7\u53d8\u4f53\uff08\u6270\u52a8\u3001\u77e5\u8bc6\u589e\u5f3a\u3001\u6570\u5b66\u62bd\u8c61\uff09\uff0c\u4ee5\u5206\u522b\u8bc4\u4f30\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u9886\u57df\u77e5\u8bc6\u548c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u968f\u7740\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u5728\u95ee\u9898\u7a0d\u4f5c\u6539\u52a8\u540e\u8868\u73b0\u53d8\u5dee\uff1b\u5728\u9ad8\u5c42\u7ea7\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8fdc\u900a\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u5de5\u7a0b\u95ee\u9898\u65f6\u4ecd\u7f3a\u4e4f\u6240\u9700\u7684\u9ad8\u9636\u63a8\u7406\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u672a\u6765\u9700\u53d1\u5c55\u66f4\u5177\u6df1\u5ea6\u548c\u53ef\u9760\u6027\u7684\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2509.16690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16690", "abs": "https://arxiv.org/abs/2509.16690", "authors": ["Xiaodong Wang", "Zijun He", "Ping Wang", "Lishun Wang", "Yanan Hu", "Xin Yuan"], "title": "Spectral Compressive Imaging via Chromaticity-Intensity Decomposition", "comment": null, "summary": "In coded aperture snapshot spectral imaging (CASSI), the captured measurement\nentangles spatial and spectral information, posing a severely ill-posed inverse\nproblem for hyperspectral images (HSIs) reconstruction. Moreover, the captured\nradiance inherently depends on scene illumination, making it difficult to\nrecover the intrinsic spectral reflectance that remains invariant to lighting\nconditions. To address these challenges, we propose a chromaticity-intensity\ndecomposition framework, which disentangles an HSI into a spatially smooth\nintensity map and a spectrally variant chromaticity cube. The chromaticity\nencodes lighting-invariant reflectance, enriched with high-frequency spatial\ndetails and local spectral sparsity. Building on this decomposition, we develop\nCIDNet, a Chromaticity-Intensity Decomposition unfolding network within a\ndual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral\nTransformer tailored to reconstruct fine-grained and sparse spectral\nchromaticity and a degradation-aware, spatially-adaptive noise estimation\nmodule that captures anisotropic noise across iterative stages. Extensive\nexperiments on both synthetic and real-world CASSI datasets demonstrate that\nour method achieves superior performance in both spectral and chromaticity\nfidelity. Code and models will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8272\u5ea6-\u5f3a\u5ea6\u5206\u89e3\u7684\u6846\u67b6\uff08CIDNet\uff09\uff0c\u7528\u4e8e\u5728\u53cc\u76f8\u673aCASSI\u7cfb\u7edf\u4e2d\u91cd\u5efa\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u6709\u6548\u5206\u79bb\u5149\u7167\u4e0d\u53d8\u7684\u53cd\u5c04\u7387\u5e76\u63d0\u5347\u9891\u8c31\u548c\u8272\u5ea6\u4fdd\u771f\u5ea6\u3002", "motivation": "CASSI\u7cfb\u7edf\u4e2d\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u9762\u4e34\u4e25\u91cd\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\uff0c\u4e14\u6355\u83b7\u7684\u8f90\u5c04\u53d7\u5149\u7167\u5f71\u54cd\uff0c\u96be\u4ee5\u6062\u590d\u56fa\u6709\u7684\u5149\u8c31\u53cd\u5c04\u7387\u3002", "method": "\u63d0\u51fa\u8272\u5ea6-\u5f3a\u5ea6\u5206\u89e3\u6846\u67b6\uff0c\u5c06\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u89e3\u4e3a\u5e73\u6ed1\u7684\u5f3a\u5ea6\u56fe\u548c\u7a00\u758f\u7684\u8272\u5ea6\u7acb\u65b9\u4f53\uff1b\u8bbe\u8ba1CIDNet\u7f51\u7edc\uff0c\u7ed3\u5408\u6df7\u5408\u7a7a\u95f4-\u5149\u8c31Transformer\u548c\u964d\u89e3\u611f\u77e5\u566a\u58f0\u4f30\u8ba1\u6a21\u5757\uff0c\u5728\u53cc\u76f8\u673aCASSI\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eCASSI\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5149\u8c31\u548c\u8272\u5ea6\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CIDNet\u901a\u8fc7\u8272\u5ea6-\u5f3a\u5ea6\u5206\u89e3\u6709\u6548\u89e3\u51b3\u4e86CASSI\u4e2d\u7684\u4e0d\u9002\u5b9a\u95ee\u9898\u548c\u5149\u7167\u4f9d\u8d56\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2509.17572", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.17572", "abs": "https://arxiv.org/abs/2509.17572", "authors": ["Vishnu Deo Mishra", "S Ganga Prasath"], "title": "Morphologies of a sagging elastica with intrinsic sensing and actuation", "comment": null, "summary": "The morphology of a slender soft-robot can be modified by sensing its shape\nvia sensors and exerting moments via actuators embedded along its body. The\nactuating moments required to morph these soft-robots to a desired shape are\noften difficult to compute due to the geometric non-linearity associated with\nthe structure, the errors in modeling the experimental system, and the\nlimitations in sensing and feedback/actuation capabilities. In this article, we\nexplore the effect of a simple feedback strategy (actuation being proportional\nto the sensed curvature) on the shape of a soft-robot, modeled as an elastica.\nThe finite number of sensors and actuators, often seen in experiments, is\ncaptured in the model via filters of specified widths. Using proportional\nfeedback, we study the simple task of straightening the device by compensating\nfor the sagging introduced by its self-weight. The device undergoes a hierarchy\nof morphological instabilities defined in the phase-space given by the\ngravito-bending number, non-dimensional sensing/feedback gain, and the scaled\nwidth of the filter. For complex shape-morphing tasks, given a perfect model of\nthe device with limited sensing and actuating capabilities, we find that a\ntrade-off arises (set by the sensor spacing & actuator size) between capturing\nthe long and short wavelength features. We show that the error in\nshape-morphing is minimal for a fixed filter width when we choose an\nappropriate actuating gain (whose magnitude goes as a square of the filter\nwidth). Our model provides a quantitative lens to study and design slender soft\ndevices with limited sensing and actuating capabilities for complex maneuvering\napplications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6bd4\u4f8b\u53cd\u9988\u7b56\u7565\u63a7\u5236\u7ec6\u957f\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5f62\u6001\uff0c\u5206\u6790\u4e86\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u6570\u91cf\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u5f62\u72b6\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u7279\u5b9a\u6ee4\u6ce2\u5668\u5bbd\u5ea6\u4e0b\u6700\u5c0f\u5316\u5f62\u53d8\u8bef\u5dee\u7684\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u51e0\u4f55\u975e\u7ebf\u6027\u3001\u5efa\u6a21\u8bef\u5dee\u4ee5\u53ca\u611f\u77e5\u4e0e\u6267\u884c\u80fd\u529b\u7684\u9650\u5236\uff0c\u8f6f\u4f53\u673a\u5668\u4eba\u8fbe\u5230\u76ee\u6807\u5f62\u72b6\u6240\u9700\u7684\u9a71\u52a8\u77e9\u96be\u4ee5\u8ba1\u7b97\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u53cd\u9988\u63a7\u5236\u7b56\u7565\u3002", "method": "\u5c06\u8f6f\u4f53\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u5f39\u6027\u4f53\uff08elastica\uff09\uff0c\u91c7\u7528\u6bd4\u4f8b\u53cd\u9988\uff08\u6267\u884c\u4e0e\u611f\u77e5\u66f2\u7387\u6210\u6b63\u6bd4\uff09\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5177\u6709\u7279\u5b9a\u5bbd\u5ea6\u7684\u6ee4\u6ce2\u5668\u6a21\u62df\u5b9e\u9a8c\u4e2d\u6709\u9650\u6570\u91cf\u7684\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u3002", "result": "\u53d1\u73b0\u8bbe\u5907\u5728\u91cd\u529b-\u5f2f\u66f2\u6570\u3001\u65e0\u91cf\u7eb2\u611f\u77e5/\u53cd\u9988\u589e\u76ca\u548c\u6ee4\u6ce2\u5668\u5c3a\u5ea6\u5bbd\u5ea6\u6784\u6210\u7684\u76f8\u7a7a\u95f4\u4e2d\u7ecf\u5386\u4e00\u7cfb\u5217\u5f62\u6001\u5931\u7a33\uff1b\u5bf9\u4e8e\u590d\u6742\u5f62\u72b6\u63a7\u5236\u4efb\u52a1\uff0c\u5b58\u5728\u6355\u6349\u957f\u77ed\u6ce2\u7279\u5f81\u4e4b\u95f4\u7684\u6743\u8861\uff1b\u5728\u56fa\u5b9a\u6ee4\u6ce2\u5668\u5bbd\u5ea6\u4e0b\uff0c\u9009\u62e9\u9002\u5f53\u7684\u9a71\u52a8\u589e\u76ca\u53ef\u4f7f\u5f62\u53d8\u8bef\u5dee\u6700\u5c0f\u5316\uff0c\u4e14\u8be5\u589e\u76ca\u5927\u5c0f\u4e0e\u6ee4\u6ce2\u5668\u5bbd\u5ea6\u7684\u5e73\u65b9\u6210\u6b63\u6bd4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u8bbe\u8ba1\u5177\u6709\u6709\u9650\u611f\u77e5\u4e0e\u6267\u884c\u80fd\u529b\u7684\u7ec6\u957f\u8f6f\u4f53\u88c5\u7f6e\u63d0\u4f9b\u4e86\u5b9a\u91cf\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u590d\u6742\u673a\u52a8\u5e94\u7528\u4e2d\u7684\u5f62\u6001\u63a7\u5236\u3002"}}
{"id": "2509.16903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16903", "abs": "https://arxiv.org/abs/2509.16903", "authors": ["Nawar Turk", "Daniele Comitogianni", "Leila Kosseim"], "title": "CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification", "comment": null, "summary": "We present our submission to Task 3 (Discourse Relation Classification) of\nthe DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse\nrelation labels across 39 corpora in 16 languages and six discourse frameworks,\nposing significant multilingual and cross-formalism challenges. We first\nbenchmark the task by fine-tuning multilingual BERT-based models (mBERT,\nXLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies\nand progressive unfreezing ratios to establish strong baselines. We then\nevaluate prompt-based large language models (namely Claude Opus 4.0) in\nzero-shot and few-shot settings to understand how LLMs respond to the newly\nproposed unified labels. Finally, we introduce HiDAC, a Hierarchical\nDual-Adapter Contrastive learning model. Results show that while larger\ntransformer models achieve higher accuracy, the improvements are modest, and\nthat unfreezing the top 75% of encoder layers yields performance comparable to\nfull fine-tuning while training far fewer parameters. Prompt-based models lag\nsignificantly behind fine-tuned transformers, and HiDAC achieves the highest\noverall accuracy (67.5%) while remaining more parameter-efficient than full\nfine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HiDAC\u6a21\u578b\u7528\u4e8e\u591a\u8bed\u8a00\u548c\u8de8\u5f62\u5f0f\u4e3b\u4e49\u7684\u7bc7\u7ae0\u5173\u7cfb\u5206\u7c7b\u4efb\u52a1\uff0c\u5728DISRPT 2025 Task 3\u4e2d\u5b9e\u73b0\u4e8667.5%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u53c2\u6570\u6548\u7387\u3002", "motivation": "\u7531\u4e8eDISRPT 2025 Task 3\u5f15\u5165\u4e86\u6db5\u76d616\u79cd\u8bed\u8a00\u548c\u516d\u79cd\u7bc7\u7ae0\u6846\u67b6\u7684\u7edf\u4e0017\u7c7b\u6807\u7b7e\u4f53\u7cfb\uff0c\u5b58\u5728\u663e\u8457\u7684\u591a\u8bed\u8a00\u548c\u8de8\u5f62\u5f0f\u4e3b\u4e49\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u5f3a\u57fa\u7ebf\u5e76\u63a2\u7d22\u65b0\u578b\u9ad8\u6548\u6a21\u578b\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5fae\u8c03\u591a\u8bed\u8a00BERT\u6a21\u578b\uff08\u5982mBERT\u3001XLM-RoBERTa\uff09\u7ed3\u5408\u4e0d\u540c\u7684\u53c2\u6570\u89e3\u51bb\u7b56\u7565\u5efa\u7acb\u57fa\u7ebf\uff1b\u7136\u540e\u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Claude Opus 4.0\uff09\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4e0b\u7684\u8868\u73b0\uff1b\u6700\u540e\u63d0\u51faHiDAC\uff0c\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u5316\u53cc\u9002\u914d\u5668\u5bf9\u6bd4\u5b66\u4e60\u7684\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u5927\u7684Transformer\u6a21\u578b\u6027\u80fd\u7565\u4f18\u4f46\u63d0\u5347\u6709\u9650\uff1b\u89e3\u51bb\u7f16\u7801\u5668\u9876\u5c4275%\u7684\u5c42\u53ef\u5728\u8bad\u7ec3\u66f4\u5c11\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u5168\u5fae\u8c03\u76f8\u5f53\u7684\u6548\u679c\uff1b\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u8868\u73b0\u660e\u663e\u843d\u540e\u4e8e\u5fae\u8c03\u6a21\u578b\uff1bHiDAC\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0867.5%\uff09\uff0c\u4e14\u6bd4\u5168\u5fae\u8c03\u66f4\u53c2\u6570\u9ad8\u6548\u3002", "conclusion": "HiDAC\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u7bc7\u7ae0\u5173\u7cfb\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u8bed\u8a00\u3001\u591a\u6846\u67b6\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7edf\u4e00\u7684\u8de8\u9886\u57df\u7bc7\u7ae0\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.17034", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17034", "abs": "https://arxiv.org/abs/2509.17034", "authors": ["Shuai Feng", "Yuxin Ge", "Yuntao Du", "Mingcai Chen", "Lei Feng"], "title": "Long-Tailed Out-of-Distribution Detection with Refined Separate Class Learning", "comment": null, "summary": "Out-of-distribution (OOD) detection is crucial for deploying robust machine\nlearning models. However, when training data follows a long-tailed\ndistribution, the model's ability to accurately detect OOD samples is\nsignificantly compromised, due to the confusion between OOD samples and\nhead/tail classes. To distinguish OOD samples from both head and tail classes,\nthe separate class learning (SCL) approach has emerged as a promising solution,\nwhich separately conduct head-specific and tail-specific class learning. To\nthis end, we examine the limitations of existing works of SCL and reveal that\nthe OOD detection performance is notably influenced by the use of static\nscaling temperature value and the presence of uninformative outliers. To\nmitigate these limitations, we propose a novel approach termed Refined Separate\nClass Learning (RSCL), which leverages dynamic class-wise temperature\nadjustment to modulate the temperature parameter for each in-distribution class\nand informative outlier mining to identify diverse types of outliers based on\ntheir affinity with head and tail classes. Extensive experiments demonstrate\nthat RSCL achieves superior OOD detection performance while improving the\nclassification accuracy on in-distribution data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5206\u79bb\u7c7b\u5b66\u4e60\u65b9\u6cd5\uff08RSCL\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u6e29\u5ea6\u548c\u6316\u6398\u4fe1\u606f\u6027\u5f02\u5e38\u503c\uff0c\u63d0\u5347\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u7684OOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u957f\u5c3e\u5206\u5e03\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u56e0\u9759\u6001\u6e29\u5ea6\u53c2\u6570\u548c\u65e0\u4fe1\u606f\u5f02\u5e38\u503c\u7684\u5b58\u5728\u800c\u6027\u80fd\u53d7\u9650\uff0c\u96be\u4ee5\u533a\u5206OOD\u6837\u672c\u4e0e\u5934/\u5c3e\u90e8\u7c7b\u522b\u3002", "method": "\u63d0\u51faRSCL\u65b9\u6cd5\uff0c\u91c7\u7528\u52a8\u6001\u7684\u9010\u7c7b\u6e29\u5ea6\u8c03\u6574\u673a\u5236\uff0c\u5e76\u8fdb\u884c\u4fe1\u606f\u6027\u5f02\u5e38\u503c\u6316\u6398\uff0c\u4ee5\u66f4\u597d\u5730\u533a\u5206OOD\u6837\u672c\u4e0e\u5934\u7c7b\u548c\u5c3e\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRSCL\u5728OOD\u68c0\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5206\u5e03\u5185\u6570\u636e\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "RSCL\u6709\u6548\u7f13\u89e3\u4e86\u957f\u5c3e\u5206\u5e03\u4e0bOOD\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u6e29\u5ea6\u8c03\u8282\u548c\u5f02\u5e38\u503c\u6316\u6398\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u5224\u522b\u80fd\u529b\u3002"}}
{"id": "2509.17706", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17706", "abs": "https://arxiv.org/abs/2509.17706", "authors": ["Pierre Montalbano", "Simon de Givry", "George Katsirelos"], "title": "Virtual Arc Consistency for Linear Constraints inCost Function Networks", "comment": null, "summary": "In Constraint Programming, solving discrete minimization problems with hard\nand soft constraints can be done either using (i) soft global constraints, (ii)\na reformulation into a linear program, or (iii) a reformulation into local cost\nfunctions. Approach (i) benefits from a vast catalog of constraints. Each soft\nconstraint propagator communicates with other soft constraints only through the\nvariable domains, resulting in weak lower bounds. Conversely, the approach (ii)\nprovides a global view with strong bounds, but the size of the reformulation\ncan be problematic. We focus on approach (iii) in which soft arc consistency\n(SAC) algorithms produce bounds of intermediate quality. Recently, the\nintroduction of linear constraints as local cost functions increases their\nmodeling expressiveness. We adapt an existing SAC algorithm to handle linear\nconstraints. We show that our algorithm significantly improves the lower bounds\ncompared to the original algorithm on several benchmarks, reducing solving time\nin some cases.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ea6\u675f\u89c4\u5212\u4e2d\u5904\u7406\u786c\u7ea6\u675f\u548c\u8f6f\u7ea6\u675f\u7684\u79bb\u6563\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u91cd\u70b9\u6539\u8fdb\u8f6f\u5f27\u4e00\u81f4\u6027\uff08SAC\uff09\u7b97\u6cd5\u4ee5\u5904\u7406\u7ebf\u6027\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u754c\u5e76\u51cf\u5c11\u4e86\u6c42\u89e3\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u5c40\u89c6\u56fe\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u8f6f\u5168\u5c40\u7ea6\u675f\u4f20\u64ad\u5f31\uff0c\u7ebf\u6027\u89c4\u5212\u91cd\u6784\u89c4\u6a21\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u4f9b\u66f4\u597d\u4e0b\u754c\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5c06\u8f6f\u7ea6\u675f\u8f6c\u5316\u4e3a\u5c40\u90e8\u4ee3\u4ef7\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u6269\u5c55\u73b0\u6709\u7684\u8f6f\u5f27\u4e00\u81f4\u6027\uff08SAC\uff09\u7b97\u6cd5\u4ee5\u652f\u6301\u7ebf\u6027\u7ea6\u675f\u4f5c\u4e3a\u5c40\u90e8\u4ee3\u4ef7\u51fd\u6570\u3002", "result": "\u6539\u8fdb\u540e\u7684SAC\u7b97\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u754c\u8d28\u91cf\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u6c42\u89e3\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u6269\u5c55SAC\u7b97\u6cd5\u5904\u7406\u7ebf\u6027\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u6bd4\u539f\u7b97\u6cd5\u66f4\u5f3a\u7684\u4e0b\u754c\uff0c\u5728\u4fdd\u6301\u5efa\u6a21\u7075\u6d3b\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6c42\u89e3\u6548\u7387\u3002"}}
{"id": "2509.16691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16691", "abs": "https://arxiv.org/abs/2509.16691", "authors": ["Qiang Xiang", "Shuang Sun", "Binglei Li", "Dejia Song", "Huaxia Li", "Nemo Chen", "Xu Tang", "Yao Hu", "Junping Zhang"], "title": "InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention", "comment": "Accepted in NeurIPS 2025", "summary": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality images. Recent advancements in Layout-to-Image (L2I) generation\nhave leveraged positional conditions and textual descriptions to facilitate\nprecise and controllable image synthesis. Despite overall progress, current L2I\nmethods still exhibit suboptimal performance. Therefore, we propose\nInstanceAssemble, a novel architecture that incorporates layout conditions via\ninstance-assembling attention, enabling position control with bounding boxes\n(bbox) and multimodal content control including texts and additional visual\ncontent. Our method achieves flexible adaption to existing DiT-based T2I models\nthrough light-weighted LoRA modules. Additionally, we propose a Layout-to-Image\nbenchmark, Denselayout, a comprehensive benchmark for layout-to-image\ngeneration, containing 5k images with 90k instances in total. We further\nintroduce Layout Grounding Score (LGS), an interpretable evaluation metric to\nmore precisely assess the accuracy of L2I generation. Experiments demonstrate\nthat our InstanceAssemble method achieves state-of-the-art performance under\ncomplex layout conditions, while exhibiting strong compatibility with diverse\nstyle LoRA modules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInstanceAssemble\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ec4\u88c5\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5e03\u5c40\u6761\u4ef6\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8fb9\u754c\u6846\u7684\u4f4d\u7f6e\u63a7\u5236\u548c\u591a\u6a21\u6001\u5185\u5bb9\u63a7\u5236\uff0c\u5728\u590d\u6742\u5e03\u5c40\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684Layout-to-Image\u751f\u6210\u65b9\u6cd5\u5728\u590d\u6742\u5e03\u5c40\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u7cbe\u786e\u7684\u5b9e\u4f8b\u7ea7\u63a7\u5236\u548c\u6709\u6548\u7684\u8bc4\u4f30\u57fa\u51c6\u4e0e\u6307\u6807\u3002", "method": "\u63d0\u51faInstanceAssemble\u67b6\u6784\uff0c\u91c7\u7528\u5b9e\u4f8b\u7ec4\u88c5\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u5e03\u5c40\u3001\u6587\u672c\u548c\u89c6\u89c9\u6761\u4ef6\uff1b\u4f7f\u7528\u8f7b\u91cf\u7ea7LoRA\u6a21\u5757\u9002\u914d\u73b0\u6709DiT-based T2I\u6a21\u578b\uff1b\u6784\u5efaDenselayout\u57fa\u51c6\u548cLayout Grounding Score\uff08LGS\uff09\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u590d\u6742\u5e03\u5c40\u6761\u4ef6\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u517c\u5bb9\u591a\u79cd\u98ce\u683cLoRA\u6a21\u5757\uff0cDenselayout\u5305\u542b5k\u56fe\u50cf\u548c90k\u5b9e\u4f8b\uff0cLGS\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u751f\u6210\u51c6\u786e\u6027\u8bc4\u4f30\u3002", "conclusion": "InstanceAssemble\u6709\u6548\u63d0\u5347\u4e86\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u7684\u7cbe\u5ea6\u4e0e\u7075\u6d3b\u6027\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u6807\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2509.17582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17582", "abs": "https://arxiv.org/abs/2509.17582", "authors": ["Vassil Atanassov", "Wanming Yu", "Siddhant Gangapurwala", "James Wilson", "Ioannis Havoutis"], "title": "GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots", "comment": "You can find an associated video here: https://youtu.be/o8Dd44MkG2E", "summary": "Most modern approaches to quadruped locomotion focus on using Deep\nReinforcement Learning (DRL) to learn policies from scratch, in an end-to-end\nmanner. Such methods often fail to scale, as every new problem or application\nrequires time-consuming and iterative reward definition and tuning. We present\nGeneralist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained\nwith Deep Reinforcement Learning that is capable of tracking arbitrary contact\npoints on a quadruped robot. The strength of our approach is that it provides a\ngeneral and modular low-level controller that can be reused for a wider range\nof high-level tasks, without the need to re-train new controllers from scratch.\nWe demonstrate the scalability and robustness of our method by evaluating on a\nwide range of locomotion and manipulation tasks in a common framework and under\na single generalist policy. These include a variety of gaits, traversing\ncomplex terrains (eg. stairs and slopes) as well as previously unseen\nstepping-stones and narrow beams, and interacting with objects (eg. pushing\nbuttons, tracking trajectories). Our framework acquires new behaviors more\nefficiently, simply by combining a task-specific high-level contact planner and\nthe pre-trained generalist policy. A supplementary video can be found at\nhttps://youtu.be/o8Dd44MkG2E.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGeCCo\u7684\u901a\u7528\u63a5\u89e6\u6761\u4ef6\u7b56\u7565\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e00\u4e2a\u53ef\u590d\u7528\u7684\u4f4e\u5c42\u63a7\u5236\u5668\uff0c\u80fd\u591f\u8ddf\u8e2a\u56db\u8db3\u673a\u5668\u4eba\u4efb\u610f\u63a5\u89e6\u70b9\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9ad8\u5c42\u4efb\u52a1\u800c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6bcf\u6b21\u65b0\u4efb\u52a1\u4e2d\u90fd\u9700\u8981\u8017\u65f6\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u4e0e\u8c03\u4f18\uff0c\u96be\u4ee5\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u3001\u6a21\u5757\u5316\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aGeneralist Contact-Conditioned Policy (GeCCo) \u7684\u4f4e\u5c42\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u8f93\u5165\u4e3a\u671f\u671b\u7684\u63a5\u89e6\u70b9\uff0c\u8f93\u51fa\u4e3a\u5173\u8282\u63a7\u5236\u6307\u4ee4\uff0c\u53ef\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5171\u4eab\u3002", "result": "\u5728\u591a\u79cd\u8fd0\u52a8\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u4e0d\u540c\u6b65\u6001\u3001\u590d\u6742\u5730\u5f62\uff08\u697c\u68af\u3001\u659c\u5761\uff09\u3001\u672a\u66fe\u89c1\u8fc7\u7684\u8e0f\u811a\u77f3\u4e0e\u7a84\u6881\u884c\u8d70\uff0c\u4ee5\u53ca\u6309\u94ae\u6309\u538b\u7b49\u7269\u4f53\u4ea4\u4e92\u4efb\u52a1\u3002\u65b0\u884c\u4e3a\u53ef\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u7684\u9ad8\u5c42\u63a5\u89e6\u89c4\u5212\u5668\u4e0e\u9884\u8bad\u7ec3\u7b56\u7565\u9ad8\u6548\u83b7\u5f97\u3002", "conclusion": "GeCCo\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9c81\u68d2\u4e14\u6a21\u5757\u5316\u7684\u56db\u8db3\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4e3a\u65b0\u4efb\u52a1\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u7684\u6210\u672c\uff0c\u63a8\u52a8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.16914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16914", "abs": "https://arxiv.org/abs/2509.16914", "authors": ["Wenhao Zhuang", "Yuan Sun"], "title": "CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages", "comment": null, "summary": "Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities\nin various NLP tasks, significantly enhancing user experience and efficiency.\nHowever, this advantage is primarily limited to resource-rich languages. For\nthe diverse array of low-resource languages, support remains inadequate, with\nthe scarcity of training corpora considered the primary cause. We construct and\nopen-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two\n25GB sets of four-language corpora (one parallel and one non-parallel),\nobtained through machine translation. CUTE encompasses two resource-rich\nlanguages (Chinese and English) and two low-resource languages (Uyghur and\nTibetan). Prior to constructing CUTE, human assessment validates that the\nmachine translation quality between Chinese-Uyghur and Chinese-Tibetan\napproaches that of Chinese-English translation. CUTE represents the largest\nopen-source corpus for Uyghur and Tibetan languages to date, and we demonstrate\nits effectiveness in enhancing LLMs' ability to process low-resource languages\nwhile investigating the role of corpus parallelism in cross-lingual transfer\nlearning. The CUTE corpus and related models are made publicly available to the\nresearch community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CUTE\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5305\u542b\u4e2d\u6587\u3001\u82f1\u6587\u3001\u7ef4\u543e\u5c14\u8bed\u548c\u85cf\u8bed\u7684\u5927\u89c4\u6a21\u5f00\u6e90\u8bed\u6599\u5e93\uff0c\u65e8\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u4e30\u5bcc\u7684\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u652f\u6301\u4e0d\u8db3\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bad\u7ec3\u8bed\u6599\u7684\u7a00\u7f3a\u3002\u56e0\u6b64\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u4ee5\u6539\u5584\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5904\u7406\u80fd\u529b\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u6784\u5efa\u4e8625GB\u7684\u5e73\u884c\u4e0e\u975e\u5e73\u884c\u56db\u8bed\u8a00\u8bed\u6599\u5e93\uff08\u4e2d\u6587\u3001\u82f1\u6587\u3001\u7ef4\u543e\u5c14\u8bed\u3001\u85cf\u8bed\uff09\uff0c\u5e76\u5728\u6784\u5efa\u524d\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u4e2d-\u7ef4\u3001\u4e2d-\u85cf\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u63a5\u8fd1\u4e2d-\u82f1\u7ffb\u8bd1\u6c34\u5e73\u3002", "result": "CUTE\u662f\u76ee\u524d\u6700\u5927\u7684\u5f00\u6e90\u7ef4\u543e\u5c14\u8bed\u548c\u85cf\u8bed\u8bed\u6599\u5e93\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bed\u6599\u5e73\u884c\u6027\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "CUTE\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u63a8\u52a8\u4e86\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u53d1\u5c55\uff0c\u4e14\u6240\u6709\u6570\u636e\u4e0e\u6a21\u578b\u5747\u5df2\u516c\u5f00\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2509.17051", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.17051", "abs": "https://arxiv.org/abs/2509.17051", "authors": ["Riccardo Doyle"], "title": "Enhancing Performance and Calibration in Quantile Hyperparameter Optimization", "comment": "19 pages, 15 figures, 1 table", "summary": "Bayesian hyperparameter optimization relies heavily on Gaussian Process (GP)\nsurrogates, due to robust distributional posteriors and strong performance on\nlimited training samples. GPs however underperform in categorical\nhyperparameter environments or when assumptions of normality,\nheteroskedasticity and symmetry are excessively challenged. Conformalized\nquantile regression can address these estimation weaknesses, while still\nproviding robust calibration guarantees. This study builds upon early work in\nthis area by addressing feedback covariate shift in sequential acquisition and\nintegrating a wider range of surrogate architectures and acquisition functions.\nProposed algorithms are rigorously benchmarked against a range of state of the\nart hyperparameter optimization methods (GP, TPE and SMAC). Findings identify\nquantile surrogate architectures and acquisition functions yielding superior\nperformance to the current quantile literature, while validating the beneficial\nimpact of conformalization on calibration and search performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\u548c\u5171\u5f62\u5316\u7684\u4ee3\u7406\u6a21\u578b\u67b6\u6784\u4e0e\u91c7\u96c6\u51fd\u6570\uff0c\u5728\u8d1d\u53f6\u65af\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u975e\u6b63\u6001\u3001\u5f02\u65b9\u5dee\u548c\u5206\u7c7b\u8d85\u53c2\u6570\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u5728\u5904\u7406\u5206\u7c7b\u8d85\u53c2\u6570\u6216\u8fdd\u53cd\u6b63\u6001\u6027\u3001\u540c\u65b9\u5dee\u6027\u548c\u5bf9\u79f0\u6027\u5047\u8bbe\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5171\u5f62\u5316\u5206\u4f4d\u6570\u56de\u5f52\u89e3\u51b3GP\u7684\u5c40\u9650\u6027\uff0c\u5f15\u5165\u591a\u79cd\u4ee3\u7406\u6a21\u578b\u67b6\u6784\u548c\u91c7\u96c6\u51fd\u6570\uff0c\u5e76\u5e94\u5bf9\u5e8f\u5217\u91c7\u6837\u4e2d\u7684\u53cd\u9988\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u4e0eGP\u3001TPE\u548cSMAC\u7b49\u5148\u8fdb\u65b9\u6cd5\u7684\u57fa\u51c6\u6bd4\u8f83\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6821\u51c6\u6027\u548c\u641c\u7d22\u6027\u80fd\u3002", "conclusion": "\u5206\u4f4d\u6570\u4ee3\u7406\u67b6\u6784\u7ed3\u5408\u5171\u5f62\u5316\u80fd\u663e\u8457\u63d0\u5347\u8d85\u53c2\u6570\u4f18\u5316\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6311\u6218\u4f20\u7edfGP\u5047\u8bbe\u7684\u573a\u666f\u3002"}}
{"id": "2509.17711", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17711", "abs": "https://arxiv.org/abs/2509.17711", "authors": ["Shenwei Kang", "Xin Zhang", "Wen Liu", "Bin Li", "Yujie Liu", "Bo Gao"], "title": "DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation", "comment": null, "summary": "Human engagement estimation in conversational scenarios is essential for\napplications such as adaptive tutoring, remote healthcare assessment, and\nsocially aware human--computer interaction. Engagement is a dynamic, multimodal\nsignal conveyed by facial expressions, speech, gestures, and behavioral cues\nover time. In this work we introduce DA-Mamba, a dialogue-aware multimodal\narchitecture that replaces attention-heavy dialogue encoders with Mamba-based\nselective state-space processing to achieve linear time and memory complexity\nwhile retaining expressive cross-modal reasoning. We design a Mamba\ndialogue-aware selective state-space model composed of three core modules: a\nDialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group\nFusion and Partner-Group Fusion, these modules achieve expressive dialogue\nunderstanding. Extensive experiments on three standard benchmarks (NoXi,\nNoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art\n(SOTA) methods in concordance correlation coefficient (CCC), while reducing\ntraining time and peak memory; these gains enable processing much longer\nsequences and facilitate real-time deployment in resource-constrained,\nmulti-party conversational settings. The source code will be available at:\nhttps://github.com/kksssssss-ssda/MMEA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u8bdd\u611f\u77e5\u7684\u591a\u6a21\u6001\u67b6\u6784DA-Mamba\uff0c\u5229\u7528Mamba-based\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u66ff\u4ee3\u4f20\u7edf\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u4f30\u8ba1\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u4eba\u7c7b\u53c2\u4e0e\u5ea6\uff0c\u9700\u8981\u5904\u7406\u591a\u6a21\u6001\u52a8\u6001\u4fe1\u53f7\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u6d88\u8017\u65b9\u9762\u5b58\u5728\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eMamba\u7684\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5305\u542b\u5bf9\u8bdd\u611f\u77e5\u7f16\u7801\u5668\u548c\u4e24\u79cdMamba-based\u878d\u5408\u673a\u5236\uff08\u6a21\u6001\u7ec4\u878d\u5408\u548c\u4f19\u4f34\u7ec4\u878d\u5408\uff09\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5728NoXi\u3001NoXi-Add\u548cMPIIGI\u4e09\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\uff0cDA-Mamba\u5728\u4e00\u81f4\u6027\u76f8\u5173\u7cfb\u6570\uff08CCC\uff09\u4e0a\u8d85\u8fc7\u4e86\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "DA-Mamba\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u548c\u591a\u65b9\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.16702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16702", "abs": "https://arxiv.org/abs/2509.16702", "authors": ["Chen Liu", "Haitao Wu", "Kafeng Wang", "Xiaowang Zhang"], "title": "Animalbooth: multimodal feature enhancement for animal subject personalization", "comment": null, "summary": "Personalized animal image generation is challenging due to rich appearance\ncues and large morphological variability. Existing approaches often exhibit\nfeature misalignment across domains, which leads to identity drift. We present\nAnimalBooth, a framework that strengthens identity preservation with an Animal\nNet and an adaptive attention module, mitigating cross domain alignment errors.\nWe further introduce a frequency controlled feature integration module that\napplies Discrete Cosine Transform filtering in the latent space to guide the\ndiffusion process, enabling a coarse to fine progression from global structure\nto detailed texture. To advance research in this area, we curate AnimalBench, a\nhigh resolution dataset for animal personalization. Extensive experiments show\nthat AnimalBooth consistently outperforms strong baselines on multiple\nbenchmarks and improves both identity fidelity and perceptual quality.", "AI": {"tldr": "\u63d0\u51faAnimalBooth\u6846\u67b6\uff0c\u901a\u8fc7Animal Net\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u8eab\u4efd\u4fdd\u6301\uff0c\u7ed3\u5408\u9891\u57df\u63a7\u5236\u7279\u5f81\u878d\u5408\u5b9e\u73b0\u4ece\u5168\u5c40\u7ed3\u6784\u5230\u7ec6\u8282\u7eb9\u7406\u7684\u751f\u6210\uff0c\u5728\u52a8\u7269\u56fe\u50cf\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u4e0a\u5b58\u5728\u9519\u4f4d\u95ee\u9898\uff0c\u5bfc\u81f4\u8eab\u4efd\u6f02\u79fb\uff0c\u96be\u4ee5\u4fdd\u6301\u52a8\u7269\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faAnimalBooth\u6846\u67b6\uff0c\u5305\u62ecAnimal Net\u3001\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\u548c\u57fa\u4e8e\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u7684\u9891\u57df\u63a7\u5236\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "AnimalBooth\u6709\u6548\u7f13\u89e3\u4e86\u8de8\u57df\u5bf9\u9f50\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u52a8\u7269\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2509.17666", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17666", "abs": "https://arxiv.org/abs/2509.17666", "authors": ["Mimo Shirasaka", "Cristian C. Beltran-Hernandez", "Masashi Hamaya", "Yoshitaka Ushiku"], "title": "Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery", "comment": null, "summary": "Object insertion tasks are prone to failures under pose uncertainties and\nenvironmental variations, traditionally requiring manual finetuning or\ncontroller retraining. We present a novel approach for robust and resilient\nobject insertion using a passively compliant soft wrist that enables safe\ncontact absorption through large deformations, without high-frequency control\nor force sensing. Our method structures insertion as compliance-enabled contact\nformations, sequential contact states that progressively constrain degrees of\nfreedom, and integrates automated failure recovery strategies. Our key insight\nis that wrist compliance permits safe, repeated recovery attempts; hence, we\nrefer to it as compliance-enabled failure recovery. We employ a pre-trained\nvision-language model (VLM) that assesses each skill execution from terminal\nposes and images, identifies failure modes, and proposes recovery actions by\nselecting skills and updating goals. In simulation, our method achieved an 83%\nsuccess rate, recovering from failures induced by randomized\nconditions--including grasp misalignments up to 5 degrees, hole-pose errors up\nto 20mm, fivefold increases in friction, and previously unseen\nsquare/rectangular pegs--and we further validate the approach on a real robot.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u88ab\u52a8\u67d4\u987a\u8f6f\u8155\u7684\u9c81\u68d2\u7269\u4f53\u63d2\u5165\u65b9\u6cd5\uff0c\u5229\u7528\u67d4\u987a\u6027\u5b9e\u73b0\u63a5\u89e6\u5438\u6536\u548c\u81ea\u52a8\u5931\u8d25\u6062\u590d\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u5931\u8d25\u6a21\u5f0f\u5e76\u9009\u62e9\u6062\u590d\u52a8\u4f5c\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u7269\u4f53\u63d2\u5165\u4efb\u52a1\u5728\u59ff\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u73af\u5883\u53d8\u5316\u4e0b\u5bb9\u6613\u5931\u8d25\uff0c\u901a\u5e38\u9700\u8981\u624b\u52a8\u8c03\u6574\u6216\u91cd\u65b0\u8bad\u7ec3\u63a7\u5236\u5668\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u81ea\u6062\u590d\u80fd\u529b\u3002", "method": "\u5c06\u63d2\u5165\u4efb\u52a1\u5efa\u6a21\u4e3a\u67d4\u987a\u6027\u652f\u6301\u7684\u63a5\u89e6\u5f62\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8f6f\u8155\u7684\u88ab\u52a8\u67d4\u987a\u6027\u5438\u6536\u63a5\u89e6\u529b\uff0c\u65e0\u9700\u9ad8\u9891\u63a7\u5236\u6216\u529b\u4f20\u611f\uff1b\u5f15\u5165\u81ea\u52a8\u5316\u5931\u8d25\u6062\u590d\u7b56\u7565\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6839\u636e\u7ec8\u7aef\u59ff\u6001\u548c\u56fe\u50cf\u8bc6\u522b\u5931\u8d25\u6a21\u5f0f\u3001\u9009\u62e9\u6062\u590d\u6280\u80fd\u5e76\u66f4\u65b0\u76ee\u6807\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u8fbe\u523083%\u7684\u6210\u529f\u7387\uff0c\u80fd\u5e94\u5bf9\u6700\u59275\u5ea6\u7684\u6293\u53d6\u504f\u79fb\u300120mm\u7684\u5b54\u4f4d\u8bef\u5dee\u3001\u4e94\u500d\u6469\u64e6\u589e\u52a0\u4ee5\u53ca\u672a\u89c1\u8fc7\u7684\u65b9\u5f62/\u77e9\u5f62\u9500\u9489\uff1b\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u6210\u529f\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u67d4\u987a\u6027\u4e0d\u4ec5\u6709\u52a9\u4e8e\u5b89\u5168\u63a5\u89e6\uff0c\u8fd8\u53ef\u652f\u6301\u591a\u6b21\u81ea\u52a8\u6062\u590d\u5c1d\u8bd5\uff0c\u6240\u63d0\u51fa\u7684\u67d4\u987a\u6027\u8d4b\u80fd\u5931\u8d25\u6062\u590d\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63d2\u5165\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.16929", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16929", "abs": "https://arxiv.org/abs/2509.16929", "authors": ["Yongrui Chen", "Yi Huang", "Yunchang Liu", "Shenyu Zhang", "Junhao He", "Tongtong Wu", "Guilin Qi", "Tianxing Wu"], "title": "K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling", "comment": "Accepted in Neurips 2025 (poster)", "summary": "Continual Structured Knowledge Reasoning (CSKR) focuses on training models to\nhandle sequential tasks, where each task involves translating natural language\nquestions into structured queries grounded in structured knowledge. Existing\ngeneral continual learning approaches face significant challenges when applied\nto this task, including poor generalization to heterogeneous structured\nknowledge and inefficient reasoning due to parameter growth as tasks increase.\nTo address these limitations, we propose a novel CSKR framework,\n\\textsc{K-DeCore}, which operates with a fixed number of tunable parameters.\nUnlike prior methods, \\textsc{K-DeCore} introduces a knowledge decoupling\nmechanism that disentangles the reasoning process into task-specific and\ntask-agnostic stages, effectively bridging the gaps across diverse tasks.\nBuilding on this foundation, \\textsc{K-DeCore} integrates a dual-perspective\nmemory consolidation mechanism for distinct stages and introduces a\nstructure-guided pseudo-data synthesis strategy to further enhance the model's\ngeneralization capabilities. Extensive experiments on four benchmark datasets\ndemonstrate the superiority of \\textsc{K-DeCore} over existing continual\nlearning methods across multiple metrics, leveraging various backbone large\nlanguage models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u6846\u67b6K-DeCore\uff0c\u901a\u8fc7\u77e5\u8bc6\u89e3\u8026\u673a\u5236\u548c\u53cc\u89c6\u89d2\u8bb0\u5fc6\u5de9\u56fa\u673a\u5236\uff0c\u5728\u56fa\u5b9a\u53c2\u6570\u91cf\u4e0b\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e0b\u7684\u6cdb\u5316\u4e0e\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u7ed3\u6784\u5316\u77e5\u8bc6\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u968f\u7740\u4efb\u52a1\u589e\u52a0\u53c2\u6570\u589e\u957f\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faK-DeCore\u6846\u67b6\uff0c\u5f15\u5165\u77e5\u8bc6\u89e3\u8026\u673a\u5236\u5c06\u63a8\u7406\u5206\u4e3a\u4efb\u52a1\u7279\u5b9a\u548c\u4efb\u52a1\u65e0\u5173\u9636\u6bb5\uff0c\u5e76\u7ed3\u5408\u53cc\u89c6\u89d2\u8bb0\u5fc6\u5de9\u56fa\u548c\u7ed3\u6784\u5f15\u5bfc\u7684\u4f2a\u6570\u636e\u751f\u6210\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86K-DeCore\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b backbone\uff0c\u4e14\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "K-DeCore\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u91cf\u548c\u89e3\u8026\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86CSKR\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17063", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17063", "abs": "https://arxiv.org/abs/2509.17063", "authors": ["Shuang Liang", "Chaochuan Hou", "Xu Yao", "Shiping Wang", "Minqi Jiang", "Songqiao Han", "Hailiang Huang"], "title": "TSGym: Design Choices for Deep Multivariate Time-Series Forecasting", "comment": null, "summary": "Recently, deep learning has driven significant advancements in multivariate\ntime series forecasting (MTSF) tasks. However, much of the current research in\nMTSF tends to evaluate models from a holistic perspective, which obscures the\nindividual contributions and leaves critical issues unaddressed. Adhering to\nthe current modeling paradigms, this work bridges these gaps by systematically\ndecomposing deep MTSF methods into their core, fine-grained components like\nseries-patching tokenization, channel-independent strategy, attention modules,\nor even Large Language Models and Time-series Foundation Models. Through\nextensive experiments and component-level analysis, our work offers more\nprofound insights than previous benchmarks that typically discuss models as a\nwhole.\n  Furthermore, we propose a novel automated solution called TSGym for MTSF\ntasks. Unlike traditional hyperparameter tuning, neural architecture searching\nor fixed model selection, TSGym performs fine-grained component selection and\nautomated model construction, which enables the creation of more effective\nsolutions tailored to diverse time series data, therefore enhancing model\ntransferability across different data sources and robustness against\ndistribution shifts. Extensive experiments indicate that TSGym significantly\noutperforms existing state-of-the-art MTSF and AutoML methods. All code is\npublicly available on https://github.com/SUFE-AILAB/TSGym.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08MTSF\uff09\u7684\u7ec6\u7c92\u5ea6\u7ec4\u4ef6\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u81ea\u52a8\u5316\u6a21\u578b\u6784\u5efa\u7cfb\u7edfTSGym\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u6e90\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709MTSF\u7814\u7a76\u591a\u4ece\u6574\u4f53\u89d2\u5ea6\u8bc4\u4f30\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u5404\u7ec4\u4ef6\u7684\u72ec\u7acb\u8d21\u732e\uff0c\u5bfc\u81f4\u5173\u952e\u95ee\u9898\u88ab\u63a9\u76d6\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ec6\u81f4\u7684\u5206\u6790\u65b9\u6cd5\u6765\u63ed\u793a\u4e0d\u540c\u7ec4\u4ef6\u7684\u4f5c\u7528\u3002", "method": "\u5c06\u6df1\u5ea6MTSF\u65b9\u6cd5\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u6838\u5fc3\u7ec4\u4ef6\uff08\u5982\u5e8f\u5217\u5206\u5757\u3001\u901a\u9053\u72ec\u7acb\u7b56\u7565\u3001\u6ce8\u610f\u529b\u6a21\u5757\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684TSGym\u6846\u67b6\u5b9e\u73b0\u7ec4\u4ef6\u7ea7\u9009\u62e9\u4e0e\u81ea\u52a8\u5316\u6a21\u578b\u6784\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTSGym\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdbMTSF\u548cAutoML\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u8de8\u6570\u636e\u6e90\u8fc1\u79fb\u80fd\u529b\u548c\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ec4\u4ef6\u7ea7\u5206\u6790\u548c\u81ea\u52a8\u5316\u7ec4\u5408\uff0cTSGym\u4e3aMTSF\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u5efa\u6a21\u8303\u5f0f\u3002"}}
{"id": "2509.17774", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.17774", "abs": "https://arxiv.org/abs/2509.17774", "authors": ["Joao Marques-Silva", "Alexey Ignatiev"], "title": "Efficient & Correct Predictive Equivalence for Decision Trees", "comment": null, "summary": "The Rashomon set of decision trees (DTs) finds importance uses. Recent work\nshowed that DTs computing the same classification function, i.e. predictive\nequivalent DTs, can represent a significant fraction of the Rashomon set. Such\nredundancy is undesirable. For example, feature importance based on the\nRashomon set becomes inaccurate due the existence of predictive equivalent DTs,\ni.e. DTs with the same prediction for every possible input. In recent work,\nMcTavish et al. proposed solutions for several computational problems related\nwith DTs, including that of deciding predictive equivalent DTs. This approach,\nwhich this paper refers to as MBDSR, consists of applying the well-known method\nof Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal\nform) representations of DTs, which are then used for comparing DTs for\npredictive equivalence. Furthermore, the minimum-size DNF representation was\nalso applied to computing explanations for the predictions made by DTs, and to\nfinding predictions in the presence of missing data. However, the problem of\nformula minimization is hard for the second level of the polynomial hierarchy,\nand the QM method may exhibit worst-case exponential running time and space.\nThis paper first demonstrates that there exist decision trees that trigger the\nworst-case exponential running time and space of the QM method. Second, the\npaper shows that the MBDSR approach can produce incorrect results for the\nproblem of deciding predictive equivalence. Third, the paper shows that any of\nthe problems to which the minimum-size DNF representation has been applied to\ncan in fact be solved in polynomial time, in the size of the DT. The\nexperiments confirm that, for DTs for which the the worst-case of the QM method\nis triggered, the algorithms proposed in this paper are orders of magnitude\nfaster than the ones proposed by McTavish et al.", "AI": {"tldr": "\u672c\u6587\u6307\u51faMcTavish\u7b49\u4eba\u63d0\u51fa\u7684\u57fa\u4e8eQuine-McCluskey\u65b9\u6cd5\u7684\u51b3\u7b56\u6811\u9884\u6d4b\u7b49\u4ef7\u5224\u5b9a\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u9519\u8bef\u7ed3\u679c\u7684\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u76f8\u5173\u95ee\u9898\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u89e3\u51b3\uff0c\u5b9e\u9a8c\u663e\u793a\u65b0\u7b97\u6cd5\u663e\u8457\u66f4\u5feb\u3002", "motivation": "\u7531\u4e8e\u51b3\u7b56\u6811\u7684Rashomon\u96c6\u4e2d\u5b58\u5728\u5927\u91cf\u9884\u6d4b\u7b49\u4ef7\u7684\u5197\u4f59\u6811\uff0c\u5bfc\u81f4\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u4e0d\u51c6\u786e\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6700\u5c0fDNF\u8868\u793a\u65f6\u590d\u6742\u5ea6\u9ad8\uff0c\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u5206\u6790Quine-McCluskey\u65b9\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6307\u6570\u7ea7\u590d\u6742\u5ea6\uff0c\u63ed\u793a\u5176\u5728\u7279\u5b9a\u51b3\u7b56\u6811\u4e0a\u89e6\u53d1\u8be5\u95ee\u9898\uff1b\u6307\u51faMBDSR\u65b9\u6cd5\u5728\u5224\u65ad\u9884\u6d4b\u7b49\u4ef7\u6027\u65f6\u7684\u9519\u8bef\uff1b\u63d0\u51fa\u53ef\u5728\u51b3\u7b56\u6811\u5927\u5c0f\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u89e3\u51b3\u76f8\u5173\u95ee\u9898\u7684\u65b0\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u67d0\u4e9b\u51b3\u7b56\u6811\u4f1a\u89e6\u53d1QM\u65b9\u6cd5\u7684\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\uff1b\u53d1\u73b0MBDSR\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7684\u9884\u6d4b\u7b49\u4ef7\u5224\u65ad\uff1b\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7684\u65b0\u7b97\u6cd5\uff0c\u5728\u5b9e\u9a8c\u4e2d\u6bd4MBDSR\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u6700\u5c0f\u5c3a\u5bf8DNF\u8868\u793a\u5e76\u975e\u89e3\u51b3\u51b3\u7b56\u6811\u9884\u6d4b\u7b49\u4ef7\u3001\u89e3\u91ca\u751f\u6210\u548c\u7f3a\u5931\u6570\u636e\u9884\u6d4b\u7b49\u95ee\u9898\u7684\u5fc5\u8981\u624b\u6bb5\uff0c\u8fd9\u4e9b\u95ee\u9898\u5747\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u9ad8\u6548\u51c6\u786e\u5730\u89e3\u51b3\uff0c\u5e94\u907f\u514d\u4f7f\u7528\u590d\u6742\u5ea6\u9ad8\u7684QM\u65b9\u6cd5\u3002"}}
{"id": "2509.16704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16704", "abs": "https://arxiv.org/abs/2509.16704", "authors": ["Pan Liu", "Jinshi Liu"], "title": "When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation", "comment": null, "summary": "While significant advances exist in pseudo-label generation for\nsemi-supervised semantic segmentation, pseudo-label selection remains\nunderstudied. Existing methods typically use fixed confidence thresholds to\nretain high-confidence predictions as pseudo-labels. However, these methods\ncannot cope with network overconfidence tendency, where correct and incorrect\npredictions overlap significantly in high-confidence regions, making separation\nchallenging and amplifying model cognitive bias. Meanwhile, the direct\ndiscarding of low-confidence predictions disrupts spatial-semantic continuity,\ncausing critical context loss. We propose Confidence Separable Learning (CSL)\nto address these limitations. CSL formulates pseudo-label selection as a convex\noptimization problem within the confidence distribution feature space,\nestablishing sample-specific decision boundaries to distinguish reliable from\nunreliable predictions. Additionally, CSL introduces random masking of reliable\npixels to guide the network in learning contextual relationships from\nlow-reliability regions, thereby mitigating the adverse effects of discarding\nuncertain predictions. Extensive experimental results on the Pascal,\nCityscapes, and COCO benchmarks show that CSL performs favorably against\nstate-of-the-art methods. Code and model weights are available at\nhttps://github.com/PanLiuCSU/CSL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f2a\u6807\u7b7e\u9009\u62e9\u65b9\u6cd5\u2014\u2014\u7f6e\u4fe1\u5ea6\u53ef\u5206\u5b66\u4e60\uff08CSL\uff09\uff0c\u901a\u8fc7\u5728\u7f6e\u4fe1\u5ea6\u5206\u5e03\u7279\u5f81\u7a7a\u95f4\u4e2d\u6784\u5efa\u6837\u672c\u7279\u5b9a\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u5f15\u5165\u968f\u673a\u63a9\u7801\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u7f51\u7edc\u8fc7\u81ea\u4fe1\u548c\u4e0a\u4e0b\u6587\u4e22\u5931\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4f2a\u6807\u7b7e\u9009\u62e9\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u9608\u503c\uff0c\u96be\u4ee5\u5e94\u5bf9\u7f51\u7edc\u8fc7\u81ea\u4fe1\u5bfc\u81f4\u7684\u9ad8\u7f6e\u4fe1\u533a\u57df\u6b63\u786e\u4e0e\u9519\u8bef\u9884\u6d4b\u91cd\u53e0\u95ee\u9898\uff0c\u4e14\u76f4\u63a5\u4e22\u5f03\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4f1a\u7834\u574f\u8bed\u4e49\u8fde\u7eed\u6027\u3002", "method": "\u5c06\u4f2a\u6807\u7b7e\u9009\u62e9\u5efa\u6a21\u4e3a\u7f6e\u4fe1\u5ea6\u5206\u5e03\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5efa\u7acb\u6837\u672c\u7279\u5b9a\u7684\u51b3\u7b56\u8fb9\u754c\uff1b\u540c\u65f6\u5bf9\u53ef\u9760\u50cf\u7d20\u8fdb\u884c\u968f\u673a\u63a9\u7801\uff0c\u4fc3\u4f7f\u7f51\u7edc\u4ece\u4f4e\u53ef\u9760\u6027\u533a\u57df\u5b66\u4e60\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002", "result": "\u5728Pascal\u3001Cityscapes\u548cCOCO\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCSL\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "CSL\u901a\u8fc7\u52a8\u6001\u51b3\u7b56\u8fb9\u754c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u6807\u7b7e\u9009\u62e9\u7684\u53ef\u9760\u6027\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u81ea\u4fe1\u548c\u4e0a\u4e0b\u6587\u4e22\u5931\u95ee\u9898\u3002"}}
{"id": "2509.17683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17683", "abs": "https://arxiv.org/abs/2509.17683", "authors": ["Jonas Gruetter", "Lorenzo Terenzi", "Pascal Egli", "Marco Hutter"], "title": "Towards Learning Boulder Excavation with Hydraulic Excavators", "comment": null, "summary": "Construction sites frequently require removing large rocks before excavation\nor grading can proceed. Human operators typically extract these boulders using\nonly standard digging buckets, avoiding time-consuming tool changes to\nspecialized grippers. This task demands manipulating irregular objects with\nunknown geometries in harsh outdoor environments where dust, variable lighting,\nand occlusions hinder perception. The excavator must adapt to varying soil\nresistance--dragging along hard-packed surfaces or penetrating soft\nground--while coordinating multiple hydraulic joints to secure rocks using a\nshovel. Current autonomous excavation focuses on continuous media (soil,\ngravel) or uses specialized grippers with detailed geometric planning for\ndiscrete objects. These approaches either cannot handle large irregular rocks\nor require impractical tool changes that interrupt workflow. We train a\nreinforcement learning policy in simulation using rigid-body dynamics and\nanalytical soil models. The policy processes sparse LiDAR points (just 20 per\nrock) from vision-based segmentation and proprioceptive feedback to control\nstandard excavator buckets. The learned agent discovers different strategies\nbased on soil resistance: dragging along the surface in hard soil and\npenetrating directly in soft conditions. Field tests on a 12-ton excavator\nachieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to\n83% for human operators. This demonstrates that standard construction equipment\ncan learn complex manipulation despite sparse perception and challenging\noutdoor conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u6316\u6398\u7b56\u7565\uff0c\u5229\u7528\u6807\u51c6\u6316\u6398\u673a\u94f2\u6597\u5728\u7a00\u758f\u611f\u77e5\u548c\u590d\u6742\u6237\u5916\u6761\u4ef6\u4e0b\u9ad8\u6548\u79fb\u9664\u5927\u5757\u5ca9\u77f3\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u6316\u6398\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5927\u578b\u4e0d\u89c4\u5219\u5ca9\u77f3\uff0c\u4e14\u4f7f\u7528\u4e13\u7528\u5939\u5177\u4f1a\u4e2d\u65ad\u4f5c\u4e1a\u6d41\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5de5\u5177\u66f4\u6362\u3001\u9002\u5e94\u590d\u6742\u73af\u5883\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7ed3\u5408\u521a\u4f53\u52a8\u529b\u5b66\u548c\u89e3\u6790\u571f\u58e4\u6a21\u578b\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8f93\u5165\u4e3a\u57fa\u4e8e\u89c6\u89c9\u5206\u5272\u5f97\u5230\u7684\u7a00\u758fLiDAR\u70b9\u4e91\uff08\u6bcf\u77f3\u4ec520\u4e2a\u70b9\uff09\u548c\u672c\u4f53\u611f\u77e5\u53cd\u9988\uff0c\u76f4\u63a5\u63a7\u5236\u6807\u51c6\u6316\u6398\u673a\u94f2\u6597\u3002", "result": "\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4e0d\u540c\u5ca9\u77f3\uff080.4-0.7\u7c73\uff09\u548c\u571f\u58e4\u7c7b\u578b\u4e0b\uff0c12\u5428\u6316\u6398\u673a\u4efb\u52a1\u6210\u529f\u7387\u8fbe70%\uff0c\u4eba\u7c7b\u64cd\u4f5c\u5458\u4e3a83%\u3002\u4ee3\u7406\u80fd\u6839\u636e\u571f\u58e4\u963b\u529b\u81ea\u52a8\u9009\u62e9\u62d6\u62fd\u6216\u7a7f\u900f\u7b56\u7565\u3002", "conclusion": "\u6807\u51c6\u5de5\u7a0b\u673a\u68b0\u53ef\u901a\u8fc7\u5b66\u4e60\u5b9e\u73b0\u590d\u6742\u64cd\u4f5c\uff0c\u5373\u4f7f\u5728\u611f\u77e5\u53d7\u9650\u548c\u6076\u52a3\u6237\u5916\u6761\u4ef6\u4e0b\u4e5f\u5177\u5907\u53ef\u884c\u6027\uff0c\u65e0\u9700\u4e13\u7528\u5de5\u5177\u5373\u53ef\u5b8c\u6210\u5927\u77f3\u6e05\u9664\u4efb\u52a1\u3002"}}
{"id": "2509.16952", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16952", "abs": "https://arxiv.org/abs/2509.16952", "authors": ["Tiancheng Huang", "Ruisheng Cao", "Yuxin Zhang", "Zhangyi Kang", "Zijian Wang", "Chenrun Wang", "Yijie Luo", "Hang Zheng", "Lirong Qian", "Lu Chen", "Kai Yu"], "title": "AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation", "comment": null, "summary": "The growing volume of academic papers has made it increasingly difficult for\nresearchers to efficiently extract key information. While large language models\n(LLMs) based agents are capable of automating question answering (QA) workflows\nfor scientific papers, there still lacks a comprehensive and realistic\nbenchmark to evaluate their capabilities. Moreover, training an interactive\nagent for this specific task is hindered by the shortage of high-quality\ninteraction trajectories. In this work, we propose AirQA, a human-annotated\ncomprehensive paper QA dataset in the field of artificial intelligence (AI),\nwith 13,948 papers and 1,246 questions, that encompasses multi-task,\nmulti-modal and instance-level evaluation. Furthermore, we propose ExTrActor,\nan automated framework for instruction data synthesis. With three LLM-based\nagents, ExTrActor can perform example generation and trajectory collection\nwithout human intervention. Evaluations of multiple open-source and proprietary\nmodels show that most models underperform on AirQA, demonstrating the quality\nof our dataset. Extensive experiments confirm that ExTrActor consistently\nimproves the multi-turn tool-use capability of small models, enabling them to\nachieve performance comparable to larger ones.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AirQA\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4eba\u5de5\u6807\u6ce8AI\u9886\u57df\u8bba\u6587\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u4ee5\u53caExTrActor\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u6307\u4ee4\u6570\u636e\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u5b66\u672f\u8bba\u6587\u6570\u91cf\u6fc0\u589e\uff0c\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u9ad8\u6548\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff1b\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7f3a\u4e4f\u5168\u9762\u4e14\u771f\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u9ad8\u8d28\u91cf\u4ea4\u4e92\u8f68\u8ff9\u6570\u636e\u3002", "method": "\u63d0\u51faAirQA\u6570\u636e\u96c6\uff08\u5305\u542b13,948\u7bc7\u8bba\u6587\u548c1,246\u4e2a\u95ee\u9898\uff09\u8fdb\u884c\u591a\u4efb\u52a1\u3001\u591a\u6a21\u6001\u548c\u5b9e\u4f8b\u7ea7\u8bc4\u4f30\uff0c\u5e76\u8bbe\u8ba1ExTrActor\u6846\u67b6\uff0c\u5229\u7528\u4e09\u4e2a\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u4ee3\u7406\u81ea\u52a8\u5408\u6210\u6307\u4ee4\u6570\u636e\u548c\u6536\u96c6\u4ea4\u4e92\u8f68\u8ff9\u3002", "result": "\u591a\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728AirQA\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6311\u6218\u6027\uff1b\u5b9e\u9a8c\u8868\u660eExTrActor\u80fd\u6301\u7eed\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4f7f\u5176\u6027\u80fd\u63a5\u8fd1\u5927\u6a21\u578b\u3002", "conclusion": "AirQA\u4e3a\u79d1\u5b66\u8bba\u6587\u95ee\u7b54\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\uff0cExTrActor\u6709\u6548\u7f13\u89e3\u4e86\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u79d1\u7814\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.17092", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17092", "abs": "https://arxiv.org/abs/2509.17092", "authors": ["Michelangelo Conserva", "Remo Sasso", "Paulo Rauber"], "title": "On the Limits of Tabular Hardness Metrics for Deep RL: A Study with the Pharos Benchmark", "comment": null, "summary": "Principled evaluation is critical for progress in deep reinforcement learning\n(RL), yet it lags behind the theory-driven benchmarks of tabular RL. While\ntabular settings benefit from well-understood hardness measures like MDP\ndiameter and suboptimality gaps, deep RL benchmarks are often chosen based on\nintuition and popularity. This raises a critical question: can tabular hardness\nmetrics be adapted to guide non-tabular benchmarking? We investigate this\nquestion and reveal a fundamental gap. Our primary contribution is\ndemonstrating that the difficulty of non-tabular environments is dominated by a\nfactor that tabular metrics ignore: representation hardness. The same\nunderlying MDP can pose vastly different challenges depending on whether the\nagent receives state vectors or pixel-based observations. To enable this\nanalysis, we introduce \\texttt{pharos}, a new open-source library for\nprincipled RL benchmarking that allows for systematic control over both\nenvironment structure and agent representations. Our extensive case study using\n\\texttt{pharos} shows that while tabular metrics offer some insight, they are\npoor predictors of deep RL agent performance on their own. This work highlights\nthe urgent need for new, representation-aware hardness measures and positions\n\\texttt{pharos} as a key tool for developing them.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u6307\u51fa\u4f20\u7edf\u7684\u8868\u683c\u578bRL\u96be\u5ea6\u5ea6\u91cf\u65e0\u6cd5\u6709\u6548\u9884\u6d4b\u975e\u8868\u683c\u578b\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u4e3b\u8981\u56e0\u4e3a\u5ffd\u7565\u4e86\u8868\u793a\u96be\u5ea6\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u5f00\u6e90\u5e93pharos\uff0c\u7528\u4e8e\u7cfb\u7edf\u63a7\u5236\u73af\u5883\u7ed3\u6784\u548c\u667a\u80fd\u4f53\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u8868\u793a\u786c\u5ea6\u5bf9\u6df1\u5ea6RL\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u591a\u57fa\u4e8e\u76f4\u89c9\u548c\u6d41\u884c\u5ea6\u9009\u62e9\uff0c\u7f3a\u4e4f\u7406\u8bba\u9a71\u52a8\u7684\u96be\u5ea6\u5ea6\u91cf\u3002\u800c\u8868\u683c\u578bRL\u4e2d\u7684\u786c\u5ea6\u6307\u6807\uff08\u5982MDP\u76f4\u5f84\u548c\u5b50\u4f18\u95f4\u9699\uff09\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u975e\u8868\u683c\u578b\u73af\u5883\u3002\u56e0\u6b64\uff0c\u9700\u8981\u63a2\u7a76\u662f\u5426\u53ef\u5c06\u8868\u683c\u578b\u786c\u5ea6\u5ea6\u91cf\u6269\u5c55\u5230\u975e\u8868\u683c\u573a\u666f\uff0c\u5e76\u8bc6\u522b\u5f71\u54cd\u6df1\u5ea6RL\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3apharos\u7684\u5f00\u6e90\u5e93\uff0c\u652f\u6301\u5bf9\u73af\u5883\u7ed3\u6784\u548c\u667a\u80fd\u4f53\u8868\u793a\u8fdb\u884c\u7cfb\u7edf\u5316\u63a7\u5236\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u8868\u793a\uff08\u5982\u72b6\u6001\u5411\u91cf\u4e0e\u50cf\u7d20\u8f93\u5165\uff09\u4e0b\u667a\u80fd\u4f53\u7684\u8868\u73b0\uff0c\u5206\u6790\u4f20\u7edf\u8868\u683c\u578b\u786c\u5ea6\u6307\u6807\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u540c\u7684MDP\u5728\u4e0d\u540c\u8868\u793a\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u540c\u7684\u5b66\u4e60\u96be\u5ea6\uff0c\u8bf4\u660e\u8868\u793a\u786c\u5ea6\u662f\u975e\u8868\u683cRL\u4e2d\u4e00\u4e2a\u4e3b\u5bfc\u6027\u7684\u96be\u5ea6\u56e0\u7d20\u3002\u4f20\u7edf\u8868\u683c\u578b\u786c\u5ea6\u6307\u6807\u867d\u6709\u4e00\u5b9a\u53c2\u8003\u4ef7\u503c\uff0c\u4f46\u5355\u72ec\u4f7f\u7528\u65f6\u5bf9\u6df1\u5ea6RL\u6027\u80fd\u7684\u9884\u6d4b\u80fd\u529b\u8f83\u5f31\u3002", "conclusion": "\u975e\u8868\u683c\u5f3a\u5316\u5b66\u4e60\u7684\u96be\u5ea6\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u73af\u5883\u672c\u8eab\u7684\u7ed3\u6784\uff0c\u8fd8\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u72b6\u6001\u8868\u793a\u7684\u8d28\u91cf\uff1b\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u65b0\u7684\u3001\u8003\u8651\u8868\u793a\u786c\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\u3002pharos\u4e3a\u5f00\u53d1\u548c\u9a8c\u8bc1\u6b64\u7c7b\u65b0\u5ea6\u91cf\u63d0\u4f9b\u4e86\u57fa\u7840\u5de5\u5177\u3002"}}
{"id": "2509.17905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17905", "abs": "https://arxiv.org/abs/2509.17905", "authors": ["Zongqian Wu", "Baoduo Xu", "Tianyu Li", "Zhu Sun", "Xiaofeng Zhu", "Lei Feng"], "title": "Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling", "comment": "23 pages, 9 figures", "summary": "Test-time scaling (TTS) has been shown to improve the performance of large\nlanguage models (LLMs) by sampling and aggregating diverse reasoning paths.\nHowever, existing research has overlooked a critical issue: selection bias of\nreasoning strategies during scaling. Specifically, when generating reasoning\nprocesses, LLMs tend to follow certain strategies (e.g., algebraic solutions\nfor math problems) while neglecting other valid alternatives (e.g., geometric\nsolutions), resulting in insufficient exploration of the solution space. To\nfurther understand the impact of this bias, we present a theoretical analysis\nthat reveals when it undermines the effectiveness of test-time scaling.\nMotivated by this theoretical insight, we introduce TTS-Uniform, a framework\ndesigned to mitigate the selection bias of reasoning strategies. It (i)\nidentifies potential strategies, (ii) uniformly allocates the sampling budget\nacross them, and (iii) filters out unstable strategies prior to aggregation.\nExperimental results show that TTS-Uniform significantly enhances scaling\neffectiveness across multiple mainstream LLMs and benchmark datasets.", "AI": {"tldr": "\u63d0\u51faTTS-Uniform\u6846\u67b6\u4ee5\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u63a8\u7406\u7b56\u7565\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u63a8\u7406\u7b56\u7565\u7684\u9009\u62e9\u504f\u5dee\uff0c\u5bfc\u81f4\u89e3\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u6f5c\u5728\u7b56\u7565\u3001\u5747\u5300\u5206\u914d\u91c7\u6837\u9884\u7b97\u3001\u8fc7\u6ee4\u4e0d\u7a33\u5b9a\u7b56\u7565\u6765\u51cf\u8f7b\u9009\u62e9\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTTS-Uniform\u5728\u591a\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6269\u5c55\u6548\u679c\u3002", "conclusion": "TTS-Uniform\u6709\u6548\u7f13\u89e3\u4e86\u63a8\u7406\u7b56\u7565\u9009\u62e9\u504f\u5dee\uff0c\u589e\u5f3a\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6027\u80fd\u3002"}}
{"id": "2509.16721", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16721", "abs": "https://arxiv.org/abs/2509.16721", "authors": ["Haoyuan Li", "Rui Liu", "Hehe Fan", "Yi Yang"], "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding", "comment": "19 pages, 12 figures, 6 tables", "summary": "Enabling agents to understand and interact with complex 3D scenes is a\nfundamental challenge for embodied artificial intelligence systems. While\nMultimodal Large Language Models (MLLMs) have achieved significant progress in\n2D image understanding, extending such capabilities to 3D scenes remains\ndifficult: 1) 3D environment involves richer concepts such as spatial\nrelationships, affordances, physics, layout, and so on, 2) the absence of\nlarge-scale 3D vision-language datasets has posed a significant obstacle. In\nthis paper, we introduce Text-Scene, a framework that automatically parses 3D\nscenes into textual descriptions for scene understanding. Given a 3D scene, our\nmodel identifies object attributes and spatial relationships, and then\ngenerates a coherent summary of the whole scene, bridging the gap between 3D\nobservation and language without requiring human-in-the-loop intervention. By\nleveraging both geometric analysis and MLLMs, Text-Scene produces descriptions\nthat are accurate, detailed, and human-interpretable, capturing object-level\ndetails and global-level context. Experimental results on benchmarks\ndemonstrate that our textual parses can faithfully represent 3D scenes and\nbenefit downstream tasks. To evaluate the reasoning capability of MLLMs, we\npresent InPlan3D, a comprehensive benchmark for 3D task planning, consisting of\n3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity\nand accessibility in our approach, aiming to make 3D scene content\nunderstandable through language. Code and datasets will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Text-Scene\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D\u573a\u666f\u81ea\u52a8\u89e3\u6790\u4e3a\u6587\u672c\u63cf\u8ff0\u6765\u5b9e\u73b0\u5bf9\u590d\u67423D\u73af\u5883\u7684\u7406\u89e3\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u5206\u6790\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u51c6\u786e\u3001\u8be6\u7ec6\u7684\u573a\u666f\u63cf\u8ff0\uff1b\u540c\u65f6\u6784\u5efa\u4e86InPlan3D\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f303D\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57282D\u56fe\u50cf\u7406\u89e3\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u57283D\u573a\u666f\u7406\u89e3\u4e0a\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec3D\u7a7a\u95f4\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u6982\u5ff5\uff08\u5982\u7a7a\u95f4\u5173\u7cfb\u3001\u7269\u7406\u5e03\u5c40\u7b49\uff09\u4ee5\u53ca\u7f3a\u4e4f\u5927\u89c4\u6a213D\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faText-Scene\u6846\u67b6\uff0c\u5229\u7528\u51e0\u4f55\u5206\u6790\u8bc6\u522b3D\u573a\u666f\u4e2d\u7684\u7269\u4f53\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u3001\u53ef\u8bfb\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u76843D\u5230\u8bed\u8a00\u7684\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cText-Scene\u751f\u6210\u7684\u6587\u672c\u63cf\u8ff0\u80fd\u51c6\u786e\u53cd\u66203D\u573a\u666f\u5185\u5bb9\uff0c\u5e76\u6709\u52a9\u4e8e\u4e0b\u6e38\u4efb\u52a1\uff1b\u63d0\u51fa\u7684InPlan3D\u5305\u542b3174\u4e2a\u957f\u671f\u89c4\u5212\u4efb\u52a1\uff0c\u8986\u76d6636\u4e2a\u5ba4\u5185\u573a\u666f\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30MLLM\u57283D\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Text-Scene\u6709\u6548\u5f25\u5408\u4e863D\u611f\u77e5\u4e0e\u8bed\u8a00\u7406\u89e3\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4f7f\u590d\u67423D\u573a\u666f\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u88ab\u7406\u89e3\u548c\u64cd\u4f5c\uff0c\u63a8\u52a8\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.17750", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17750", "abs": "https://arxiv.org/abs/2509.17750", "authors": ["Inkyu Jang", "Jonghae Park", "Chams E. Mballo", "Sihyun Cho", "Claire J. Tomlin", "H. Jin Kim"], "title": "EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering", "comment": "Workshop on Safe and Robust Robot Learning for Operation in the Real\n  World (SAFE-ROL) at CoRL 2025", "summary": "We present EigenSafe, an operator-theoretic framework for learning-enabled\nsafety-critical control for stochastic systems. In many robotic systems where\ndynamics are best modeled as stochastic systems due to factors such as sensing\nnoise and environmental disturbances, it is challenging for conventional\nmethods such as Hamilton-Jacobi reachability and control barrier functions to\nprovide a holistic measure of safety. We derive a linear operator governing the\ndynamic programming principle for safety probability, and find that its\ndominant eigenpair provides information about safety for both individual states\nand the overall closed-loop system. The proposed learning framework, called\nEigenSafe, jointly learns this dominant eigenpair and a safe backup policy in\nan offline manner. The learned eigenfunction is then used to construct a safety\nfilter that detects potentially unsafe situations and falls back to the backup\npolicy. The framework is validated in three simulated stochastic\nsafety-critical control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u5b50\u7406\u8bba\u7684EigenSafe\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u968f\u673a\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\uff0c\u901a\u8fc7\u4e3b\u5bfc\u7279\u5f81\u5bf9\u548c\u5907\u4efd\u7b56\u7565\u5b9e\u73b0\u79bb\u7ebf\u5b66\u4e60\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982Hamilton-Jacobi\u53ef\u8fbe\u6027\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u96be\u4ee5\u5168\u9762\u8861\u91cf\u968f\u673a\u7cfb\u7edf\uff08\u5982\u53d7\u611f\u77e5\u566a\u58f0\u548c\u73af\u5883\u6270\u52a8\u5f71\u54cd\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff09\u7684\u5b89\u5168\u6027\u3002", "method": "\u63a8\u5bfc\u4e86\u4e00\u4e2a\u7ebf\u6027\u7b97\u5b50\u6765\u63cf\u8ff0\u5b89\u5168\u6982\u7387\u7684\u52a8\u6001\u89c4\u5212\u539f\u7406\uff0c\u5e76\u5229\u7528\u5176\u4e3b\u5bfc\u7279\u5f81\u5bf9\u4fe1\u606f\u8054\u5408\u5b66\u4e60\u5b89\u5168\u7279\u5f81\u51fd\u6570\u548c\u5b89\u5168\u5907\u4efd\u7b56\u7565\uff0c\u6784\u5efa\u5b89\u5168\u8fc7\u6ee4\u5668\u3002", "result": "EigenSafe\u6846\u67b6\u5728\u4e09\u4e2a\u6a21\u62df\u7684\u968f\u673a\u5b89\u5168\u5173\u952e\u63a7\u5236\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u4e0d\u5b89\u5168\u60c5\u51b5\u5e76\u89e6\u53d1\u5907\u4efd\u7b56\u7565\u3002", "conclusion": "EigenSafe\u4e3a\u968f\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u517c\u5177\u72b6\u6001\u7ea7\u548c\u7cfb\u7edf\u7ea7\u7684\u5b89\u5168\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2509.16965", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16965", "abs": "https://arxiv.org/abs/2509.16965", "authors": ["Minchan Kwon", "Junwon Ko", "Kangil Kim", "Junmo Kim"], "title": "Preference Distillation via Value based Reinforcement Learning", "comment": "20 page", "summary": "Direct Preference Optimization (DPO) is a powerful paradigm to align language\nmodels with human preferences using pairwise comparisons. However, its binary\nwin-or-loss supervision often proves insufficient for training small models\nwith limited capacity. Prior works attempt to distill information from large\nteacher models using behavior cloning or KL divergence. These methods often\nfocus on mimicking current behavior and overlook distilling reward modeling. To\naddress this issue, we propose \\textit{Teacher Value-based Knowledge\nDistillation} (TVKD), which introduces an auxiliary reward from the value\nfunction of the teacher model to provide a soft guide. This auxiliary reward is\nformulated to satisfy potential-based reward shaping, ensuring that the global\nreward structure and optimal policy of DPO are preserved. TVKD can be\nintegrated into the standard DPO training framework and does not require\nadditional rollouts. Our experimental results show that TVKD consistently\nimproves performance across various benchmarks and model sizes.", "AI": {"tldr": "\u63d0\u51fa\u6559\u5e08\u503c\u77e5\u8bc6\u84b8\u998f\uff08TVKD\uff09\uff0c\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u503c\u51fd\u6570\u63d0\u4f9b\u8f85\u52a9\u5956\u52b1\uff0c\u6539\u8fdb\u5c0f\u6a21\u578b\u5728\u76f4\u63a5\u504f\u597d\u4f18\u5316\u4e2d\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "DPO\u7684\u4e8c\u5143\u76d1\u7763\u5bf9\u5c0f\u5bb9\u91cf\u6a21\u578b\u4e0d\u8db3\uff0c\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u5ffd\u7565\u5956\u52b1\u5efa\u6a21\u7684\u63d0\u70bc\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6559\u5e08\u6a21\u578b\u503c\u51fd\u6570\u7684\u8f85\u52a9\u5956\u52b1\uff0c\u6ee1\u8db3\u52bf\u80fd\u5956\u52b1\u5851\u5f62\uff0c\u4fdd\u6301DPO\u7684\u5168\u5c40\u7ed3\u6784\u548c\u6700\u4f18\u7b56\u7565\u3002", "result": "TVKD\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916rollouts\u3002", "conclusion": "TVKD\u6709\u6548\u589e\u5f3a\u5c0f\u6a21\u578b\u7684\u504f\u597d\u5b66\u4e60\uff0c\u517c\u987e\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2509.17095", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17095", "abs": "https://arxiv.org/abs/2509.17095", "authors": ["Jinbao Wang", "Jun Liu", "Shiliang Zhang", "Xuehui Ma"], "title": "Ultra-short-term solar power forecasting by deep learning and data reconstruction", "comment": null, "summary": "The integration of solar power has been increasing as the green energy\ntransition rolls out. The penetration of solar power challenges the grid\nstability and energy scheduling, due to its intermittent energy generation.\nAccurate and near real-time solar power prediction is of critical importance to\ntolerant and support the permeation of distributed and volatile solar power\nproduction in the energy system. In this paper, we propose a deep-learning\nbased ultra-short-term solar power prediction with data reconstruction. We\ndecompose the data for the prediction to facilitate extensive exploration of\nthe spatial and temporal dependencies within the data. Particularly, we\nreconstruct the data into low- and high-frequency components, using ensemble\nempirical model decomposition with adaptive noise (CEEMDAN). We integrate\nmeteorological data with those two components, and employ deep-learning models\nto capture long- and short-term dependencies towards the target prediction\nperiod. In this way, we excessively exploit the features in historical data in\npredicting a ultra-short-term solar power production. Furthermore, as\nultra-short-term prediction is vulnerable to local optima, we modify the\noptimization in our deep-learning training by penalizing long prediction\nintervals. Numerical experiments with diverse settings demonstrate that,\ncompared to baseline models, the proposed method achieves improved\ngeneralization in data reconstruction and higher prediction accuracy for\nultra-short-term solar power production.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8d85\u77ed\u671f\u592a\u9633\u80fd\u529f\u7387\u9884\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408CEEMDAN\u6570\u636e\u5206\u89e3\u548c\u6c14\u8c61\u6570\u636e\u878d\u5408\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u592a\u9633\u80fd\u53d1\u7535\u5177\u6709\u95f4\u6b47\u6027\uff0c\u5176\u9ad8\u6e17\u900f\u7387\u5bf9\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u80fd\u6e90\u8c03\u5ea6\u6784\u6210\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u51c6\u786e\u4e14\u63a5\u8fd1\u5b9e\u65f6\u7684\u8d85\u77ed\u671f\u592a\u9633\u80fd\u529f\u7387\u9884\u6d4b\u6765\u652f\u6301\u80fd\u6e90\u7cfb\u7edf\u7684\u53ef\u9760\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u96c6\u5408\u7ecf\u9a8c\u6a21\u6001\u5206\u89e3\uff08CEEMDAN\uff09\u5c06\u5386\u53f2\u6570\u636e\u5206\u89e3\u4e3a\u9ad8\u4f4e\u9891\u6210\u5206\uff0c\u5e76\u4e0e\u6c14\u8c61\u6570\u636e\u878d\u5408\uff1b\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u60e9\u7f5a\u957f\u9884\u6d4b\u533a\u95f4\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u4ee5\u907f\u514d\u5c40\u90e8\u6700\u4f18\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u66f4\u597d\u7684\u6570\u636e\u91cd\u6784\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u9ad8\u7684\u8d85\u77ed\u671f\u592a\u9633\u80fd\u529f\u7387\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8d85\u77ed\u671f\u592a\u9633\u80fd\u529f\u7387\u9884\u6d4b\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7535\u7f51\u5bf9\u5206\u5e03\u5f0f\u3001\u6ce2\u52a8\u6027\u592a\u9633\u80fd\u53d1\u7535\u7684\u63a5\u7eb3\u80fd\u529b\u3002"}}
{"id": "2509.17907", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17907", "abs": "https://arxiv.org/abs/2509.17907", "authors": ["Xiaojing Dong", "Weilin Huang", "Liang Li", "Yiying Li", "Shu Liu", "Tongtong Ou", "Shuang Ouyang", "Yu Tian", "Fengxuan Zhao"], "title": "MEF: A Systematic Evaluation Framework for Text-to-Image Models", "comment": null, "summary": "Rapid advances in text-to-image (T2I) generation have raised higher\nrequirements for evaluation methodologies. Existing benchmarks center on\nobjective capabilities and dimensions, but lack an application-scenario\nperspective, limiting external validity. Moreover, current evaluations\ntypically rely on either ELO for overall ranking or MOS for dimension-specific\nscoring, yet both methods have inherent shortcomings and limited\ninterpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),\na systematic and practical approach for evaluating T2I models. First, we\npropose a structured taxonomy encompassing user scenarios, elements, element\ncompositions, and text expression forms to construct the Magic-Bench-377, which\nsupports label-level assessment and ensures a balanced coverage of both user\nscenarios and capabilities. On this basis, we combine ELO and\ndimension-specific MOS to generate model rankings and fine-grained assessments\nrespectively. This joint evaluation method further enables us to quantitatively\nanalyze the contribution of each dimension to user satisfaction using\nmultivariate logistic regression. By applying MEF to current T2I models, we\nobtain a leaderboard and key characteristics of the leading models. We release\nour evaluation framework and make Magic-Bench-377 fully open-source to advance\nresearch in the evaluation of visual generative models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMagic Evaluation Framework (MEF) \u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408ELO\u548cMOS\u8bc4\u5206\u5e76\u5f15\u5165\u591a\u53d8\u91cf\u903b\u8f91\u56de\u5f52\u5206\u6790\uff0c\u63d0\u5347\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5916\u90e8\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u5e94\u7528\u573a\u666f\u89c6\u89d2\uff0c\u4e14\u4f9d\u8d56ELO\u6216MOS\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5177\u5916\u90e8\u6709\u6548\u6027\u548c\u7ec6\u7c92\u5ea6\u89e3\u91ca\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7528\u6237\u573a\u666f\u3001\u5143\u7d20\u3001\u5143\u7d20\u7ec4\u5408\u548c\u6587\u672c\u8868\u8fbe\u5f62\u5f0f\u7684\u7ed3\u6784\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bbe\u8ba1Magic-Bench-377\u57fa\u51c6\uff1b\u7ed3\u5408ELO\u6574\u4f53\u6392\u540d\u4e0e\u7ef4\u5ea6\u7279\u5b9a\u7684MOS\u8bc4\u5206\uff0c\u5e76\u91c7\u7528\u591a\u53d8\u91cf\u903b\u8f91\u56de\u5f52\u91cf\u5316\u5404\u7ef4\u5ea6\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u8d21\u732e\u3002", "result": "\u5728\u5f53\u524dT2I\u6a21\u578b\u4e0a\u5e94\u7528MEF\u5f97\u5230\u4e86\u6a21\u578b\u6392\u884c\u699c\u548c\u5173\u952e\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u6807\u7b7e\u7ea7\u8bc4\u4f30\u548c\u66f4\u4f18\u7684\u573a\u666f\u4e0e\u80fd\u529b\u8986\u76d6\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "MEF\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u3001\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684T2I\u6a21\u578b\u8bc4\u4f30\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u89c6\u89c9\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u76f8\u5173\u6846\u67b6\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.16727", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16727", "abs": "https://arxiv.org/abs/2509.16727", "authors": ["Xin Lei Lin", "Soroush Mehraban", "Abhishek Moturu", "Babak Taati"], "title": "Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment", "comment": null, "summary": "Automated pain assessment from facial expressions is crucial for\nnon-communicative patients, such as those with dementia. Progress has been\nlimited by two challenges: (i) existing datasets exhibit severe demographic and\nlabel imbalance due to ethical constraints, and (ii) current generative models\ncannot precisely control facial action units (AUs), facial structure, or\nclinically validated pain levels.\n  We present 3DPain, a large-scale synthetic dataset specifically designed for\nautomated pain assessment, featuring unprecedented annotation richness and\ndemographic diversity. Our three-stage framework generates diverse 3D meshes,\ntextures them with diffusion models, and applies AU-driven face rigging to\nsynthesize multi-view faces with paired neutral and pain images, AU\nconfigurations, PSPI scores, and the first dataset-level annotations of\npain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain\nexpression heatmaps and 2,500 synthetic identities balanced by age, gender, and\nethnicity.\n  We further introduce ViTPain, a Vision Transformer based cross-modal\ndistillation framework in which a heatmap-trained teacher guides a student\ntrained on RGB images, enhancing accuracy, interpretability, and clinical\nreliability. Together, 3DPain and ViTPain establish a controllable, diverse,\nand clinically grounded foundation for generalizable automated pain assessment.", "AI": {"tldr": "\u63d0\u51fa3DPain\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u548cViTPain\u8de8\u6a21\u6001\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u975e\u4ea4\u6d41\u60a3\u8005\u75bc\u75db\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u4e0e\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u6807\u7b7e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e14\u751f\u6210\u6a21\u578b\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u3001\u7ed3\u6784\u548c\u4e34\u5e8a\u9a8c\u8bc1\u7684\u75bc\u75db\u6c34\u5e73\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u75bc\u75db\u8bc4\u4f30\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u6846\u67b6\u751f\u6210\u5305\u542b3D\u7f51\u683c\u3001\u6269\u6563\u6a21\u578b\u7eb9\u7406\u548cAU\u9a71\u52a8\u9762\u90e8\u7ed1\u5b9a\u7684\u591a\u89c6\u89d2\u4eba\u8138\u56fe\u50cf\uff0c\u6784\u5efa\u5177\u6709\u4e30\u5bcc\u6807\u6ce8\u548c\u4eba\u53e3\u591a\u6837\u6027\u76843DPain\u6570\u636e\u96c6\uff1b\u5e76\u63d0\u51fa\u57fa\u4e8eVision Transformer\u7684\u8de8\u6a21\u6001\u84b8\u998f\u6a21\u578bViTPain\uff0c\u5229\u7528\u70ed\u56fe\u6559\u5e08\u6a21\u578b\u6307\u5bfcRGB\u56fe\u50cf\u5b66\u751f\u6a21\u578b\u3002", "result": "3DPain\u5305\u542b82,500\u4e2a\u6837\u672c\uff0c\u8986\u76d625,000\u4e2a\u75bc\u75db\u70ed\u56fe\u548c2,500\u4e2a\u5408\u6210\u8eab\u4efd\uff0c\u5b9e\u73b0\u5e74\u9f84\u3001\u6027\u522b\u548c\u79cd\u65cf\u5e73\u8861\uff1bViTPain\u63d0\u5347\u4e86\u75bc\u75db\u8bc6\u522b\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "conclusion": "3DPain\u548cViTPain\u5171\u540c\u4e3a\u53ef\u6cdb\u5316\u7684\u81ea\u52a8\u5316\u75bc\u75db\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u63a7\u3001\u591a\u6837\u4e14\u4e34\u5e8a\u53ef\u9760\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2509.17759", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17759", "abs": "https://arxiv.org/abs/2509.17759", "authors": ["Chengbo Yuan", "Rui Zhou", "Mengzhen Liu", "Yingdong Hu", "Shengjie Wang", "Li Yi", "Chuan Wen", "Shanghang Zhang", "Yang Gao"], "title": "MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies", "comment": null, "summary": "Scaling real robot data is a key bottleneck in imitation learning, leading to\nthe use of auxiliary data for policy training. While other aspects of robotic\nmanipulation such as image or language understanding may be learned from\ninternet-based datasets, acquiring motion knowledge remains challenging. Human\ndata, with its rich diversity of manipulation behaviors, offers a valuable\nresource for this purpose. While previous works show that using human data can\nbring benefits, such as improving robustness and training efficiency, it\nremains unclear whether it can realize its greatest advantage: enabling robot\npolicies to directly learn new motions for task completion. In this paper, we\nsystematically explore this potential through multi-task human-robot\ncotraining. We introduce MotionTrans, a framework that includes a data\ncollection system, a human data transformation pipeline, and a weighted\ncotraining strategy. By cotraining 30 human-robot tasks simultaneously, we\ndirecly transfer motions of 13 tasks from human data to deployable end-to-end\nrobot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot\nmanner. MotionTrans also significantly enhances pretraining-finetuning\nperformance (+40% success rate). Through ablation study, we also identify key\nfactors for successful motion learning: cotraining with robot data and broad\ntask-related motion coverage. These findings unlock the potential of\nmotion-level learning from human data, offering insights into its effective use\nfor training robotic manipulation policies. All data, code, and model weights\nare open-sourced https://motiontrans.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MotionTrans\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u4eba\u673a\u534f\u540c\u8bad\u7ec3\uff0c\u7cfb\u7edf\u6027\u63a2\u7d22\u4e86\u4ece\u4eba\u7c7b\u6570\u636e\u4e2d\u8fc1\u79fb\u8fd0\u52a8\u77e5\u8bc6\u4ee5\u8bad\u7ec3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u4eba\u7c7b\u64cd\u4f5c\u884c\u4e3a\u5177\u6709\u4e30\u5bcc\u7684\u591a\u6837\u6027\uff0c\u56e0\u6b64\u5229\u7528\u4eba\u7c7b\u6570\u636e\u6765\u5f25\u8865\u673a\u5668\u4eba\u8fd0\u52a8\u77e5\u8bc6\u5b66\u4e60\u7684\u4e0d\u8db3\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u5c1a\u4e0d\u6e05\u695a\u662f\u5426\u80fd\u901a\u8fc7\u4eba\u7c7b\u6570\u636e\u8ba9\u673a\u5668\u4eba\u76f4\u63a5\u5b66\u4f1a\u65b0\u7684\u5b8c\u6210\u4efb\u52a1\u7684\u52a8\u4f5c\u3002", "method": "\u63d0\u51faMotionTrans\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\u3001\u4eba\u7c7b\u6570\u636e\u8f6c\u6362\u6d41\u7a0b\u548c\u52a0\u6743\u534f\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u572830\u4e2a\u4eba\u673a\u534f\u540c\u4efb\u52a1\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0c\u5c0613\u4e2a\u4efb\u52a1\u7684\u4eba\u7c7b\u52a8\u4f5c\u8fc1\u79fb\u5230\u7aef\u5230\u7aef\u7684\u673a\u5668\u4eba\u7b56\u7565\u4e2d\u3002", "result": "9\u4e2a\u4efb\u52a1\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u975e\u5e73\u51e1\u7684\u6210\u529f\u7387\uff1b\u9884\u8bad\u7ec3-\u5fae\u8c03\u6027\u80fd\u63d0\u534740%\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u4e0e\u673a\u5668\u4eba\u6570\u636e\u534f\u540c\u8bad\u7ec3\u548c\u5e7f\u6cdb\u7684\u4efb\u52a1\u76f8\u5173\u8fd0\u52a8\u8986\u76d6\u662f\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "MotionTrans\u9a8c\u8bc1\u4e86\u4ece\u4eba\u7c7b\u6570\u636e\u4e2d\u8fdb\u884c\u8fd0\u52a8\u7ea7\u5b66\u4e60\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u52bf\uff0c\u4e3a\u5229\u7528\u4eba\u7c7b\u6570\u636e\u8bad\u7ec3\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u548c\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2509.16990", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16990", "abs": "https://arxiv.org/abs/2509.16990", "authors": ["Avishai Elmakies", "Hagai Aronowitz", "Nimrod Shabtay", "Eli Schwartz", "Ron Hoory", "Avihu Dekel"], "title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO", "comment": null, "summary": "In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based\nmethod for training Speech-Aware Large Language Models (SALLMs) on open-format\nspeech understanding tasks, such as Spoken Question Answering and Automatic\nSpeech Translation. SALLMs have proven highly effective for speech\nunderstanding tasks. GRPO has recently gained traction for its efficiency in\ntraining LLMs, and prior work has explored its application to SALLMs, primarily\nin multiple-choice tasks. Building on this, we focus on open-format tasks that\nbetter reflect the generative abilities of the models. Our approach leverages\nGRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate\nempirically that it surpasses standard SFT across several key metrics. Finally,\nwe explore the potential of incorporating off-policy samples within GRPO for\nthese tasks, highlighting avenues for further improvement and further research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f00\u653e\u683c\u5f0f\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e0a\u8bad\u7ec3\u8bed\u97f3\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\uff08SALLMs\uff09\uff0c\u5e76\u5229\u7528BLEU\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u7684SFT\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c06GRPO\u5e94\u7528\u4e8e\u591a\u9009\u4efb\u52a1\uff0c\u800c\u5f00\u653e\u683c\u5f0f\u4efb\u52a1\u66f4\u80fd\u4f53\u73b0\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5176\u5728\u8be5\u7c7b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u4ee5BLEU\u5206\u6570\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6765\u4f18\u5316SALLMs\u3002", "result": "\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\u3002", "conclusion": "GRPO\u7ed3\u5408BLEU\u5956\u52b1\u4fe1\u53f7\u80fd\u6709\u6548\u63d0\u5347SALLMs\u5728\u5f00\u653e\u683c\u5f0f\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f15\u5165\u79bb\u7b56\u7565\u6837\u672c\u7684\u6539\u8fdb\u6f5c\u529b\u3002"}}
{"id": "2509.17105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17105", "abs": "https://arxiv.org/abs/2509.17105", "authors": ["Haoxin Guo", "Jiawen Pan", "Weixin Zhai"], "title": "GRPOformer: Advancing Hyperparameter Optimization via Group Relative Policy Optimization", "comment": null, "summary": "Hyperparameter optimization (HPO) plays a critical role in improving model\nperformance. Transformer-based HPO methods have shown great potential; however,\nexisting approaches rely heavily on large-scale historical optimization\ntrajectories and lack effective reinforcement learning (RL) techniques, thereby\nlimiting their efficiency and performance improvements. Inspired by the success\nof Group Relative Policy Optimization (GRPO) in large language models (LLMs),\nwe propose GRPOformer -- a novel hyperparameter optimization framework that\nintegrates reinforcement learning (RL) with Transformers. In GRPOformer,\nTransformers are employed to generate new hyperparameter configurations from\nhistorical optimization trajectories, while GRPO enables rapid trajectory\nconstruction and optimization strategy learning from scratch. Moreover, we\nintroduce Policy Churn Regularization (PCR) to enhance the stability of GRPO\ntraining. Experimental results on OpenML demonstrate that GRPOformer\nconsistently outperforms baseline methods across diverse tasks, offering new\ninsights into the application of RL for HPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d85\u53c2\u6570\u4f18\u5316\u6846\u67b6GRPOformer\uff0c\u7ed3\u5408Transformer\u4e0e\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7b56\u7565\u53d8\u5316\u6b63\u5219\u5316\uff08PCR\uff09\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5728OpenML\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u5386\u53f2\u8f68\u8ff9\u6570\u636e\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faGRPOformer\u6846\u67b6\uff0c\u4f7f\u7528Transformer\u751f\u6210\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u5e76\u7ed3\u5408GRPO\u8fdb\u884c\u5feb\u901f\u7b56\u7565\u5b66\u4e60\u4e0e\u8f68\u8ff9\u6784\u5efa\uff0c\u540c\u65f6\u5f15\u5165Policy Churn Regularization\uff08PCR\uff09\u4ee5\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728OpenML\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRPOformer\u5728\u4e0d\u540c\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "GRPOformer\u6709\u6548\u878d\u5408\u4e86Transformer\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u8303\u5f0f\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17917", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17917", "abs": "https://arxiv.org/abs/2509.17917", "authors": ["Junyu Lu", "Songxin Zhang", "Zejian Xie", "Zhuoyang Song", "Jiaxing Zhang"], "title": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent", "comment": null, "summary": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Orcust\u6846\u67b6\uff0c\u7ed3\u5408\u539f\u7406\u7ea6\u675f\u5956\u52b1\u5efa\u6a21\uff08PCRM\uff09\u548c\u5728\u7ebf\u865a\u62df\u673a\u9a71\u52a8\u7684\u8f68\u8ff9\u6784\u5efa\uff08OVTC\uff09\uff0c\u4ee5\u63d0\u5347GUI\u667a\u80fd\u4f53\u5728\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u53ef\u9760\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709GUI\u667a\u80fd\u4f53\u6a21\u578b\u9762\u4e34\u5956\u52b1\u4fe1\u53f7\u4e0d\u53ef\u9760\u548c\u5728\u7ebf\u8f68\u8ff9\u751f\u6210\u6709\u9650\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u4fdd\u8bc1\u957f\u94fe\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u4efb\u52a1\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faPCRM\u5229\u7528\u73af\u5883\u53ef\u9a8c\u8bc1\u548cLLM\u751f\u6210\u7684\u539f\u5219\u6765\u7ea6\u675f\u5956\u52b1\u4fe1\u53f7\uff0c\u589e\u5f3a\u63a8\u7406\u53ef\u89e3\u91ca\u6027\uff1b\u901a\u8fc7OVTC\u5728\u4eea\u5668\u5316\u865a\u62df\u673a\u4e2d\u81ea\u4e3b\u6536\u96c6\u5177\u6709\u660e\u786e\u7a0b\u5e8f\u548c\u7ed3\u6784\u76ee\u6807\u7684GUI\u4ea4\u4e92\u8f68\u8ff9\uff0c\u8bad\u7ec3\u9010\u6b65\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728ScreenSpot\u548cScreenSpot-Pro\u57fa\u51c6\u4e0a\u5206\u522b\u6bd4\u57fa\u7840\u6a21\u578bQwen2.5-VL-7B\u63d0\u534722.2%\u548c23.9%\uff0c\u5e76\u5728\u591a\u4e2aGUI\u4efb\u52a1\u4e2d\u5c55\u73b0\u4f18\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u3001\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Orcust\u663e\u8457\u63d0\u5347\u4e86GUI\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8f68\u8ff9\u751f\u6210\u548c\u539f\u5219\u7ea6\u675f\u5956\u52b1\u5efa\u6a21\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u7684\u4ea4\u4e92\u51b3\u7b56\u3002"}}
{"id": "2509.16738", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16738", "abs": "https://arxiv.org/abs/2509.16738", "authors": ["Kai Jiang", "Zhengyan Shi", "Dell Zhang", "Hongyuan Zhang", "Xuelong Li"], "title": "Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning", "comment": "Accepted by NeurIPS 2025. Source Code will be released in the next\n  version", "summary": "Class Incremental Learning (CIL) aims to continuously learn new categories\nwhile retaining the knowledge of old ones. Pre-trained models (PTMs) show\npromising capabilities in CIL. However, existing approaches that apply\nlightweight fine-tuning to backbones still induce parameter drift, thereby\ncompromising the generalization capability of pre-trained models. Parameter\ndrift can be conceptualized as a form of noise that obscures critical patterns\nlearned for previous tasks. However, recent researches have shown that noise is\nnot always harmful. For example, the large number of visual patterns learned\nfrom pre-training can be easily abused by a single task, and introducing\nappropriate noise can suppress some low-correlation features, thus leaving a\nmargin for future tasks. To this end, we propose learning beneficial noise for\nCIL guided by information theory and propose Mixture of Noise (Min), aiming to\nmitigate the degradation of backbone generalization from adapting new tasks.\nSpecifically, task-specific noise is learned from high-dimension features of\nnew tasks. Then, a set of weights is adjusted dynamically for optimal mixture\nof different task noise. Finally, Min embeds the beneficial noise into the\nintermediate features to mask the response of inefficient patterns. Extensive\nexperiments on six benchmark datasets demonstrate that Min achieves\nstate-of-the-art performance in most incremental settings, with particularly\noutstanding results in 50-steps incremental settings. This shows the\nsignificant potential for beneficial noise in continual learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u6307\u5bfc\u7684\u201c\u6709\u76ca\u566a\u58f0\u201d\u5b66\u4e60\u65b9\u6cd5Mixture of Noise (Min)\uff0c\u7528\u4e8e\u7f13\u89e3\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u9000\u5316\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u572850\u6b65\u589e\u91cf\u8bbe\u7f6e\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5728\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u4f1a\u5f15\u8d77\u53c2\u6570\u6f02\u79fb\uff0c\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u800c\u73b0\u6709\u7814\u7a76\u53d1\u73b0\u566a\u58f0\u5e76\u4e0d\u603b\u662f\u6709\u5bb3\u7684\uff0c\u9002\u5f53\u5f15\u5165\u566a\u58f0\u53ef\u6291\u5236\u4f4e\u76f8\u5173\u6027\u7279\u5f81\uff0c\u4e3a\u540e\u7eed\u4efb\u52a1\u4fdd\u7559\u7a7a\u95f4\u3002", "method": "\u63d0\u51faMixture of Noise (Min)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u65b0\u4efb\u52a1\u7684\u9ad8\u7ef4\u7279\u5f81\u4e2d\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u566a\u58f0\uff0c\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u4efb\u52a1\u566a\u58f0\u7684\u6df7\u5408\u6743\u91cd\uff0c\u5e76\u5c06\u6709\u76ca\u566a\u58f0\u5d4c\u5165\u4e2d\u95f4\u7279\u5f81\u4ee5\u5c4f\u853d\u65e0\u6548\u6a21\u5f0f\u7684\u54cd\u5e94\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cMin\u5728\u5927\u591a\u6570\u589e\u91cf\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c24\u5176\u572850\u6b65\u589e\u91cf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5408\u7406\u5229\u7528\u566a\u58f0\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u9000\u5316\uff0c\u9a8c\u8bc1\u4e86\u2018\u6709\u76ca\u566a\u58f0\u2019\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.17760", "categories": ["cs.RO", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17760", "abs": "https://arxiv.org/abs/2509.17760", "authors": ["Austin Wilson", "Sahar Kapasi", "Zane Greene", "Alexis E. Block"], "title": "Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research", "comment": null, "summary": "Many research groups face challenges when legacy (unsupported) robotic\nplatforms lose manufacturer support and cannot accommodate modern sensing,\nspeech, and interaction capabilities. We present the Enhanced NAO, a\nrevitalized version of Aldebaran's NAO robot that uses upgraded microphones,\nRGB-D and thermal cameras, and additional compute resources in a fully\nself-contained package. This system combines cloud and local models for\nperception and dialogue, while preserving the NAO's expressive body and\nbehaviors. In a pilot validation study, the Enhanced NAO delivered\nsignificantly higher conversational quality and stronger user preference\ncompared to the NAO AI Edition, without increasing response latency. Key\nupgrades, such as beamforming microphones and low-latency audio processing,\nreduced artifacts like self-hearing and improved multi-party separation.\nExpanded visual and thermal sensing established a foundation for future\ninteraction capabilities. Beyond the NAO, our framework provides a\nplatform-agnostic strategy for extending the lifespan and research utility of\nlegacy robots, ensuring they remain valuable tools for human-robot interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7248NAO\u673a\u5668\u4eba\uff08Enhanced NAO\uff09\uff0c\u901a\u8fc7\u786c\u4ef6\u5347\u7ea7\u548c\u4e91-\u672c\u5730\u6df7\u5408\u8ba1\u7b97\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u65e7\u6b3e\u673a\u5668\u4eba\u7684\u611f\u77e5\u4e0e\u4ea4\u4e92\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u539f\u6709\u8868\u73b0\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5bf9\u8bdd\u8d28\u91cf\u548c\u7528\u6237\u504f\u597d\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u8bb8\u591a\u7814\u7a76\u56e2\u961f\u9762\u4e34\u65e7\u578b\u673a\u5668\u4eba\u5e73\u53f0\u56e0\u5931\u53bb\u5382\u5546\u652f\u6301\u800c\u96be\u4ee5\u96c6\u6210\u73b0\u4ee3\u611f\u77e5\u3001\u8bed\u97f3\u548c\u4ea4\u4e92\u529f\u80fd\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u4e2d\u7684\u6301\u7eed\u5e94\u7528\u3002", "method": "\u5728\u539fNAO\u673a\u5668\u4eba\u57fa\u7840\u4e0a\u96c6\u6210\u9ad8\u6027\u80fd\u9ea6\u514b\u98ce\u3001RGB-D\u4e0e\u70ed\u6210\u50cf\u6444\u50cf\u5934\u53ca\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\uff0c\u6784\u5efa\u5b8c\u5168\u81ea\u5305\u542b\u7cfb\u7edf\uff1b\u91c7\u7528\u4e91\u7aef\u4e0e\u672c\u5730\u534f\u540c\u7684\u611f\u77e5\u4e0e\u5bf9\u8bdd\u6a21\u578b\uff0c\u4fdd\u7559NAO\u539f\u6709\u7684\u52a8\u4f5c\u8868\u73b0\u80fd\u529b\u3002", "result": "\u5728\u521d\u6b65\u9a8c\u8bc1\u5b9e\u9a8c\u4e2d\uff0cEnhanced NAO\u76f8\u6bd4NAO AI Edition\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u8d28\u91cf\u4e0e\u7528\u6237\u504f\u597d\uff0c\u672a\u589e\u52a0\u54cd\u5e94\u5ef6\u8fdf\uff1b\u6ce2\u675f\u6210\u5f62\u9ea6\u514b\u98ce\u548c\u4f4e\u5ef6\u8fdf\u97f3\u9891\u5904\u7406\u51cf\u5c11\u4e86\u81ea\u542c\u56de\u6388\u5e76\u6539\u5584\u4e86\u591a\u65b9\u8bed\u97f3\u5206\u79bb\uff1b\u6269\u5c55\u7684\u89c6\u89c9\u4e0e\u70ed\u611f\u4f20\u611f\u4e3a\u672a\u6765\u4ea4\u4e92\u529f\u80fd\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u6210\u529f\u5ef6\u957f\u4e86NAO\u673a\u5668\u4eba\u7684\u7814\u7a76\u5bff\u547d\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5176\u4ed6\u65e7\u5e73\u53f0\u7684\u901a\u7528\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u7ef4\u6301 legacy \u673a\u5668\u4eba\u5728\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.17030", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17030", "abs": "https://arxiv.org/abs/2509.17030", "authors": ["Hinata Tezuka", "Naoya Inoue"], "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs", "comment": "57 pages, 47 figures and 41 tables; Accepted to EMNLP 2025 Main", "summary": "Recent studies have suggested a processing framework for multilingual inputs\nin decoder-based LLMs: early layers convert inputs into English-centric and\nlanguage-agnostic representations; middle layers perform reasoning within an\nEnglish-centric latent space; and final layers generate outputs by transforming\nthese representations back into language-specific latent spaces. However, the\ninternal dynamics of such transformation and the underlying mechanism remain\nunderexplored. Towards a deeper understanding of this framework, we propose and\nempirically validate The Transfer Neurons Hypothesis: certain neurons in the\nMLP module are responsible for transferring representations between\nlanguage-specific latent spaces and a shared semantic latent space.\nFurthermore, we show that one function of language-specific neurons, as\nidentified in recent studies, is to facilitate movement between latent spaces.\nFinally, we show that transfer neurons are critical for reasoning in\nmultilingual LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u8f6c\u79fb\u795e\u7ecf\u5143\u5047\u8bf4\u201d\uff0c\u8ba4\u4e3a\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e2d\u67d0\u4e9bMLP\u795e\u7ecf\u5143\u8d1f\u8d23\u5728\u8bed\u8a00\u7279\u5b9a\u7a7a\u95f4\u4e0e\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u4e4b\u95f4\u8f6c\u6362\u8868\u5f81\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u795e\u7ecf\u5143\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u8f93\u5165\u7684\u5904\u7406\u6846\u67b6\uff0c\u4f46\u5176\u5185\u90e8\u8f6c\u6362\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u63a2\u7a76\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e2d\u8bed\u8a00\u95f4\u8868\u5f81\u8f6c\u6362\u7684\u5e95\u5c42\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u5728\u89e3\u7801\u5668\u578b\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e2d\u8bc6\u522b\u5e76\u9a8c\u8bc1\u8d1f\u8d23\u5728\u8bed\u8a00\u7279\u5b9a\u7a7a\u95f4\u4e0e\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u4e4b\u95f4\u8f6c\u6362\u7684\u2018\u8f6c\u79fb\u795e\u7ecf\u5143\u2019\uff0c\u5e76\u5206\u6790\u5176\u529f\u80fd\u4e0e\u91cd\u8981\u6027\u3002", "result": "\u53d1\u73b0\u4e86\u652f\u6301\u2018\u8f6c\u79fb\u795e\u7ecf\u5143\u5047\u8bf4\u2019\u7684\u8bc1\u636e\uff0c\u8868\u660e\u8fd9\u4e9b\u795e\u7ecf\u5143\u5728\u8bed\u8a00\u8868\u5f81\u8f6c\u6362\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4e14\u8bed\u8a00\u7279\u5f02\u6027\u795e\u7ecf\u5143\u7684\u529f\u80fd\u4e4b\u4e00\u662f\u4fc3\u8fdb\u4e0d\u540c\u6f5c\u5728\u7a7a\u95f4\u4e4b\u95f4\u7684\u8fc1\u79fb\u3002", "conclusion": "\u8f6c\u79fb\u795e\u7ecf\u5143\u5728\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u8868\u793a\u8f6c\u6362\u7684\u5185\u5728\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u591a\u8bed\u8a00LLMs\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.17119", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17119", "abs": "https://arxiv.org/abs/2509.17119", "authors": ["Yifei Wu", "Bo Wang", "Jingshi Cui", "Pei-chun Lin", "Junzo Watada"], "title": "ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware Renewable Scenario Forecasting", "comment": null, "summary": "To address the intermittency of renewable energy source (RES) generation,\nscenario forecasting offers a series of stochastic realizations for predictive\nobjects with superior flexibility and direct views. Based on a long time-series\nperspective, this paper explores uncertainties in the realms of renewable power\nand deep learning. Then, an uncertainty-aware model is meticulously designed\nfor renewable scenario forecasting, which leverages an attention mechanism and\ngenerative adversarial networks (GANs) to precisely capture complex\nspatial-temporal dynamics. To improve the interpretability of uncertain\nbehavior in RES generation, Bayesian deep learning and adaptive instance\nnormalization (AdaIN) are incorporated to simulate typical patterns and\nvariations. Additionally, the integration of meteorological information,\nforecasts, and historical trajectories in the processing layer improves the\nsynergistic forecasting capability for multiscale periodic regularities.\nNumerical experiments and case analyses demonstrate that the proposed approach\nprovides an appropriate interpretation for renewable uncertainty\nrepresentation, including both aleatoric and epistemic uncertainties, and shows\nsuperior performance over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578b\uff0c\u7528\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u573a\u666f\u9884\u6d4b\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\uff0c\u63d0\u5347\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u7684\u95f4\u6b47\u6027\u95ee\u9898\uff0c\u63d0\u5347\u9884\u6d4b\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u4e0e\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\uff08AdaIN\uff09\uff0c\u5e76\u878d\u5408\u6c14\u8c61\u4fe1\u606f\u3001\u9884\u62a5\u548c\u5386\u53f2\u8f68\u8ff9\u8fdb\u884c\u591a\u5c3a\u5ea6\u5468\u671f\u89c4\u5f8b\u534f\u540c\u9884\u6d4b\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u548c\u6848\u4f8b\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8868\u5f81\u968f\u673a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u53ef\u518d\u751f\u80fd\u6e90\u7684\u590d\u6742\u65f6\u7a7a\u52a8\u6001\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u573a\u666f\u9884\u6d4b\u7ed3\u679c\u3002"}}
{"id": "2509.17956", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17956", "abs": "https://arxiv.org/abs/2509.17956", "authors": ["Lin Luo", "Yuri Nakao", "Mathieu Chollet", "Hiroya Inakoshi", "Simone Stumpf"], "title": "\"I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment", "comment": null, "summary": "Assessing fairness in artificial intelligence (AI) typically involves AI\nexperts who select protected features, fairness metrics, and set fairness\nthresholds. However, little is known about how stakeholders, particularly those\naffected by AI outcomes but lacking AI expertise, assess fairness. To address\nthis gap, we conducted a qualitative study with 30 stakeholders without AI\nexpertise, representing potential decision subjects in a credit rating\nscenario, to examine how they assess fairness when placed in the role of\ndeciding on features with priority, metrics, and thresholds. We reveal that\nstakeholders' fairness decisions are more complex than typical AI expert\npractices: they considered features far beyond legally protected features,\ntailored metrics for specific contexts, set diverse yet stricter fairness\nthresholds, and even preferred designing customized fairness. Our results\nextend the understanding of how stakeholders can meaningfully contribute to AI\nfairness governance and mitigation, underscoring the importance of\nincorporating stakeholders' nuanced fairness judgments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e00\u9879\u9488\u5bf930\u540d\u65e0AI\u4e13\u4e1a\u77e5\u8bc6\u7684\u5229\u76ca\u76f8\u5173\u8005\u7684\u5b9a\u6027\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u4ed6\u4eec\u5728\u4fe1\u7528\u8bc4\u7ea7\u60c5\u5883\u4e2d\u5982\u4f55\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u5176\u516c\u5e73\u6027\u5224\u65ad\u6bd4\u4e13\u5bb6\u5e38\u89c4\u505a\u6cd5\u66f4\u590d\u6742\u4e14\u4e25\u683c\u3002", "motivation": "\u4e86\u89e3\u975e\u4e13\u5bb6\u5229\u76ca\u76f8\u5173\u8005\u5982\u4f55\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u516c\u5e73\u6027\uff0c\u5f25\u8865\u5f53\u524d\u516c\u5e73\u6027\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e13\u5bb6\u800c\u5ffd\u89c6\u53d7\u5f71\u54cd\u7fa4\u4f53\u89c2\u70b9\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u8ba930\u540d\u4ee3\u8868\u6f5c\u5728\u51b3\u7b56\u5bf9\u8c61\u7684\u975eAI\u4e13\u5bb6\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u4fe1\u7528\u8bc4\u7ea7\u573a\u666f\uff0c\u5206\u6790\u4ed6\u4eec\u5728\u9009\u62e9\u7279\u5f81\u3001\u516c\u5e73\u6027\u6307\u6807\u548c\u9608\u503c\u65f6\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5229\u76ca\u76f8\u5173\u8005\u4e0d\u4ec5\u8003\u8651\u6cd5\u5f8b\u4fdd\u62a4\u7279\u5f81\u4e4b\u5916\u7684\u66f4\u591a\u56e0\u7d20\uff0c\u8fd8\u6839\u636e\u5177\u4f53\u60c5\u5883\u5b9a\u5236\u6307\u6807\uff0c\u8bbe\u5b9a\u66f4\u4e25\u683c\u591a\u6837\u7684\u516c\u5e73\u6027\u9608\u503c\uff0c\u5e76\u503e\u5411\u4e8e\u8bbe\u8ba1\u4e2a\u6027\u5316\u7684\u516c\u5e73\u6027\u65b9\u6848\u3002", "conclusion": "\u5229\u76ca\u76f8\u5173\u8005\u7684\u516c\u5e73\u6027\u5224\u65ad\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e94\u5c06\u5176\u7eb3\u5165AI\u516c\u5e73\u6027\u6cbb\u7406\u4e0e\u7f13\u89e3\u5b9e\u8df5\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u548c\u7ec6\u81f4\u7684\u516c\u5e73\u6027\u4fdd\u969c\u3002"}}
{"id": "2509.16745", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16745", "abs": "https://arxiv.org/abs/2509.16745", "authors": ["Ritabrata Chakraborty", "Avijit Dasgupta", "Sandeep Chaurasia"], "title": "CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding", "comment": "9 pages, 5 figures, 6 tables", "summary": "Visual explanations are often plausible but not structurally faithful. We\nintroduce CAMBench-QR, a structure-aware benchmark that leverages the canonical\ngeometry of QR codes (finder patterns, timing lines, module grid) to test\nwhether CAM methods place saliency on requisite substructures while avoiding\nbackground. CAMBench-QR synthesizes QR/non-QR data with exact masks and\ncontrolled distortions, and reports structure-aware metrics (Finder/Timing Mass\nRatios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside\ncausal occlusion, insertion/deletion faithfulness, robustness, and latency. We\nbenchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)\nunder two practical regimes of zero-shot and last-block fine-tuning. The\nbenchmark, metrics, and training recipes provide a simple, reproducible\nyardstick for structure-aware evaluation of visual explanations. Hence we\npropose that CAMBENCH-QR can be used as a litmus test of whether visual\nexplanations are truly structure-aware.", "AI": {"tldr": "CAMBench-QR\u662f\u4e00\u4e2a\u7ed3\u6784\u611f\u77e5\u7684\u57fa\u51c6\uff0c\u5229\u7528QR\u7801\u7684\u51e0\u4f55\u7279\u5f81\u8bc4\u4f30\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\uff08\u5982CAM\uff09\u662f\u5426\u5173\u6ce8\u5fc5\u8981\u5b50\u7ed3\u6784\u5e76\u907f\u514d\u80cc\u666f\u5e72\u6270\uff0c\u63d0\u4f9b\u591a\u79cd\u7ed3\u6784\u611f\u77e5\u6307\u6807\u548c\u6d4b\u8bd5\u573a\u666f\uff0c\u53ef\u4f5c\u4e3a\u68c0\u9a8c\u89c6\u89c9\u89e3\u91ca\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u7684\u8bd5\u91d1\u77f3\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u867d\u7136\u770b\u4f3c\u5408\u7406\uff0c\u4f46\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u4e0a\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u8bc4\u4f30\u5176\u662f\u5426\u5173\u6ce8\u5173\u952e\u7ed3\u6784\u800c\u975e\u80cc\u666f\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faCAMBench-QR\uff0c\u5229\u7528QR\u7801\u7684\u89c4\u8303\u51e0\u4f55\u7ed3\u6784\uff08\u5982\u5b9a\u4f4d\u56fe\u6848\u3001\u5b9a\u65f6\u7ebf\u3001\u6a21\u5757\u7f51\u683c\uff09\u751f\u6210\u5e26\u6709\u7cbe\u786e\u63a9\u7801\u548c\u53ef\u63a7\u5931\u771f\u7684QR/\u975eQR\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u7ed3\u6784\u611f\u77e5\u6307\u6807\uff08\u5982Finder/Timing Mass Ratios\u3001Background Leakage\u7b49\uff09\u4ee5\u53ca\u56e0\u679c\u906e\u853d\u3001\u63d2\u5165/\u5220\u9664\u4fdd\u771f\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u5ef6\u8fdf\u7b49\u8bc4\u6d4b\u7ef4\u5ea6\u3002", "result": "\u5728\u96f6\u6837\u672c\u548c\u6700\u540e\u4e00\u5757\u5fae\u8c03\u4e24\u79cd\u5b9e\u9645\u8bbe\u7f6e\u4e0b\uff0c\u5bf9LayerCAM\u3001EigenGrad-CAM\u3001XGrad-CAM\u7b49\u9ad8\u6548CAM\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u6d4b\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u57fa\u51c6\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u65b9\u6cd5\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "CAMBench-QR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u53ef\u590d\u73b0\u7684\u7ed3\u6784\u611f\u77e5\u89c6\u89c9\u89e3\u91ca\u8bc4\u4f30\u6807\u51c6\uff0c\u53ef\u4f5c\u4e3a\u5224\u65ad\u89c6\u89c9\u89e3\u91ca\u662f\u5426\u771f\u6b63\u5173\u6ce8\u7ed3\u6784\u7684\u8bd5\u91d1\u77f3\u3002"}}
{"id": "2509.17783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17783", "abs": "https://arxiv.org/abs/2509.17783", "authors": ["Yibo Peng", "Jiahao Yang", "Shenhao Yan", "Ziyu Huang", "Shuang Li", "Shuguang Cui", "Yiming Zhao", "Yatong Han"], "title": "RoboSeek: You Need to Interact with Your Objects", "comment": null, "summary": "Optimizing and refining action execution through\n  exploration and interaction is a promising way for robotic\n  manipulation. However, practical approaches to interaction driven robotic\nlearning are still underexplored, particularly for\n  long-horizon tasks where sequential decision-making, physical\n  constraints, and perceptual uncertainties pose significant chal lenges.\nMotivated by embodied cognition theory, we propose\n  RoboSeek, a framework for embodied action execution that\n  leverages interactive experience to accomplish manipulation\n  tasks. RoboSeek optimizes prior knowledge from high-level\n  perception models through closed-loop training in simulation\n  and achieves robust real-world execution via a real2sim2real\n  transfer pipeline. Specifically, we first replicate real-world\n  environments in simulation using 3D reconstruction to provide\n  visually and physically consistent environments., then we train\n  policies in simulation using reinforcement learning and the\n  cross-entropy method leveraging visual priors. The learned\n  policies are subsequently deployed on real robotic platforms\n  for execution. RoboSeek is hardware-agnostic and is evaluated\n  on multiple robotic platforms across eight long-horizon ma nipulation tasks\ninvolving sequential interactions, tool use, and\n  object handling. Our approach achieves an average success rate\n  of 79%, significantly outperforming baselines whose success\n  rates remain below 50%, highlighting its generalization and\n  robustness across tasks and platforms. Experimental results\n  validate the effectiveness of our training framework in complex,\n  dynamic real-world settings and demonstrate the stability of the\n  proposed real2sim2real transfer mechanism, paving the way for\n  more generalizable embodied robotic learning. Project Page:\n  https://russderrick.github.io/Roboseek/", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRoboSeek\uff0c\u4e00\u79cd\u57fa\u4e8e\u5177\u8eab\u8ba4\u77e5\u7406\u8bba\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u771f\u4e2d\u7684\u95ed\u73af\u8bad\u7ec3\u548creal2sim2real\u8fc1\u79fb\u673a\u5236\u4f18\u5316\u52a8\u4f5c\u6267\u884c\uff0c\u5728\u516b\u9879\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u8fbe79%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u5468\u671f\u4efb\u52a1\u4e2d\u9762\u4e34\u5e8f\u5217\u51b3\u7b56\u3001\u7269\u7406\u7ea6\u675f\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7b49\u6311\u6218\uff0c\u7f3a\u4e4f\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u89c6\u89c9\u4e0e\u7269\u7406\u4e00\u81f4\u7684\u4eff\u771f\u73af\u5883\uff0c\u7ed3\u54083D\u91cd\u5efa\u4e0e\u9ad8\u9636\u611f\u77e5\u6a21\u578b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u4e0e\u4ea4\u53c9\u71b5\u65b9\u6cd5\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7real2sim2real\u8fc1\u79fb\u5c06\u7b56\u7565\u90e8\u7f72\u81f3\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u3002", "result": "\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b8c\u6210\u516b\u9879\u957f\u5468\u671f\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e73\u5747\u6210\u529f\u7387\u4e3a79%\uff0c\u660e\u663e\u9ad8\u4e8e\u6210\u529f\u7387\u4f4e\u4e8e50%\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "RoboSeek\u6846\u67b6\u901a\u8fc7\u5177\u8eab\u4ea4\u4e92\u7ecf\u9a8c\u4e0e\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u95ed\u73af\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u4e0e\u901a\u7528\u6027\uff0c\u4e3a\u53ef\u6cdb\u5316\u7684\u5177\u8eab\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.17047", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17047", "abs": "https://arxiv.org/abs/2509.17047", "authors": ["Cui Ding", "Yanning Yin", "Lena A. J\u00e4ger", "Ethan Gotlieb Wilcox"], "title": "Modeling Bottom-up Information Quality during Language Processing", "comment": null, "summary": "Contemporary theories model language processing as integrating both top-down\nexpectations and bottom-up inputs. One major prediction of such models is that\nthe quality of the bottom-up inputs modulates ease of processing -- noisy\ninputs should lead to difficult and effortful comprehension. We test this\nprediction in the domain of reading. First, we propose an information-theoretic\noperationalization for the \"quality\" of bottom-up information as the mutual\ninformation (MI) between visual information and word identity. We formalize\nthis prediction in a mathematical model of reading as a Bayesian update.\nSecond, we test our operationalization by comparing participants' reading times\nin conditions where words' information quality has been reduced, either by\noccluding their top or bottom half, with full words. We collect data in English\nand Chinese. We then use multimodal language models to estimate the mutual\ninformation between visual inputs and words. We use these data to estimate the\nspecific effect of reduced information quality on reading times. Finally, we\ncompare how information is distributed across visual forms. In English and\nChinese, the upper half contains more information about word identity than the\nlower half. However, the asymmetry is more pronounced in English, a pattern\nwhich is reflected in the reading times.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u9605\u8bfb\u4e2d\u81ea\u4e0b\u800c\u4e0a\u4fe1\u606f\u8d28\u91cf\u7684\u64cd\u4f5c\u5316\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u89c6\u89c9\u8f93\u5165\u7684\u4fe1\u606f\u8d28\u91cf\uff08\u7279\u522b\u662f\u5b57\u6bcd\u4e0a\u4e0b\u534a\u90e8\u5206\uff09\u5bf9\u9605\u8bfb\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u82f1\u6587\u548c\u4e2d\u6587\u4e2d\u4e0a\u534a\u90e8\u5206\u5305\u542b\u66f4\u591a\u8bcd\u8eab\u4efd\u4fe1\u606f\uff0c\u4e14\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u5728\u82f1\u8bed\u4e2d\u66f4\u660e\u663e\u3002", "motivation": "\u4e3a\u4e86\u68c0\u9a8c\u8bed\u8a00\u5904\u7406\u6a21\u578b\u4e2d\u5173\u4e8e\u81ea\u4e0b\u800c\u4e0a\u8f93\u5165\u8d28\u91cf\u5f71\u54cd\u52a0\u5de5\u96be\u5ea6\u7684\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u566a\u58f0\u8f93\u5165\u662f\u5426\u5bfc\u81f4\u7406\u89e3\u56f0\u96be\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u65b9\u6cd5\u5b9a\u4e49\u89c6\u89c9\u8f93\u5165\u4e0e\u8bcd\u8eab\u4efd\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u4f5c\u4e3a\u4fe1\u606f\u8d28\u91cf\u6307\u6807\uff0c\u5efa\u7acb\u8d1d\u53f6\u65af\u66f4\u65b0\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u906e\u6321\u5355\u8bcd\u4e0a\u4e0b\u534a\u90e8\u5206\u7684\u5b9e\u9a8c\u6d4b\u91cf\u82f1\u8bed\u548c\u4e2d\u6587\u8bfb\u8005\u7684\u9605\u8bfb\u65f6\u95f4\uff0c\u5229\u7528\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u4e92\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4fe1\u606f\u8d28\u91cf\u964d\u4f4e\u4f1a\u663e\u8457\u589e\u52a0\u9605\u8bfb\u65f6\u95f4\uff1b\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u4e2d\uff0c\u5355\u8bcd\u4e0a\u534a\u90e8\u5206\u6bd4\u4e0b\u534a\u90e8\u5206\u5305\u542b\u66f4\u591a\u5173\u4e8e\u8bcd\u8eab\u4efd\u7684\u4fe1\u606f\uff0c\u4e14\u8be5\u4e0d\u5bf9\u79f0\u6027\u5728\u82f1\u8bed\u4e2d\u66f4\u4e3a\u663e\u8457\uff0c\u8fd9\u4e00\u6a21\u5f0f\u4e5f\u53cd\u6620\u5728\u5b9e\u9645\u9605\u8bfb\u65f6\u95f4\u4e2d\u3002", "conclusion": "\u81ea\u4e0b\u800c\u4e0a\u4fe1\u606f\u7684\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u9605\u8bfb\u52a0\u5de5\u96be\u5ea6\uff0c\u4e14\u89c6\u89c9\u5f62\u5f0f\u4e2d\u7684\u4fe1\u606f\u5206\u5e03\u5177\u6709\u4e0a\u4e0b\u4e0d\u5bf9\u79f0\u6027\uff0c\u652f\u6301\u4e86\u6574\u5408\u81ea\u4e0a\u800c\u4e0b\u9884\u671f\u4e0e\u81ea\u4e0b\u800c\u4e0a\u8f93\u5165\u7684\u8bed\u8a00\u5904\u7406\u6a21\u578b\u3002"}}
{"id": "2509.17145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17145", "abs": "https://arxiv.org/abs/2509.17145", "authors": ["Amaan Ansari", "Lukas Kirchdorfer", "Raheleh Hadian"], "title": "On the Simplification of Neural Network Architectures for Predictive Process Monitoring", "comment": null, "summary": "Predictive Process Monitoring (PPM) aims to forecast the future behavior of\nongoing process instances using historical event data, enabling proactive\ndecision-making. While recent advances rely heavily on deep learning models\nsuch as LSTMs and Transformers, their high computational cost hinders practical\nadoption. Prior work has explored data reduction techniques and alternative\nfeature encodings, but the effect of simplifying model architectures themselves\nremains underexplored. In this paper, we analyze how reducing model complexity,\nboth in terms of parameter count and architectural depth, impacts predictive\nperformance, using two established PPM approaches. Across five diverse event\nlogs, we show that shrinking the Transformer model by 85% results in only a\n2-3% drop in performance across various PPM tasks, while the LSTM proves\nslightly more sensitive, particularly for waiting time prediction. Overall, our\nfindings suggest that substantial model simplification can preserve predictive\naccuracy, paving the way for more efficient and scalable PPM solutions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7b80\u5316\u6a21\u578b\u67b6\u6784\u5bf9\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\uff08PPM\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c06Transformer\u6a21\u578b\u538b\u7f2985%\u4ec5\u5bfc\u81f42-3%\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u660e\u7b80\u5316\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u8f83\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982LSTM\u548cTransformer\u5728PPM\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u800c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u6a21\u578b\u67b6\u6784\u672c\u8eab\u7684\u7b80\u5316\u3002", "method": "\u901a\u8fc7\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u7f51\u7edc\u6df1\u5ea6\u6765\u7b80\u5316\u4e24\u79cd\u4e3b\u6d41PPM\u6a21\u578b\uff08Transformer\u548cLSTM\uff09\uff0c\u5e76\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u8bc4\u4f30\u5176\u5728\u591a\u79cdPPM\u4efb\u52a1\u4e2d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "result": "Transformer\u6a21\u578b\u538b\u7f2985%\u540e\u6027\u80fd\u4ec5\u4e0b\u964d2-3%\uff0c\u5728\u5404\u7c7bPPM\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\uff1bLSTM\u5bf9\u7ed3\u6784\u7b80\u5316\u66f4\u654f\u611f\uff0c\u5c24\u5176\u5728\u7b49\u5f85\u65f6\u95f4\u9884\u6d4b\u4e0a\u6027\u80fd\u4e0b\u964d\u66f4\u660e\u663e\u3002", "conclusion": "\u5927\u5e45\u7b80\u5316PPM\u6a21\u578b\u67b6\u6784\u53ef\u5728\u51e0\u4e4e\u4e0d\u727a\u7272\u9884\u6d4b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684PPM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.17957", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.17957", "abs": "https://arxiv.org/abs/2509.17957", "authors": ["David Hyland", "Mahault Albarracin"], "title": "On the Variational Costs of Changing Our Minds", "comment": "Accepted as a full paper at the 6th International Workshop on Active\n  Inference", "summary": "The human mind is capable of extraordinary achievements, yet it often appears\nto work against itself. It actively defends its cherished beliefs even in the\nface of contradictory evidence, conveniently interprets information to conform\nto desired narratives, and selectively searches for or avoids information to\nsuit its various purposes. Despite these behaviours deviating from common\nnormative standards for belief updating, we argue that such 'biases' are not\ninherently cognitive flaws, but rather an adaptive response to the significant\npragmatic and cognitive costs associated with revising one's beliefs. This\npaper introduces a formal framework that aims to model the influence of these\ncosts on our belief updating mechanisms.\n  We treat belief updating as a motivated variational decision, where agents\nweigh the perceived 'utility' of a belief against the informational cost\nrequired to adopt a new belief state, quantified by the Kullback-Leibler\ndivergence from the prior to the variational posterior. We perform\ncomputational experiments to demonstrate that simple instantiations of this\nresource-rational model can be used to qualitatively emulate commonplace human\nbehaviours, including confirmation bias and attitude polarisation. In doing so,\nwe suggest that this framework makes steps toward a more holistic account of\nthe motivated Bayesian mechanics of belief change and provides practical\ninsights for predicting, compensating for, and correcting deviations from\ndesired belief updating processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5c06\u4fe1\u5ff5\u66f4\u65b0\u89c6\u4e3a\u4e00\u79cd\u6743\u8861\u4fe1\u5ff5\u6548\u7528\u4e0e\u4fe1\u606f\u6210\u672c\u7684\u6709\u52a8\u673a\u53d8\u5206\u51b3\u7b56\uff0c\u7528\u4ee5\u89e3\u91ca\u4eba\u7c7b\u5e38\u89c1\u7684\u8ba4\u77e5\u504f\u5dee\uff08\u5982\u786e\u8ba4\u504f\u8bef\u548c\u6001\u5ea6\u6781\u5316\uff09\u5e76\u975e\u8ba4\u77e5\u7f3a\u9677\uff0c\u800c\u662f\u5bf9\u9ad8\u6602\u4fe1\u5ff5\u4fee\u6b63\u6210\u672c\u7684\u9002\u5e94\u6027\u53cd\u5e94\u3002", "motivation": "\u4eba\u7c7b\u5e38\u8868\u73b0\u51fa\u8fdd\u80cc\u7406\u6027\u4fe1\u5ff5\u66f4\u65b0\u6807\u51c6\u7684\u884c\u4e3a\uff0c\u5982\u575a\u6301\u9519\u8bef\u4fe1\u5ff5\u6216\u9009\u62e9\u6027\u5730\u5904\u7406\u4fe1\u606f\u3002\u4f20\u7edf\u89c2\u70b9\u89c6\u5176\u4e3a\u8ba4\u77e5\u504f\u5dee\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u884c\u4e3a\u53ef\u80fd\u6e90\u4e8e\u4fe1\u5ff5\u66f4\u65b0\u7684\u5b9e\u9645\u4ee3\u4ef7\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u89e3\u91ca\u6b64\u7c7b\u73b0\u8c61\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53d8\u5206\u8d1d\u53f6\u65af\u51b3\u7b56\u7684\u6a21\u578b\uff0c\u5c06\u4fe1\u5ff5\u66f4\u65b0\u5efa\u6a21\u4e3a\u5728\u4fe1\u5ff5\u6548\u7528\u4e0e\u4fe1\u606f\u6210\u672c\uff08\u901a\u8fc7KL\u6563\u5ea6\u91cf\u5316\uff09\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u80fd\u5426\u5b9a\u6027\u6a21\u62df\u786e\u8ba4\u504f\u8bef\u548c\u6001\u5ea6\u6781\u5316\u7b49\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u6210\u529f\u590d\u73b0\u4e86\u591a\u79cd\u5e38\u89c1\u7684\u4eba\u7c7b\u4fe1\u5ff5\u66f4\u65b0\u504f\u5dee\uff0c\u8868\u660e\u8fd9\u4e9b\u2018\u504f\u5dee\u2019\u53ef\u80fd\u662f\u8d44\u6e90\u6709\u9650\u6761\u4ef6\u4e0b\u7684\u7406\u6027\u9002\u5e94\u7ed3\u679c\uff0c\u800c\u975e\u5355\u7eaf\u7684\u8ba4\u77e5\u7f3a\u9677\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u52a8\u673a\u9a71\u52a8\u7684\u4fe1\u5ff5\u6539\u53d8\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89d2\uff0c\u652f\u6301\u2018\u8ba4\u77e5\u504f\u5dee\u2019\u53ef\u80fd\u662f\u8d44\u6e90\u5408\u7406\u6027\u7684\u4f53\u73b0\uff0c\u5e76\u4e3a\u9884\u6d4b\u548c\u7ea0\u6b63\u4fe1\u5ff5\u66f4\u65b0\u504f\u5dee\u63d0\u4f9b\u4e86\u5b9e\u7528\u6d1e\u89c1\u3002"}}
{"id": "2509.16748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16748", "abs": "https://arxiv.org/abs/2509.16748", "authors": ["Heyuan Li", "Kenkun Liu", "Lingteng Qiu", "Qi Zuo", "Keru Zheng", "Zilong Dong", "Xiaoguang Han"], "title": "HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis", "comment": "Accepted by NeurIPS 2025", "summary": "Tri-plane-like representations have been widely adopted in 3D-aware GANs for\nhead image synthesis and other 3D object/scene modeling tasks due to their\nefficiency. However, querying features via Cartesian coordinate projection\noften leads to feature entanglement, which results in mirroring artifacts. A\nrecent work, SphereHead, attempted to address this issue by introducing\nspherical tri-planes based on a spherical coordinate system. While it\nsuccessfully mitigates feature entanglement, SphereHead suffers from uneven\nmapping between the square feature maps and the spherical planes, leading to\ninefficient feature map utilization during rendering and difficulties in\ngenerating fine image details. Moreover, both tri-plane and spherical tri-plane\nrepresentations share a subtle yet persistent issue: feature penetration across\nconvolutional channels can cause interference between planes, particularly when\none plane dominates the others. These challenges collectively prevent\ntri-plane-based methods from reaching their full potential. In this paper, we\nsystematically analyze these problems for the first time and propose innovative\nsolutions to address them. Specifically, we introduce a novel hybrid-plane\n(hy-plane for short) representation that combines the strengths of both planar\nand spherical planes while avoiding their respective drawbacks. We further\nenhance the spherical plane by replacing the conventional theta-phi warping\nwith a novel near-equal-area warping strategy, which maximizes the effective\nutilization of the square feature map. In addition, our generator synthesizes a\nsingle-channel unified feature map instead of multiple feature maps in separate\nchannels, thereby effectively eliminating feature penetration. With a series of\ntechnical improvements, our hy-plane representation enables our method,\nHyPlaneHead, to achieve state-of-the-art performance in full-head image\nsynthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u5e73\u9762\uff08hy-plane\uff09\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5e73\u9762\u548c\u7403\u9762\u8868\u793a\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e09\u5e73\u9762\u53ca\u5176\u7403\u9762\u53d8\u4f53\u5728\u5934\u50cf\u751f\u6210\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u3001\u6620\u5c04\u4e0d\u5747\u548c\u7279\u5f81\u901a\u9053\u6e17\u900f\u7b49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u611f\u77e5\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e09\u5e73\u9762\u53ca\u7403\u9762\u4e09\u5e73\u9762\u8868\u793a\u6cd5\u5b58\u5728\u7279\u5f81\u7ea0\u7f20\u3001\u7279\u5f81\u56fe\u5229\u7528\u4e0d\u5747\u548c\u8de8\u901a\u9053\u7279\u5f81\u6e17\u900f\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u57283D-aware GAN\u4e2d\u5934\u50cf\u5408\u6210\u7684\u6f5c\u529b\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u5206\u6790\u4e0e\u6539\u8fdb\u3002", "method": "\u63d0\u51fahy-plane\u8868\u793a\u6cd5\uff0c\u878d\u5408\u5e73\u9762\u4e0e\u7403\u9762\u4f18\u52bf\uff1b\u91c7\u7528\u8fd1\u4f3c\u7b49\u9762\u79ef\u6620\u5c04\u7b56\u7565\u4f18\u5316\u7403\u9762\u7279\u5f81\u5229\u7528\uff1b\u8bbe\u8ba1\u5355\u901a\u9053\u7edf\u4e00\u7279\u5f81\u56fe\u4ee5\u6d88\u9664\u8de8\u901a\u9053\u7279\u5f81\u6e17\u900f\u3002", "result": "\u6240\u63d0HyPlaneHead\u5728\u5168\u5934\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u955c\u50cf\u4f2a\u5f71\uff0c\u63d0\u5347\u4e86\u7ec6\u8282\u751f\u6210\u8d28\u91cf\u4e0e\u7279\u5f81\u5229\u7528\u7387\u3002", "conclusion": "hy-plane\u8868\u793a\u6cd5\u7cfb\u7edf\u6027\u89e3\u51b3\u4e86\u4e09\u5e73\u9762\u65b9\u6cd5\u7684\u5173\u952e\u7f3a\u9677\uff0c\u5728\u6548\u7387\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\uff0c\u4e3a3D-aware\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9690\u5f0f\u8868\u793a\u65b9\u6848\u3002"}}
{"id": "2509.17812", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17812", "abs": "https://arxiv.org/abs/2509.17812", "authors": ["Yitaek Kim", "Casper Hewson Rask", "Christoffer Sloth"], "title": "Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation", "comment": "This paper has submitted to Dexterous Humanoid Manipulation Workshop,\n  Humanoid 2025", "summary": "This paper proposes Tac2Motion, a contact-aware reinforcement learning\nframework to facilitate the learning of contact-rich in-hand manipulation\ntasks, such as removing a lid. To this end, we propose tactile sensing-based\nreward shaping and incorporate the sensing into the observation space through\nembedding. The designed rewards encourage an agent to ensure firm grasping and\nsmooth finger gaiting at the same time, leading to higher data efficiency and\nrobust performance compared to the baseline. We verify the proposed framework\non the opening a lid scenario, showing generalization of the trained policy\ninto a couple of object types and various dynamics such as torsional friction.\nLastly, the learned policy is demonstrated on the multi-fingered robot, Shadow\nRobot, showing that the control policy can be transferred to the real world.\nThe video is available: https://youtu.be/poeJBPR7urQ.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTac2Motion\u7684\u89e6\u89c9\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4fc3\u8fdb\u5bcc\u542b\u63a5\u89e6\u7684\u624b\u4e2d\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u5f00\u76d6\uff09\u7684\u5b66\u4e60\u3002\u901a\u8fc7\u57fa\u4e8e\u89e6\u89c9\u611f\u77e5\u7684\u5956\u52b1\u5851\u9020\u548c\u5d4c\u5165\u89c2\u6d4b\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728Shadow Robot\u4e0a\u6210\u529f\u9a8c\u8bc1\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u7b56\u7565\u8fc1\u79fb\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u590d\u6742\u63a5\u89e6\u4efb\u52a1\uff08\u5982\u5f00\u76d6\uff09\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u9c81\u68d2\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u89e6\u89c9\u4fe1\u606f\u7684\u6709\u6548\u5229\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u878d\u5408\u89e6\u89c9\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86Tac2Motion\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u89e6\u89c9\u7684\u5956\u52b1 shaping \u65b9\u6cd5\uff0c\u5e76\u5c06\u89e6\u89c9\u8f93\u5165\u901a\u8fc7\u5d4c\u5165\u65b9\u5f0f\u878d\u5165\u89c2\u6d4b\u7a7a\u95f4\uff0c\u4ee5\u540c\u65f6\u9f13\u52b1\u7a33\u56fa\u6293\u63e1\u548c\u6d41\u7545\u7684\u624b\u6307\u6b65\u6001\u3002", "result": "\u5728\u5f00\u76d6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u5bf9\u4e0d\u540c\u7269\u4f53\u7c7b\u578b\u548c\u52a8\u6001\u7279\u6027\uff08\u5982\u626d\u8f6c\u6469\u64e6\uff09\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u591a\u6307\u673a\u5668\u4ebaShadow Robot\u7684\u771f\u5b9e\u4e16\u754c\u7b56\u7565\u8fc1\u79fb\u3002", "conclusion": "Tac2Motion\u901a\u8fc7\u6709\u6548\u6574\u5408\u89e6\u89c9\u4fe1\u606f\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\u4e0e\u6f5c\u529b\u3002"}}
{"id": "2509.17054", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17054", "abs": "https://arxiv.org/abs/2509.17054", "authors": ["Yiwei Liu", "Emma Jane Pretty", "Jiahao Huang", "Saku Sugawara"], "title": "TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?", "comment": null, "summary": "While recent studies explore Large Language Models' (LLMs) performance on\nTheory of Mind (ToM) reasoning tasks, research on ToM abilities that require\nmore nuanced social context is limited, such as white lies. We introduce\nTactfulToM, a novel English benchmark designed to evaluate LLMs' ability to\nunderstand white lies within real-life conversations and reason about prosocial\nmotivations behind them, particularly when they are used to spare others'\nfeelings and maintain social harmony. Our benchmark is generated through a\nmulti-stage human-in-the-loop pipeline where LLMs expand manually designed seed\nstories into conversations to maintain the information asymmetry between\nparticipants necessary for authentic white lies. We show that TactfulToM is\nchallenging for state-of-the-art models, which perform substantially below\nhumans, revealing shortcomings in their ability to fully comprehend the ToM\nreasoning that enables true understanding of white lies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TactfulToM\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u7406\u89e3\u5584\u610f\u8c0e\u8a00\u53ca\u5176\u80cc\u540e\u4eb2\u793e\u4f1a\u52a8\u673a\u7684\u65b0\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u901a\u8fc7\u591a\u9636\u6bb5\u4eba\u673a\u534f\u540c\u6d41\u7a0b\u6784\u5efa\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u66b4\u9732\u51fa\u5176\u5728\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u7ec6\u81f4\u793e\u4f1a\u8bed\u5883\u7684\u5fc3\u667a\u7406\u8bba\u4efb\u52a1\uff08\u5982\u5584\u610f\u8c0e\u8a00\uff09\u4e0a\u7684\u8868\u73b0\u63a2\u7d22\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5408\u9002\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faTactfulToM\u57fa\u51c6\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u4eba\u673a\u534f\u540c\u6d41\u7a0b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u4eba\u5de5\u8bbe\u8ba1\u7684\u79cd\u5b50\u6545\u4e8b\u751f\u6210\u5305\u542b\u4fe1\u606f\u4e0d\u5bf9\u79f0\u7684\u771f\u5b9e\u5bf9\u8bdd\uff0c\u4ee5\u6a21\u62df\u5584\u610f\u8c0e\u8a00\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728TactfulToM\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u96be\u4ee5\u5145\u5206\u7406\u89e3\u5584\u610f\u8c0e\u8a00\u80cc\u540e\u7684\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u3002", "conclusion": "TactfulToM\u63ed\u793a\u4e86\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u793e\u4ea4\u60c5\u5883\u4e2d\u7684\u5584\u610f\u8c0e\u8a00\u65f6\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u8868\u660e\u5176\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u5347\u3002"}}
{"id": "2509.17153", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17153", "abs": "https://arxiv.org/abs/2509.17153", "authors": ["Moule Lin", "Andrea Patane", "Weipeng Jing", "Shuhao Guan", "Goetz Botterweck"], "title": "Flow-Induced Diagonal Gaussian Processes", "comment": "15 pages", "summary": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression\nframework that incorporates a compact inducing weight matrix to project a\nneural network's weight uncertainty into a lower-dimensional subspace.\nCritically, FiD-GP relies on normalising-flow priors and spectral\nregularisations to augment its expressiveness and align the inducing subspace\nwith feature-gradient geometry through a numerically stable projection\nmechanism objective. Furthermore, we demonstrate how the prediction framework\nin FiD-GP can help to design a single-pass projection for Out-of-Distribution\n(OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation\nability on various tasks compared with SVGP-based baselines, satisfies tight\nspectral residual bounds with theoretically guaranteed OoD detection, and\nsignificantly compresses the neural network's storage requirements at the cost\nof increased inference computation dependent on the number of inducing weights\nemployed. Specifically, in a comprehensive empirical study spanning regression,\nimage classification, semantic segmentation, and out-of-distribution detection\nbenchmarks, it cuts Bayesian training cost by several orders of magnitude,\ncompresses parameters by roughly 51%, reduces model size by about 75%, and\nmatches state-of-the-art accuracy and uncertainty estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlow-Induced Diagonal Gaussian Processes (FiD-GP)\u7684\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u7684\u8bf1\u5bfc\u6743\u91cd\u77e9\u9635\u6765\u5efa\u6a21\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u5f52\u4e00\u5316\u6d41\u5148\u9a8c\u548c\u8c31\u6b63\u5219\u5316\uff0c\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u5e76\u5b9e\u73b0\u7a33\u5b9a\u7684OoD\u68c0\u6d4b\u6295\u5f71\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u8d1d\u53f6\u65af\u8bad\u7ec3\u6210\u672c\u3001\u538b\u7f29\u6a21\u578b\u53c2\u6570\u7684\u540c\u65f6\u4fdd\u6301\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u5b58\u50a8\u548c\u8bad\u7ec3\u6210\u672c\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u548cOoD\u68c0\u6d4b\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u538b\u7f29\u6a21\u578b\u53c8\u4fdd\u6301\u9ad8\u8868\u8fbe\u80fd\u529b\u7684\u8d1d\u53f6\u65af\u65b9\u6cd5\u3002", "method": "FiD-GP\u5f15\u5165\u4e00\u4e2a\u7d27\u51d1\u7684\u8bf1\u5bfc\u6743\u91cd\u77e9\u9635\uff0c\u5c06\u6743\u91cd\u4e0d\u786e\u5b9a\u6027\u6295\u5f71\u5230\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff1b\u91c7\u7528\u5f52\u4e00\u5316\u6d41\u5148\u9a8c\u589e\u5f3a\u5148\u9a8c\u5206\u5e03\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u8c31\u6b63\u5219\u5316\u4f7f\u8bf1\u5bfc\u5b50\u7a7a\u95f4\u4e0e\u7279\u5f81\u68af\u5ea6\u51e0\u4f55\u5bf9\u9f50\uff0c\u901a\u8fc7\u6570\u503c\u7a33\u5b9a\u7684\u6295\u5f71\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u63a8\u65ad\u548c\u5355\u6b21OoD\u68c0\u6d4b\u3002", "result": "FiD-GP\u5728\u56de\u5f52\u3001\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548cOoD\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u964d\u4f4e\u8d1d\u53f6\u65af\u8bad\u7ec3\u6210\u672c\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u53c2\u6570\u538b\u7f29\u7ea651%\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u7ea675%\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6027\u80fd\uff0c\u5e76\u5177\u5907\u7406\u8bba\u4fdd\u8bc1\u7684OoD\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "FiD-GP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u663e\u8457\u538b\u7f29\u6a21\u578b\u548c\u8bad\u7ec3\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u826f\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548cOoD\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17978", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.17978", "abs": "https://arxiv.org/abs/2509.17978", "authors": ["Antoni Guasch", "Maria Isabel Valdez"], "title": "The STAR-XAI Protocol: An Interactive Framework for Inducing Second-Order Agency in AI Agents", "comment": "Paper 1 of 4 in The STAR-XAI Protocol series. Paper 2\n  [arXiv:ID_to_be_added], Paper 3 [arXiv:ID_to_be_added], Paper 4\n  [arXiv:ID_to_be_added]", "summary": "Current Large Reasoning Models (LRMs) exhibit significant limitations in\nreliability and transparency, often showing a collapse in reasoning\ncapabilities when faced with high-complexity, long-horizon tasks. This\n\"illusion of thinking\" is frequently an artifact of non-agentic, black-box\nevaluation paradigms that fail to cultivate robust problem-solving processes.\nIn response, we introduce The STAR-XAI Protocol (Socratic, Transparent,\nAgentic, Reasoning - for eXplainable Artificial Intelligence), a novel\nmethodology for training and operating verifiably reliable AI agents. Our\nmethod reframes the human-AI interaction as a structured, Socratic dialogue,\ngoverned by an explicit and evolving rulebook, the Consciousness Transfer\nPackage (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc\nstrategic justification and a state-locking Checksum that prevents error\naccumulation, the protocol transforms a powerful but opaque LRM into a\ndisciplined \"Clear Box\" agent. We demonstrate the efficacy of this method\nthrough an exhaustive 25-move case study in the complex strategic game \"Caps i\nCaps\". The agent not only solved the high-complexity puzzle but also\ndemonstrated Second-Order Agency, identifying flaws in its own\nsupervisor-approved plans and adapting its core integrity protocols mid-task.\nThe STAR-XAI Protocol offers a practical pathway to creating AI agents that are\nnot just high-performing, but also transparent, auditable, and trustworthy by\ndesign.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86STAR-XAI\u534f\u8bae\uff0c\u901a\u8fc7\u82cf\u683c\u62c9\u5e95\u5f0f\u5bf9\u8bdd\u548c\u900f\u660e\u5316\u673a\u5236\uff0c\u5c06\u5927\u63a8\u7406\u6a21\u578b\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7684\u201c\u900f\u660e\u76d2\u201d\u667a\u80fd\u4f53\uff0c\u663e\u8457\u63d0\u5347\u5176\u5728\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524d\u5927\u63a8\u7406\u6a21\u578b\u5728\u9762\u5bf9\u9ad8\u590d\u6742\u6027\u3001\u957f\u89c6\u91ce\u4efb\u52a1\u65f6\u5b58\u5728\u63a8\u7406\u80fd\u529b\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u5bfc\u81f4\u5176\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faSTAR-XAI\u534f\u8bae\uff0c\u91c7\u7528\u82cf\u683c\u62c9\u5e95\u5f0f\u4eba\u673a\u5bf9\u8bdd\u6846\u67b6\uff0c\u7ed3\u5408\u610f\u8bc6\u8f6c\u79fb\u5305\uff08CTP\uff09\u89c4\u5219\u4e66\u3001\u524d\u7f6e\u7b56\u7565\u9a8c\u8bc1\u7684\u6e38\u620f\u5faa\u73af\u673a\u5236\u548c\u72b6\u6001\u9501\u5b9a\u6821\u9a8c\u548c\uff0c\u5b9e\u73b0\u5bf9AI\u63a8\u7406\u8fc7\u7a0b\u7684\u7ed3\u6784\u5316\u76d1\u7763\u4e0e\u9519\u8bef\u63a7\u5236\u3002", "result": "\u5728\u2018Caps i Caps\u2019\u8fd9\u4e00\u590d\u6742\u6218\u7565\u6e38\u620f\u768425\u6b65\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6210\u529f\u89e3\u51b3\u9ad8\u590d\u6742\u5ea6\u96be\u9898\uff0c\u8fd8\u5c55\u73b0\u51fa\u4e8c\u9636\u4ee3\u7406\u80fd\u529b\uff0c\u5373\u80fd\u8bc6\u522b\u5e76\u4fee\u6b63\u4e0a\u7ea7\u6279\u51c6\u8ba1\u5212\u4e2d\u7684\u7f3a\u9677\uff0c\u5e76\u5728\u4efb\u52a1\u4e2d\u52a8\u6001\u8c03\u6574\u6838\u5fc3\u8bda\u4fe1\u534f\u8bae\u3002", "conclusion": "STAR-XAI\u534f\u8bae\u4e3a\u6784\u5efa\u9ad8\u6027\u80fd\u3001\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u4e14\u672c\u8d28\u53ef\u4fe1\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u63a8\u52a8AI\u4ece\u9ed1\u7bb1\u6a21\u578b\u5411\u53ef\u89e3\u91ca\u3001\u8d1f\u8d23\u4efb\u7684\u667a\u80fd\u4f53\u8f6c\u53d8\u3002"}}
{"id": "2509.16767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16767", "abs": "https://arxiv.org/abs/2509.16767", "authors": ["Ozgur Kara", "Harris Nisar", "James M. Rehg"], "title": "DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images", "comment": "Accepted to NeurIPS 2025", "summary": "Numerous models have been developed for scanpath and saliency prediction,\nwhich are typically trained on scanpaths, which model eye movement as a\nsequence of discrete fixation points connected by saccades, while the rich\ninformation contained in the raw trajectories is often discarded. Moreover,\nmost existing approaches fail to capture the variability observed among human\nsubjects viewing the same image. They generally predict a single scanpath of\nfixed, pre-defined length, which conflicts with the inherent diversity and\nstochastic nature of real-world visual attention. To address these challenges,\nwe propose DiffEye, a diffusion-based training framework designed to model\ncontinuous and diverse eye movement trajectories during free viewing of natural\nimages. Our method builds on a diffusion model conditioned on visual stimuli\nand introduces a novel component, namely Corresponding Positional Embedding\n(CPE), which aligns spatial gaze information with the patch-based semantic\nfeatures of the visual input. By leveraging raw eye-tracking trajectories\nrather than relying on scanpaths, DiffEye captures the inherent variability in\nhuman gaze behavior and generates high-quality, realistic eye movement\npatterns, despite being trained on a comparatively small dataset. The generated\ntrajectories can also be converted into scanpaths and saliency maps, resulting\nin outputs that more accurately reflect the distribution of human visual\nattention. DiffEye is the first method to tackle this task on natural images\nusing a diffusion model while fully leveraging the richness of raw eye-tracking\ndata. Our extensive evaluation shows that DiffEye not only achieves\nstate-of-the-art performance in scanpath generation but also enables, for the\nfirst time, the generation of continuous eye movement trajectories. Project\nwebpage: https://diff-eye.github.io/", "AI": {"tldr": "\u63d0\u51faDiffEye\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u81ea\u7136\u56fe\u50cf\u81ea\u7531\u89c2\u770b\u8fc7\u7a0b\u4e2d\u7684\u8fde\u7eed\u4e14\u591a\u6837\u5316\u7684\u773c\u52a8\u8f68\u8ff9\uff0c\u9996\u6b21\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u5229\u7528\u6269\u6563\u6a21\u578b\u5145\u5206\u5229\u7528\u539f\u59cb\u773c\u52a8\u6570\u636e\u7684\u4e30\u5bcc\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u901a\u5e38\u4e22\u5f03\u539f\u59cb\u773c\u52a8\u8f68\u8ff9\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u89c2\u770b\u540c\u4e00\u56fe\u50cf\u65f6\u773c\u52a8\u884c\u4e3a\u7684\u591a\u6837\u6027\uff0c\u5f80\u5f80\u9884\u6d4b\u56fa\u5b9a\u957f\u5ea6\u7684\u5355\u4e00\u626b\u63cf\u8def\u5f84\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u89c6\u89c9\u6ce8\u610f\u7684\u968f\u673a\u6027\u548c\u5dee\u5f02\u6027\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u89c9\u523a\u6fc0\u6761\u4ef6\u5316\uff0c\u5e76\u5f15\u5165\u5bf9\u5e94\u4f4d\u7f6e\u7f16\u7801\uff08CPE\uff09\uff0c\u5c06\u6ce8\u89c6\u70b9\u7a7a\u95f4\u4fe1\u606f\u4e0e\u57fa\u4e8epatch\u7684\u8bed\u4e49\u7279\u5f81\u5bf9\u9f50\uff0c\u76f4\u63a5\u5229\u7528\u539f\u59cb\u773c\u52a8\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "DiffEye\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4ecd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u903c\u771f\u7684\u773c\u52a8\u6a21\u5f0f\uff0c\u53ef\u8f6c\u5316\u4e3a\u626b\u63cf\u8def\u5f84\u548c\u663e\u8457\u56fe\uff0c\u6027\u80fd\u8fbe\u5230SOTA\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u8fde\u7eed\u773c\u52a8\u8f68\u8ff9\u7684\u751f\u6210\u3002", "conclusion": "DiffEye\u662f\u9996\u4e2a\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u8fde\u7eed\u4e14\u591a\u6837\u5316\u773c\u52a8\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u6355\u6349\u4eba\u7c7b\u773c\u52a8\u53d8\u5f02\u6027\uff0c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u5e03\u3002"}}
{"id": "2509.17850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17850", "abs": "https://arxiv.org/abs/2509.17850", "authors": ["Xiao Zhou", "Zengqi Peng", "Jun Ma"], "title": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model", "comment": null, "summary": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for\nautonomous driving systems to avoid misguided decisions and potential\naccidents. However, achieving reliable predictions in highly dynamic and\ncomplex traffic scenarios remains a significant challenge. One of the key\nimpediments lies in the limited effectiveness of current approaches to capture\nthe multi-modal behaviors of drivers, which leads to predicted trajectories\nthat deviate from actual future motions. To address this issue, we propose\nSocialTraj, a novel trajectory prediction framework integrating social\npsychology principles through social value orientation (SVO). By utilizing\nBayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we\nobtain the critical social context to infer the future interaction trend. To\nensure modal consistency in predicted behaviors, the estimated SVOs of SVs are\nembedded into a conditional denoising diffusion model that aligns generated\ntrajectories with historical driving styles. Additionally, the planned future\ntrajectory of the ego vehicle (EV) is explicitly incorporated to enhance\ninteraction modeling. Extensive experiments on NGSIM and HighD datasets\ndemonstrate that SocialTraj is capable of adapting to highly dynamic and\ninteractive scenarios while generating socially compliant and behaviorally\nconsistent trajectory predictions, outperforming existing baselines. Ablation\nstudies demonstrate that dynamic SVO estimation and explicit ego-planning\ncomponents notably improve prediction accuracy and substantially reduce\ninference time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u793e\u4f1a\u5fc3\u7406\u5b66\u4e2d\u793e\u4f1a\u4ef7\u503c\u53d6\u5411\uff08SVO\uff09\u7684\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6SocialTraj\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u9006\u5f3a\u5316\u5b66\u4e60\u4f30\u8ba1\u5468\u56f4\u8f66\u8f86\u7684SVO\uff0c\u5e76\u5c06\u5176\u878d\u5165\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u4ee5\u751f\u6210\u7b26\u5408\u5b9e\u9645\u9a7e\u9a76\u884c\u4e3a\u4e14\u5177\u793e\u4ea4\u4e00\u81f4\u6027\u7684\u8f68\u8ff9\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u9a7e\u9a76\u5458\u7684\u591a\u6a21\u6001\u884c\u4e3a\uff0c\u5bfc\u81f4\u9884\u6d4b\u8f68\u8ff9\u504f\u79bb\u771f\u5b9e\u8fd0\u52a8\uff0c\u5c24\u5176\u5728\u590d\u6742\u52a8\u6001\u4ea4\u901a\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f15\u5165\u80fd\u591f\u523b\u753b\u793e\u4f1a\u4ea4\u4e92\u56e0\u7d20\u7684\u65b0\u673a\u5236\u6765\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u5408\u7406\u6027\u3002", "method": "\u63d0\u51faSocialTraj\u6846\u67b6\uff1a1\uff09\u5229\u7528\u8d1d\u53f6\u65af\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u5728\u7ebf\u4f30\u8ba1\u5468\u56f4\u8f66\u8f86\u7684\u793e\u4f1a\u4ef7\u503c\u53d6\u5411\uff08SVO\uff09\uff0c\u83b7\u53d6\u793e\u4f1a\u4ea4\u4e92\u4e0a\u4e0b\u6587\uff1b2\uff09\u5c06\u4f30\u8ba1\u7684SVO\u5d4c\u5165\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u786e\u4fdd\u751f\u6210\u8f68\u8ff9\u4e0e\u5386\u53f2\u9a7e\u9a76\u98ce\u683c\u4e00\u81f4\uff1b3\uff09\u663e\u5f0f\u878d\u5408\u81ea\u8f66\u89c4\u5212\u8f68\u8ff9\u4ee5\u589e\u5f3a\u4ea4\u4e92\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728NGSIM\u548cHighD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSocialTraj\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u4ea4\u4e92\u6027\u5f3a\u7684\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5177\u5907\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff0c\u52a8\u6001SVO\u4f30\u8ba1\u548c\u663e\u5f0f\u81ea\u8f66\u89c4\u5212\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u7387\u5e76\u5927\u5e45\u7f29\u77ed\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u793e\u4f1a\u5fc3\u7406\u5b66\u4e2d\u7684SVO\u6982\u5ff5\u5e76\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u81ea\u8f66\u89c4\u5212\u4fe1\u606f\uff0cSocialTraj\u5b9e\u73b0\u4e86\u66f4\u7b26\u5408\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u3001\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.17167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17167", "abs": "https://arxiv.org/abs/2509.17167", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Joseph Skrovan", "Mehak Beri", "Hitakshi Modi", "Andrew Well", "Liu Leqi", "Mia Markey", "Ying Ding"], "title": "SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis", "comment": null, "summary": "Thematic Analysis (TA) is a widely used qualitative method that provides a\nstructured yet flexible framework for identifying and reporting patterns in\nclinical interview transcripts. However, manual thematic analysis is\ntime-consuming and limits scalability. Recent advances in LLMs offer a pathway\nto automate thematic analysis, but alignment with human results remains\nlimited. To address these limitations, we propose SFT-TA, an automated thematic\nanalysis framework that embeds supervised fine-tuned (SFT) agents within a\nmulti-agent system. Our framework outperforms existing frameworks and the\ngpt-4o baseline in alignment with human reference themes. We observed that SFT\nagents alone may underperform, but achieve better results than the baseline\nwhen embedded within a multi-agent system. Our results highlight that embedding\nSFT agents in specific roles within a multi-agent system is a promising pathway\nto improve alignment with desired outputs for thematic analysis.", "AI": {"tldr": "\u63d0\u51faSFT-TA\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5d4c\u5165\u76d1\u7763\u5fae\u8c03\u7684\u4ee3\u7406\uff0c\u63d0\u5347\u4e3b\u9898\u5206\u6790\u81ea\u52a8\u5316\u4e2d\u4e0e\u4eba\u7c7b\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u624b\u52a8\u4e3b\u9898\u5206\u6790\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u73b0\u6709\u5927\u6a21\u578b\u81ea\u52a8\u5316\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u5206\u6790\u7ed3\u679c\u4e00\u81f4\u6027\u6709\u9650\u3002", "method": "\u6784\u5efa\u540d\u4e3aSFT-TA\u7684\u81ea\u52a8\u5316\u4e3b\u9898\u5206\u6790\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee3\u7406\u5d4c\u5165\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u63d0\u5347\u5206\u6790\u6027\u80fd\u3002", "result": "SFT-TA\u5728\u4e0e\u4eba\u5de5\u53c2\u8003\u4e3b\u9898\u7684\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u548cgpt-4o\u57fa\u7ebf\uff1b\u5355\u72ec\u4f7f\u7528SFT\u4ee3\u7406\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e3aSFT\u4ee3\u7406\u5206\u914d\u7279\u5b9a\u89d2\u8272\u662f\u63d0\u5347\u4e3b\u9898\u5206\u6790\u81ea\u52a8\u5316\u6548\u679c\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.17156", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17156", "abs": "https://arxiv.org/abs/2509.17156", "authors": ["Samar Hadou", "Alejandro Ribeiro"], "title": "Unrolled Graph Neural Networks for Constrained Optimization", "comment": null, "summary": "In this paper, we unroll the dynamics of the dual ascent (DA) algorithm in\ntwo coupled graph neural networks (GNNs) to solve constrained optimization\nproblems. The two networks interact with each other at the layer level to find\na saddle point of the Lagrangian. The primal GNN finds a stationary point for a\ngiven dual multiplier, while the dual network iteratively refines its estimates\nto reach an optimal solution. We force the primal and dual networks to mirror\nthe dynamics of the DA algorithm by imposing descent and ascent constraints. We\npropose a joint training scheme that alternates between updating the primal and\ndual networks. Our numerical experiments demonstrate that our approach yields\nnear-optimal near-feasible solutions and generalizes well to\nout-of-distribution (OOD) problems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cc\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u5076\u4e0a\u5347\u7b97\u6cd5\u89e3\u8026\u65b9\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5bf9\u5076\u4e0a\u5347\u7b97\u6cd5\u5728\u5904\u7406\u590d\u6742\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u65f6\u6536\u655b\u6162\u4e14\u96be\u4ee5\u6cdb\u5316\uff0c\u5e0c\u671b\u5229\u7528GNN\u5efa\u6a21\u7ed3\u6784\u4fe1\u606f\u5e76\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "method": "\u5c06\u5bf9\u5076\u4e0a\u5347\u7b97\u6cd5\u5c55\u5f00\u4e3a\u4e24\u4e2a\u8026\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\uff0c\u5206\u522b\u5bf9\u5e94\u539f\u59cb\u548c\u5bf9\u5076\u53d8\u91cf\uff1b\u901a\u8fc7\u5c42\u95f4\u4ea4\u4e92\u5bfb\u627e\u62c9\u683c\u6717\u65e5\u51fd\u6570\u7684\u978d\u70b9\uff0c\u5e76\u65bd\u52a0\u4e0b\u964d\u4e0e\u4e0a\u5347\u7ea6\u675f\u4ee5\u6a21\u62dfDA\u52a8\u6001\uff0c\u91c7\u7528\u4ea4\u66ff\u66f4\u65b0\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u63a5\u8fd1\u6700\u4f18\u4e14\u8fd1\u4e4e\u53ef\u884c\u7684\u89e3\uff0c\u5e76\u5728\u5916\u5206\u5e03\uff08OOD\uff09\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86GNN\u4e0e\u5bf9\u5076\u4e0a\u5347\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u7ea6\u675f\u6ee1\u8db3\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u7ea6\u675f\u95ee\u9898\u7684\u6c42\u89e3\u3002"}}
{"id": "2509.18076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18076", "abs": "https://arxiv.org/abs/2509.18076", "authors": ["Hy Dang", "Tianyi Liu", "Zhuofeng Wu", "Jingfeng Yang", "Haoming Jiang", "Tao Yang", "Pei Chen", "Zhengyang Wang", "Helen Wang", "Huasheng Li", "Bing Yin", "Meng Jiang"], "title": "Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u677f\u6765\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u51fd\u6570\u8c03\u7528\uff0c\u663e\u8457\u51cf\u5c11\u5de5\u5177\u4f7f\u7528\u9519\u8bef\uff0c\u63d0\u5347AI\u52a9\u624b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5de5\u5177\u4ea4\u4e92\u4e2d\u5e38\u56e0\u53c2\u6570\u9519\u8bef\u3001\u5de5\u5177\u9009\u62e9\u4e0d\u5f53\u6216\u7528\u6237\u610f\u56fe\u8bef\u89e3\u800c\u5931\u8d25\uff0c\u4e3b\u8981\u6e90\u4e8e\u5bf9\u7528\u6237\u76ee\u6807\u548c\u5de5\u5177\u6587\u6863\u7406\u89e3\u4e0d\u8db3\u3002\u4f20\u7edf\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\u751a\u81f3\u9002\u5f97\u5176\u53cd\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u53d7\u8bfe\u7a0b\u5b66\u4e60\u542f\u53d1\u7684\u6846\u67b6\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u677f\uff0c\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5206\u6b65\u63a8\u7406\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u51fd\u6570\u8c03\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u548c\u57fa\u51c6\u4e0a\u76f8\u5bf9\u51cf\u5c11\u4e863-12%\u7684\u5de5\u5177\u4f7f\u7528\u9519\u8bef\uff0c\u5e76\u63d0\u5347\u4e86\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u677f\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u63a8\u52a8\u4e86\u9762\u5411\u5b9e\u9645\u5e94\u7528\u7684AI\u52a9\u624b\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.16768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16768", "abs": "https://arxiv.org/abs/2509.16768", "authors": ["Omid Bonakdar", "Nasser Mozayani"], "title": "MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation", "comment": null, "summary": "Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,\nmetaverse, and robotics. However, most methods represent the target object as a\nclosed mesh devoid of any structural information, limiting editing, animation,\nand semantic understanding. Part-aware 3D generation addresses this problem by\ndecomposing objects into meaningful components, but existing pipelines face\nchallenges: in existing methods, the user has no control over which objects are\nseparated and how model imagine the occluded parts in isolation phase. In this\npaper, we introduce MMPart, an innovative framework for generating part-aware\n3D models from a single image. We first use a VLM to generate a set of prompts\nbased on the input image and user descriptions. In the next step, a generative\nmodel generates isolated images of each object based on the initial image and\nthe previous step's prompts as supervisor (which control the pose and guide\nmodel how imagine previously occluded areas). Each of those images then enters\nthe multi-view generation stage, where a number of consistent images from\ndifferent views are generated. Finally, a reconstruction model converts each of\nthese multi-view images into a 3D model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMMPart\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5177\u6709\u90e8\u4ef6\u611f\u77e5\u76843D\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u7269\u4f53\u90e8\u4ef6\u5206\u89e3\u7684\u53ef\u63a7\u6027\uff0c\u5e76\u63d0\u5347\u906e\u6321\u90e8\u5206\u7684\u5408\u7406\u60f3\u8c61\u4e0e\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u91cd\u5efa\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u65b9\u6cd5\u5927\u591a\u5c06\u7269\u4f53\u8868\u793a\u4e3a\u65e0\u7ed3\u6784\u4fe1\u606f\u7684\u95ed\u5408\u7f51\u683c\uff0c\u9650\u5236\u4e86\u7f16\u8f91\u3001\u52a8\u753b\u548c\u8bed\u4e49\u7406\u89e3\uff1b\u540c\u65f6\uff0c\u90e8\u4ef6\u611f\u77e5\u751f\u6210\u4e2d\u7528\u6237\u7f3a\u4e4f\u5bf9\u5206\u5272\u548c\u906e\u6321\u90e8\u5206\u60f3\u8c61\u7684\u63a7\u5236\u3002", "method": "\u9996\u5148\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6839\u636e\u8f93\u5165\u56fe\u50cf\u548c\u7528\u6237\u63cf\u8ff0\u751f\u6210\u63d0\u793a\uff1b\u7136\u540e\u751f\u6210\u6a21\u578b\u5728\u63d0\u793a\u6307\u5bfc\u4e0b\u751f\u6210\u5404\u90e8\u4ef6\u7684\u9694\u79bb\u56fe\u50cf\uff1b\u63a5\u7740\u8fdb\u884c\u591a\u89c6\u89d2\u4e00\u81f4\u56fe\u50cf\u751f\u6210\uff1b\u6700\u540e\u901a\u8fc7\u91cd\u5efa\u6a21\u578b\u751f\u6210\u6bcf\u4e2a\u90e8\u4ef6\u76843D\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u90e8\u4ef6\u611f\u77e5\u76843D\u6a21\u578b\uff0c\u652f\u6301\u7528\u6237\u63a7\u5236\u90e8\u4ef6\u5206\u79bb\u65b9\u5f0f\u548c\u906e\u6321\u533a\u57df\u7684\u8865\u5168\uff0c\u63d0\u5347\u4e86\u751f\u6210\u90e8\u4ef6\u7684\u8bed\u4e49\u5408\u7406\u6027\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "conclusion": "MMPart\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u90e8\u4ef6\u611f\u77e53D\u751f\u6210\u4e2d\u7528\u6237\u63a7\u5236\u4e0d\u8db3\u548c\u906e\u6321\u63a8\u7406\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u4e3a\u53ef\u7f16\u8f91\u3001\u53ef\u89e3\u91ca\u76843D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17877", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17877", "abs": "https://arxiv.org/abs/2509.17877", "authors": ["Richard Kuhlmann", "Jakob Wolfram", "Boyang Sun", "Jiaxu Xing", "Davide Scaramuzza", "Marc Pollefeys", "Cesar Cadena"], "title": "Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection", "comment": null, "summary": "Autonomous inspection is a central problem in robotics, with applications\nranging from industrial monitoring to search-and-rescue. Traditionally,\ninspection has often been reduced to navigation tasks, where the objective is\nto reach a predefined location while avoiding obstacles. However, this\nformulation captures only part of the real inspection problem. In real-world\nenvironments, the inspection targets may become visible well before their exact\ncoordinates are reached, making further movement both redundant and\ninefficient. What matters more for inspection is not simply arriving at the\ntarget's position, but positioning the robot at a viewpoint from which the\ntarget becomes observable. In this work, we revisit inspection from a\nperception-aware perspective. We propose an end-to-end reinforcement learning\nframework that explicitly incorporates target visibility as the primary\nobjective, enabling the robot to find the shortest trajectory that guarantees\nvisual contact with the target without relying on a map. The learned policy\nleverages both perceptual and proprioceptive sensing and is trained entirely in\nsimulation, before being deployed to a real-world robot. We further develop an\nalgorithm to compute ground-truth shortest inspection paths, which provides a\nreference for evaluation. Through extensive experiments, we show that our\nmethod outperforms existing classical and learning-based navigation approaches,\nyielding more efficient inspection trajectories in both simulated and\nreal-world settings. The project is avialable at\nhttps://sight-over-site.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u7684\u81ea\u4e3b\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5c06\u76ee\u6807\u53ef\u89c1\u6027\u4f5c\u4e3a\u4e3b\u8981\u4f18\u5316\u76ee\u6807\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u65e0\u9700\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\u627e\u5230\u6700\u77ed\u4e14\u4fdd\u8bc1\u89c6\u89c9\u63a5\u89e6\u7684\u68c0\u6d4b\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u4efb\u52a1\u901a\u5e38\u88ab\u7b80\u5316\u4e3a\u5230\u8fbe\u9884\u5b9a\u4e49\u4f4d\u7f6e\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u4e2d\u76ee\u6807\u53ef\u80fd\u5728\u5230\u8fbe\u5176\u7cbe\u786e\u5750\u6807\u524d\u5c31\u5df2\u53ef\u89c1\uff0c\u5bfc\u81f4\u79fb\u52a8\u5197\u4f59\u548c\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4ee5\u611f\u77e5\u4e3a\u4e2d\u5fc3\u7684\u65b0\u65b9\u6cd5\u6765\u63d0\u9ad8\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u5f0f\u5730\u5c06\u76ee\u6807\u53ef\u89c1\u6027\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\uff0c\u7ed3\u5408\u611f\u77e5\u548c\u672c\u4f53\u611f\u89c9\u4fe1\u606f\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u5b8c\u6210\u5168\u90e8\u8bad\u7ec3\u540e\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u79cd\u7b97\u6cd5\u8ba1\u7b97\u6700\u77ed\u68c0\u6d4b\u8def\u5f84\u7684\u771f\u503c\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u7ecf\u5178\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u6548\u7684\u68c0\u6d4b\u8f68\u8ff9\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u611f\u77e5\u610f\u8bc6\u7684\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u68c0\u6d4b\u95ee\u9898\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u79fb\u52a8\uff0c\u63d0\u5347\u68c0\u6d4b\u4efb\u52a1\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17177", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17177", "abs": "https://arxiv.org/abs/2509.17177", "authors": ["Bowen Qin", "Chen Yue", "Fang Yin", "Hui Wang", "JG Yao", "Jiakang Liu", "Jing-Shu Zheng", "Miguel Hu Chen", "Richeng Xuan", "Shibei Meng", "Shiqi Zhou", "Teng Dai", "Tong-Shuai Ren", "Wei Cui", "Xi Yang", "Xialin Du", "Xiaojing Xu", "Xue Sun", "Xuejing Li", "Yaming Liu", "Yesheng Liu", "Ying Liu", "Yonghua Lin", "Yu Zhao", "Yunduo Zhang", "Yuwen Luo", "Zheqi He", "Zhiyuan He", "Zhongyuan Wang"], "title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions", "comment": "23 pages in main text", "summary": "We conduct a moderate-scale contamination-free (to some extent) evaluation of\ncurrent large reasoning models (LRMs) with some preliminary findings. We also\nrelease ROME, our evaluation benchmark for vision language models intended to\ntest reasoning from visual clues. We attach links to the benchmark, evaluation\ndata, and other updates on this website:\nhttps://flageval-baai.github.io/LRM-Eval/", "AI": {"tldr": "\u672c\u6587\u5bf9\u5f53\u524d\u7684\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u4e86\u9002\u5ea6\u89c4\u6a21\u7684\u65e0\u6c61\u67d3\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86\u540d\u4e3aROME\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u57fa\u4e8e\u89c6\u89c9\u7ebf\u7d22\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u5728\u65e0\u6570\u636e\u6c61\u67d3\u60c5\u51b5\u4e0b\u7684\u771f\u5b9e\u6027\u80fd\uff0c\u5e76\u63a8\u52a8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u76f8\u5bf9\u65e0\u6c61\u67d3\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u4f7f\u7528\u65b0\u63d0\u51fa\u7684ROME\u57fa\u51c6\u5bf9\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u91cd\u70b9\u8003\u5bdf\u5176\u4ece\u89c6\u89c9\u7ebf\u7d22\u4e2d\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\u3002", "result": "\u5f97\u5230\u4e86\u5f53\u524d\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u4e00\u4e9b\u521d\u6b65\u8bc4\u4f30\u7ed3\u679c\uff0c\u5e76\u516c\u5f00\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6570\u636e\u53ca\u76f8\u5173\u8d44\u6e90\u94fe\u63a5\u3002", "conclusion": "ROME\u57fa\u51c6\u6709\u52a9\u4e8e\u672a\u6765\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u516c\u5e73\u8bc4\u4f30\uff0c\u5f53\u524d\u6a21\u578b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2509.17165", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17165", "abs": "https://arxiv.org/abs/2509.17165", "authors": ["Sahar Koohfar", "Wubeshet Woldemariam"], "title": "Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding Denoising Auto Encoder Transformer", "comment": null, "summary": "Time series data is a prevalent form of data found in various fields. It\nconsists of a series of measurements taken over time. Forecasting is a crucial\napplication of time series models, where future values are predicted based on\nhistorical data. Accurate forecasting is essential for making well-informed\ndecisions across industries. When it comes to electric vehicles (EVs), precise\npredictions play a key role in planning infrastructure development, load\nbalancing, and energy management. This study introduces a BI-LSTM embedding\ndenoising autoencoder model (BDM) designed to address time series problems,\nfocusing on short-term EV charging load prediction. The performance of the\nproposed model is evaluated by comparing it with benchmark models like\nTransformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the\nproposed model outperforms the benchmark models in four of the five-time steps,\ndemonstrating its effectiveness for time series forecasting. This research\nmakes a significant contribution to enhancing time series forecasting, thereby\nimproving decision-making processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBI-LSTM\u5d4c\u5165\u53bb\u566a\u81ea\u7f16\u7801\u5668\u7684\u6a21\u578b\uff08BDM\uff09\uff0c\u7528\u4e8e\u77ed\u671f\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u8d1f\u8377\u9884\u6d4b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u6570\u65f6\u95f4\u6b65\u4e0a\u4f18\u4e8eTransformer\u3001CNN\u3001RNN\u7b49\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u652f\u6301\u7535\u52a8\u6c7d\u8f66\u9886\u57df\u7684\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u3001\u8d1f\u8f7d\u5e73\u8861\u548c\u80fd\u6e90\u7ba1\u7406\u51b3\u7b56\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e00\u79cdBI-LSTM\u5d4c\u5165\u53bb\u566a\u81ea\u7f16\u7801\u5668\u6a21\u578b\uff08BDM\uff09\uff0c\u7528\u4e8e\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5e76\u5e94\u7528\u4e8e\u77ed\u671fEV\u5145\u7535\u8d1f\u8377\u9884\u6d4b\u3002", "result": "\u5728\u4e94\u4e2a\u65f6\u95f4\u6b65\u4e2d\u6709\u56db\u4e2a\u65f6\u95f4\u6b65\u7684\u8868\u73b0\u4f18\u4e8eTransformer\u3001CNN\u3001RNN\u3001LSTM\u548cGRU\u7b49\u57fa\u51c6\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86BDM\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "BDM\u6a21\u578b\u5728\u77ed\u671fEV\u5145\u7535\u8d1f\u8377\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2509.18083", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18083", "abs": "https://arxiv.org/abs/2509.18083", "authors": ["Valentin Lacombe", "Valentin Quesnel", "Damien Sileo"], "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning", "comment": null, "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.", "AI": {"tldr": "Reasoning Core\u662f\u4e00\u4e2a\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u65b0\u53ef\u6269\u5c55\u73af\u5883\uff0c\u65e8\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u96c6\u4e2d\u4e8e\u6e38\u620f\u6216\u5b64\u7acb\u8c1c\u9898\uff0c\u7f3a\u4e4f\u5bf9\u57fa\u7840\u5f62\u5f0f\u5316\u9886\u57df\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u6db5\u76d6PDDL\u89c4\u5212\u3001\u4e00\u9636\u903b\u8f91\u3001\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u89e3\u6790\u3001\u56e0\u679c\u63a8\u7406\u548c\u7cfb\u7edf\u65b9\u7a0b\u6c42\u89e3\u7b49\u95ee\u9898\uff0c\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u9a8c\u8bc1\u548c\u8fde\u7eed\u96be\u5ea6\u63a7\u5236\u6765\u6784\u5efa\u9ad8\u901a\u7528\u6027\u7684\u95ee\u9898\u5206\u5e03\u3002", "result": "\u521d\u6b65\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u663e\u793a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u73af\u5883\u4e0a\u7684\u4efb\u52a1\u8868\u73b0\u8f83\u5dee\uff0c\u8868\u660e\u5176\u5177\u6709\u8f83\u9ad8\u96be\u5ea6\u3002", "conclusion": "Reasoning Core\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65e0\u9650\u8d44\u6e90\u3002"}}
{"id": "2509.16771", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2509.16771", "abs": "https://arxiv.org/abs/2509.16771", "authors": ["Xiaohan Chen", "Hongrui Gu", "Cunshi Wang", "Haiyang Mu", "Jie Zheng", "Junju Du", "Jing Ren", "Zhou Fan", "Jing Li"], "title": "Artificial Satellite Trails Detection Using U-Net Deep Neural Network and Line Segment Detector Algorithm", "comment": "15 pages, 7 figures, 2 tables, PASP accepted", "summary": "With the rapid increase in the number of artificial satellites, astronomical\nimaging is experiencing growing interference. When these satellites reflect\nsunlight, they produce streak-like artifacts in photometry images. Such\nsatellite trails can introduce false sources and cause significant photometric\nerrors. As a result, accurately identifying the positions of satellite trails\nin observational data has become essential. In this work, we propose a\nsatellite trail detection model that combines the U-Net deep neural network for\nimage segmentation with the Line Segment Detector (LSD) algorithm. The model is\ntrained on 375 simulated images of satellite trails, generated using data from\nthe Mini-SiTian Array. Experimental results show that for trails with a\nsignal-to-noise ratio (SNR) greater than 3, the detection rate exceeds 99.\nAdditionally, when applied to real observational data from the Mini-SiTian\nArray, the model achieves a recall of 79.57 and a precision of 74.56.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408U-Net\u548cLSD\u7b97\u6cd5\u7684\u536b\u661f\u8f68\u8ff9\u68c0\u6d4b\u6a21\u578b\uff0c\u57fa\u4e8e\u6a21\u62df\u548c\u771f\u5b9e\u5929\u6587\u56fe\u50cf\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u7387\u548c\u8f83\u597d\u7684\u53ec\u56de\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740\u4eba\u9020\u536b\u661f\u6570\u91cf\u8fc5\u901f\u589e\u52a0\uff0c\u5176\u5728\u5929\u6587\u6210\u50cf\u4e2d\u4ea7\u751f\u7684\u6761\u7eb9\u72b6\u8f68\u8ff9\u5e72\u6270\u65e5\u76ca\u4e25\u91cd\uff0c\u5f71\u54cd\u5149\u5ea6\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u4e9f\u9700\u6709\u6548\u65b9\u6cd5\u51c6\u786e\u8bc6\u522b\u536b\u661f\u8f68\u8ff9\u4f4d\u7f6e\u3002", "method": "\u91c7\u7528U-Net\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u56fe\u50cf\u5206\u5272\uff0c\u5e76\u7ed3\u5408\u7ebf\u6bb5\u68c0\u6d4b\u7b97\u6cd5\uff08LSD\uff09\uff0c\u5728375\u5f20\u57fa\u4e8eMini-SiTian\u9635\u5217\u6570\u636e\u751f\u6210\u7684\u6a21\u62df\u536b\u661f\u8f68\u8ff9\u56fe\u50cf\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5bf9\u4e8e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u5927\u4e8e3\u7684\u8f68\u8ff9\uff0c\u68c0\u6d4b\u7387\u8d85\u8fc799%\uff1b\u5728Mini-SiTian\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u4e0a\uff0c\u53ec\u56de\u7387\u8fbe\u523079.57%\uff0c\u7cbe\u5ea6\u4e3a74.56%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u878d\u5408U-Net\u4e0eLSD\u7684\u6a21\u578b\u80fd\u9ad8\u6548\u68c0\u6d4b\u536b\u661f\u8f68\u8ff9\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5904\u7406\u5f53\u524d\u53ca\u672a\u6765\u5929\u6587\u89c2\u6d4b\u4e2d\u7684\u536b\u661f\u5e72\u6270\u95ee\u9898\u3002"}}
{"id": "2509.17884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17884", "abs": "https://arxiv.org/abs/2509.17884", "authors": ["Arun L. Bishop", "Juan Alvarez-Padilla", "Sam Schoedel", "Ibrahima Sory Sow", "Juee Chandrachud", "Sheitej Sharma", "Will Kraus", "Beomyeong Park", "Robert J. Griffin", "John M. Dolan", "Zachary Manchester"], "title": "The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control", "comment": "Accepted to IEEE Humanoids 2025. For videos and code visit\n  https://linearwalking.github.io/", "summary": "When do locomotion controllers require reasoning about nonlinearities? In\nthis work, we show that a whole-body model-predictive controller using a simple\nlinear time-invariant approximation of the whole-body dynamics is able to\nexecute basic locomotion tasks on complex legged robots. The formulation\nrequires no online nonlinear dynamics evaluations or matrix inversions. We\ndemonstrate walking, disturbance rejection, and even navigation to a goal\nposition without a separate footstep planner on a quadrupedal robot. In\naddition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with\nsignificant limb inertia, complex actuator dynamics, and large sim-to-real gap.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u65f6\u4e0d\u53d8\u8fd1\u4f3c\u52a8\u529b\u5b66\u7684\u5168\u8eab\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5728\u590d\u6742\u817f\u5f0f\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u57fa\u672c\u7684\u8fd0\u52a8\u4efb\u52a1\uff0c\u65e0\u9700\u5728\u7ebf\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8ba1\u7b97\u6216\u77e9\u9635\u6c42\u9006\u3002", "motivation": "\u63a2\u8ba8\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u8fd0\u52a8\u63a7\u5236\u9700\u8981\u8003\u8651\u975e\u7ebf\u6027\u56e0\u7d20\uff0c\u5e76\u9a8c\u8bc1\u7b80\u5316\u7ebf\u6027\u6a21\u578b\u5728\u590d\u6742\u673a\u5668\u4eba\u4e0a\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u7ebf\u6027\u65f6\u4e0d\u53d8\uff08LTI\uff09\u8fd1\u4f3c\u6765\u5efa\u6a21\u5168\u8eab\u52a8\u529b\u5b66\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u4e0d\u4f9d\u8d56\u5728\u7ebf\u975e\u7ebf\u6027\u8ba1\u7b97\u6216\u77e9\u9635\u6c42\u9006\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u884c\u8d70\u3001\u6297\u5e72\u6270\u548c\u76ee\u6807\u5bfc\u822a\uff08\u65e0\u9700\u5355\u72ec\u7684\u843d\u811a\u70b9\u89c4\u5212\uff09\uff0c\u5e76\u5728\u5177\u6709\u5927\u60ef\u6027\u3001\u590d\u6742\u6267\u884c\u5668\u52a8\u6001\u548c\u663e\u8457\u4eff\u771f-\u73b0\u5b9e\u5dee\u8ddd\u7684\u6db2\u538b\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u52a8\u6001\u884c\u8d70\u3002", "conclusion": "\u5373\u4f7f\u4f7f\u7528\u7b80\u5316\u7684\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5168\u8eabMPC\u4e5f\u80fd\u6709\u6548\u5b8c\u6210\u591a\u79cd\u590d\u6742\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u57fa\u672c\u8fd0\u52a8\u4efb\u52a1\uff0c\u8868\u660e\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u65e0\u9700\u663e\u5f0f\u5904\u7406\u975e\u7ebf\u6027\u3002"}}
{"id": "2509.17178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17178", "abs": "https://arxiv.org/abs/2509.17178", "authors": ["Tian Lan", "Jinyuan Xu", "Xue He", "Jenq-Neng Hwang", "Lei Li"], "title": "Attention Consistency for LLMs Explanation", "comment": null, "summary": "Understanding the decision-making processes of large language models (LLMs)\nis essential for their trustworthy development and deployment. However, current\ninterpretability methods often face challenges such as low resolution and high\ncomputational cost. To address these limitations, we propose the\n\\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,\nand easily deployable heuristic for estimating the importance of input tokens\nin decoder-based models. MACS measures contributions of input tokens based on\nthe consistency of maximal attention. Empirical evaluations demonstrate that\nMACS achieves a favorable trade-off between interpretability quality and\ncomputational efficiency, showing faithfulness comparable to complex techniques\nwith a 22\\% decrease in VRAM usage and 30\\% reduction in latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u591a\u5c42\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u5206\u6570\uff08MACS\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u8f93\u5165\u4ee4\u724c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5e38\u9762\u4e34\u5206\u8fa8\u7387\u4f4e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6700\u5927\u6ce8\u610f\u529b\u7684\u4e00\u81f4\u6027\u6765\u8861\u91cf\u8f93\u5165\u4ee4\u724c\u7684\u8d21\u732e\uff0c\u63d0\u51faMulti-Layer Attention Consistency Score (MACS)\u3002", "result": "MACS\u5728\u53ef\u89e3\u91ca\u6027\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u6743\u8861\uff0c\u76f8\u6bd4\u590d\u6742\u65b9\u6cd5\u51cf\u5c11\u4e8622%\u7684\u663e\u5b58\u4f7f\u7528\u548c30%\u7684\u5ef6\u8fdf\u3002", "conclusion": "MACS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5fe0\u5b9e\u4e14\u8d44\u6e90\u6d88\u8017\u4f4e\u7684\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u4fe1\u5f00\u53d1\u4e0e\u90e8\u7f72\u3002"}}
{"id": "2509.17175", "categories": ["cs.LG", "stat.AP", "62P12", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.17175", "abs": "https://arxiv.org/abs/2509.17175", "authors": ["Ni\u00e1l Perry", "Peter P. Pedersen", "Charles N. Christensen", "Emanuel Nussli", "Sanelma Heinonen", "Lorena Gordillo Dagallier", "Rapha\u00ebl Jacquat", "Sebastian Horstmann", "Christoph Franck"], "title": "Detecting Urban PM$_{2.5}$ Hotspots with Mobile Sensing and Gaussian Process Regression", "comment": "39 pages, 12 figures", "summary": "Low-cost mobile sensors can be used to collect PM$_{2.5}$ concentration data\nthroughout an entire city. However, identifying air pollution hotspots from the\ndata is challenging due to the uneven spatial sampling, temporal variations in\nthe background air quality, and the dynamism of urban air pollution sources.\nThis study proposes a method to identify urban PM$_{2.5}$ hotspots that\naddresses these challenges, involving four steps: (1) equip citizen scientists\nwith mobile PM$_{2.5}$ sensors while they travel; (2) normalise the raw data to\nremove the influence of background ambient pollution levels; (3) fit a Gaussian\nprocess regression model to the normalised data and (4) calculate a grid of\nspatially explicit 'hotspot scores' using the probabilistic framework of\nGaussian processes, which conveniently summarise the relative pollution levels\nthroughout the city. We apply our method to create the first ever map of\nPM$_{2.5}$ pollution in Kigali, Rwanda, at a 200m resolution. Our results\nsuggest that the level of ambient PM$_{2.5}$ pollution in Kigali is dangerously\nhigh, and we identify the hotspots in Kigali where pollution consistently\nexceeds the city-wide average. We also evaluate our method using simulated\nmobile sensing data for Beijing, China, where we find that the hotspot scores\nare probabilistically well calibrated and accurately reflect the 'ground truth'\nspatial profile of PM$_{2.5}$ pollution. Thanks to the use of open-source\nsoftware, our method can be re-applied in cities throughout the world with a\nhandful of low-cost sensors. The method can help fill the gap in urban air\nquality information and empower public health officials.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79fb\u52a8\u4f20\u611f\u5668\u7684\u9ad8\u5206\u8fa8\u7387\u57ce\u5e02PM2.5\u70ed\u70b9\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5efa\u6a21\uff0c\u5728\u5362\u65fa\u8fbe\u57fa\u52a0\u5229\u9996\u6b21\u7ed8\u5236\u51fa200\u7c73\u5206\u8fa8\u7387\u6c61\u67d3\u56fe\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u6a21\u62df\u5317\u4eac\u6570\u636e\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u7a7a\u95f4\u91c7\u6837\u4e0d\u5747\u3001\u80cc\u666f\u7a7a\u6c14\u8d28\u91cf\u65f6\u53d8\u6027\u548c\u6c61\u67d3\u6e90\u52a8\u6001\u6027\uff0c\u5229\u7528\u4f4e\u6210\u672c\u79fb\u52a8\u4f20\u611f\u5668\u6570\u636e\u8bc6\u522b\u57ce\u5e02PM2.5\u70ed\u70b9\u5177\u6709\u6311\u6218\u6027\u3002", "method": "1\uff09\u7531\u5e02\u6c11\u79d1\u5b66\u5bb6\u643a\u5e26\u79fb\u52a8\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff1b2\uff09\u5bf9\u539f\u59cb\u6570\u636e\u8fdb\u884c\u80cc\u666f\u6c61\u67d3\u6c34\u5e73\u5f52\u4e00\u5316\uff1b3\uff09\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6a21\u578b\u62df\u5408\u5f52\u4e00\u5316\u6570\u636e\uff1b4\uff09\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u6982\u7387\u6846\u67b6\u8ba1\u7b97\u7a7a\u95f4\u663e\u5f0f\u7684\u2018\u70ed\u70b9\u8bc4\u5206\u2019\u7f51\u683c\u3002", "result": "\u5728\u57fa\u52a0\u5229\u751f\u6210\u4e86\u9996\u4e2a200\u7c73\u5206\u8fa8\u7387PM2.5\u6c61\u67d3\u5730\u56fe\uff0c\u53d1\u73b0\u8be5\u5e02\u7a7a\u6c14\u8d28\u91cf\u5371\u9669\uff0c\u8bc6\u522b\u51fa\u6301\u7eed\u9ad8\u4e8e\u5168\u5e02\u5e73\u5747\u6c34\u5e73\u7684\u6c61\u67d3\u70ed\u70b9\uff1b\u5728\u5317\u4eac\u7684\u6a21\u62df\u6570\u636e\u4e2d\u9a8c\u8bc1\u663e\u793a\u70ed\u70b9\u8bc4\u5206\u5177\u6709\u826f\u597d\u6982\u7387\u6821\u51c6\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u501f\u52a9\u5f00\u6e90\u8f6f\u4ef6\u548c\u5c11\u91cf\u4f4e\u6210\u672c\u4f20\u611f\u5668\u5728\u5168\u7403\u57ce\u5e02\u63a8\u5e7f\u5e94\u7528\uff0c\u6709\u52a9\u4e8e\u586b\u8865\u57ce\u5e02\u7a7a\u6c14\u8d28\u91cf\u4fe1\u606f\u7a7a\u767d\uff0c\u652f\u6301\u516c\u5171\u536b\u751f\u51b3\u7b56\u3002"}}
{"id": "2403.09548", "categories": ["cs.LG", "cs.AI", "cs.CY", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2403.09548", "abs": "https://arxiv.org/abs/2403.09548", "authors": ["Jo\u00e3o Manoel Herrera Pinheiro", "Marcelo Becker"], "title": "Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability", "comment": "9 pages, 16 figures", "summary": "Cancer is one of the diseases that kill the most women in the world, with\nbreast cancer being responsible for the highest number of cancer cases and\nconsequently deaths. However, it can be prevented by early detection and,\nconsequently, early treatment. Any development for detection or perdition this\nkind of cancer is important for a better healthy life. Many studies focus on a\nmodel with high accuracy in cancer prediction, but sometimes accuracy alone may\nnot always be a reliable metric. This study implies an investigative approach\nto studying the performance of different machine learning algorithms based on\nboosting to predict breast cancer focusing on the recall metric. Boosting\nmachine learning algorithms has been proven to be an effective tool for\ndetecting medical diseases. The dataset of the University of California, Irvine\n(UCI) repository has been utilized to train and test the model classifier that\ncontains their attributes. The main objective of this study is to use\nstate-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and\nLightGBM to predict and diagnose breast cancer and to find the most effective\nmetric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study\nis the first to use these four boosting algorithms with Optuna, a library for\nhyperparameter optimization, and the SHAP method to improve the\ninterpretability of our model, which can be used as a support to identify and\npredict breast cancer. We were able to improve AUC or recall for all the models\nand reduce the False Negative for AdaBoost and LigthGBM the final AUC were more\nthan 99.41\\% for all models.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528AdaBoost\u3001XGBoost\u3001CatBoost\u548cLightGBM\u56db\u79cd\u5148\u8fdb\u7684\u63d0\u5347\u7b97\u6cd5\uff0c\u7ed3\u5408Optuna\u8d85\u53c2\u6570\u4f18\u5316\u548cSHAP\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u57fa\u4e8eUCI\u4e73\u817a\u764c\u6570\u636e\u96c6\u8fdb\u884c\u9884\u6d4b\uff0c\u91cd\u70b9\u5173\u6ce8\u53ec\u56de\u7387\u7b49\u6307\u6807\uff0c\u5728\u6240\u6709\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc799.41%\u7684AUC\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5047\u9634\u6027\u7387\u3002", "motivation": "\u4e73\u817a\u764c\u662f\u5bfc\u81f4\u5973\u6027\u6b7b\u4ea1\u7684\u4e3b\u8981\u764c\u75c7\u4e4b\u4e00\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u63d0\u9ad8\u6cbb\u6108\u7387\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u8bb8\u591a\u7814\u7a76\u5173\u6ce8\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5355\u4e00\u51c6\u786e\u7387\u53ef\u80fd\u4e0d\u8db3\u4ee5\u53cd\u6620\u6a21\u578b\u5728\u4e34\u5e8a\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982\u53ec\u56de\u7387\uff09\u6765\u51cf\u5c11\u6f0f\u8bca\u3002", "method": "\u91c7\u7528\u56db\u79cd\u57fa\u4e8e\u63d0\u5347\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08AdaBoost\u3001XGBoost\u3001CatBoost\u3001LightGBM\uff09\uff0c\u4f7f\u7528UCI\u4e73\u817a\u764c\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\uff1b\u5f15\u5165Optuna\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u5e94\u7528SHAP\u65b9\u6cd5\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff1b\u91cd\u70b9\u8bc4\u4f30\u53ec\u56de\u7387\u3001ROC-AUC\u548c\u6df7\u6dc6\u77e9\u9635\u7b49\u6027\u80fd\u6307\u6807\u3002", "result": "\u6240\u6709\u6a21\u578b\u7ecf\u4f18\u5316\u540eAUC\u5747\u8d85\u8fc799.41%\uff0c\u53ec\u56de\u7387\u5f97\u5230\u63d0\u5347\uff0c\u7279\u522b\u662fAdaBoost\u548cLightGBM\u663e\u8457\u964d\u4f4e\u4e86\u5047\u9634\u6027\u7387\u3002\u8fd9\u662f\u9996\u6b21\u5c06\u8fd9\u56db\u79cd\u63d0\u5347\u7b97\u6cd5\u4e0eOptuna\u53caSHAP\u65b9\u6cd5\u7ed3\u5408\u7528\u4e8e\u4e73\u817a\u764c\u9884\u6d4b\u7684\u7814\u7a76\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u5347\u7684\u7b97\u6cd5\u5728\u4e73\u817a\u764c\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u8d85\u53c2\u6570\u4f18\u5316\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e0e\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u6709\u52a9\u4e8e\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u65e9\u671f\u8bca\u65ad\u3002"}}
{"id": "2509.16805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16805", "abs": "https://arxiv.org/abs/2509.16805", "authors": ["Md. Atabuzzaman", "Ali Asgarov", "Chris Thomas"], "title": "Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models", "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Large Vision-Language Models (LVLMs) have achieved strong performance on\nvision-language tasks, particularly Visual Question Answering (VQA). While\nprior work has explored unimodal biases in VQA, the problem of selection bias\nin Multiple-Choice Question Answering (MCQA), where models may favor specific\noption tokens (e.g., \"A\") or positions, remains underexplored. In this paper,\nwe investigate both the presence and nature of selection bias in LVLMs through\nfine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,\ndefined by the semantic similarity of the options. We further propose an\ninference-time logit-level debiasing method that estimates an ensemble bias\nvector from general and contextual prompts and applies confidence-adaptive\ncorrections to the model's output. Our method mitigates bias without retraining\nand is compatible with frozen LVLMs. Extensive experiments across several\nstate-of-the-art models reveal consistent selection biases that intensify with\ntask difficulty, and show that our mitigation approach significantly reduces\nbias while improving accuracy in challenging settings. This work offers new\ninsights into the limitations of LVLMs in MCQA and presents a practical\napproach to improve their robustness in fine-grained visual reasoning. Datasets\nand code are available at:\nhttps://github.com/Atabuzzaman/Selection-Bias-of-LVLMs", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u591a\u9879\u9009\u62e9\u9898\u56de\u7b54\uff08MCQA\uff09\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u504f\u597d\u7279\u5b9a\u9009\u9879\u6807\u8bb0\u6216\u4f4d\u7f6e\uff0c\u4e14\u504f\u5dee\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u52a0\u5267\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u63a8\u7406\u9636\u6bb5\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7logit\u5c42\u9762\u7684\u81ea\u9002\u5e94\u6821\u6b63\u6709\u6548\u51cf\u8f7b\u504f\u5dee\u5e76\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u5c3d\u7ba1LVLMs\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5728MCQA\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u9009\u62e9\u504f\u5dee\uff08\u5982\u504f\u597d\u9009\u9879'A'\u6216\u7279\u5b9a\u4f4d\u7f6e\uff09\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u8fd9\u79cd\u504f\u5dee\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5e76\u63d0\u51fa\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u6784\u5efa\u4e86\u6309\u9009\u9879\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5212\u5206\u96be\u6613\u7a0b\u5ea6\u7684\u7ec6\u7c92\u5ea6MCQA\u57fa\u51c6\uff0c\u8bc4\u4f30LVLM\u7684\u9009\u62e9\u504f\u5dee\uff1b\u63d0\u51fa\u4e00\u79cd\u63a8\u7406\u65f6logit\u7ea7\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7\u901a\u7528\u548c\u4e0a\u4e0b\u6587\u63d0\u793a\u4f30\u8ba1\u96c6\u6210\u504f\u5dee\u5411\u91cf\uff0c\u5e76\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u7684\u8f93\u51fa\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u591a\u79cd\u5148\u8fdbLVLM\u5b58\u5728\u663e\u8457\u9009\u62e9\u504f\u5dee\uff0c\u4e14\u504f\u5dee\u968f\u4efb\u52a1\u96be\u5ea6\u4e0a\u5347\uff1b\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u504f\u5dee\uff0c\u5728\u4e0d\u91cd\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u5728\u56f0\u96be\u6837\u672c\u4e0a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "LVLMs\u5728MCQA\u4e2d\u5b58\u5728\u4e0d\u53ef\u5ffd\u89c6\u7684\u9009\u62e9\u504f\u5dee\uff0c\u5f71\u54cd\u5176\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff1b\u63d0\u51fa\u7684\u53bb\u504f\u65b9\u6cd5\u65e0\u9700\u5fae\u8c03\uff0c\u5177\u6709\u826f\u597d\u7684\u517c\u5bb9\u6027\u548c\u5b9e\u7528\u6027\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u516c\u5e73\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.17940", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17940", "abs": "https://arxiv.org/abs/2509.17940", "authors": ["Shuyao Shang", "Yuntao Chen", "Yuqi Wang", "Yingyan Li", "Zhaoxiang Zhang"], "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving", "comment": "NeurIPS 2025", "summary": "End-to-end autonomous driving has substantially progressed by directly\npredicting future trajectories from raw perception inputs, which bypasses\ntraditional modular pipelines. However, mainstream methods trained via\nimitation learning suffer from critical safety limitations, as they fail to\ndistinguish between trajectories that appear human-like but are potentially\nunsafe. Some recent approaches attempt to address this by regressing multiple\nrule-driven scores but decoupling supervision from policy optimization,\nresulting in suboptimal performance. To tackle these challenges, we propose\nDriveDPO, a Safety Direct Preference Optimization Policy Learning framework.\nFirst, we distill a unified policy distribution from human imitation similarity\nand rule-based safety scores for direct policy optimization. Further, we\nintroduce an iterative Direct Preference Optimization stage formulated as\ntrajectory-level preference alignment. Extensive experiments on the NAVSIM\nbenchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of\n90.0. Furthermore, qualitative results across diverse challenging scenarios\nhighlight DriveDPO's ability to produce safer and more reliable driving\nbehaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDriveDPO\uff0c\u4e00\u79cd\u57fa\u4e8e\u5b89\u5168\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u4eba\u7c7b\u6a21\u4eff\u76f8\u4f3c\u6027\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5b89\u5168\u8bc4\u5206\u8fdb\u884c\u8f68\u8ff9\u7ea7\u504f\u597d\u5bf9\u9f50\uff0c\u5728NAVSIM\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8690.0\u7684PDMS\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u4e3b\u6d41\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u96be\u4ee5\u533a\u5206\u770b\u4f3c\u4eba\u7c7b\u884c\u4e3a\u4f46\u5b9e\u9645\u4e0d\u5b89\u5168\u7684\u8f68\u8ff9\uff0c\u5bfc\u81f4\u5b89\u5168\u9690\u60a3\uff1b\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u5c06\u5b89\u5168\u8bc4\u5206\u4e0e\u7b56\u7565\u4f18\u5316\u5206\u79bb\uff0c\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faDriveDPO\u6846\u67b6\uff1a\u9996\u5148\u878d\u5408\u4eba\u7c7b\u6a21\u4eff\u76f8\u4f3c\u6027\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5b89\u5168\u8bc4\u5206\uff0c\u63d0\u70bc\u7edf\u4e00\u7b56\u7565\u5206\u5e03\u7528\u4e8e\u76f4\u63a5\u4f18\u5316\uff1b\u7136\u540e\u5f15\u5165\u8fed\u4ee3\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u9636\u6bb5\uff0c\u5b9e\u73b0\u8f68\u8ff9\u7ea7\u7684\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u4e0a\u53d6\u5f9790.0\u7684PDMS\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u6c34\u5e73\uff1b\u5b9a\u6027\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\u80fd\u751f\u6210\u66f4\u5b89\u5168\u3001\u53ef\u9760\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "conclusion": "DriveDPO\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u6a21\u4eff\u76f8\u4f3c\u6027\u4e0e\u5b89\u5168\u8bc4\u5206\uff0c\u5e76\u7ed3\u5408\u8f68\u8ff9\u7ea7\u504f\u597d\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u5b89\u5168\u6027\u4e0e\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2509.17183", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17183", "abs": "https://arxiv.org/abs/2509.17183", "authors": ["Junsong Li", "Jie Zhou", "Bihao Zhan", "Yutao Yang", "Qianjun Pan", "Shilian Chen", "Tianyu Huai", "Xin Li", "Qin Chen", "Liang He"], "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization", "comment": null, "summary": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning\nwith human preferences on a specific task/domain. Traditional alignment methods\nsuffer from catastrophic forgetting, where models lose previously acquired\nknowledge when adapting to new preferences or domains. We introduce LifeAlign,\na novel framework for lifelong alignment that enables LLMs to maintain\nconsistent human preference alignment across sequential learning tasks without\nforgetting previously learned knowledge. Our approach consists of two key\ninnovations. First, we propose a focalized preference optimization strategy\nthat aligns LLMs with new preferences while preventing the erosion of knowledge\nacquired from previous tasks. Second, we develop a short-to-long memory\nconsolidation mechanism that merges denoised short-term preference\nrepresentations into stable long-term memory using intrinsic dimensionality\nreduction, enabling efficient storage and retrieval of alignment patterns\nacross diverse domains. We evaluate LifeAlign across multiple sequential\nalignment tasks spanning different domains and preference types. Experimental\nresults demonstrate that our method achieves superior performance in\nmaintaining both preference alignment quality and knowledge retention compared\nto existing lifelong learning approaches. The codes and datasets will be\nreleased on GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLifeAlign\u7684\u65b0\u578b\u7ec8\u8eab\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6301\u7eed\u4fdd\u6301\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5728\u9002\u5e94\u65b0\u504f\u597d\u6216\u9886\u57df\u65f6\u5bb9\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5bfc\u81f4\u6a21\u578b\u4e22\u5931\u5148\u524d\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u805a\u7126\u504f\u597d\u4f18\u5316\u7b56\u7565\u548c\u77ed\u957f\u671f\u8bb0\u5fc6\u6574\u5408\u673a\u5236\uff0c\u901a\u8fc7\u5185\u5728\u7ef4\u5ea6\u964d\u4f4e\u5b9e\u73b0\u9ad8\u6548\u5b58\u50a8\u4e0e\u68c0\u7d22\u8de8\u57df\u5bf9\u9f50\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u8fde\u7eed\u5bf9\u9f50\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0cLifeAlign\u5728\u4fdd\u6301\u5bf9\u9f50\u8d28\u91cf\u548c\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LifeAlign\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u7684\u4e00\u81f4\u504f\u597d\u5bf9\u9f50\u3002"}}
{"id": "2509.17176", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17176", "abs": "https://arxiv.org/abs/2509.17176", "authors": ["Ganesh Khekare", "Shivam Sunda", "Yash Bothra"], "title": "A Comprehensive Performance Comparison of Traditional and Ensemble Machine Learning Models for Online Fraud Detection", "comment": "6 pages, 6 figures. Presented at IEEE INTERNATIONAL CONFERENCE ON\n  COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT), 2025", "summary": "In the era of the digitally driven economy, where there has been an\nexponential surge in digital payment systems and other online activities,\nvarious forms of fraudulent activities have accompanied the digital growth, out\nof which credit card fraud has become an increasingly significant threat. To\ndeal with this, real-time fraud detection is essential for financial security\nbut remains challenging due to high transaction volumes and the complexity of\nmodern fraud patterns. This study presents a comprehensive performance\ncomparison between traditional machine learning models like Random Forest, SVM,\nLogistic Regression, XGBoost, and ensemble methods like Stacking and Voting\nClassifier for detecting credit card fraud on a heavily imbalanced public\ndataset, where the number of fraudulent transactions is 492 out of 284,807\ntotal transactions. Application-specific preprocessing techniques were applied,\nand the models were evaluated using various performance metrics. The ensemble\nmethods achieved an almost perfect precision of around 0.99, but traditional\nmethods demonstrated superior performance in terms of recall, which highlights\nthe trade-off between false positives and false negatives. The comprehensive\ncomparison reveals distinct performance strengths and limitations for each\nalgorithm, offering insights to guide practitioners in selecting the most\neffective model for robust fraud detection applications in real-world settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u96c6\u6210\u65b9\u6cd5\u5728\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u4e00\u4e2a\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u663e\u793a\u96c6\u6210\u65b9\u6cd5\u5177\u6709\u9ad8\u7cbe\u786e\u7387\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5728\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5e94\u5bf9\u6570\u5b57\u7ecf\u6d4e\u53d1\u5c55\u4e2d\u4fe1\u7528\u5361\u6b3a\u8bc8\u65e5\u76ca\u4e25\u91cd\u7684\u5a01\u80c1\uff0c\u9700\u5b9e\u73b0\u5b9e\u65f6\u4e14\u9ad8\u6548\u7684\u6b3a\u8bc8\u68c0\u6d4b\u3002", "method": "\u91c7\u7528Random Forest\u3001SVM\u3001Logistic Regression\u3001XGBoost\u4ee5\u53caStacking\u548cVoting Classifier\u7b49\u4f20\u7edf\u4e0e\u96c6\u6210\u6a21\u578b\uff0c\u5728\u7ecf\u8fc7\u7279\u5b9a\u9884\u5904\u7406\u7684\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\uff08\u5982Stacking\u548cVoting\uff09\u8fbe\u5230\u7ea60.99\u7684\u7cbe\u786e\u7387\uff0c\u4f46\u5728\u53ec\u56de\u7387\u4e0a\u4f4e\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u7cbe\u786e\u7387\u4e0e\u53ec\u56de\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u4e0d\u540c\u6a21\u578b\u5404\u6709\u4f18\u52a3\uff0c\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u9009\u62e9\u6700\u9002\u5408\u7684\u6b3a\u8bc8\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2509.16806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16806", "abs": "https://arxiv.org/abs/2509.16806", "authors": ["Kacper Marzol", "Ignacy Kolton", "Weronika Smolak-Dy\u017cewska", "Joanna Kaleta", "Marcin Mazur", "Przemys\u0142aw Spurek"], "title": "MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging", "comment": null, "summary": "Multi-modal three-dimensional (3D) medical imaging data, derived from\nultrasound, magnetic resonance imaging (MRI), and potentially computed\ntomography (CT), provide a widely adopted approach for non-invasive anatomical\nvisualization. Accurate modeling, registration, and visualization in this\nsetting depend on surface reconstruction and frame-to-frame interpolation.\nTraditional methods often face limitations due to image noise and incomplete\ninformation between frames. To address these challenges, we present MedGS, a\nsemi-supervised neural implicit surface reconstruction framework that employs a\nGaussian Splatting (GS)-based interpolation mechanism. In this framework,\nmedical imaging data are represented as consecutive two-dimensional (2D) frames\nembedded in 3D space and modeled using Gaussian-based distributions. This\nrepresentation enables robust frame interpolation and high-fidelity surface\nreconstruction across imaging modalities. As a result, MedGS offers more\nefficient training than traditional neural implicit methods. Its explicit\nGS-based representation enhances noise robustness, allows flexible editing, and\nsupports precise modeling of complex anatomical structures with fewer\nartifacts. These features make MedGS highly suitable for scalable and practical\napplications in medical imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedGS\u7684\u534a\u76d1\u7763\u795e\u7ecf\u9690\u5f0f\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u9ad8\u65af\u70b9\u9635\uff08Gaussian Splatting\uff09\u7684\u63d2\u503c\u673a\u5236\uff0c\u5b9e\u73b0\u591a\u6a21\u60013D\u533b\u5b66\u56fe\u50cf\u7684\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u8868\u9762\u91cd\u5efa\u4e0e\u5e27\u95f4\u63d2\u503c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u533b\u5b66\u56fe\u50cf\u65f6\u53d7\u9650\u4e8e\u56fe\u50cf\u566a\u58f0\u548c\u5e27\u95f4\u4fe1\u606f\u4e0d\u5b8c\u6574\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u8868\u9762\u91cd\u5efa\u4e0e\u63d2\u503c\u3002", "method": "\u5c06\u533b\u5b66\u56fe\u50cf\u6570\u636e\u8868\u793a\u4e3a\u5d4c\u51653D\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed2D\u5e27\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af\u70b9\u9635\u63d2\u503c\u673a\u5236\u8fdb\u884c\u8868\u9762\u91cd\u5efa\u3002", "result": "MedGS\u76f8\u6bd4\u4f20\u7edf\u795e\u7ecf\u9690\u5f0f\u65b9\u6cd5\u8bad\u7ec3\u66f4\u9ad8\u6548\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u6297\u566a\u80fd\u529b\uff0c\u652f\u6301\u7075\u6d3b\u7f16\u8f91\u548c\u590d\u6742\u89e3\u5256\u7ed3\u6784\u7684\u7cbe\u786e\u5efa\u6a21\uff0c\u4e14\u4f2a\u5f71\u66f4\u5c11\u3002", "conclusion": "MedGS\u5728\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8868\u9762\u91cd\u5efa\u4e0e\u63d2\u503c\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u548c\u5b9e\u9645\u7684\u533b\u5b66\u6210\u50cf\u5e94\u7528\u3002"}}
{"id": "2509.17941", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17941", "abs": "https://arxiv.org/abs/2509.17941", "authors": ["Zichao Hu", "Chen Tang", "Michael J. Munje", "Yifeng Zhu", "Alex Liu", "Shuijing Liu", "Garrett Warnell", "Peter Stone", "Joydeep Biswas"], "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion", "comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://amrl.cs.utexas.edu/ComposableNav/", "summary": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/", "AI": {"tldr": "\u672c\u6587\u63d0\u51faComposableNav\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u7ec4\u5408\u5b66\u4e60\u5230\u7684\u8fd0\u52a8\u539f\u8bed\u6765\u6ee1\u8db3\u6307\u4ee4\u4e2d\u7684\u591a\u79cd\u89c4\u8303\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e0b\u7684\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6307\u4ee4\u4e2d\u591a\u79cd\u89c4\u8303\u7ec4\u5408\u5e26\u6765\u7684\u6307\u6570\u7ea7\u590d\u6742\u6027\uff0c\u4e14\u9700\u8981\u5927\u91cf\u9488\u5bf9\u6bcf\u4e2a\u8fd0\u52a8\u539f\u8bed\u7684\u6f14\u793a\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53ef\u7ec4\u5408\u7684\u65b9\u5f0f\u5efa\u6a21\u6307\u4ee4\u4e3a\u591a\u4e2a\u72ec\u7acb\u8fd0\u52a8\u539f\u8bed\u7684\u7ec4\u5408\uff1b\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5206\u522b\u5b66\u4e60\u5404\u539f\u8bed\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u76d1\u7763\u9884\u8bad\u7ec3\u4e00\u4e2a\u57fa\u7840\u5bfc\u822a\u6a21\u578b\uff0c\u518d\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u51fa\u4e0d\u540c\u8fd0\u52a8\u539f\u8bed\uff1b\u90e8\u7f72\u65f6\u5e76\u884c\u7ec4\u5408\u8fd9\u4e9b\u539f\u8bed\u4ee5\u6ee1\u8db3\u65b0\u89c4\u8303\u7ec4\u5408\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cComposableNav\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u975e\u7ec4\u5408\u7b56\u7565\u548c\u6210\u672c\u56fe\u7ec4\u5408\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u751f\u6210\u6ee1\u8db3\u591a\u6837\u4e14\u672a\u89c1\u89c4\u8303\u7ec4\u5408\u7684\u8f68\u8ff9\u3002", "conclusion": "ComposableNav\u901a\u8fc7\u89e3\u8026\u548c\u7ec4\u5408\u8fd0\u52a8\u539f\u8bed\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u3001\u672a\u89c1\u6307\u4ee4\u7684\u9ad8\u6548\u6cdb\u5316\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u6307\u4ee4\u9a71\u52a8\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17196", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17196", "abs": "https://arxiv.org/abs/2509.17196", "authors": ["Xuyang Ge", "Wentao Shu", "Jiaxing Wu", "Yunhua Zhou", "Zhengfu He", "Xipeng Qiu"], "title": "Evolution of Concepts in Language Model Pre-Training", "comment": "30 pages, 25 figures", "summary": "Language models obtain extensive capabilities through pre-training. However,\nthe pre-training process remains a black box. In this work, we track linear\ninterpretable feature evolution across pre-training snapshots using a sparse\ndictionary learning method called crosscoders. We find that most features begin\nto form around a specific point, while more complex patterns emerge in later\ntraining stages. Feature attribution analyses reveal causal connections between\nfeature evolution and downstream performance. Our feature-level observations\nare highly consistent with previous findings on Transformer's two-stage\nlearning process, which we term a statistical learning phase and a feature\nlearning phase. Our work opens up the possibility to track fine-grained\nrepresentation progress during language model learning dynamics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8de8\u7f16\u7801\u5668\uff08crosscoders\uff09\u65b9\u6cd5\u8ffd\u8e2a\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ebf\u6027\u53ef\u89e3\u91ca\u7279\u5f81\u6f14\u5316\uff0c\u63ed\u793a\u4e86\u7279\u5f81\u5f62\u6210\u7684\u4e24\u4e2a\u9636\u6bb5\uff1a\u7edf\u8ba1\u5b66\u4e60\u9636\u6bb5\u548c\u7279\u5f81\u5b66\u4e60\u9636\u6bb5\uff0c\u5e76\u53d1\u73b0\u7279\u5f81\u6f14\u5316\u4e0e\u4e0b\u6e38\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u56e0\u679c\u5173\u7cfb\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u52a8\u6001\uff0c\u63ed\u5f00\u9884\u8bad\u7ec3\u4f5c\u4e3a\u201c\u9ed1\u7bb1\u201d\u7684\u673a\u5236\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\uff08crosscoders\uff09\u5206\u6790\u9884\u8bad\u7ec3\u4e0d\u540c\u9636\u6bb5\u7684\u5feb\u7167\uff0c\u8ffd\u8e2a\u7ebf\u6027\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u6f14\u5316\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0\u5927\u591a\u6570\u7279\u5f81\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u5f00\u59cb\u5f62\u6210\uff0c\u590d\u6742\u6a21\u5f0f\u5728\u540e\u671f\u51fa\u73b0\uff1b\u7279\u5f81\u5f52\u56e0\u5206\u6790\u663e\u793a\u7279\u5f81\u6f14\u5316\u4e0e\u4e0b\u6e38\u6027\u80fd\u6709\u56e0\u679c\u8054\u7cfb\uff1b\u7ed3\u679c\u4e0eTransformer\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\u4e00\u81f4\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u8ffd\u8e2a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8868\u793a\u8fdb\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e24\u9636\u6bb5\u5b66\u4e60\u5047\u8bf4\u3002"}}
{"id": "2509.17180", "categories": ["cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.17180", "abs": "https://arxiv.org/abs/2509.17180", "authors": ["David Arbour", "Harsh Parikh", "Bijan Niknam", "Elizabeth Stuart", "Kara Rudolph", "Avi Feller"], "title": "Regularizing Extrapolation in Causal Inference", "comment": null, "summary": "Many common estimators in machine learning and causal inference are linear\nsmoothers, where the prediction is a weighted average of the training outcomes.\nSome estimators, such as ordinary least squares and kernel ridge regression,\nallow for arbitrarily negative weights, which improve feature imbalance but\noften at the cost of increased dependence on parametric modeling assumptions\nand higher variance. By contrast, estimators like importance weighting and\nrandom forests (sometimes implicitly) restrict weights to be non-negative,\nreducing dependence on parametric modeling and variance at the cost of worse\nimbalance. In this paper, we propose a unified framework that directly\npenalizes the level of extrapolation, replacing the current practice of a hard\nnon-negativity constraint with a soft constraint and corresponding\nhyperparameter. We derive a worst-case extrapolation error bound and introduce\na novel \"bias-bias-variance\" tradeoff, encompassing biases due to feature\nimbalance, model misspecification, and estimator variance; this tradeoff is\nespecially pronounced in high dimensions, particularly when positivity is poor.\nWe then develop an optimization procedure that regularizes this bound while\nminimizing imbalance and outline how to use this approach as a sensitivity\nanalysis for dependence on parametric modeling assumptions. We demonstrate the\neffectiveness of our approach through synthetic experiments and a real-world\napplication, involving the generalization of randomized controlled trial\nestimates to a target population of interest.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u7ea6\u675f\u800c\u975e\u786c\u6027\u975e\u8d1f\u7ea6\u675f\u6765\u76f4\u63a5\u60e9\u7f5a\u5916\u63a8\u6c34\u5e73\uff0c\u4ee5\u5e73\u8861\u7279\u5f81\u4e0d\u5e73\u8861\u3001\u6a21\u578b\u8bef\u8bbe\u548c\u4f30\u8ba1\u65b9\u5dee\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u6b63\u5b9a\u6027\u5dee\u7684\u60c5\u5f62\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u5e73\u6ed1\u4f30\u8ba1\u5668\u5728\u5904\u7406\u7279\u5f81\u4e0d\u5e73\u8861\u4e0e\u6a21\u578b\u5047\u8bbe\u4f9d\u8d56\u53ca\u65b9\u5dee\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u6539\u8fdb\u5bf9\u53c2\u6570\u6a21\u578b\u5047\u8bbe\u7684\u4f9d\u8d56\u5e76\u964d\u4f4e\u65b9\u5dee\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f6f\u7ea6\u675f\u548c\u8d85\u53c2\u6570\u6765\u76f4\u63a5\u60e9\u7f5a\u5916\u63a8\u7a0b\u5ea6\uff0c\u5e76\u63a8\u5bfc\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u5916\u63a8\u8bef\u5dee\u754c\uff0c\u5f62\u6210\u2018\u504f\u5dee-\u504f\u5dee-\u65b9\u5dee\u2019\u6743\u8861\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u5408\u6210\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u5bf9\u5916\u63a8\u7684\u4f9d\u8d56\uff0c\u5728\u63a8\u5e7f\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u7ed3\u679c\u5230\u76ee\u6807\u4eba\u7fa4\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u654f\u611f\u6027\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u5728\u964d\u4f4e\u6a21\u578b\u5047\u8bbe\u4f9d\u8d56\u7684\u540c\u65f6\u63a7\u5236\u7279\u5f81\u4e0d\u5e73\u8861\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u975e\u8d1f\u6743\u91cd\u9650\u5236\u65b9\u6cd5\u3002"}}
{"id": "2509.16822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16822", "abs": "https://arxiv.org/abs/2509.16822", "authors": ["Townim Faisal Chowdhury", "Vu Minh Hieu Phan", "Kewen Liao", "Nanyu Dong", "Minh-Son To", "Anton Hengel", "Johan Verjans", "Zhibin Liao"], "title": "Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models", "comment": "Accepted at IEEE/CVF International Conference on Computer Vision\n  (ICCV), 2025", "summary": "Counterfactual explanations (CFE) for deep image classifiers aim to reveal\nhow minimal input changes lead to different model decisions, providing critical\ninsights for model interpretation and improvement. However, existing CFE\nmethods often rely on additional image encoders and generative models to create\nplausible images, neglecting the classifier's own feature space and decision\nboundaries. As such, they do not explain the intrinsic feature space and\ndecision boundaries learned by the classifier. To address this limitation, we\npropose Mirror-CFE, a novel method that generates faithful counterfactual\nexplanations by operating directly in the classifier's feature space, treating\ndecision boundaries as mirrors that ``reflect'' feature representations in the\nmirror. Mirror-CFE learns a mapping function from feature space to image space\nwhile preserving distance relationships, enabling smooth transitions between\nsource images and their counterfactuals. Through extensive experiments on four\nimage datasets, we demonstrate that Mirror-CFE achieves superior performance in\nvalidity while maintaining input resemblance compared to state-of-the-art\nexplanation methods. Finally, mirror-CFE provides interpretable visualization\nof the classifier's decision process by generating step-wise transitions that\nreveal how features evolve as classification confidence changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMirror-CFE\u7684\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u5206\u7c7b\u5668\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u751f\u6210\u53ef\u4fe1\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u901a\u8fc7\u5c06\u51b3\u7b56\u8fb9\u754c\u89c6\u4e3a\u201c\u955c\u5b50\u201d\u6765\u53cd\u6620\u7279\u5f81\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u8f93\u5165\u76f8\u4f3c\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5206\u7c7b\u5668\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89c6\u5316\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u989d\u5916\u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u751f\u6210\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u5206\u7c7b\u5668\u81ea\u8eab\u7684\u7279\u5f81\u7a7a\u95f4\u548c\u51b3\u7b56\u8fb9\u754c\uff0c\u65e0\u6cd5\u89e3\u91ca\u5176\u5185\u5728\u7279\u6027\u3002", "method": "Mirror-CFE\u5728\u5206\u7c7b\u5668\u7684\u7279\u5f81\u7a7a\u95f4\u5185\u64cd\u4f5c\uff0c\u5c06\u51b3\u7b56\u8fb9\u754c\u89c6\u4e3a\u53cd\u5c04\u955c\u9762\uff0c\u5b66\u4e60\u4ece\u7279\u5f81\u7a7a\u95f4\u5230\u56fe\u50cf\u7a7a\u95f4\u7684\u6620\u5c04\u51fd\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u8ddd\u79bb\u5173\u7cfb\uff0c\u5b9e\u73b0\u6e90\u56fe\u50cf\u4e0e\u53cd\u4e8b\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u5728\u56db\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMirror-CFE\u5728\u6709\u6548\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u66f4\u597d\u7684\u8f93\u5165\u76f8\u4f3c\u6027\uff0c\u5e76\u80fd\u751f\u6210\u9010\u6b65\u8fc7\u6e21\u7684\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u63ed\u793a\u5206\u7c7b\u7f6e\u4fe1\u5ea6\u53d8\u5316\u65f6\u7279\u5f81\u7684\u6f14\u5316\u8fc7\u7a0b\u3002", "conclusion": "Mirror-CFE\u80fd\u591f\u66f4\u5fe0\u5b9e\u5730\u89e3\u91ca\u6df1\u5ea6\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u51b3\u7b56\u673a\u5236\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u5f3a\u3001\u89c6\u89c9\u8fde\u8d2f\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u7406\u89e3\u548c\u6539\u8fdb\u3002"}}
{"id": "2509.17952", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.17952", "abs": "https://arxiv.org/abs/2509.17952", "authors": ["Mahdi Nobar", "J\u00fcrg Keller", "Alessandro Forino", "John Lygeros", "Alisa Rupenyan"], "title": "Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller Tuning with Digital Twins", "comment": "This preprint is intended for submission to IEEE Robotics and\n  Automation Letters (RA-L)", "summary": "We propose a \\textit{guided multi-fidelity Bayesian optimization} framework\nfor data-efficient controller tuning that integrates corrected digital twin\n(DT) simulations with real-world measurements. The method targets closed-loop\nsystems with limited-fidelity simulations or inexpensive approximations. To\naddress model mismatch, we build a multi-fidelity surrogate with a learned\ncorrection model that refines DT estimates from real data. An adaptive\ncost-aware acquisition function balances expected improvement, fidelity, and\nsampling cost. Our method ensures adaptability as new measurements arrive. The\naccuracy of DTs is re-estimated, dynamically adapting both cross-source\ncorrelations and the acquisition function. This ensures that accurate DTs are\nused more frequently, while inaccurate DTs are appropriately downweighted.\nExperiments on robotic drive hardware and supporting numerical studies\ndemonstrate that our method enhances tuning efficiency compared to standard\nBayesian optimization (BO) and multi-fidelity methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f15\u5bfc\u5f0f\u591a\u4fdd\u771f\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6570\u636e\u9ad8\u6548\u7684\u63a7\u5236\u5668\u8c03\u4f18\uff0c\u7ed3\u5408\u4e86\u6821\u6b63\u540e\u7684\u6570\u5b57\u5b6a\u751f\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u6d4b\u91cf\uff0c\u63d0\u5347\u4e86\u8c03\u4f18\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u95ed\u73af\u7cfb\u7edf\u4e2d\u4eff\u771f\u6a21\u578b\u7cbe\u5ea6\u6709\u9650\u6216\u8fd1\u4f3c\u6a21\u578b\u6210\u672c\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u6709\u6548\u7ed3\u5408\u591a\u6e90\u6570\u636e\u4ee5\u63d0\u5347\u63a7\u5236\u5668\u8c03\u4f18\u7684\u6570\u636e\u6548\u7387\u3002", "method": "\u6784\u5efa\u5e26\u6709\u5b66\u4e60\u6821\u6b63\u6a21\u578b\u7684\u591a\u4fdd\u771f\u4ee3\u7406\u6a21\u578b\uff0c\u4fee\u6b63\u6570\u5b57\u5b6a\u751f\u4f30\u8ba1\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u6210\u672c\u611f\u77e5\u83b7\u53d6\u51fd\u6570\u5e73\u8861\u6539\u8fdb\u671f\u671b\u3001\u4fdd\u771f\u5ea6\u548c\u91c7\u6837\u6210\u672c\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u8de8\u6e90\u76f8\u5173\u6027\u548c\u83b7\u53d6\u7b56\u7565\u3002", "result": "\u5728\u673a\u5668\u4eba\u9a71\u52a8\u786c\u4ef6\u548c\u6570\u503c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u591a\u4fdd\u771f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8c03\u4f18\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u878d\u5408\u771f\u5b9e\u6570\u636e\u4e0e\u6821\u6b63\u540e\u7684\u6570\u5b57\u5b6a\u751f\u4eff\u771f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u63a7\u5236\u5668\u8c03\u4f18\uff0c\u9002\u7528\u4e8e\u6a21\u578b\u4e0d\u7cbe\u786e\u4e14\u6570\u636e\u53d7\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2509.17209", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17209", "abs": "https://arxiv.org/abs/2509.17209", "authors": ["Lourdes Moreno", "Jesus M. Sanchez-Gomez", "Marco Antonio Sanchez-Escudero", "Paloma Mart\u00ednez"], "title": "Prompt-Based Simplification for Plain Language using Spanish Language Models", "comment": "11 pages, 7 tables,", "summary": "This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask\n1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies\nbased on models trained on Spanish texts, including a zero-shot configuration\nusing prompt engineering and a fine-tuned version with Low-Rank Adaptation\n(LoRA). Different strategies were evaluated on representative internal subsets\nof the training data, using the official task metrics, cosine similarity (SIM)\nand the Fern\\'andez-Huerta readability index (FH) to guide the selection of the\noptimal model and prompt combination. The final system was selected for its\nbalanced and consistent performance, combining normalization steps, the\nRigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in\nsemantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).\nWe also discuss key challenges related to training data heterogeneity and the\nlimitations of current evaluation metrics in capturing both linguistic clarity\nand content preservation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HULAT-UC3M\u56e2\u961f\u5728CLEARS 2025\u5b50\u4efb\u52a11\u4e2d\u7684\u53c2\u4e0e\uff0c\u65e8\u5728\u5c06\u897f\u73ed\u7259\u8bed\u6587\u672c\u9002\u5e94\u4e3a\u7b80\u6613\u8bed\u8a00\uff08PL\uff09\u3002\u56e2\u961f\u63a2\u7d22\u4e86\u57fa\u4e8e\u897f\u73ed\u7259\u8bed\u6587\u672c\u8bad\u7ec3\u6a21\u578b\u7684\u7b56\u7565\uff0c\u5305\u62ec\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u7684\u96f6\u6837\u672c\u914d\u7f6e\u548c\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5fae\u8c03\u7684\u7248\u672c\u3002\u901a\u8fc7\u5185\u90e8\u6570\u636e\u5b50\u96c6\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u5b98\u65b9\u6307\u6807\uff08\u4f59\u5f26\u76f8\u4f3c\u5ea6SIM\u548cFern\u00e1ndez-Huerta\u53ef\u8bfb\u6027\u6307\u6570FH\uff09\u9009\u62e9\u6700\u4f18\u6a21\u578b\u4e0e\u63d0\u793a\u7ec4\u5408\u3002\u6700\u7ec8\u7cfb\u7edf\u7ed3\u5408\u5f52\u4e00\u5316\u6b65\u9aa4\u3001RigoChat-7B-v2\u6a21\u578b\u53ca\u9762\u5411PL\u7684\u63d0\u793a\uff0c\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4e0a\u6392\u540d\u7b2c\u4e00\uff08SIM=0.75\uff09\uff0c\u4f46\u5728\u53ef\u8bfb\u6027\u4e0a\u6392\u540d\u7b2c\u56db\uff08FH=69.72\uff09\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u8bad\u7ec3\u6570\u636e\u5f02\u8d28\u6027\u548c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5728\u8bed\u8a00\u6e05\u6670\u5ea6\u4e0e\u5185\u5bb9\u4fdd\u7559\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63d0\u5347\u897f\u73ed\u7259\u8bed\u590d\u6742\u6587\u672c\u5411\u7b80\u6613\u8bed\u8a00\u8f6c\u6362\u7684\u8d28\u91cf\uff0c\u4ee5\u5e2e\u52a9\u9605\u8bfb\u969c\u788d\u8005\u6216\u975e\u6bcd\u8bed\u8005\u66f4\u597d\u5730\u7406\u89e3\u5185\u5bb9\uff0c\u540c\u65f6\u63a2\u7d22\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6a21\u578b\u7b56\u7565\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u63d0\u793a\u5de5\u7a0b\u548c\u57fa\u4e8eLoRA\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528RigoChat-7B-v2\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u5f52\u4e00\u5316\u9884\u5904\u7406\u548c\u4e13\u7528PL\u63d0\u793a\u8bbe\u8ba1\uff1b\u5728\u5185\u90e8\u9a8c\u8bc1\u96c6\u4e0a\u4f7f\u7528SIM\u548cFH\u6307\u6807\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u3002", "result": "\u6700\u7ec8\u7cfb\u7edf\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff08SIM=0.75\uff0c\u6392\u540d\u7b2c\u4e00\uff09\uff0c\u4f46\u5728\u53ef\u8bfb\u6027\u65b9\u9762\u4ec5\u6392\u7b2c\u56db\uff08FH=69.72\uff09\uff1b\u7cfb\u7edf\u5c55\u73b0\u51fa\u826f\u597d\u7684\u5e73\u8861\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u548cLoRA\u5fae\u8c03\u7684\u7b56\u7565\u5728\u897f\u73ed\u7259\u8bed\u7b80\u5316\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u4fdd\u6301\u65b9\u9762\u4f18\u52bf\u660e\u663e\uff0c\u4f46\u73b0\u6709\u53ef\u8bfb\u6027\u6307\u6807\u4ecd\u6709\u5c40\u9650\uff0c\u672a\u6765\u9700\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u5e76\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\u3002"}}
{"id": "2509.17182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17182", "abs": "https://arxiv.org/abs/2509.17182", "authors": ["Sam Jacob Jacob", "Markus Mrosek", "Carsten Othmer", "Harald K\u00f6stler"], "title": "PMRT: A Training Recipe for Fast, 3D High-Resolution Aerodynamic Prediction", "comment": null, "summary": "The aerodynamic optimization of cars requires close collaboration between\naerodynamicists and stylists, while slow, expensive simulations remain a\nbottleneck. Surrogate models have been shown to accurately predict aerodynamics\nwithin the design space for which they were trained. However, many of these\nmodels struggle to scale to higher resolutions because of the 3D nature of the\nproblem and data scarcity. We propose Progressive Multi-Resolution Training\n(PMRT), a probabilistic multi-resolution training schedule that enables\ntraining a U-Net to predict the drag coefficient ($c_d$) and high-resolution\nvelocity fields (512 x 128 x 128) in 24 hours on a single NVIDIA H100 GPU, 7x\ncheaper than the high-resolution-only baseline, with similar accuracy. PMRT\nsamples batches from three resolutions based on probabilities that change\nduring training, starting with an emphasis on lower resolutions and gradually\nshifting toward higher resolutions. Since this is a training methodology, it\ncan be adapted to other high-resolution-focused backbones. We also show that a\nsingle model can be trained across five datasets from different solvers,\nincluding a real-world dataset, by conditioning on the simulation parameters.\nIn the DrivAerML dataset, our models achieve a $c_d$ $R^2$ of 0.975, matching\nliterature baselines at a fraction of the training cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u65b9\u6cd5\uff08PMRT\uff09\uff0c\u53ef\u5728\u5355\u4e2aGPU\u4e0a\u9ad8\u6548\u8bad\u7ec3U-Net\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u6c7d\u8f66\u6c14\u52a8\u6027\u80fd\u9884\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u6c7d\u8f66\u6c14\u52a8\u4f18\u5316\u4f9d\u8d56\u6602\u8d35\u4e14\u8017\u65f6\u7684\u4eff\u771f\uff0c\u73b0\u6709\u4ee3\u7406\u6a21\u578b\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u5206\u8fa8\u7387\uff0c\u53d7\u9650\u4e8e\u4e09\u7ef4\u95ee\u9898\u590d\u6742\u6027\u548c\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\uff08PMRT\uff09\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u4ece\u4f4e\u3001\u4e2d\u3001\u9ad8\u4e09\u79cd\u5206\u8fa8\u7387\u91c7\u6837\u7684\u6982\u7387\uff0c\u9010\u6b65\u8fc7\u6e21\u5230\u9ad8\u5206\u8fa8\u7387\uff1b\u91c7\u7528U-Net\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u8f93\u5165\u652f\u6301\u591a\u6570\u636e\u96c6\uff08\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff09\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u572824\u5c0f\u65f6\u5185\u4e8e\u5355\u4e2aNVIDIA H100 GPU\u4e0a\u5b8c\u6210512x128x128\u9ad8\u5206\u8fa8\u7387\u901f\u5ea6\u573a\u548c\u963b\u529b\u7cfb\u6570\uff08c_d\uff09\u9884\u6d4b\u8bad\u7ec3\uff0c\u6210\u672c\u4ec5\u4e3a\u9ad8\u5206\u8fa8\u7387\u57fa\u7ebf\u76841/7\uff1b\u5728DrivAerML\u6570\u636e\u96c6\u4e0ac_d\u7684R\u00b2\u8fbe\u52300.975\uff0c\u6027\u80fd\u4e0e\u6587\u732e\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "PMRT\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u9ad8\u5206\u8fa8\u7387\u6c14\u52a8\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u6c7d\u8f66\u8bbe\u8ba1\u4e2d\u7684\u5feb\u901f\u6c14\u52a8\u8bc4\u4f30\u3002"}}
{"id": "2509.16832", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.16832", "abs": "https://arxiv.org/abs/2509.16832", "authors": ["Ziyang Xu", "Benedikt Schwab", "Yihui Yang", "Thomas H. Kolbe", "Christoph Holst"], "title": "L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models", "comment": "submit to ISPRS Journal of Photogrammetry and Remote Sensing", "summary": "Accurate registration between LiDAR (Light Detection and Ranging) point\nclouds and semantic 3D city models is a fundamental topic in urban digital\ntwinning and a prerequisite for downstream tasks, such as digital construction,\nchange detection and model refinement. However, achieving accurate\nLiDAR-to-Model registration at individual building level remains challenging,\nparticularly due to the generalization uncertainty in semantic 3D city models\nat the Level of Detail 2 (LoD2). This paper addresses this gap by proposing\nL2M-Reg, a plane-based fine registration method that explicitly accounts for\nmodel uncertainty. L2M-Reg consists of three key steps: establishing reliable\nplane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,\nand adaptively estimating vertical translation. Experiments on three real-world\ndatasets demonstrate that L2M-Reg is both more accurate and computationally\nefficient than existing ICP-based and plane-based methods. Overall, L2M-Reg\nprovides a novel building-level solution regarding LiDAR-to-Model registration\nwhen model uncertainty is present.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u9762\u7684\u7cbe\u7ec6\u5316\u914d\u51c6\u65b9\u6cd5L2M-Reg\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u4e493D\u57ce\u5e02\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684LiDAR\u70b9\u4e91\u4e0e\u6a21\u578b\u4e4b\u95f4\u7684\u7cbe\u786e\u914d\u51c6\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eLoD2\u7ea7\u522b\u8bed\u4e493D\u57ce\u5e02\u6a21\u578b\u5b58\u5728\u6cdb\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e2a\u4f53\u5efa\u7b51\u7269\u5c42\u9762\u7684\u7cbe\u786eLiDAR\u5230\u6a21\u578b\u914d\u51c6\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "L2M-Reg\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u5efa\u7acb\u53ef\u9760\u7684\u5e73\u9762\u5bf9\u5e94\u5173\u7cfb\u3001\u6784\u5efa\u4f2a\u5e73\u9762\u7ea6\u675f\u7684Gauss-Helmert\u6a21\u578b\u3001\u81ea\u9002\u5e94\u4f30\u8ba1\u5782\u76f4\u5e73\u79fb\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cL2M-Reg\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684ICP\u548c\u5e73\u9762\u914d\u51c6\u65b9\u6cd5\u3002", "conclusion": "L2M-Reg\u4e3a\u5b58\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u5efa\u7b51\u7269\u7ea7LiDAR\u5230\u6a21\u578b\u914d\u51c6\u63d0\u4f9b\u4e86\u65b0\u9896\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18005", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18005", "abs": "https://arxiv.org/abs/2509.18005", "authors": ["Yanxin Zhang", "Liang He", "Zeyi Kang", "Zuheng Ming", "Kaixing Zhao"], "title": "M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer", "comment": "8 pages", "summary": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u5b66\u4e60\u6a21\u578bM3ET\uff0c\u7ed3\u5408Mamba\u6a21\u5757\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7279\u5f81\u878d\u5408\u4e0e\u6a21\u6001\u91cd\u5efa\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u673a\u5668\u4eba\u73af\u5883\u4e2d\u96be\u4ee5\u5145\u5206\u5229\u7528\u6587\u672c\u6a21\u6001\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u591a\u6a21\u6001\u5b66\u4e60\u5e94\u7528\u3002", "method": "\u5f15\u5165Mamba\u6a21\u5757\u548c\u8bed\u4e49\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u6a21\u578bM3ET\uff0c\u4f18\u5316\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u3001\u5bf9\u9f50\u4e0e\u6a21\u6001\u91cd\u5efa\u3002", "result": "M3ET\u5728\u9884\u8bad\u7ec3\u63a8\u7406\u901f\u5ea6\u4e0a\u63d0\u53472.3\u500d\uff0c\u53c2\u6570\u51cf\u5c1167%\uff0cVQA\u4efb\u52a1\u51c6\u786e\u7387\u8fbe0.74\uff0c\u4f46EQA\u4efb\u52a1\u8868\u73b0\u6709\u9650\u3002", "conclusion": "M3ET\u5728\u4fdd\u6301\u826f\u597d\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u5408\u79fb\u52a8\u548c\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u90e8\u7f72\u3002"}}
{"id": "2509.17249", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17249", "abs": "https://arxiv.org/abs/2509.17249", "authors": ["Kuang-Da Wang", "Shuoyang Ding", "Chao-Han Huck Yang", "Ping-Chun Hsieh", "Wen-Chih Peng", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Extending Automatic Machine Translation Evaluation to Book-Length Documents", "comment": "Accepted for EMNLP 2025 main conference", "summary": "Despite Large Language Models (LLMs) demonstrating superior translation\nperformance and long-context capabilities, evaluation methodologies remain\nconstrained to sentence-level assessment due to dataset limitations, token\nnumber restrictions in metrics, and rigid sentence boundary requirements. We\nintroduce SEGALE, an evaluation scheme that extends existing automatic metrics\nto long-document translation by treating documents as continuous text and\napplying sentence segmentation and alignment methods. Our approach enables\npreviously unattainable document-level evaluation, handling translations of\narbitrary length generated with document-level prompts while accounting for\nunder-/over-translations and varied sentence boundaries. Experiments show our\nscheme significantly outperforms existing long-form document evaluation\nschemes, while being comparable to evaluations performed with groundtruth\nsentence alignments. Additionally, we apply our scheme to book-length texts and\nnewly demonstrate that many open-weight LLMs fail to effectively translate\ndocuments at their reported maximum context lengths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SEGALE\uff0c\u4e00\u79cd\u7528\u4e8e\u957f\u6587\u6863\u7ffb\u8bd1\u7684\u8bc4\u4f30\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u6587\u6863\u89c6\u4e3a\u8fde\u7eed\u6587\u672c\u5e76\u7ed3\u5408\u53e5\u5b50\u5206\u5272\u4e0e\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u957f\u5ea6\u7ffb\u8bd1\u7ed3\u679c\u7684\u6587\u6863\u7ea7\u8bc4\u4f30\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u7ffb\u8bd1\u8bc4\u4f30\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u3001\u6307\u6807\u7684token\u9650\u5236\u548c\u4e25\u683c\u7684\u53e5\u5b50\u8fb9\u754c\u8981\u6c42\uff0c\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u957f\u6587\u6863\u7ffb\u8bd1\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6587\u6863\u7ea7\u7ffb\u8bd1\u4e14\u9002\u5e94\u4e0d\u540c\u53e5\u5b50\u8fb9\u754c\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "method": "\u63d0\u51faSEGALE\u8bc4\u4f30\u65b9\u6848\uff0c\u5c06\u6587\u6863\u89c6\u4e3a\u8fde\u7eed\u6587\u672c\uff0c\u91c7\u7528\u53e5\u5b50\u5206\u5272\u4e0e\u5bf9\u9f50\u6280\u672f\uff0c\u6269\u5c55\u4f20\u7edf\u81ea\u52a8\u6307\u6807\uff08\u5982BLEU\u3001COMET\uff09\u81f3\u6587\u6863\u7ea7\u522b\uff0c\u652f\u6301\u4efb\u610f\u957f\u5ea6\u8f93\u5165\uff0c\u5e76\u80fd\u5904\u7406\u6f0f\u8bd1\u3001\u591a\u8bd1\u548c\u53e5\u5b50\u8fb9\u754c\u53d8\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEGALE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u957f\u6587\u6863\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u771f\u5b9e\u53e5\u5b50\u5bf9\u9f50\u7684\u8bc4\u4f30\u7ed3\u679c\uff1b\u5e94\u7528\u4e8e\u4e66\u7c4d\u957f\u5ea6\u6587\u672c\u65f6\u53d1\u73b0\uff0c\u8bb8\u591a\u5f00\u6e90\u5927\u6a21\u578b\u5728\u5176\u5ba3\u79f0\u7684\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u65e0\u6cd5\u6709\u6548\u8fdb\u884c\u6587\u6863\u7ffb\u8bd1\u3002", "conclusion": "SEGALE\u4e3a\u957f\u6587\u6863\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u5f0f\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u7ffb\u8bd1\u4e2d\u7684\u5b9e\u9645\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u672a\u6765\u7814\u7a76\u5173\u6ce8\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2509.17186", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17186", "abs": "https://arxiv.org/abs/2509.17186", "authors": ["Dehao Zhang", "Malu Zhang", "Shuai Wang", "Jingya Wang", "Wenjie Wei", "Zeyu Ma", "Guoqing Wang", "Yang Yang", "HaiZhou Li"], "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling", "comment": null, "summary": "The explosive growth in sequence length has intensified the demand for\neffective and efficient long sequence modeling. Benefiting from intrinsic\noscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently\nextract frequency components from input signals and encode them into\nspatiotemporal spike trains, making them well-suited for long sequence\nmodeling. However, RF neurons exhibit limited effective memory capacity and a\ntrade-off between energy efficiency and training speed on complex temporal\ntasks. Inspired by the dendritic structure of biological neurons, we propose a\nDendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a\nmulti-dendritic and soma architecture. Each dendritic branch encodes specific\nfrequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons,\nthereby collectively achieving comprehensive frequency representation.\nFurthermore, we introduce an adaptive threshold mechanism into the soma\nstructure that adjusts the threshold based on historical spiking activity,\nreducing redundant spikes while maintaining training efficiency in long\nsequence tasks. Extensive experiments demonstrate that our method maintains\ncompetitive accuracy while substantially ensuring sparse spikes without\ncompromising computational efficiency during training. These results underscore\nits potential as an effective and efficient solution for long sequence modeling\non edge platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u7a81\u7ed3\u6784\u7684\u5171\u632f-\u653e\u7535\u795e\u7ecf\u5143\u6a21\u578b\uff08D-RF\uff09\uff0c\u901a\u8fc7\u591a\u6811\u7a81\u548c\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\u63d0\u5347\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u8bb0\u5fc6\u80fd\u529b\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5171\u632f-\u653e\u7535\u795e\u7ecf\u5143\u5728\u590d\u6742\u65f6\u5e8f\u4efb\u52a1\u4e2d\u5b58\u5728\u8bb0\u5fc6\u5bb9\u91cf\u6709\u9650\u3001\u80fd\u6548\u4e0e\u8bad\u7ec3\u901f\u5ea6\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u4ee5\u9002\u5e94\u957f\u5e8f\u5217\u5efa\u6a21\u9700\u6c42\u3002", "method": "\u53d7\u751f\u7269\u795e\u7ecf\u5143\u6811\u7a81\u7ed3\u6784\u542f\u53d1\uff0c\u8bbe\u8ba1\u591a\u6811\u7a81\u5206\u652f\u5206\u522b\u7f16\u7801\u7279\u5b9a\u9891\u6bb5\uff0c\u5e76\u5728\u80de\u4f53\u4e2d\u5f15\u5165\u57fa\u4e8e\u5386\u53f2\u8109\u51b2\u6d3b\u52a8\u8c03\u6574\u7684\u81ea\u9002\u5e94\u9608\u503c\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5197\u4f59\u8109\u51b2\uff0c\u8bad\u7ec3\u6548\u7387\u9ad8\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "conclusion": "D-RF\u6a21\u578b\u5728\u4fdd\u8bc1\u7a00\u758f\u8109\u51b2\u548c\u8ba1\u7b97\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\u3002"}}
{"id": "2509.16853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16853", "abs": "https://arxiv.org/abs/2509.16853", "authors": ["Jinhao Wang", "Cihan Ruan", "Nam Ling", "Wei Wang", "Wei Jiang"], "title": "ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression", "comment": null, "summary": "Prior studies in learned image compression (LIC) consistently show that only\na small subset of latent channels is critical for reconstruction, while many\nothers carry limited information. Exploiting this imbalance could improve both\ncoding and computational efficiency, yet existing approaches often rely on\ncostly, dataset-specific ablation tests and typically analyze channels in\nisolation, ignoring their interdependencies.\n  We propose a generalizable, dataset-agnostic method to identify and organize\nimportant channels in pretrained VAE-based LIC models. Instead of brute-force\nempirical evaluations, our approach leverages intrinsic parameter\nstatistics-weight variances, bias magnitudes, and pairwise correlations-to\nestimate channel importance. This analysis reveals a consistent organizational\nstructure, termed the Invariant Salient Channel Space (ISCS), where\nSalient-Core channels capture dominant structures and Salient-Auxiliary\nchannels provide complementary details. Building on ISCS, we introduce a\ndeterministic channel ordering and grouping strategy that enables\nslice-parallel decoding, reduces redundancy, and improves bitrate efficiency.\n  Experiments across multiple LIC architectures demonstrate that our method\neffectively reduces bitrate and computation while maintaining reconstruction\nquality, providing a practical and modular enhancement to existing learned\ncompression frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u3001\u6570\u636e\u96c6\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u53c2\u6570\u7edf\u8ba1\u8bc6\u522b\u9884\u8bad\u7ec3VAE-based\u56fe\u50cf\u538b\u7f29\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6f5c\u5728\u901a\u9053\uff0c\u5e76\u53d1\u73b0\u4e86\u4e00\u81f4\u7684\u7ec4\u7ec7\u7ed3\u6784\uff08ISCS\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u4e0e\u89e3\u7801\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u636e\u96c6\u7279\u5b9a\u6d88\u878d\u5b9e\u9a8c\uff0c\u4e14\u5ffd\u7565\u901a\u9053\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u96be\u4ee5\u9ad8\u6548\u8bc6\u522b\u5bf9\u91cd\u5efa\u5173\u952e\u7684\u6f5c\u5728\u901a\u9053\u3002", "method": "\u5229\u7528\u6743\u91cd\u65b9\u5dee\u3001\u504f\u7f6e\u5927\u5c0f\u548c\u6210\u5bf9\u76f8\u5173\u6027\u7b49\u5185\u5728\u53c2\u6570\u7edf\u8ba1\u6765\u4f30\u8ba1\u901a\u9053\u91cd\u8981\u6027\uff0c\u6784\u5efa\u4e0d\u53d8\u663e\u8457\u901a\u9053\u7a7a\u95f4\uff08ISCS\uff09\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u786e\u5b9a\u6027\u7684\u901a\u9053\u6392\u5e8f\u4e0e\u5206\u7ec4\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u5b66\u4e60\u578b\u56fe\u50cf\u538b\u7f29\u67b6\u6784\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u6bd4\u7279\u7387\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u6709\u5b66\u4e60\u578b\u538b\u7f29\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6a21\u5757\u5316\u7684\u589e\u5f3a\u65b9\u5f0f\uff0c\u63d0\u5347\u4e86\u7f16\u7801\u6548\u7387\u548c\u89e3\u7801\u5e76\u884c\u6027\u3002"}}
{"id": "2509.18043", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18043", "abs": "https://arxiv.org/abs/2509.18043", "authors": ["Yinlong Dai", "Andre Keyser", "Dylan P. Losey"], "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States", "comment": null, "summary": "Imitation learning (IL) has proven effective across a wide range of\nmanipulation tasks. However, IL policies often struggle when faced with\nout-of-distribution observations; for instance, when the target object is in a\npreviously unseen position or occluded by other objects. In these cases,\nextensive demonstrations are needed for current IL methods to reach robust and\ngeneralizable behaviors. But when humans are faced with these sorts of atypical\ninitial states, we often rearrange the environment for more favorable task\nexecution. For example, a person might rotate a coffee cup so that it is easier\nto grasp the handle, or push a box out of the way so they can directly grasp\ntheir target object. In this work we seek to equip robot learners with the same\ncapability: enabling robots to prepare the environment before executing their\ngiven policy. We propose ReSET, an algorithm that takes initial states -- which\nare outside the policy's distribution -- and autonomously modifies object poses\nso that the restructured scene is similar to training data. Theoretically, we\nshow that this two step process (rearranging the environment before rolling out\nthe given policy) reduces the generalization gap. Practically, our ReSET\nalgorithm combines action-agnostic human videos with task-agnostic\nteleoperation data to i) decide when to modify the scene, ii) predict what\nsimplifying actions a human would take, and iii) map those predictions into\nrobot action primitives. Comparisons with diffusion policies, VLAs, and other\nbaselines show that using ReSET to prepare the environment enables more robust\ntask execution with equal amounts of total training data. See videos at our\nproject website: https://reset2025paper.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReSET\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u6267\u884c\u7b56\u7565\u524d\u81ea\u4e3b\u91cd\u6784\u73af\u5883\uff08\u5982\u8c03\u6574\u7269\u4f53\u4f4d\u59ff\uff09\uff0c\u4f7f\u521d\u59cb\u72b6\u6001\u66f4\u63a5\u8fd1\u8bad\u7ec3\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u9762\u5bf9\u5206\u5e03\u5916\u89c2\u6d4b\uff08\u5982\u7269\u4f53\u4f4d\u7f6e\u53d8\u5316\u6216\u906e\u6321\uff09\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u4eba\u7c7b\u4f1a\u4e3b\u52a8\u8c03\u6574\u73af\u5883\u4ee5\u5229\u4e8e\u4efb\u52a1\u6267\u884c\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u8d4b\u4e88\u673a\u5668\u4eba\u7c7b\u4f3c\u7684\u80fd\u529b\u3002", "method": "ReSET\u7ed3\u5408\u52a8\u4f5c\u65e0\u5173\u7684\u4eba\u7c7b\u89c6\u9891\u548c\u4efb\u52a1\u65e0\u5173\u7684\u9065\u64cd\u4f5c\u6570\u636e\uff0c\u5224\u65ad\u4f55\u65f6\u9700\u8981\u8c03\u6574\u573a\u666f\u3001\u9884\u6d4b\u4eba\u7c7b\u53ef\u80fd\u91c7\u53d6\u7684\u7b80\u5316\u52a8\u4f5c\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\u57fa\u5143\uff0c\u5728\u6267\u884c\u539f\u7b56\u7565\u524d\u5bf9\u73af\u5883\u8fdb\u884c\u91cd\u6784\u3002", "result": "\u7406\u8bba\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u7f29\u5c0f\u6cdb\u5316\u5dee\u8ddd\uff1b\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u76f8\u6bd4\u6269\u6563\u7b56\u7565\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u57fa\u7ebf\uff0cReSET\u80fd\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6267\u884c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u5bf9\u73af\u5883\u7684\u9884\u8c03\u6574\u884c\u4e3a\uff0cReSET\u6709\u6548\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.17276", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17276", "abs": "https://arxiv.org/abs/2509.17276", "authors": ["Runjia Zeng", "James Chenhao Liang", "Cheng Han", "Zhiwen Cao", "Jiahao Liu", "Xiaojun Quan", "Yingjie Victor Chen", "Lifu Huang", "Tong Geng", "Qifan Wang", "Dongfang Liu"], "title": "Probabilistic Token Alignment for Large Language Model Fusion", "comment": "NeurIPS 2025", "summary": "Training large language models (LLMs) from scratch can yield models with\nunique functionalities and strengths, but it is costly and often leads to\nredundant capabilities. A more cost-effective alternative is to fuse existing\npre-trained LLMs with different architectures into a more powerful model.\nHowever, a key challenge in existing model fusion is their dependence on\nmanually predefined vocabulary alignment, which may not generalize well across\ndiverse contexts, leading to performance degradation in several evaluation. To\nsolve this, we draw inspiration from distribution learning and propose the\nprobabilistic token alignment method as a general and soft mapping for\nalignment, named as PTA-LLM. Our approach innovatively reformulates token\nalignment into a classic mathematical problem: optimal transport, seamlessly\nleveraging distribution-aware learning to facilitate more coherent model\nfusion. Apart from its inherent generality, PTA-LLM exhibits interpretability\nfrom a distributional perspective, offering insights into the essence of the\ntoken alignment. Empirical results demonstrate that probabilistic token\nalignment enhances the target model's performance across multiple capabilities.\nOur code is avaliable at https://runjia.tech/neurips_pta-llm/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u6027 token \u5bf9\u9f50\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5 PTA-LLM\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u5b9e\u73b0\u5206\u5e03\u611f\u77e5\u7684\u8f6f\u5bf9\u9f50\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u878d\u5408\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u9884\u5b9a\u4e49\u7684\u8bcd\u6c47\u5bf9\u9f50\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5c06 token \u5bf9\u9f50\u91cd\u6784\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u91c7\u7528\u6982\u7387\u6027\u5bf9\u9f50\uff08PTA\uff09\u4f5c\u4e3a\u8f6f\u6620\u5c04\uff0c\u5b9e\u73b0\u5206\u5e03\u611f\u77e5\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e PTA-LLM \u5728\u591a\u79cd\u80fd\u529b\u4e0a\u63d0\u5347\u4e86\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "PTA-LLM \u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u4f9d\u8d56\u786c\u5bf9\u9f50\u7684\u65b9\u5f0f\u3002"}}
{"id": "2509.17197", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17197", "abs": "https://arxiv.org/abs/2509.17197", "authors": ["Junlong Ke", "Qiying Hu", "Shenghai Yuan", "Yuecong Xu", "Jianfei Yang"], "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing", "comment": "11 pages", "summary": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SignalLLM\uff0c\u9996\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u4fe1\u53f7\u5904\u7406\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u548c\u5206\u5c42\u89c4\u5212\u5b9e\u73b0\u5bf9\u591a\u79cd\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u4e0e\u6cdb\u5316\u3002", "motivation": "\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u6d41\u7a0b\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u3001\u624b\u52a8\u8bbe\u8ba1\uff0c\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u63a8\u7406\u3001\u8de8\u6a21\u6001\u8fc1\u79fb\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u6709\u671b\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002", "method": "SignalLLM\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u9886\u57df\u7279\u5b9a\u68c0\u7d22\u5c06\u9ad8\u5c42\u76ee\u6807\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8fdb\u884c\u5206\u5c42\u89c4\u5212\uff0c\u5e76\u7ed3\u5408\u63d0\u793a\u63a8\u7406\u3001\u8de8\u6a21\u6001\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u3001\u6a21\u578b\u8c03\u7528\u7b49\u65b9\u5f0f\u6267\u884c\u4efb\u52a1\u3002", "result": "\u5728\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u3001\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u3001\u6587\u672c\u538b\u7f29\u7b49\u4e94\u4e2a\u901a\u4fe1\u4e0e\u611f\u77e5\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86SignalLLM\u7684\u6709\u6548\u6027\uff0c\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709LLM-based\u65b9\u6cd5\u3002", "conclusion": "SignalLLM\u5b9e\u73b0\u4e86\u901a\u7528\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u4fe1\u53f7\u5904\u7406\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5c55\u73b0\u51fa\u5728\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u548c\u4f4e\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16863", "categories": ["cs.CV", "68T20, 68U20"], "pdf": "https://arxiv.org/pdf/2509.16863", "abs": "https://arxiv.org/abs/2509.16863", "authors": ["Amanuel T. Dufera", "Yuan-Li Cai"], "title": "ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM", "comment": null, "summary": "We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM\nsystem for robust, highfidelity RGB-only reconstruction. Addressing geometric\ninaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable\ndepth estimation, ConfidentSplat incorporates a core innovation: a\nconfidence-weighted fusion mechanism. This mechanism adaptively integrates\ndepth cues from multiview geometry with learned monocular priors (Omnidata\nViT), dynamically weighting their contributions based on explicit reliability\nestimates-derived predominantly from multi-view geometric consistency-to\ngenerate high-fidelity proxy depth for map supervision. The resulting proxy\ndepth guides the optimization of a deformable 3DGS map, which efficiently\nadapts online to maintain global consistency following pose updates from a\nDROID-SLAM-inspired frontend and backend optimizations (loop closure, global\nbundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,\nScanNet) and diverse custom mobile datasets demonstrates significant\nimprovements in reconstruction accuracy (L1 depth error) and novel view\nsynthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in\nchallenging conditions. ConfidentSplat underscores the efficacy of principled,\nconfidence-aware sensor fusion for advancing state-of-the-art dense visual\nSLAM.", "AI": {"tldr": "ConfidentSplat\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u70b9\u9635\u7684SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u878d\u5408\u673a\u5236\u7ed3\u5408\u591a\u89c6\u56fe\u51e0\u4f55\u4e0e\u5355\u76ee\u5148\u9a8c\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u4ee3\u7406\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-only\u573a\u666f\u4e0b\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709RGB-only 3DGS SLAM\u65b9\u6cd5\u56e0\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u53ef\u9760\u5bfc\u81f4\u51e0\u4f55\u4e0d\u51c6\u786e\uff0c\u9700\u63d0\u5347\u91cd\u5efa\u9c81\u68d2\u6027\u4e0e\u7cbe\u5ea6\u3002", "method": "\u5f15\u5165\u7f6e\u4fe1\u5ea6\u52a0\u6743\u878d\u5408\u673a\u5236\uff0c\u52a8\u6001\u7ed3\u5408\u591a\u89c6\u56fe\u51e0\u4f55\u6df1\u5ea6\u7ebf\u7d22\u4e0e\u5b66\u4e60\u7684\u5355\u76ee\u5148\u9a8c\uff08Omnidata ViT\uff09\uff0c\u5229\u7528\u51e0\u4f55\u4e00\u81f4\u6027\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7406\u6df1\u5ea6\u7528\u4e8e\u5730\u56fe\u4f18\u5316\uff1b\u91c7\u7528\u53ef\u53d8\u5f623DGS\u5730\u56fe\u5e76\u7ed3\u5408DROID-SLAM\u542f\u53d1\u7684\u524d\u540e\u7aef\u4f18\u5316\u3002", "result": "\u5728TUM-RGBD\u3001ScanNet\u53ca\u81ea\u5efa\u79fb\u52a8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u964d\u4f4eL1\u6df1\u5ea6\u8bef\u5dee\uff0c\u63d0\u5347PSNR\u3001SSIM\u3001LPIPS\u7b49\u6307\u6807\uff0c\u5c24\u5176\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u4f20\u611f\u5668\u878d\u5408\u7b56\u7565\u6709\u6548\u63a8\u52a8\u4e86\u7a20\u5bc6\u89c6\u89c9SLAM\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u4e3aRGB-only SLAM\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18046", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18046", "abs": "https://arxiv.org/abs/2509.18046", "authors": ["Yinuo Wang", "Yuanyang Qi", "Jinzhao Zhou", "Gavin Tao"], "title": "HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba", "comment": "10 pages", "summary": "End-to-end reinforcement learning (RL) for humanoid locomotion is appealing\nfor its compact perception-action mapping, yet practical policies often suffer\nfrom training instability, inefficient feature fusion, and high actuation cost.\nWe present HuMam, a state-centric end-to-end RL framework that employs a\nsingle-layer Mamba encoder to fuse robot-centric states with oriented footstep\ntargets and a continuous phase clock. The policy outputs joint position targets\ntracked by a low-level PD loop and is optimized with PPO. A concise six-term\nreward balances contact quality, swing smoothness, foot placement, posture, and\nbody stability while implicitly promoting energy saving. On the JVRC-1 humanoid\nin mc-mujoco, HuMam consistently improves learning efficiency, training\nstability, and overall task performance over a strong feedforward baseline,\nwhile reducing power consumption and torque peaks. To our knowledge, this is\nthe first end-to-end humanoid RL controller that adopts Mamba as the fusion\nbackbone, demonstrating tangible gains in efficiency, stability, and control\neconomy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u72b6\u6001\u4e2d\u5fc3\u578b\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6HuMam\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u63a7\u5236\u7ecf\u6d4e\u6027\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u7279\u5f81\u878d\u5408\u6548\u7387\u4f4e\u548c\u6267\u884c\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5355\u5c42Mamba\u7f16\u7801\u5668\u878d\u5408\u673a\u5668\u4eba\u72b6\u6001\u3001\u76ee\u6807\u843d\u811a\u70b9\u548c\u76f8\u4f4d\u65f6\u949f\uff0c\u8f93\u51fa\u7531\u4f4e\u5c42PD\u73af\u8def\u8ddf\u8e2a\u7684\u5173\u8282\u4f4d\u7f6e\u76ee\u6807\uff0c\u5e76\u4f7f\u7528PPO\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728JVRC-1\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5f3a\u524d\u9988\u57fa\u7ebf\uff0cHuMam\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u529f\u8017\u548c\u626d\u77e9\u5cf0\u503c\u3002", "conclusion": "HuMam\u662f\u9996\u4e2a\u91c7\u7528Mamba\u4f5c\u4e3a\u878d\u5408\u4e3b\u5e72\u7684\u7aef\u5230\u7aef\u4eba\u5f62\u673a\u5668\u4ebaRL\u63a7\u5236\u5668\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63a7\u5236\u7684\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u80fd\u8017\u8868\u73b0\u3002"}}
{"id": "2509.17289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17289", "abs": "https://arxiv.org/abs/2509.17289", "authors": ["Sydney Anuyah", "Mehedi Mahmud Kaushik", "Krishna Dwarampudi", "Rakesh Shiradkar", "Arjan Durresi", "Sunandan Chakraborty"], "title": "Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling", "comment": null, "summary": "We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting\nsentence-level knowledge graphs by combining robust coreference resolution with\nsyntactic sentence decomposition. Using our model, we contribute a dataset of\nover 150,000 knowledge triples, which is open source. We also contribute a\ntraining corpus of 7248 rows for sentence complexity, 190 rows of gold human\nannotations for co-reference resolution using open source lung-cancer abstracts\nfrom PubMed, 900 rows of gold human annotations for sentence conversion\npolicies, and 398 triples of gold human annotations. We systematically select\noptimal prompt-model pairs across five complexity categories, showing that\nhybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match\naccuracy on sentence simplification. On relation extraction (RE), our pipeline\nachieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the\nart, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on\nWiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference\nand decomposition increases recall on rare relations by over 20%. Code and\ndataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CoDe-KG\uff0c\u4e00\u4e2a\u7ed3\u5408\u5171\u6307\u6d88\u89e3\u548c\u53e5\u6cd5\u5206\u89e3\u7684\u7aef\u5230\u7aef\u77e5\u8bc6\u56fe\u8c31\u62bd\u53d6 pipeline\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b15\u4e07\u4ee5\u4e0a\u77e5\u8bc6\u4e09\u5143\u7ec4\u7684\u5f00\u6e90\u6570\u636e\u96c6\u53ca\u76f8\u5173\u6807\u6ce8\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u53e5\u5b50\u7ea7\u77e5\u8bc6\u56fe\u8c31\u62bd\u53d6\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u53e5\u5b50\u548c\u5171\u6307\u73b0\u8c61\u65f6\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5206\u89e3\u4e0e\u6d88\u89e3\u673a\u5236\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faCoDe-KG\u6846\u67b6\uff0c\u7ed3\u5408\u6838\u5fc3ference resolution\u4e0e\u53e5\u6cd5\u5206\u89e3\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u601d\u7ef4\u94fe\u4e0e\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u4f18\u5316\u53e5\u5b50\u7b80\u5316\uff1b\u5728\u591a\u4e2a\u590d\u6742\u5ea6\u7c7b\u522b\u4e2d\u7cfb\u7edf\u9009\u62e9\u6700\u4f18\u63d0\u793a-\u6a21\u578b\u7ec4\u5408\u3002", "result": "\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e0a\uff0cCoDe-KG\u5728REBEL\u4e0a\u8fbe\u523065.8% macro-F1\uff08\u6bd4\u5148\u524dSOTA\u63d0\u53478\u70b9\uff09\uff0c\u5728WebNLG2\u4e0a\u8fbe75.7% micro-F1\uff0c\u5728Wiki-NRE\u548cCaRB\u4e0a\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u4f18\uff1b\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u6574\u5408\u5171\u6307\u4e0e\u5206\u89e3\u4f7f\u7f55\u89c1\u5173\u7cfb\u53ec\u56de\u7387\u63d0\u5347\u8d8520%\uff1b\u53e5\u5b50\u7b80\u5316\u51c6\u786e\u7387\u6700\u9ad8\u8fbe99.8%\u3002", "conclusion": "CoDe-KG\u901a\u8fc7\u878d\u5408\u5171\u6307\u6d88\u89e3\u4e0e\u53e5\u6cd5\u5206\u89e3\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u62bd\u53d6\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u53e5\u5b50\u548c\u7f55\u89c1\u5173\u7cfb\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u5177\u6709\u826f\u597d\u7684\u5f00\u6e90\u4ef7\u503c\u4e0e\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17205", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17205", "abs": "https://arxiv.org/abs/2509.17205", "authors": ["Wook Lee", "Frans A. Oliehoek"], "title": "Conditional Policy Generator for Dynamic Constraint Satisfaction and Optimization", "comment": null, "summary": "Leveraging machine learning methods to solve constraint satisfaction problems\nhas shown promising, but they are mostly limited to a static situation where\nthe problem description is completely known and fixed from the beginning. In\nthis work we present a new approach to constraint satisfaction and optimization\nin dynamically changing environments, particularly when variables in the\nproblem are statistically independent. We frame it as a reinforcement learning\nproblem and introduce a conditional policy generator by borrowing the idea of\nclass conditional generative adversarial networks (GANs). Assuming that the\nproblem includes both static and dynamic constraints, the former are used in a\nreward formulation to guide the policy training such that it learns to map to a\nprobabilistic distribution of solutions satisfying static constraints from a\nnoise prior, which is similar to a generator in GANs. On the other hand,\ndynamic constraints in the problem are encoded to different class labels and\nfed with the input noise. The policy is then simultaneously updated for maximum\nlikelihood of correctly classifying given the dynamic conditions in a\nsupervised manner. We empirically demonstrate a proof-of-principle experiment\nwith a multi-modal constraint satisfaction problem and compare between\nunconditional and conditional cases.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5229\u7528\u6761\u4ef6\u7b56\u7565\u751f\u6210\u5668\u89e3\u51b3\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e0b\u7684\u7ea6\u675f\u6ee1\u8db3\u4e0e\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u9759\u6001\u548c\u52a8\u6001\u7ea6\u675f\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6c42\u89e3\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u65f6\u591a\u5c40\u9650\u4e8e\u9759\u6001\u73af\u5883\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u95ee\u9898\u63cf\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u53d8\u91cf\u7edf\u8ba1\u72ec\u7acb\u65f6\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u501f\u9274\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u601d\u60f3\u8bbe\u8ba1\u6761\u4ef6\u7b56\u7565\u751f\u6210\u5668\uff1b\u9759\u6001\u7ea6\u675f\u7528\u4e8e\u5956\u52b1\u51fd\u6570\u4ee5\u5f15\u5bfc\u7b56\u7565\u5b66\u4e60\u6ee1\u8db3\u7ea6\u675f\u7684\u89e3\u5206\u5e03\uff0c\u52a8\u6001\u7ea6\u675f\u7f16\u7801\u4e3a\u7c7b\u522b\u6807\u7b7e\u5e76\u4e0e\u8f93\u5165\u566a\u58f0\u4e00\u540c\u8f93\u5165\uff0c\u7b56\u7565\u901a\u8fc7\u76d1\u7763\u65b9\u5f0f\u8054\u5408\u66f4\u65b0\u4ee5\u6700\u5927\u5316\u5206\u7c7b\u4f3c\u7136\u3002", "result": "\u5728\u591a\u6a21\u6001\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u539f\u7406\u9a8c\u8bc1\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u6761\u4ef6\u7b56\u7565\u751f\u6210\u5668\u76f8\u6bd4\u65e0\u6761\u4ef6\u60c5\u51b5\u80fd\u66f4\u6709\u6548\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7ea6\u675f\u6761\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u878d\u5408\u9759\u6001\u4e0e\u52a8\u6001\u7ea6\u675f\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u6ee1\u8db3\u7ea6\u675f\u7684\u89e3\u5206\u5e03\u3002"}}
{"id": "2509.16873", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16873", "abs": "https://arxiv.org/abs/2509.16873", "authors": ["Yuanzhi Li", "Lebin Zhou", "Nam Ling", "Zhenghao Chen", "Wei Wang", "Wei Jiang"], "title": "$\\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation", "comment": null, "summary": "The gaming and entertainment industry is rapidly evolving, driven by\nimmersive experiences and the integration of generative AI (GAI) technologies.\nTraining such models effectively requires large-scale datasets that capture the\ndiversity and context of gaming environments. However, existing datasets are\noften limited to specific domains or rely on artificial degradations, which do\nnot accurately capture the unique characteristics of gaming content. Moreover,\nbenchmarks for controllable video generation remain absent.\n  To address these limitations, we introduce $\\mathtt{M^3VIR}$, a large-scale,\nmulti-modal, multi-view dataset specifically designed to overcome the\nshortcomings of current resources. Unlike existing datasets, $\\mathtt{M^3VIR}$\nprovides diverse, high-fidelity gaming content rendered with Unreal Engine 5,\noffering authentic ground-truth LR-HR paired and multi-view frames across 80\nscenes in 8 categories. It includes $\\mathtt{M^3VIR\\_MR}$ for super-resolution\n(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and\n$\\mathtt{M^3VIR\\_{MS}}$, the first multi-style, object-level ground-truth set\nenabling research on controlled video generation. Additionally, we benchmark\nseveral state-of-the-art SR and NVS methods to establish performance baselines.\nWhile no existing approaches directly handle controlled video generation,\n$\\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing\nthe dataset, we aim to facilitate research in AI-powered restoration,\ncompression, and controllable content generation for next-generation cloud\ngaming and entertainment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86M^3VIR\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u3001\u591a\u89c6\u89d2\u7684\u6e38\u620f\u5185\u5bb9\u6570\u636e\u96c6\uff0c\u57fa\u4e8eUnreal Engine 5\u751f\u6210\uff0c\u5305\u542b\u771f\u5b9e\u914d\u5bf9\u7684\u4f4e/\u9ad8\u5206\u8fa8\u7387\u548c\u591a\u89c6\u89d2\u5e27\uff0c\u652f\u6301\u8d85\u5206\u8fa8\u7387\u3001\u65b0\u89c6\u89d2\u5408\u6210\u53ca\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u9762\u5411\u6e38\u620f\u573a\u666f\u7684\u591a\u98ce\u683c\u5bf9\u8c61\u7ea7\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u6e38\u620f\u76f8\u5173\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u6216\u4f7f\u7528\u4eba\u5de5\u9000\u5316\u65b9\u6cd5\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u6e38\u620f\u5185\u5bb9\u7279\u5f81\uff0c\u4e14\u7f3a\u4e4f\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u771f\u5b9e\u3001\u591a\u6837\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8AI\u5728\u6e38\u620f\u4e0e\u5a31\u4e50\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aM^3VIR\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b80\u4e2a\u573a\u666f\u30018\u4e2a\u7c7b\u522b\uff0c\u4f7f\u7528Unreal Engine 5\u6e32\u67d3\uff0c\u63d0\u4f9b\u771f\u5b9e\u7684LR-HR\u914d\u5bf9\u548c\u591a\u89c6\u89d2\u5e27\uff1b\u8bbe\u8ba1\u4e86M^3VIR_MR\uff08\u7528\u4e8eSR\u3001NVS\u53ca\u8054\u5408\u4efb\u52a1\uff09\u548cM^3VIR_MS\uff08\u9996\u4e2a\u652f\u6301\u591a\u98ce\u683c\u63a7\u5236\u7684\u5bf9\u8c61\u7ea7\u6570\u636e\u96c6\uff09\u4e24\u4e2a\u5b50\u96c6\uff0c\u5e76\u5bf9\u5f53\u524d\u4e3b\u6d41SR\u548cNVS\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5efa\u7acb\u4e86\u8d85\u5206\u8fa8\u7387\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u7684\u6027\u80fd\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff1bM^3VIR\u6210\u4e3a\u9996\u4e2a\u652f\u6301\u53ef\u63a7\u89c6\u9891\u751f\u6210\u7814\u7a76\u7684\u591a\u98ce\u683c\u3001\u5bf9\u8c61\u7ea7\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002", "conclusion": "M^3VIR\u4e3a\u6e38\u620f\u548c\u5a31\u4e50\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8AI\u9a71\u52a8\u7684\u56fe\u50cf\u6062\u590d\u3001\u538b\u7f29\u4ee5\u53ca\u53ef\u63a7\u5185\u5bb9\u751f\u6210\u5728\u4e0b\u4e00\u4ee3\u4e91\u6e38\u620f\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.18053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18053", "abs": "https://arxiv.org/abs/2509.18053", "authors": ["Hsu-kuang Chiu", "Ryo Hachiuma", "Chien-Yi Wang", "Yu-Chiang Frank Wang", "Min-Hung Chen", "Stephen F. Smith"], "title": "V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts", "comment": null, "summary": "Current state-of-the-art autonomous vehicles could face safety-critical\nsituations when their local sensors are occluded by large nearby objects on the\nroad. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed\nas a means of addressing this problem, and one recently introduced framework\nfor cooperative autonomous driving has further adopted an approach that\nincorporates a Multimodal Large Language Model (MLLM) to integrate cooperative\nperception and planning processes. However, despite the potential benefit of\napplying graph-of-thoughts reasoning to the MLLM, this idea has not been\nconsidered by previous cooperative autonomous driving research. In this paper,\nwe propose a novel graph-of-thoughts framework specifically designed for\nMLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our\nproposed novel ideas of occlusion-aware perception and planning-aware\nprediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for\ntraining and testing the cooperative driving graph-of-thoughts. Our\nexperimental results show that our method outperforms other baselines in\ncooperative perception, prediction, and planning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u8f66\u8054\u7f51\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u56fe\u601d\u7ef4\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u906e\u6321\u611f\u77e5\u611f\u77e5\u548c\u89c4\u5212\u611f\u77e5\u9884\u6d4b\u7684\u65b0\u6982\u5ff5\uff0c\u5e76\u6784\u5efa\u4e86V2V-GoT-QA\u6570\u636e\u96c6\u4e0eV2V-GoT\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u534f\u540c\u611f\u77e5\u3001\u9884\u6d4b\u4e0e\u89c4\u5212\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4f20\u611f\u5668\u88ab\u906e\u6321\u65f6\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u5c3d\u7ba1\u8f66\u8054\u7f51\u534f\u540c\u9a7e\u9a76\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u5c1a\u672a\u63a2\u7d22\u56fe\u601d\u7ef4\u63a8\u7406\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411MLLM\u7684\u56fe\u601d\u7ef4\u6846\u67b6\uff0c\u878d\u5408\u906e\u6321\u611f\u77e5\u611f\u77e5\u4e0e\u89c4\u5212\u611f\u77e5\u9884\u6d4b\u673a\u5236\uff0c\u5e76\u6784\u5efa\u4e13\u7528\u6570\u636e\u96c6V2V-GoT-QA\u53caV2V-GoT\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u65b9\u6cd5\u5728\u534f\u540c\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u56fe\u601d\u7ef4\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86MLLM\u5728\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u4f20\u611f\u5668\u906e\u6321\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17292", "abs": "https://arxiv.org/abs/2509.17292", "authors": ["Jun Seo Kim", "Hyemi Kim", "Woo Joo Oh", "Hongjin Cho", "Hochul Lee", "Hye Hyeon Kim"], "title": "Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection", "comment": null, "summary": "Cognitive distortions have been closely linked to mental health disorders,\nyet their automatic detection remained challenging due to contextual ambiguity,\nco-occurrence, and semantic overlap. We proposed a novel framework that\ncombines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)\narchitecture to enhance interpretability and expression-level reasoning. Each\nutterance was decomposed into Emotion, Logic, and Behavior (ELB) components,\nwhich were processed by LLMs to infer multiple distortion instances, each with\na predicted type, expression, and model-assigned salience score. These\ninstances were integrated via a Multi-View Gated Attention mechanism for final\nclassification. Experiments on Korean (KoACD) and English (Therapist QA)\ndatasets demonstrate that incorporating ELB and LLM-inferred salience scores\nimproves classification performance, especially for distortions with high\ninterpretive ambiguity. Our results suggested a psychologically grounded and\ngeneralizable approach for fine-grained reasoning in mental health NLP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u3001\u903b\u8f91\u548c\u884c\u4e3a\uff08ELB\uff09\u5206\u89e3\u4e0e\u663e\u8457\u6027\u8bc4\u5206\u63d0\u5347\u8ba4\u77e5\u626d\u66f2\u81ea\u52a8\u68c0\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u3002", "motivation": "\u8ba4\u77e5\u626d\u66f2\u7684\u81ea\u52a8\u68c0\u6d4b\u56e0\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u3001\u5171\u73b0\u548c\u8bed\u4e49\u91cd\u53e0\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e14\u53ef\u89e3\u91ca\u7684\u8bc6\u522b\u3002", "method": "\u5c06\u6bcf\u6761\u8bdd\u8bed\u5206\u89e3\u4e3a\u60c5\u611f\u3001\u903b\u8f91\u548c\u884c\u4e3a\uff08ELB\uff09\u4e09\u4e2a\u6210\u5206\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u65ad\u6bcf\u4e2a\u6210\u5206\u4e2d\u7684\u626d\u66f2\u5b9e\u4f8b\u53ca\u5176\u7c7b\u578b\u3001\u8868\u8fbe\u548c\u663e\u8457\u6027\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u89d2\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u591a\u5b9e\u4f8b\u5b66\u4e60\u6574\u5408\u4ee5\u5b8c\u6210\u5206\u7c7b\u3002", "result": "\u5728\u97e9\u8bed\uff08KoACD\uff09\u548c\u82f1\u8bed\uff08Therapist QA\uff09\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f15\u5165ELB\u5206\u89e3\u548cLLM\u63a8\u65ad\u7684\u663e\u8457\u6027\u8bc4\u5206\u80fd\u6709\u6548\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u89e3\u91ca\u6a21\u7cca\u6027\u5f3a\u7684\u8ba4\u77e5\u626d\u66f2\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5fc3\u7406\u5b66\u57fa\u7840\u624e\u5b9e\u3001\u53ef\u6cdb\u5316\u7684\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u3002"}}
{"id": "2509.17208", "categories": ["cs.LG", "physics.atm-clus", "I.6.5; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.17208", "abs": "https://arxiv.org/abs/2509.17208", "authors": ["Kevin Bachelor", "Sanya Murdeshwar", "Daniel Sabo", "Razvan Marinescu"], "title": "Active Learning for Machine Learning Driven Molecular Dynamics", "comment": "8 pages, 4 figures, for Neurips Workshop: Machine Learning and the\n  Physical Sciences 2025", "summary": "Machine learned coarse grained (CG) potentials are fast, but degrade over\ntime when simulations reach undersampled biomolecular conformations, and\ngenerating widespread all atom (AA) data to combat this is computationally\ninfeasible. We propose a novel active learning framework for CG neural network\npotentials in molecular dynamics (MD). Building on the CGSchNet model, our\nmethod employs root mean squared deviation (RMSD) based frame selection from MD\nsimulations in order to generate data on the fly by querying an oracle during\nthe training of a neural network potential. This framework preserves CG level\nefficiency while correcting the model at precise, RMSD identified coverage\ngaps. By training CGSchNet, a coarse grained neural network potential, we\nempirically show that our framework explores previously unseen configurations\nand trains the model on unexplored regions of conformational space. Our active\nlearning framework enables a CGSchNet model trained on the Chignolin protein to\nachieve a 33.05% improvement in the Wasserstein 1 (W1) metric in Time lagged\nIndependent Component Analysis (TICA) space on an in house benchmark suite.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u7c97\u7c92\u5316\u795e\u7ecf\u7f51\u7edc\u52bf\u51fd\u6570\u6846\u67b6\uff0c\u901a\u8fc7RMSD\u9a71\u52a8\u7684\u5e27\u9009\u62e9\u5728\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u52a8\u6001\u751f\u6210\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u6784\u8c61\u7a7a\u95f4\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7c97\u7c92\u5316\u52bf\u51fd\u6570\u5728\u6a21\u62df\u8fdb\u5165\u91c7\u6837\u4e0d\u8db3\u7684\u751f\u7269\u5206\u5b50\u6784\u8c61\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5e7f\u6cdb\u751f\u6210\u5168\u539f\u5b50\u6570\u636e\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u80fd\u52a8\u6001\u6539\u8fdb\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eCGSchNet\u6a21\u578b\uff0c\u91c7\u7528RMSD\u4f5c\u4e3a\u6307\u6807\u8fdb\u884c\u5206\u5b50\u52a8\u529b\u5b66\u8f68\u8ff9\u4e2d\u7684\u5e27\u9009\u62e9\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u67e5\u8be2\u5168\u539f\u5b50\u6a21\u578b\uff08oracle\uff09\u751f\u6210\u5173\u952e\u6570\u636e\uff0c\u5b9e\u73b0\u4e3b\u52a8\u5b66\u4e60\u3002", "result": "\u8be5\u6846\u67b6\u4f7fCGSchNet\u5728Chignolin\u86cb\u767d\u4e0a\u7684Wasserstein 1\u8ddd\u79bb\uff08W1\uff09\u6307\u6807\u5728TICA\u7a7a\u95f4\u4e2d\u63d0\u5347\u4e8633.05%\uff0c\u663e\u8457\u6539\u5584\u4e86\u5bf9\u6784\u8c61\u7a7a\u95f4\u7684\u8986\u76d6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u6301\u7c97\u7c92\u5316\u6548\u7387\u7684\u540c\u65f6\uff0c\u6709\u6548\u8bc6\u522b\u5e76\u586b\u8865\u6a21\u578b\u8986\u76d6\u7a7a\u767d\uff0c\u63d0\u5347\u6a21\u62df\u7684\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.16886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16886", "abs": "https://arxiv.org/abs/2509.16886", "authors": ["Yingzhen Hu", "Yiheng Zhong", "Ruobing Li", "Yingxue Su", "Jiabao An", "Feilong Tang", "Jionglong Su", "Imran Razzak"], "title": "SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation", "comment": null, "summary": "The Segment Anything Model (SAM) demonstrates impressive zero-shot\nsegmentation ability on natural images but encounters difficulties in medical\nimaging due to domain shifts, anatomical variability, and its reliance on\nuser-provided prompts. Recent prompt-free adaptations alleviate the need for\nexpert intervention, yet still suffer from limited robustness and adaptability,\noften overlooking the issues of semantic over-smoothing and token uniformity.\nWe propose SAM-DCE, which balances local discrimination and global semantics\nwhile mitigating token uniformity, enhancing inter-class separability, and\nenriching mask decoding with fine-grained, consistent representations.\nExtensive experiments on diverse medical benchmarks validate its effectiveness.", "AI": {"tldr": "\u63d0\u51faSAM-DCE\u6a21\u578b\uff0c\u901a\u8fc7\u5e73\u8861\u5c40\u90e8\u5224\u522b\u4e0e\u5168\u5c40\u8bed\u4e49\uff0c\u7f13\u89e3\u6807\u8bb0\u5747\u5300\u6027\u95ee\u9898\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u7c7b\u95f4\u53ef\u5206\u6027\u548c\u89e3\u7801\u7cbe\u5ea6\u3002", "motivation": "SAM\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u56e0\u9886\u57df\u5dee\u5f02\u3001\u89e3\u5256\u53d8\u5f02\u548c\u4f9d\u8d56\u4eba\u5de5\u63d0\u793a\u800c\u53d7\u9650\uff1b\u73b0\u6709\u65e0\u63d0\u793a\u65b9\u6cd5\u4ecd\u5b58\u5728\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u5ffd\u89c6\u8bed\u4e49\u8fc7\u5e73\u6ed1\u548c\u6807\u8bb0\u5747\u5300\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faSAM-DCE\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u673a\u5236\u5e73\u8861\u5c40\u90e8\u5224\u522b\u529b\u4e0e\u5168\u5c40\u8bed\u4e49\uff0c\u7f13\u89e3\u6807\u8bb0\u5747\u5300\u6027\uff0c\u589e\u5f3a\u7c7b\u95f4\u53ef\u5206\u6027\uff0c\u5e76\u5728\u63a9\u7801\u89e3\u7801\u4e2d\u878d\u5165\u7ec6\u7c92\u5ea6\u3001\u4e00\u81f4\u6027\u7684\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAM-DCE\u5728\u5206\u5272\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "SAM-DCE\u6709\u6548\u63d0\u5347\u4e86SAM\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u96f6\u6837\u672c\u5206\u5272\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u8fc7\u5e73\u6ed1\u548c\u7279\u5f81\u5747\u5300\u6027\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18068", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18068", "abs": "https://arxiv.org/abs/2509.18068", "authors": ["Bin Zhao", "Nakul Garg"], "title": "RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds", "comment": null, "summary": "Millimeter-wave radar provides perception robust to fog, smoke, dust, and low\nlight, making it attractive for size, weight, and power constrained robotic\nplatforms. Current radar imaging methods, however, rely on synthetic aperture\nor multi-frame aggregation to improve resolution, which is impractical for\nsmall aerial, inspection, or wearable systems. We present RadarSFD, a\nconditional latent diffusion framework that reconstructs dense LiDAR-like point\nclouds from a single radar frame without motion or SAR. Our approach transfers\ngeometric priors from a pretrained monocular depth estimator into the diffusion\nbackbone, anchors them to radar inputs via channel-wise latent concatenation,\nand regularizes outputs with a dual-space objective combining latent and\npixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer\nDistance and 28 cm Modified Hausdorff Distance, improving over the single-frame\nRadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame\nmethods using 5-41 frames. Qualitative results show recovery of fine walls and\nnarrow gaps, and experiments across new environments confirm strong\ngeneralization. Ablation studies highlight the importance of pretrained\ninitialization, radar BEV conditioning, and the dual-space loss. Together,\nthese results establish the first practical single-frame, no-SAR mmWave radar\npipeline for dense point cloud perception in compact robotic systems.", "AI": {"tldr": "\u63d0\u51faRadarSFD\uff0c\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u53ef\u4ece\u5355\u5e27\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u91cd\u5efa\u5bc6\u96c6LiDAR-like\u70b9\u4e91\uff0c\u65e0\u9700\u5408\u6210\u5b54\u5f84\u6216\u8fd0\u52a8\uff0c\u9002\u7528\u4e8e\u5c0f\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u96f7\u8fbe\u6210\u50cf\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u5b54\u5f84\u6216\u591a\u5e27\u805a\u5408\u4ee5\u63d0\u5347\u5206\u8fa8\u7387\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5c0f\u578b\u7a7a\u4e2d\u3001\u68c0\u6d4b\u6216\u53ef\u7a7f\u6234\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u7684\u5355\u5e27\u9ad8\u5206\u8fa8\u7387\u96f7\u8fbe\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRadarSFD\uff0c\u4e00\u79cd\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u9053\u7ea7\u6f5c\u5728\u62fc\u63a5\u5c06\u9884\u8bad\u7ec3\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u7684\u51e0\u4f55\u5148\u9a8c\u8fc1\u79fb\u5230\u6269\u6563\u4e3b\u5e72\u4e2d\uff0c\u5e76\u7ed3\u5408\u6f5c\u7a7a\u95f4\u4e0e\u50cf\u7d20\u7a7a\u95f4\u635f\u5931\u7684\u53cc\u7a7a\u95f4\u76ee\u6807\u5bf9\u8f93\u51fa\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u4ece\u5355\u5e27\u96f7\u8fbe\u6570\u636e\u91cd\u5efa\u5bc6\u96c6\u70b9\u4e91\u3002", "result": "\u5728RadarHD\u57fa\u51c6\u4e0a\uff0cRadarSFD\u8fbe\u523035 cm Chamfer\u8ddd\u79bb\u548c28 cm Modified Hausdorff\u8ddd\u79bb\uff0c\u4f18\u4e8e\u5355\u5e27\u57fa\u7ebf\uff0856 cm, 45 cm\uff09\uff0c\u5e76\u4e0e\u4f7f\u75285-41\u5e27\u7684\u591a\u5e27\u65b9\u6cd5\u76f8\u5f53\uff1b\u5b9a\u6027\u7ed3\u679c\u6062\u590d\u51fa\u7ec6\u5899\u548c\u7a84\u7f1d\uff0c\u4e14\u5728\u65b0\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RadarSFD\u9996\u6b21\u5b9e\u73b0\u4e86\u9002\u7528\u4e8e\u7d27\u51d1\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b9e\u7528\u5316\u5355\u5e27\u3001\u65e0SAR\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5bc6\u96c6\u70b9\u4e91\u611f\u77e5 pipeline\u3002"}}
{"id": "2509.17317", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17317", "abs": "https://arxiv.org/abs/2509.17317", "authors": ["Dan John Velasco", "Matthew Theodore Roque"], "title": "Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text", "comment": "Under review", "summary": "Most languages lack sufficient data for large-scale monolingual pretraining,\ncreating a \"data wall.\" Multilingual pretraining helps but is limited by\nlanguage imbalance and the \"curse of multilinguality.\" An alternative is to\ntranslate high-resource text with machine translation (MT), which raises three\nquestions: (1) How does MT-derived data scale with model capacity? (2) Can\nsource-side transformations (e.g., simplifying English with an LLM) improve\ngeneralization to native text? (3) How well do models pretrained on MT-derived\ndata adapt when continually trained on limited native text? We investigate\nthese questions by translating English into Indonesian and Tamil--two\ntypologically distant, lower-resource languages--and pretraining GPT-2 models\n(124M-774M) on native or MT-derived corpora from raw and LLM-simplified\nEnglish. We evaluate cross-entropy loss on native text, along with accuracy on\nsyntactic probes and downstream tasks. Our results show that (1) MT-pretrained\nmodels benefit from scaling; (2) source-side simplification harms\ngeneralization to native text; and (3) adapting MT-pretrained models on native\ntext often yields better performance than native-only models, even with less\nnative data. However, tasks requiring cultural nuance (e.g., toxicity\ndetection) demand more exposure to native data.", "AI": {"tldr": "\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u5c06\u9ad8\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u8f6c\u5316\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u968f\u89c4\u6a21\u63d0\u5347\u800c\u6539\u5584\uff0c\u4f46\u6e90\u7aef\u7b80\u5316\u4f1a\u635f\u5bb3\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u5728\u9700\u8981\u6587\u5316\u654f\u611f\u6027\u7684\u4efb\u52a1\u4e0a\u4ecd\u9700\u66f4\u591a\u539f\u751f\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u56e0\u6570\u636e\u4e0d\u8db3\u96be\u4ee5\u8fdb\u884c\u5927\u89c4\u6a21\u5355\u8bed\u9884\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u4e4b\u5916\u7684\u53ef\u884c\u65b9\u6848\u3002", "method": "\u5c06\u82f1\u8bed\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u8f6c\u5316\u4e3a\u5370\u5c3c\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed\uff0c\u5e76\u4f7f\u7528\u539f\u59cb\u53ca\u7ecf\u5927\u8bed\u8a00\u6a21\u578b\u7b80\u5316\u7684\u82f1\u6587\u751f\u6210MT\u884d\u751f\u8bed\u6599\uff0c\u5bf9GPT-2\u6a21\u578b\uff08124M-774M\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8bc4\u4f30\u5176\u5728\u539f\u751f\u6587\u672c\u4e0a\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u3001\u53e5\u6cd5\u63a2\u9488\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "result": "MT\u884d\u751f\u6570\u636e\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u968f\u6a21\u578b\u5bb9\u91cf\u589e\u52a0\u800c\u8868\u73b0\u63d0\u5347\uff1b\u6e90\u7aef\u7b80\u5316\u635f\u5bb3\u5bf9\u539f\u751f\u6587\u672c\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u5728\u5c11\u91cf\u539f\u751f\u6570\u636e\u4e0a\u7ee7\u7eed\u8bad\u7ec3MT\u9884\u8bad\u7ec3\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u4ec5\u4f7f\u7528\u539f\u751f\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u5229\u7528MT\u751f\u6210\u7684\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u662f\u7a81\u7834\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u58c1\u5792\u7684\u6709\u6548\u9014\u5f84\uff0c\u5c24\u5176\u5728\u7ed3\u5408\u540e\u7eed\u539f\u751f\u6570\u636e\u5fae\u8c03\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5728\u6587\u5316\u76f8\u5173\u4efb\u52a1\u4e0a\u4ecd\u4f9d\u8d56\u8db3\u591f\u539f\u751f\u6570\u636e\u3002"}}
{"id": "2509.17228", "categories": ["cs.LG", "cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.17228", "abs": "https://arxiv.org/abs/2509.17228", "authors": ["Zihan Liang", "Ziwen Pan", "Ruoxuan Xiong"], "title": "Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness", "comment": "To appear in Proc. of EMNLP 2025 (18 pages)", "summary": "Clinical notes contain rich patient information, such as diagnoses or\nmedications, making them valuable for patient representation learning. Recent\nadvances in large language models have further improved the ability to extract\nmeaningful representations from clinical texts. However, clinical notes are\noften missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of\npatients have no available discharge summaries. In such cases, representations\ncan be learned from other modalities such as structured data, chest X-rays, or\nradiology reports. Yet the availability of these modalities is influenced by\nclinical decision-making and varies across patients, resulting in modality\nmissing-not-at-random (MMNAR) patterns. We propose a causal representation\nlearning framework that leverages observed data and informative missingness in\nmultimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion\ncomponent that integrates structured data, imaging, and text while conditioning\non missingness patterns to capture patient health and clinician-driven\nassignment; (2) a modality reconstruction component with contrastive learning\nto ensure semantic sufficiency in representation learning; and (3) a multitask\noutcome prediction model with a rectifier that corrects for residual bias from\nspecific modality observation patterns. Comprehensive evaluations across\nMIMIC-IV and eICU show consistent gains over the strongest baselines, achieving\nup to 13.8% AUC improvement for hospital readmission and 13.1% for ICU\nadmission.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u4e34\u5e8a\u8bb0\u5f55\u4e2d\u7684\u89c2\u6d4b\u6570\u636e\u548c\u7f3a\u5931\u6a21\u5f0f\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u60a3\u8005\u8868\u793a\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u7b49\u591a\u6a21\u6001\u6570\u636e\u5e38\u5b58\u5728\u975e\u968f\u673a\u7f3a\u5931\u95ee\u9898\uff0c\u5f71\u54cd\u60a3\u8005\u8868\u793a\u5b66\u4e60\u7684\u51c6\u786e\u6027\uff0c\u9700\u8003\u8651\u7f3a\u5931\u673a\u5236\u5bf9\u8868\u5f81\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2aMMNAR\u611f\u77e5\u7684\u6a21\u6001\u878d\u5408\u7ec4\u4ef6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6570\u636e\u3001\u5f71\u50cf\u548c\u6587\u672c\uff0c\u5e76\u4ee5\u7f3a\u5931\u6a21\u5f0f\u4e3a\u6761\u4ef6\uff1b\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u6a21\u6001\u91cd\u5efa\uff1b\u6784\u5efa\u5e26\u6821\u6b63\u5668\u7684\u591a\u4efb\u52a1\u9884\u6d4b\u6a21\u578b\u4ee5\u7ea0\u6b63\u6b8b\u4f59\u504f\u5dee\u3002", "result": "\u5728MIMIC-IV\u548ceICU\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff0c\u5728\u518d\u5165\u9662\u9884\u6d4bAUC\u63d0\u5347\u8fbe13.8%\uff0cICU\u5165\u9662\u9884\u6d4b\u63d0\u534713.1%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5229\u7528\u7f3a\u5931\u6a21\u5f0f\u4e2d\u7684\u4fe1\u606f\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5177\u6709\u8f83\u5f3a\u9c81\u68d2\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.16888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16888", "abs": "https://arxiv.org/abs/2509.16888", "authors": ["Youwei Pang", "Xiaoqi Zhao", "Lihe Zhang", "Huchuan Lu", "Georges El Fakhri", "Xiaofeng Liu", "Shijian Lu"], "title": "Rethinking Evaluation of Infrared Small Target Detection", "comment": "NeurIPS 2025; Evaluation Toolkit:\n  https://github.com/lartpang/PyIRSTDMetrics", "summary": "As an essential vision task, infrared small target detection (IRSTD) has seen\nsignificant advancements through deep learning. However, critical limitations\nin current evaluation protocols impede further progress. First, existing\nmethods rely on fragmented pixel- and target-level specific metrics, which\nfails to provide a comprehensive view of model capabilities. Second, an\nexcessive emphasis on overall performance scores obscures crucial error\nanalysis, which is vital for identifying failure modes and improving real-world\nsystem performance. Third, the field predominantly adopts dataset-specific\ntraining-testing paradigms, hindering the understanding of model robustness and\ngeneralization across diverse infrared scenarios. This paper addresses these\nissues by introducing a hybrid-level metric incorporating pixel- and\ntarget-level performance, proposing a systematic error analysis method, and\nemphasizing the importance of cross-dataset evaluation. These aim to offer a\nmore thorough and rational hierarchical analysis framework, ultimately\nfostering the development of more effective and robust IRSTD models. An\nopen-source toolkit has be released to facilitate standardized benchmarking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff08IRSTD\uff09\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u50cf\u7d20\u7ea7\u548c\u76ee\u6807\u7ea7\u7684\u6df7\u5408\u5ea6\u91cf\u6307\u6807\uff0c\u7cfb\u7edf\u5316\u7684\u8bef\u5dee\u5206\u6790\u65b9\u6cd5\u4ee5\u53ca\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u6709\u6548\u548c\u9c81\u68d2\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90\u5de5\u5177\u5305\u652f\u6301\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u788e\u7247\u5316\u6307\u6807\u3001\u8fc7\u5ea6\u5173\u6ce8\u603b\u4f53\u6027\u80fd\u800c\u5ffd\u89c6\u8bef\u5dee\u5206\u6790\u3001\u4ee5\u53ca\u7f3a\u4e4f\u8de8\u6570\u636e\u96c6\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86IRSTD\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u878d\u5408\u50cf\u7d20\u7ea7\u548c\u76ee\u6807\u7ea7\u6027\u80fd\u7684\u6df7\u5408\u5c42\u6b21\u8bc4\u4f30\u6307\u6807\uff0c\u63d0\u51fa\u7cfb\u7edf\u6027\u8bef\u5dee\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u8de8\u6570\u636e\u96c6\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u5408\u7406\u7684\u5206\u5c42\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u63ed\u793a\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\u5e76\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u8303\u5f0f\u6709\u52a9\u4e8e\u63a8\u52a8IRSTD\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u4e0e\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u5f00\u653e\u7684\u5de5\u5177\u5305\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u7814\u7a76\u3002"}}
{"id": "2509.18084", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18084", "abs": "https://arxiv.org/abs/2509.18084", "authors": ["Jiawen Tian", "Liqun Huang", "Zhongren Cui", "Jingchao Qiao", "Jiafeng Xu", "Xiao Ma", "Zeyu Ren"], "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces", "comment": "Tech Report.13 pages, 9 figures. Project page:\n  https://bytewrist.github.io/", "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aByteWrist\u7684\u65b0\u578b\u9ad8\u7075\u6d3b\u6027\u3001\u4eff\u4eba\u5e76\u8054\u8155\u90e8\u673a\u6784\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u4e09\u9636\u6bb5\u5e76\u8054\u9a71\u52a8\u673a\u5236\u548c\u5f27\u5f62\u672b\u7aef\u8fde\u6746\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e32\u5e76\u8054\u8155\u90e8\u5728\u72ed\u5c0f\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u8155\u90e8\u5728\u72ed\u5c0f\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u5b58\u5728\u4f53\u79ef\u5927\u3001\u7075\u6d3b\u6027\u4e0e\u521a\u5ea6\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bb6\u5ead\u670d\u52a1\u3001\u533b\u7597\u8f85\u52a9\u7b49\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5305\u542b\u4e09\u9636\u6bb5\u7535\u673a\u9a71\u52a8\u5d4c\u5957\u8fde\u6746\u3001\u5f27\u5f62\u672b\u7aef\u8fde\u6746\u548c\u4e2d\u5fc3\u7403\u652f\u6491\u7ed3\u6784\u7684\u5e76\u8054\u8155\u90e8\uff0c\u5e76\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5305\u62ec\u6b63/\u9006\u8fd0\u52a8\u5b66\u53ca\u6570\u503c\u96c5\u53ef\u6bd4\u77e9\u9635\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cByteWrist\u5728\u72ed\u5c0f\u7a7a\u95f4\u673a\u52a8\u6027\u548c\u53cc\u81c2\u534f\u540c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4Kinova\u7cfb\u7edf\u5177\u6709\u66f4\u9ad8\u7684\u7d27\u51d1\u6027\u3001\u6548\u7387\u548c\u7ed3\u6784\u521a\u5ea6\u3002", "conclusion": "ByteWrist\u901a\u8fc7\u521b\u65b0\u7684\u5e76\u8054\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u7075\u6d3b\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7d27\u51d1\u6027\u548c\u521a\u5ea6\uff0c\u4e3a\u53d7\u9650\u73af\u5883\u4e0b\u7684\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17348", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17348", "abs": "https://arxiv.org/abs/2509.17348", "authors": ["Yujie Feng", "Jian Li", "Xiaoyu Dong", "Pengfei Xu", "Xiaohui Zhou", "Yujia Zhang", "Zexin LU", "Yasha Wang", "Alan Zhao", "Xu Chu", "Xiao-Ming Wu"], "title": "AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning", "comment": "EMNLP 2025", "summary": "Continual learning (CL) is essential for deploying large language models\n(LLMs) in dynamic real-world environments without the need for costly\nretraining. Recent model merging-based methods have attracted significant\nattention, but they still struggle to effectively manage the trade-off between\nlearning new knowledge and preventing forgetting, a challenge largely stemming\nfrom suboptimal number of merges and merging frequency. In this paper, we\nintroduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework\nthat utilizes learning and forgetting signals from the training trajectory to\ndynamically monitor the model's training status. Guided by dynamic monitoring,\nthe training trajectory-guided merge controller adaptively determines the\ntiming and frequency of iterative fusion, while the rehearsal-based knowledge\nfusion module computes the merging weights and executes the fusion.\nComprehensive experiments on three CL benchmarks with various model sizes (from\n770M to 13B) demonstrate that AimMerging achieves significant performance\nimprovements over existing state-of-the-art methods, with an average relative\nimprovement of 80% and 59% on FWT and BWT, respectively. The source code is\nprovided for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AimMerging\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u8f68\u8ff9\u52a8\u6001\u76d1\u63a7\u7684\u81ea\u9002\u5e94\u8fed\u4ee3\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5b66\u4e60\u65b0\u77e5\u8bc6\u4e0e\u9632\u6b62\u9057\u5fd8\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u96be\u4ee5\u6709\u6548\u5e73\u8861\u5b66\u4e60\u65b0\u77e5\u8bc6\u548c\u9632\u6b62\u9057\u5fd8\uff0c\u4e3b\u8981\u7531\u4e8e\u5408\u5e76\u6b21\u6570\u548c\u9891\u7387\u4e0d\u591f\u4f18\u5316\u3002", "method": "\u63d0\u51faAimMerging\u6846\u67b6\uff0c\u5229\u7528\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u4e0e\u9057\u5fd8\u4fe1\u53f7\u52a8\u6001\u76d1\u63a7\u6a21\u578b\u72b6\u6001\uff0c\u5e76\u7531\u63a7\u5236\u5668\u81ea\u9002\u5e94\u51b3\u5b9a\u878d\u5408\u65f6\u673a\u4e0e\u9891\u7387\uff0c\u7ed3\u5408\u57fa\u4e8e\u56de\u653e\u7684\u77e5\u8bc6\u878d\u5408\u6a21\u5757\u8ba1\u7b97\u878d\u5408\u6743\u91cd\u5e76\u6267\u884c\u878d\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u4e0a\u3001\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u4e0b\uff08770M\u523013B\uff09\uff0cAimMerging\u5728\u524d\u5411\u8fc1\u79fb\uff08FWT\uff09\u548c\u540e\u5411\u8fc1\u79fb\uff08BWT\uff09\u4e0a\u5206\u522b\u5e73\u5747\u76f8\u5bf9\u63d0\u534780%\u548c59%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "AimMerging\u901a\u8fc7\u8bad\u7ec3\u8f68\u8ff9\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2509.17235", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17235", "abs": "https://arxiv.org/abs/2509.17235", "authors": ["Jiazhen Chen", "Mingbin Feng", "Tony S. Wirjanto"], "title": "Prospective Multi-Graph Cohesion for Multivariate Time Series Anomaly Detection", "comment": "Accepted by the 18th ACM International Conference on Web Search and\n  Data Mining (ACM WSDM 2025)", "summary": "Anomaly detection in high-dimensional time series data is pivotal for\nnumerous industrial applications. Recent advances in multivariate time series\nanomaly detection (TSAD) have increasingly leveraged graph structures to model\ninter-variable relationships, typically employing Graph Neural Networks (GNNs).\nDespite their promising results, existing methods often rely on a single graph\nrepresentation, which are insufficient for capturing the complex, diverse\nrelationships inherent in multivariate time series. To address this, we propose\nthe Prospective Multi-Graph Cohesion (PMGC) framework for multivariate TSAD.\nPMGC exploits spatial correlations by integrating a long-term static graph with\na series of short-term instance-wise dynamic graphs, regulated through a graph\ncohesion loss function. Our theoretical analysis shows that this loss function\npromotes diversity among dynamic graphs while aligning them with the stable\nlong-term relationships encapsulated by the static graph. Additionally, we\nintroduce a \"prospective graphing\" strategy to mitigate the limitations of\ntraditional forecasting-based TSAD methods, which often struggle with\nunpredictable future variations. This strategy allows the model to accurately\nreflect concurrent inter-series relationships under normal conditions, thereby\nenhancing anomaly detection efficacy. Empirical evaluations on real-world\ndatasets demonstrate the superior performance of our method compared to\nexisting TSAD techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u56fe\u878d\u5408\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6PMGC\uff0c\u7ed3\u5408\u9759\u6001\u56fe\u548c\u52a8\u6001\u56fe\u5efa\u6a21\u53d8\u91cf\u95f4\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u524d\u77bb\u6027\u56fe\u7b56\u7565\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u56fe\u7ed3\u6784\uff0c\u96be\u4ee5\u6355\u6349\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u590d\u6742\u591a\u6837\u7684\u53d8\u91cf\u5173\u7cfb\uff0c\u4e14\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u5bf9\u4e0d\u53ef\u9884\u77e5\u7684\u672a\u6765\u53d8\u5316\u9002\u5e94\u80fd\u529b\u5dee\u3002", "method": "PMGC\u6846\u67b6\u878d\u5408\u957f\u671f\u9759\u6001\u56fe\u548c\u77ed\u671f\u5b9e\u4f8b\u52a8\u6001\u56fe\uff0c\u901a\u8fc7\u56fe\u51dd\u805a\u635f\u5931\u51fd\u6570\u8c03\u8282\uff0c\u5f15\u5165\u524d\u77bb\u6027\u56fe\u7b56\u7565\u4ee5\u53cd\u6620\u6b63\u5e38\u60c5\u51b5\u4e0b\u7684\u5b9e\u65f6\u53d8\u91cf\u5173\u7cfb\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPMGC\u5728\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684TSAD\u65b9\u6cd5\u3002", "conclusion": "PMGC\u80fd\u66f4\u6709\u6548\u5730\u5efa\u6a21\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.16892", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16892", "abs": "https://arxiv.org/abs/2509.16892", "authors": ["Jiahe Qian", "Yaoyu Fang", "Ziqiao Weng", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "title": "Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning", "comment": "9 pages, 3 figures", "summary": "Spatial transcriptomics aims to connect high-resolution histology images with\nspatially resolved gene expression. To achieve better performance on downstream\ntasks such as gene expression prediction, large-scale pre-training is required\nto obtain generalisable representations that can bridge histology and\ntranscriptomics across tissues, protocols, and laboratories. Existing\ncross-modal pre-training approaches for spatial transcriptomics rely on either\ngene names or expression values in isolation, which strips the gene branch of\nessential semantics and breaks the association between each gene and its\nquantitative magnitude. In addition, by restricting supervision to image-text\nalignment, these methods ignore intrinsic visual cues that are critical for\nlearning robust image features. We present CoMTIP, the first Contrastive Masked\nText-Image Pretraining framework that jointly learns from images, gene names,\nand expression values while capturing fine-grained visual context for spatial\ntranscriptomics. The vision branch uses Masked Feature Modeling to reconstruct\noccluded patches and learn context-aware image embeddings. The text branch\napplies a scalable Gene-Text Encoder that processes all gene sentences in\nparallel, enriches each gene and its numerical value with dedicated embeddings,\nand employs Pair-aware Adversarial Training (PAAT) to preserve correct\ngene-value associations. Image and text representations are aligned in a shared\nInfoNCE-optimised space. Experiments on public spatial transcriptomics datasets\nshow that CoMTIP not only surpasses previous methods on diverse downstream\ntasks but also achieves zero-shot gene expression prediction, a capability that\nexisting approaches do not provide.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CoMTIP\uff0c\u9996\u4e2a\u7ed3\u5408\u56fe\u50cf\u3001\u57fa\u56e0\u540d\u79f0\u548c\u8868\u8fbe\u503c\u8fdb\u884c\u5bf9\u6bd4\u63a9\u7801\u6587\u672c-\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u5728\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u548c\u96f6\u6837\u672c\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u5355\u72ec\u4f7f\u7528\u57fa\u56e0\u540d\u79f0\u6216\u8868\u8fbe\u503c\uff0c\u5ffd\u7565\u4e86\u57fa\u56e0\u8bed\u4e49\u4e0e\u5b9a\u91cf\u5e45\u5ea6\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u4e14\u4ec5\u4f9d\u8d56\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u800c\u5ffd\u89c6\u4e86\u5173\u952e\u7684\u5185\u5728\u89c6\u89c9\u7ebf\u7d22\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u6355\u6349\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCoMTIP\u6846\u67b6\uff1a\u89c6\u89c9\u5206\u652f\u91c7\u7528\u63a9\u7801\u7279\u5f81\u5efa\u6a21\u6765\u91cd\u5efa\u88ab\u906e\u6321\u56fe\u50cf\u5757\u5e76\u5b66\u4e60\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u56fe\u50cf\u5d4c\u5165\uff1b\u6587\u672c\u5206\u652f\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u57fa\u56e0-\u6587\u672c\u7f16\u7801\u5668\uff0c\u5e76\u884c\u5904\u7406\u6240\u6709\u57fa\u56e0\u53e5\u5b50\uff0c\u4e3a\u6bcf\u4e2a\u57fa\u56e0\u53ca\u5176\u6570\u503c\u6dfb\u52a0\u4e13\u7528\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u6210\u5bf9\u611f\u77e5\u5bf9\u6297\u8bad\u7ec3\uff08PAAT\uff09\u4fdd\u6301\u57fa\u56e0-\u503c\u7684\u6b63\u786e\u5173\u8054\uff1b\u56fe\u50cf\u4e0e\u6587\u672c\u8868\u5f81\u5728\u5171\u4eab\u7684InfoNCE\u4f18\u5316\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u3002", "result": "\u5728\u516c\u5171\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoMTIP\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u3002", "conclusion": "CoMTIP\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u56fe\u50cf\u3001\u57fa\u56e0\u540d\u79f0\u548c\u8868\u8fbe\u503c\uff0c\u5e76\u5f15\u5165\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4e0e\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u4e2d\u7684\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6548\u679c\uff0c\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17349", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17349", "abs": "https://arxiv.org/abs/2509.17349", "authors": ["Peter Pol\u00e1k", "Sara Papi", "Luisa Bentivogli", "Ond\u0159ej Bojar"], "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation", "comment": null, "summary": "Simultaneous speech-to-text translation (SimulST) systems have to balance\ntranslation quality with latency--the delay between speech input and the\ntranslated output. While quality evaluation is well established, accurate\nlatency measurement remains a challenge. Existing metrics often produce\ninconsistent or misleading results, especially in the widely used short-form\nsetting, where speech is artificially presegmented. In this paper, we present\nthe first comprehensive analysis of SimulST latency metrics across language\npairs, systems, and both short- and long-form regimes. We uncover a structural\nbias in current metrics related to segmentation that undermines fair and\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\nLagging), a refined latency metric that delivers more accurate evaluations in\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\npropose SoftSegmenter, a novel resegmentation tool based on word-level\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\nevaluation, together enabling more reliable assessments of SimulST systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684SimulST\u5ef6\u8fdf\u5ea6\u91cfYAAL\u548cLongYAAL\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8bcd\u7ea7\u5bf9\u9f50\u7684\u91cd\u5206\u6bb5\u5de5\u5177SoftSegmenter\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5ef6\u8fdf\u5ea6\u91cf\u5728\u77ed\u6587\u672c\u548c\u957f\u6587\u672c\u573a\u666f\u4e0b\u7684\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684SimulST\u5ef6\u8fdf\u5ea6\u91cf\u5728\u77ed\u6587\u672c\u8bbe\u7f6e\u4e0b\u5b58\u5728\u7ed3\u6784\u6027\u504f\u5dee\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4e00\u81f4\u6216\u8bef\u5bfc\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u8bed\u8a00\u5bf9\u3001\u7cfb\u7edf\u53ca\u957f\u77ed\u6587\u672c\u573a\u666f\u4e0b\u5ef6\u8fdf\u5ea6\u91cf\u7684\u5168\u9762\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u5ef6\u8fdf\u5ea6\u91cf\u7684\u7ed3\u6784\u6027\u504f\u5dee\uff0c\u63d0\u51fa\u6539\u8fdb\u7684YAAL\uff08\u9002\u7528\u4e8e\u77ed\u6587\u672c\uff09\u548cLongYAAL\uff08\u9002\u7528\u4e8e\u957f\u6587\u672c\u672a\u5206\u6bb5\u97f3\u9891\uff09\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u8bcd\u7ea7\u5bf9\u9f50\u7684SoftSegmenter\u91cd\u5206\u6bb5\u5de5\u5177\u4ee5\u63d0\u5347\u957f\u6587\u672c\u8bc4\u4f30\u7684\u5bf9\u9f50\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cYAAL\u548cLongYAAL\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u6d41\u884c\u7684\u5ef6\u8fdf\u5ea6\u91cf\u6307\u6807\uff0cSoftSegmenter\u663e\u8457\u63d0\u5347\u957f\u6587\u672c\u8bc4\u4f30\u4e2d\u7684\u5bf9\u9f50\u8d28\u91cf\uff0c\u6574\u4f53\u5b9e\u73b0\u66f4\u53ef\u9760\u7684SimulST\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "YAAL\u548cLongYAAL\u80fd\u66f4\u51c6\u786e\u5730\u8861\u91cfSimulST\u7cfb\u7edf\u7684\u5ef6\u8fdf\uff0cSoftSegmenter\u6709\u6548\u6539\u5584\u957f\u6587\u672c\u4e0b\u7684\u5206\u6bb5\u4e0e\u5bf9\u9f50\uff0c\u4e09\u8005\u7ed3\u5408\u4e3aSimulST\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u3001\u4e00\u81f4\u7684\u8bc4\u4f30\u65b9\u6848\u3002"}}
{"id": "2509.17241", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17241", "abs": "https://arxiv.org/abs/2509.17241", "authors": ["Ali Faraji", "Manos Papagelis"], "title": "TraceHiding: Scalable Machine Unlearning for Mobility Data", "comment": null, "summary": "This work introduces TraceHiding, a scalable, importance-aware machine\nunlearning framework for mobility trajectory data. Motivated by privacy\nregulations such as GDPR and CCPA granting users \"the right to be forgotten,\"\nTraceHiding removes specified user trajectories from trained deep models\nwithout full retraining. It combines a hierarchical data-driven importance\nscoring scheme with teacher-student distillation. Importance scores--computed\nat token, trajectory, and user levels from statistical properties (coverage\ndiversity, entropy, length)--quantify each training sample's impact, enabling\ntargeted forgetting of high-impact data while preserving common patterns. The\nstudent model retains knowledge on remaining data and unlearns targeted\ntrajectories through an importance-weighted loss that amplifies forgetting\nsignals for unique samples and attenuates them for frequent ones. We validate\non Trajectory--User Linking (TUL) tasks across three real-world higher-order\nmobility datasets (HO-Rome, HO-Geolife, HO-NYC) and multiple architectures\n(GRU, LSTM, BERT, ModernBERT, GCN-TULHOR), against strong unlearning baselines\nincluding SCRUB, NegGrad, NegGrad+, Bad-T, and Finetuning. Experiments under\nuniform and targeted user deletion show TraceHiding, especially its\nentropy-based variant, achieves superior unlearning accuracy, competitive\nmembership inference attack (MIA) resilience, and up to 40\\times speedup over\nretraining with minimal test accuracy loss. Results highlight robustness to\nadversarial deletion of high-information users and consistent performance\nacross models. To our knowledge, this is the first systematic study of machine\nunlearning for trajectory data, providing a reproducible pipeline with public\ncode and preprocessing tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TraceHiding\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u57fa\u4e8e\u91cd\u8981\u6027\u611f\u77e5\u7684\u8f68\u8ff9\u6570\u636e\u673a\u5668\u9057\u5fd8\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u91cd\u8981\u6027\u8bc4\u5206\u4e0e\u5e08\u751f\u84b8\u998f\u673a\u5236\uff0c\u5728\u65e0\u9700\u5b8c\u5168\u91cd\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u79fb\u9664\u7528\u6237\u8f68\u8ff9\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u5177\u6709\u4f18\u8d8a\u7684\u9057\u5fd8\u7cbe\u5ea6\u3001\u6297\u6210\u5458\u63a8\u65ad\u653b\u51fb\u80fd\u529b\u53ca\u663e\u8457\u901f\u5ea6\u4f18\u52bf\u3002", "motivation": "\u53d7GDPR\u548cCCPA\u7b49\u9690\u79c1\u6cd5\u89c4\u4e2d\u201c\u88ab\u9057\u5fd8\u6743\u201d\u7684\u9a71\u52a8\uff0c\u9700\u4ece\u5df2\u8bad\u7ec3\u6a21\u578b\u4e2d\u5220\u9664\u7279\u5b9a\u7528\u6237\u8f68\u8ff9\u6570\u636e\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faTraceHiding\u6846\u67b6\uff1a1\uff09\u8bbe\u8ba1\u57fa\u4e8e\u8986\u76d6\u591a\u6837\u6027\u3001\u71b5\u548c\u957f\u5ea6\u7684\u5c42\u7ea7\uff08token\u3001\u8f68\u8ff9\u3001\u7528\u6237\uff09\u91cd\u8981\u6027\u8bc4\u5206\u673a\u5236\uff1b2\uff09\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u84b8\u998f\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u52a0\u6743\u635f\u5931\u51fd\u6570\u589e\u5f3a\u5bf9\u72ec\u7279\u6837\u672c\u7684\u9057\u5fd8\u4fe1\u53f7\u3001\u51cf\u5f31\u5bf9\u5e38\u89c1\u6837\u672c\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u9ad8\u9636\u79fb\u52a8\u6570\u636e\u96c6\uff08HO-Rome, HO-Geolife, HO-NYC\uff09\u548c\u591a\u79cd\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cTraceHiding\uff08\u5c24\u5176\u662f\u57fa\u4e8e\u71b5\u7684\u53d8\u4f53\uff09\u76f8\u6bd4SCRUB\u3001NegGrad\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5747\u5300\u548c\u9488\u5bf9\u6027\u5220\u9664\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u9057\u5fd8\u51c6\u786e\u6027\u3001\u8f83\u5f3a\u7684MIA\u9632\u5fa1\u80fd\u529b\uff0c\u5e76\u6bd4\u91cd\u65b0\u8bad\u7ec3\u5feb\u8fbe40\u500d\u4e14\u6d4b\u8bd5\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "TraceHiding\u662f\u9996\u4e2a\u9488\u5bf9\u8f68\u8ff9\u6570\u636e\u7684\u7cfb\u7edf\u6027\u673a\u5668\u9057\u5fd8\u7814\u7a76\uff0c\u6709\u6548\u5e73\u8861\u4e86\u9057\u5fd8\u6548\u679c\u3001\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u5177\u5907\u826f\u597d\u9c81\u68d2\u6027\u548c\u8de8\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u4e3a\u8f68\u8ff9\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.16897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16897", "abs": "https://arxiv.org/abs/2509.16897", "authors": ["Xuewan He", "Jielei Wang", "Zihan Cheng", "Yuchen Su", "Shiyue Huang", "Guoming Lu"], "title": "PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion", "comment": null, "summary": "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to\na student without access to the real in-distribution (ID) data. While existing\nmethods perform well on small-scale images, they suffer from mode collapse when\nsynthesizing large-scale images, resulting in limited knowledge transfer.\nRecently, leveraging advanced generative models to synthesize photorealistic\nimages has emerged as a promising alternative. Nevertheless, directly using\noff-the-shelf diffusion to generate datasets faces the precision-recall\nchallenges: 1) ensuring synthetic data aligns with the real distribution, and\n2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a\nprecision-recall informed synthesis method. Specifically, we introduce\nEnergy-guided Distribution Alignment to avoid the generation of\nout-of-distribution samples, and design the Diversified Prompt Engineering to\nenhance coverage of the real ID manifold. Extensive experiments on various\nlarge-scale image datasets demonstrate the superiority of PRISM. Moreover, we\ndemonstrate that models trained with PRISM exhibit strong domain\ngeneralization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRISM\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u80fd\u91cf\u5f15\u5bfc\u5206\u5e03\u5bf9\u9f50\u548c\u591a\u6837\u5316\u63d0\u793a\u5de5\u7a0b\u89e3\u51b3\u751f\u6210\u56fe\u50cf\u65f6\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u4e0a\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u4e14\u751f\u6210\u56fe\u50cf\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5206\u5e03\u5bf9\u9f50\u548c\u8986\u76d6\u771f\u5b9e\u6570\u636e\u6d41\u5f62\u3002", "method": "\u63d0\u51faPRISM\uff0c\u5305\u62ec\u80fd\u91cf\u5f15\u5bfc\u5206\u5e03\u5bf9\u9f50\u4ee5\u907f\u514d\u751f\u6210\u5206\u5e03\u5916\u6837\u672c\uff0c\u4ee5\u53ca\u591a\u6837\u5316\u63d0\u793a\u5de5\u7a0b\u4ee5\u589e\u5f3a\u5bf9\u771f\u5b9e\u6570\u636e\u6d41\u5f62\u7684\u8986\u76d6\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660ePRISM\u6027\u80fd\u4f18\u8d8a\uff0c\u5e76\u663e\u793a\u51fa\u5f3a\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PRISM\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u4e2d\u751f\u6210\u56fe\u50cf\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u77db\u76fe\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002"}}
{"id": "2509.17367", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17367", "abs": "https://arxiv.org/abs/2509.17367", "authors": ["Haoyang Chen", "Kumiko Tanaka-Ishii"], "title": "Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs", "comment": "to be published in Text, Speech, and Dialogue (TSD 2025)", "summary": "We present a comparative analysis of text complexity across domains using\nscale-free metrics. We quantify linguistic complexity via Heaps' exponent\n$\\beta$ (vocabulary growth), Taylor's exponent $\\alpha$ (word-frequency\nfluctuation scaling), compression rate $r$ (redundancy), and entropy. Our\ncorpora span three domains: legal documents (statutes, cases, deeds) as a\nspecialized domain, general natural language texts (literature, Wikipedia), and\nAI-generated (GPT) text. We find that legal texts exhibit slower vocabulary\ngrowth (lower $\\beta$) and higher term consistency (higher $\\alpha$) than\ngeneral texts. Within legal domain, statutory codes have the lowest $\\beta$ and\nhighest $\\alpha$, reflecting strict drafting conventions, while cases and deeds\nshow higher $\\beta$ and lower $\\alpha$. In contrast, GPT-generated text shows\nthe statistics more aligning with general language patterns. These results\ndemonstrate that legal texts exhibit domain-specific structures and\ncomplexities, which current generative models do not fully replicate.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u65e0\u6807\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u6bd4\u8f83\u4e86\u4e0d\u540c\u9886\u57df\u6587\u672c\u7684\u590d\u6742\u6027\uff0c\u53d1\u73b0\u6cd5\u5f8b\u6587\u672c\u5177\u6709\u8f83\u6162\u7684\u8bcd\u6c47\u589e\u957f\u548c\u8f83\u9ad8\u7684\u672f\u8bed\u4e00\u81f4\u6027\uff0c\u800cAI\u751f\u6210\u7684\u6587\u672c\u5219\u66f4\u63a5\u8fd1\u666e\u901a\u8bed\u8a00\u6a21\u5f0f\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u9886\u57df\u6587\u672c\uff08\u7279\u522b\u662f\u6cd5\u5f8b\u6587\u672c\uff09\u7684\u8bed\u8a00\u590d\u6742\u6027\u7279\u5f81\uff0c\u8bc4\u4f30\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728\u590d\u5236\u8fd9\u4e9b\u7279\u5f81\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528Heaps\u6307\u6570\u03b2\u3001Taylor\u6307\u6570\u03b1\u3001\u538b\u7f29\u7387r\u548c\u71b5\u7b49\u6307\u6807\uff0c\u5bf9\u6cd5\u5f8b\u6587\u672c\u3001\u901a\u7528\u81ea\u7136\u8bed\u8a00\u6587\u672c\u548cAI\u751f\u6210\u6587\u672c\u8fdb\u884c\u91cf\u5316\u5206\u6790\u3002", "result": "\u6cd5\u5f8b\u6587\u672c\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u03b2\u503c\u548c\u8f83\u9ad8\u7684\u03b1\u503c\uff0c\u5176\u4e2d\u6cd5\u89c4\u6761\u6587\u6700\u4e3a\u663e\u8457\uff1bAI\u751f\u6210\u6587\u672c\u7684\u8bed\u8a00\u7edf\u8ba1\u7279\u5f81\u4e0e\u901a\u7528\u6587\u672c\u66f4\u4e3a\u63a5\u8fd1\u3002", "conclusion": "\u6cd5\u5f8b\u6587\u672c\u5177\u6709\u9886\u57df\u7279\u5b9a\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\uff0c\u5f53\u524d\u7684\u751f\u6210\u5f0f\u6a21\u578b\u5c1a\u672a\u5b8c\u5168\u590d\u5236\u8fd9\u4e9b\u7279\u5f81\u3002"}}
{"id": "2509.17250", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17250", "abs": "https://arxiv.org/abs/2509.17250", "authors": ["Yigit Berkay Uslu", "Samar Hadou", "Sergio Rozada", "Shirin Saeedi Bidokhti", "Alejandro Ribeiro"], "title": "Graph Signal Generative Diffusion Models", "comment": "Submitted to 2026 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2026)", "summary": "We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for\nstochastic graph signal generation using denoising diffusion processes. The\narchitecture learns node features at different resolutions with skip\nconnections between the encoder and decoder paths, analogous to the\nconvolutional U-Net for image generation. The U-GNN is prominent for a pooling\noperation that leverages zero-padding and avoids arbitrary graph coarsening,\nwith graph convolutions layered on top to capture local dependencies. This\ntechnique permits learning feature embeddings for sampled nodes at deeper\nlevels of the architecture that remain convolutional with respect to the\noriginal graph. Applied to stock price prediction -- where deterministic\nforecasts struggle to capture uncertainties and tail events that are paramount\n-- we demonstrate the effectiveness of the diffusion model in probabilistic\nforecasting of stock prices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u968f\u673a\u56fe\u4fe1\u53f7\u751f\u6210\u7684U\u5f62\u7f16\u7801\u5668-\u89e3\u7801\u5668\u56fe\u795e\u7ecf\u7f51\u7edc\uff08U-GNN\uff09\uff0c\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u5728\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6982\u7387\u9884\u6d4b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u91d1\u878d\u5e02\u573a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6781\u7aef\u4e8b\u4ef6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fdb\u884c\u6982\u7387\u9884\u6d4b\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1U-GNN\u67b6\u6784\uff0c\u7ed3\u5408\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u4e0e\u8df3\u8dc3\u8fde\u63a5\uff0c\u91c7\u7528\u96f6\u586b\u5145\u6c60\u5316\u64cd\u4f5c\u907f\u514d\u4efb\u610f\u56fe\u7c97\u5316\uff0c\u5e76\u5728\u591a\u5206\u8fa8\u7387\u4e0b\u5b66\u4e60\u8282\u70b9\u7279\u5f81\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u6355\u83b7\u5c40\u90e8\u4f9d\u8d56\u3002", "result": "U-GNN\u80fd\u591f\u5728\u4e0d\u540c\u6df1\u5ea6\u4fdd\u6301\u5bf9\u539f\u59cb\u56fe\u7684\u5377\u79ef\u6027\uff0c\u5b66\u4e60\u91c7\u6837\u8282\u70b9\u7684\u7279\u5f81\u5d4c\u5165\uff0c\u5e76\u5728\u80a1\u7968\u4ef7\u683c\u7684\u6982\u7387\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "U-GNN\u7ed3\u5408\u6269\u6563\u6a21\u578b\uff0c\u5728\u56fe\u4fe1\u53f7\u751f\u6210\u548c\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u6982\u7387\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u4e0e\u5c3e\u90e8\u98ce\u9669\u7684\u573a\u666f\u3002"}}
{"id": "2509.16900", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16900", "abs": "https://arxiv.org/abs/2509.16900", "authors": ["Chengsheng Zhang", "Linhao Qu", "Xiaoyu Liu", "Zhijian Song"], "title": "ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis", "comment": null, "summary": "Survival analysis using whole-slide images (WSIs) is crucial in cancer\nresearch. Despite significant successes, pathology images typically only\nprovide slide-level labels, which hinders the learning of discriminative\nrepresentations from gigapixel WSIs. With the rapid advancement of\nhigh-throughput sequencing technologies, multimodal survival analysis\nintegrating pathology images and genomics data has emerged as a promising\napproach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures\ndiscriminative pathological and genomic features while enabling efficient\nintegration of both modalities. This approach achieves complementary\ninformation fusion without losing critical information from individual\nmodalities, thereby facilitating accurate cancer survival analysis.\nSpecifically, we first introduce a Pathology Expert and a Genomics Expert to\nprocess unimodal data separately. Both experts are designed with Mamba\narchitectures that incorporate conventional scanning and attention-based\nscanning mechanisms, allowing them to extract discriminative features from long\ninstance sequences containing substantial redundant or irrelevant information.\nSecond, we design a Synergistic Expert responsible for modality fusion. It\nexplicitly learns token-level local correspondences between the two modalities\nvia Optimal Transport, and implicitly enhances distribution consistency through\na global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused\nfeature representations are then passed to a mamba backbone for further\nintegration. Through the collaboration of the Pathology Expert, Genomics\nExpert, and Synergistic Expert, our method achieves stable and accurate\nsurvival analysis with relatively low computational complexity. Extensive\nexperimental results on five datasets in The Cancer Genome Atlas (TCGA)\ndemonstrate our state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aME-Mamba\u7684\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408\u75c5\u7406\u56fe\u50cf\u548c\u57fa\u56e0\u7ec4\u6570\u636e\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u4e0e\u878d\u5408\uff0c\u5728\u4e94\u4e2aTCGA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u75c5\u7406\u56fe\u50cf\u901a\u5e38\u53ea\u6709\u5207\u7247\u7ea7\u6807\u7b7e\uff0c\u96be\u4ee5\u4ece\u4e2d\u5b66\u4e60\u5224\u522b\u6027\u8868\u793a\uff1b\u540c\u65f6\uff0c\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u5f15\u5165\u4e3a\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u5982\u4f55\u6709\u6548\u878d\u5408\u4e24\u79cd\u6a21\u6001\u5e76\u4fdd\u7559\u5404\u81ea\u5173\u952e\u4fe1\u606f\u4ecd\u5177\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff1a\u75c5\u7406\u4e13\u5bb6\u548c\u57fa\u56e0\u7ec4\u4e13\u5bb6\u5206\u522b\u4f7f\u7528Mamba\u67b6\u6784\u5904\u7406\u5355\u6a21\u6001\u6570\u636e\uff0c\u5229\u7528\u626b\u63cf\u548c\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u957f\u5e8f\u5217\u4e2d\u7684\u5224\u522b\u7279\u5f81\uff1b\u534f\u540c\u4e13\u5bb6\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5b66\u4e60\u6a21\u6001\u95f4token\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\u7684\u5168\u5c40\u8de8\u6a21\u6001\u635f\u5931\u589e\u5f3a\u5206\u5e03\u4e00\u81f4\u6027\uff0c\u6700\u540e\u5c06\u878d\u5408\u7279\u5f81\u8f93\u5165Mamba\u4e3b\u5e72\u7f51\u7edc\u8fdb\u884c\u6574\u5408\u3002", "result": "\u5728TCGA\u7684\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u764c\u75c7\u751f\u5b58\u5206\u6790\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u4f4e\u3002", "conclusion": "ME-Mamba\u901a\u8fc7\u591a\u4e13\u5bb6\u534f\u540c\u673a\u5236\u6709\u6548\u5b9e\u73b0\u4e86\u75c5\u7406\u548c\u57fa\u56e0\u7ec4\u6570\u636e\u7684\u4e92\u8865\u878d\u5408\uff0c\u5728\u4e0d\u4e22\u5931\u5355\u6a21\u6001\u5173\u952e\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u4e86\u751f\u5b58\u9884\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u764c\u75c7\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u53ef\u9760\u7684\u6846\u67b6\u3002"}}
{"id": "2509.17377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17377", "abs": "https://arxiv.org/abs/2509.17377", "authors": ["Hannah Bansal", "Kemal Kurniawan", "Lea Frermann"], "title": "Robustness of Neurosymbolic Reasoners on First-Order Logic Problems", "comment": null, "summary": "Recent trends in NLP aim to improve reasoning capabilities in Large Language\nModels (LLMs), with key focus on generalization and robustness to variations in\ntasks. Counterfactual task variants introduce minimal but semantically\nmeaningful changes to otherwise valid first-order logic (FOL) problem instances\naltering a single predicate or swapping roles of constants to probe whether a\nreasoning system can maintain logical consistency under perturbation. Previous\nstudies showed that LLMs becomes brittle on counterfactual variations,\nsuggesting that they often rely on spurious surface patterns to generate\nresponses. In this work, we explore if a neurosymbolic (NS) approach that\nintegrates an LLM and a symbolic logical solver could mitigate this problem.\nExperiments across LLMs of varying sizes show that NS methods are more robust\nbut perform worse overall that purely neural methods. We then propose NSCoT\nthat combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate\nthat while it improves performance, NSCoT still lags behind standard CoT. Our\nanalysis opens research directions for future work.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u53cd\u4e8b\u5b9e\u4efb\u52a1\u53d8\u4f53\u7684\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5c3d\u7ba1\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u66f4\u7a33\u5065\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4ecd\u4f4e\u4e8e\u7eaf\u795e\u7ecf\u65b9\u6cd5\uff1b\u63d0\u51fa\u7684NSCoT\u7ed3\u5408\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u6027\u80fd\u6709\u6240\u63d0\u5347\u4f46\u4ecd\u4e0d\u53ca\u6807\u51c6CoT\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u4efb\u52a1\u53d8\u4f53\u4e0a\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u8868\u9762\u6a21\u5f0f\u800c\u975e\u771f\u6b63\u903b\u8f91\u63a8\u7406\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u903b\u8f91\u6c42\u89e3\u5668\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5e76\u63d0\u51faNSCoT\uff0c\u5c06\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4e0e\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u76f8\u7ed3\u5408\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u53cd\u4e8b\u5b9e\u53d8\u4f53\u4e0a\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4f4e\u4e8e\u7eaf\u795e\u7ecf\u65b9\u6cd5\uff1bNSCoT\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u4ecd\u843d\u540e\u4e8e\u6807\u51c6CoT\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u589e\u5f3a\u4e86\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f46\u5f53\u524d\u6574\u5408\u65b9\u5f0f\u5c1a\u4e0d\u8db3\u4ee5\u8d85\u8d8a\u7eaf\u795e\u7ecf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.17281", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17281", "abs": "https://arxiv.org/abs/2509.17281", "authors": ["Raisa Amiruddin", "Nikolay Y. Yordanov", "Nazanin Maleki", "Pascal Fehringer", "Athanasios Gkampenis", "Anastasia Janas", "Kiril Krantchev", "Ahmed Moawad", "Fabian Umeh", "Salma Abosabie", "Sara Abosabie", "Albara Alotaibi", "Mohamed Ghonim", "Mohanad Ghonim", "Sedra Abou Ali Mhana", "Nathan Page", "Marko Jakovljevic", "Yasaman Sharifi", "Prisha Bhatia", "Amirreza Manteghinejad", "Melisa Guelen", "Michael Veronesi", "Virginia Hill", "Tiffany So", "Mark Krycia", "Bojan Petrovic", "Fatima Memon", "Justin Cramer", "Elizabeth Schrickel", "Vilma Kosovic", "Lorenna Vidal", "Gerard Thompson", "Ichiro Ikuta", "Basimah Albalooshy", "Ali Nabavizadeh", "Nourel Hoda Tahon", "Karuna Shekdar", "Aashim Bhatia", "Claudia Kirsch", "Gennaro D'Anna", "Philipp Lohmann", "Amal Saleh Nour", "Andriy Myronenko", "Adam Goldman-Yassen", "Janet R. Reid", "Sanjay Aneja", "Spyridon Bakas", "Mariam Aboian"], "title": "Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challenge education platform", "comment": "23 pages, 9 figures, 1 table, 3 supplementary tables", "summary": "High-quality reference standard image data creation by neuroradiology experts\nfor automated clinical tools can be a powerful tool for neuroradiology &\nartificial intelligence education. We developed a multimodal educational\napproach for students and trainees during the MICCAI Brain Tumor Segmentation\nLighthouse Challenge 2025, a landmark initiative to develop accurate brain\ntumor segmentation algorithms. Fifty-six medical students & radiology trainees\nvolunteered to annotate brain tumor MR images for the BraTS challenges of 2023\n& 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56\nannotators, 14 select volunteers were then paired with neuroradiology faculty\nfor guided one-on-one annotation sessions for BraTS 2025. Lectures on\nneuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were\norganized online. Annotators & audience members completed surveys on their\nperceived knowledge before & after annotations & lectures respectively.\nFourteen coordinators, each paired with a neuroradiologist, completed the data\nannotation process, averaging 1322.9+/-760.7 hours per dataset per pair and\n1200 segmentations in total. On a scale of 1-10, annotation coordinators\nreported significant increase in familiarity with image segmentation software\npre- and post-annotation, moving from initial average of 6+/-2.9 to final\naverage of 8.9+/-1.1, and significant increase in familiarity with brain tumor\nfeatures pre- and post-annotation, moving from initial average of 6.2+/-2.4 to\nfinal average of 8.1+/-1.2. We demonstrate an innovative offering for providing\nneuroradiology & AI education through an image segmentation challenge to\nenhance understanding of algorithm development, reinforce the concept of data\nreference standard, and diversify opportunities for AI-driven image analysis\namong future physicians.", "AI": {"tldr": "\u901a\u8fc7\u53c2\u4e0e\u8111\u80bf\u7624\u5206\u5272\u6311\u6218\u8d5b\uff0c\u533b\u5b66\u751f\u548c\u653e\u5c04\u79d1\u57f9\u8bad\u4eba\u5458\u5728\u795e\u7ecf\u653e\u5c04\u5b66\u4e13\u5bb6\u6307\u5bfc\u4e0b\u8fdb\u884c\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u56fe\u50cf\u5206\u5272\u8f6f\u4ef6\u548c\u8111\u80bf\u7624\u7279\u5f81\u7684\u7406\u89e3\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u4e0e\u533b\u5b66\u6559\u80b2\u7684\u878d\u5408\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u672a\u6765\u533b\u751f\u5bf9\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u5f00\u53d1\u548c\u6570\u636e\u53c2\u8003\u6807\u51c6\u7684\u7406\u89e3\uff0c\u5e76\u62d3\u5c55AI\u9a71\u52a8\u5f71\u50cf\u5206\u6790\u7684\u5e94\u7528\u673a\u4f1a\uff0c\u63a2\u7d22\u4e00\u79cd\u521b\u65b0\u7684\u795e\u7ecf\u653e\u5c04\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u6559\u80b2\u6a21\u5f0f\u3002", "method": "\u7ec4\u7ec756\u540d\u533b\u5b66\u751f\u548c\u653e\u5c04\u79d1\u57f9\u8bad\u4eba\u5458\u53c2\u4e0eBraTS 2023\u548c2024\u7684\u8111\u80bf\u7624MRI\u56fe\u50cf\u6807\u6ce8\uff0c\u5176\u4e2d14\u540d\u5fd7\u613f\u8005\u4e0e\u795e\u7ecf\u653e\u5c04\u5b66\u4e13\u5bb6\u4e00\u5bf9\u4e00\u5408\u4f5c\u5b8c\u6210BraTS 2025\u7684\u6807\u6ce8\u4efb\u52a1\uff1b\u7ed3\u5408\u7ebf\u4e0a\u795e\u7ecf\u89e3\u5256\u3001\u75c5\u7406\u4e0eAI\u8bb2\u5ea7\u3001\u671f\u520a\u4ff1\u4e50\u90e8\u548c\u6570\u636e\u79d1\u5b66\u5bb6\u4e3b\u5bfc\u7684\u5de5\u4f5c\u574a\u8fdb\u884c\u591a\u6a21\u6001\u6559\u5b66\uff0c\u5e76\u901a\u8fc7\u524d\u540e\u95ee\u5377\u8c03\u67e5\u8bc4\u4f30\u77e5\u8bc6\u638c\u63e1\u60c5\u51b5\u3002", "result": "\u6bcf\u7ec4\u5e73\u5747\u8017\u65f61322.9\u00b1760.7\u5c0f\u65f6\u5b8c\u6210\u6570\u636e\u96c6\u6807\u6ce8\uff0c\u5171\u5b8c\u62101200\u6b21\u5206\u5272\uff1b\u6807\u6ce8\u534f\u8c03\u5458\u5bf9\u56fe\u50cf\u5206\u5272\u8f6f\u4ef6\u7684\u719f\u6089\u5ea6\u4ece6\u00b12.9\u63d0\u5347\u81f38.9\u00b11.1\uff08\u6ee1\u520610\uff09\uff0c\u5bf9\u8111\u80bf\u7624\u7279\u5f81\u7684\u719f\u6089\u5ea6\u4ece6.2\u00b12.4\u63d0\u5347\u81f38.1\u00b11.2\uff0c\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u5206\u5272\u6311\u6218\u7684\u591a\u6a21\u6001\u6559\u80b2\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5b66\u5458\u5bf9\u795e\u7ecf\u653e\u5c04\u5b66\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u7406\u89e3\u4e0e\u5b9e\u8df5\u80fd\u529b\uff0c\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u533b\u5b66AI\u6559\u80b2\u6a21\u5f0f\u3002"}}
{"id": "2509.16909", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16909", "abs": "https://arxiv.org/abs/2509.16909", "authors": ["Yijun Yuan", "Zhuoguang Chen", "Kenan Li", "Weibang Wang", "Hang Zhao"], "title": "SLAM-Former: Putting SLAM into One Transformer", "comment": "Project Page:https://tsinghua-mars-lab.github.io/SLAM-Former", "summary": "We present SLAM-Former, a novel neural approach that integrates full SLAM\ncapabilities into a single transformer. Similar to traditional SLAM systems,\nSLAM-Former comprises both a frontend and a backend that operate in tandem. The\nfrontend processes sequential monocular images in real-time for incremental\nmapping and tracking, while the backend performs global refinement to ensure a\ngeometrically consistent result. This alternating execution allows the frontend\nand backend to mutually promote one another, enhancing overall system\nperformance. Comprehensive experimental results demonstrate that SLAM-Former\nachieves superior or highly competitive performance compared to\nstate-of-the-art dense SLAM methods.", "AI": {"tldr": "SLAM-Former\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u5c06\u5b8c\u6574\u7684SLAM\u529f\u80fd\u96c6\u6210\u4e8e\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u524d\u540e\u7aef\u4ea4\u66ff\u6267\u884c\u5b9e\u73b0\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u5b9e\u65f6\u5355\u76eeSLAM\u3002", "motivation": "\u4f20\u7edfSLAM\u7cfb\u7edf\u901a\u5e38\u6a21\u5757\u5316\u4e14\u590d\u6742\uff0c\u96be\u4ee5\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\uff1b\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u79cd\u7edf\u4e00\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5c06\u524d\u7aef\u8ddf\u8e2a\u4e0e\u540e\u7aef\u4f18\u5316\u540c\u65f6\u96c6\u6210\uff0c\u63d0\u5347\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faSLAM-Former\uff0c\u91c7\u7528\u5355\u4e00Transformer\u67b6\u6784\uff0c\u5305\u542b\u5b9e\u65f6\u5904\u7406\u5355\u76ee\u56fe\u50cf\u5e8f\u5217\u7684\u524d\u7aef\uff08\u7528\u4e8e\u589e\u91cf\u5efa\u56fe\u4e0e\u4f4d\u59ff\u4f30\u8ba1\uff09\u548c\u6267\u884c\u5168\u5c40\u4f18\u5316\u7684\u540e\u7aef\uff0c\u524d\u540e\u7aef\u4ea4\u66ff\u8fd0\u884c\u5e76\u76f8\u4e92\u4fc3\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLAM-Former\u5728\u5bc6\u96c6SLAM\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SLAM-Former\u6210\u529f\u5c06\u5b8c\u6574SLAM\u6d41\u7a0b\u96c6\u6210\u5230\u4e00\u4e2aTransformer\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u7aef\u5230\u7aefSLAM\uff0c\u4e3a\u672a\u6765\u795e\u7ecfSLAM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17395", "abs": "https://arxiv.org/abs/2509.17395", "authors": ["Tianshi Cai", "Guanxu Li", "Nijia Han", "Ce Huang", "Zimu Wang", "Changyu Zeng", "Yuqi Wang", "Jingshi Zhou", "Haiyang Zhang", "Qi Chen", "Yushan Pan", "Shuihua Wang", "Wei Wang"], "title": "FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis", "comment": "Accepted at FinNLP@EMNLP 2025. Camera-ready version", "summary": "We introduce FinDebate, a multi-agent framework for financial analysis,\nintegrating collaborative debate with domain-specific Retrieval-Augmented\nGeneration (RAG). Five specialized agents, covering earnings, market,\nsentiment, valuation, and risk, run in parallel to synthesize evidence into\nmulti-dimensional insights. To mitigate overconfidence and improve reliability,\nwe introduce a safe debate protocol that enables agents to challenge and refine\ninitial conclusions while preserving coherent recommendations. Experimental\nresults, based on both LLM-based and human evaluations, demonstrate the\nframework's efficacy in producing high-quality analysis with calibrated\nconfidence levels and actionable investment strategies across multiple time\nhorizons.", "AI": {"tldr": "\u63d0\u51faFinDebate\uff0c\u4e00\u79cd\u7ed3\u5408\u534f\u4f5c\u5f0f\u8fa9\u8bba\u548c\u9886\u57df\u7279\u5b9a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u591a\u667a\u80fd\u4f53\u91d1\u878d\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u5e76\u884c\u5de5\u4f5c\uff0c\u63d0\u5347\u5206\u6790\u8d28\u91cf\u4e0e\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u91d1\u878d\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u5f15\u5165\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u4ee5\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u6536\u76ca\u3001\u5e02\u573a\u3001\u60c5\u7eea\u3001\u4f30\u503c\u548c\u98ce\u9669\u4e94\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u5e76\u884c\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5b89\u5168\u8fa9\u8bba\u534f\u8bae\uff0c\u4f7f\u667a\u80fd\u4f53\u53ef\u76f8\u4e92\u6311\u6218\u5e76\u4fee\u6b63\u7ed3\u8bba\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728LLM\u548c\u4eba\u5de5\u8bc4\u4f30\u4e0b\u5747\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u826f\u597d\u7684\u5206\u6790\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u8de8\u65f6\u95f4\u5c3a\u5ea6\u7684\u53ef\u64cd\u4f5c\u6295\u8d44\u7b56\u7565\u3002", "conclusion": "FinDebate\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u91d1\u878d\u5206\u6790\u7684\u53ef\u9760\u6027\u4e0e\u591a\u7ef4\u6d1e\u5bdf\u529b\uff0c\u901a\u8fc7\u5b89\u5168\u8fa9\u8bba\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.17291", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17291", "abs": "https://arxiv.org/abs/2509.17291", "authors": ["Rahul Nandakumar", "Deepayan Chakrabarti"], "title": "GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories", "comment": "18 pages, 4 figures. Accepted at ECML-PKDD 2025", "summary": "Given a set of graphs from some unknown family, we want to generate new\ngraphs from that family. Recent methods use diffusion on either graph\nembeddings or the discrete space of nodes and edges. However, simple changes to\nembeddings (say, adding noise) can mean uninterpretable changes in the graph.\nIn discrete-space diffusion, each step may add or remove many nodes/edges. It\nis hard to predict what graph patterns we will observe after many diffusion\nsteps. Our proposed method, called GraphWeave, takes a different approach. We\nseparate pattern generation and graph construction. To find patterns in the\ntraining graphs, we see how they transform vectors during random walks. We then\ngenerate new graphs in two steps. First, we generate realistic random walk\n\"trajectories\" which match the learned patterns. Then, we find the optimal\ngraph that fits these trajectories. The optimization infers all edges jointly,\nwhich improves robustness to errors. On four simulated and five real-world\nbenchmark datasets, GraphWeave outperforms existing methods. The most\nsignificant differences are on large-scale graph structures such as PageRank,\ncuts, communities, degree distributions, and flows. GraphWeave is also 10x\nfaster than its closest competitor. Finally, GraphWeave is simple, needing only\na transformer and standard optimizers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGraphWeave\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u6a21\u5f0f\u751f\u6210\u548c\u56fe\u6784\u5efa\uff0c\u5229\u7528\u968f\u673a\u6e38\u8d70\u8f68\u8ff9\u751f\u6210\u5e76\u4f18\u5316\u62df\u5408\u56fe\u7ed3\u6784\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u56fe\u7ed3\u6784\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\u3001\u5b9e\u73b0\u66f4\u7b80\u5355\u3002", "motivation": "\u73b0\u6709\u56fe\u751f\u6210\u65b9\u6cd5\u5728\u5d4c\u5165\u7a7a\u95f4\u6216\u79bb\u6563\u8282\u70b9\u8fb9\u7a7a\u95f4\u8fdb\u884c\u6269\u6563\u65f6\u5b58\u5728\u4e0d\u53ef\u89e3\u91ca\u6027\u6216\u96be\u4ee5\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u56fe\u7ed3\u6784\u6a21\u5f0f\u7684\u53ef\u63a7\u751f\u6210\u3002", "method": "\u5c06\u56fe\u751f\u6210\u5206\u4e3a\u4e24\u6b65\uff1a\u9996\u5148\u5b66\u4e60\u8bad\u7ec3\u56fe\u4e2d\u968f\u673a\u6e38\u8d70\u7684\u5411\u91cf\u53d8\u6362\u6a21\u5f0f\uff0c\u751f\u6210\u7b26\u5408\u8fd9\u4e9b\u6a21\u5f0f\u7684\u968f\u673a\u6e38\u8d70\u8f68\u8ff9\uff1b\u7136\u540e\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63a8\u65ad\u51fa\u6700\u5339\u914d\u8fd9\u4e9b\u8f68\u8ff9\u7684\u56fe\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u6a21\u62df\u548c\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cGraphWeave\u5728PageRank\u3001\u5272\u3001\u793e\u533a\u3001\u5ea6\u5206\u5e03\u548c\u6d41\u7b49\u5927\u89c4\u6a21\u7ed3\u6784\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u6bd4\u6700\u63a5\u8fd1\u7684\u7ade\u4e89\u8005\u5feb10\u500d\u3002", "conclusion": "GraphWeave\u901a\u8fc7\u89e3\u8026\u6a21\u5f0f\u751f\u6210\u4e0e\u56fe\u6784\u9020\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u3001\u66f4\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u56fe\u751f\u6210\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u56fe\u751f\u6210\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.16935", "categories": ["cs.CV", "68T07", "I.2.10; I.4.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.16935", "abs": "https://arxiv.org/abs/2509.16935", "authors": ["Lavish Ramchandani", "Gunjan Deotale", "Dev Kumar Das"], "title": "Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification", "comment": "MIDOG'25", "summary": "Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated\nwith tumor aggressiveness and poor prognosis. Their detection remains a\nsignificant challenge due to subtle morphological cues, class imbalance, and\ninter-observer variability among pathologists. The MIDOG 2025 challenge\nintroduced a dedicated track for atypical mitosis classification, enabling\nsystematic evaluation of deep learning methods. In this study, we investigated\nthe use of large vision foundation models, including Virchow, Virchow2, and\nUNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We\nconducted extensive experiments with different LoRA ranks, as well as random\nand group-based data splits, to analyze robustness under varied conditions. Our\nbest approach, Virchow with LoRA rank 8 and ensemble of three-fold\ncross-validation, achieved a balanced accuracy of 88.37% on the preliminary\ntest set, ranking joint 9th in the challenge leaderboard. These results\nhighlight the promise of foundation models with efficient adaptation strategies\nfor the classification of atypical mitosis, while underscoring the need for\nimprovements in specificity and domain generalization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982Virchow\u3001Virchow2\u548cUNI\uff09\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\uff0c\u5bf9\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u5e94\u5bf9MIDOG 2025\u6311\u6218\u8d5b\u4e2d\u7684\u68c0\u6d4b\u96be\u9898\u3002\u6700\u4f73\u6a21\u578b\u5728\u521d\u6b65\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523088.37%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u6392\u540d\u7b2c9\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u63d0\u5347\u7279\u5f02\u6027\u548c\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\uff08AMFs\uff09\u4e0e\u80bf\u7624\u4fb5\u88ad\u6027\u548c\u4e0d\u826f\u9884\u540e\u76f8\u5173\uff0c\u4f46\u5176\u68c0\u6d4b\u56e0\u5f62\u6001\u5b66\u7279\u5f81\u7ec6\u5fae\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u53ca\u75c5\u7406\u533b\u5e08\u95f4\u5dee\u5f02\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08Virchow\u3001Virchow2\u3001UNI\uff09\u7ed3\u5408LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u4e0d\u540cLoRA\u79e9\u548c\u6570\u636e\u5212\u5206\u65b9\u5f0f\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e09\u6298\u4ea4\u53c9\u9a8c\u8bc1\u96c6\u6210\u7b56\u7565\u3002", "result": "\u6700\u4f73\u65b9\u6cd5\uff08Virchow + LoRA\u79e98 + \u4e09\u6298\u96c6\u6210\uff09\u5728\u521d\u6b65\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8688.37%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u5728\u6311\u6218\u8d5b\u4e2d\u5e76\u5217\u7b2c9\u540d\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u9ad8\u6548\u7684\u9002\u914d\u7b56\u7565\u5728\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6f5c\u529b\uff0c\u4f46\u5728\u7279\u5f02\u6027\u4e0e\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2509.17396", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17396", "abs": "https://arxiv.org/abs/2509.17396", "authors": ["Minsoo Kim", "Arnav Kundu", "Han-Byul Kim", "Richa Dixit", "Minsik Cho"], "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering", "comment": null, "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.", "AI": {"tldr": "EpiCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u9884\u586b\u5145\u548c\u57fa\u4e8e\u4f1a\u8bdd\u7247\u6bb5\u7684\u538b\u7f29\uff0c\u5728\u56fa\u5b9a\u5185\u5b58\u9884\u7b97\u4e0b\u6709\u6548\u652f\u6301\u957f\u5bf9\u8bdd\u95ee\u7b54\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5728\u5168\u4e0a\u4e0b\u6587\u9884\u586b\u5145\u540e\u5bfc\u81f4\u5cf0\u503c\u5185\u5b58\u65e0\u754c\uff0c\u4e14\u67e5\u8be2\u4f9d\u8d56\u7684\u9a71\u9010\u7b56\u7565\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u635f\u5bb3\u51c6\u786e\u6027\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u8fd0\u884c\u3002", "method": "\u63d0\u51faEpiCache\uff0c\u91c7\u7528\u5206\u5757\u9884\u586b\u5145\u63a7\u5236\u7f13\u5b58\u589e\u957f\uff0c\u5c06\u5bf9\u8bdd\u5386\u53f2\u805a\u7c7b\u4e3a\u8fde\u8d2f\u7684\u7247\u6bb5\uff0c\u5e76\u8fdb\u884c\u7247\u6bb5\u7279\u5b9a\u7684KV\u7f13\u5b58\u9a71\u9010\uff1b\u8bbe\u8ba1\u81ea\u9002\u5e94\u9010\u5c42\u9884\u7b97\u5206\u914d\u7b56\u7565\uff0c\u6839\u636e\u5404\u5c42\u5bf9\u9a71\u9010\u7684\u654f\u611f\u5ea6\u52a8\u6001\u5206\u914d\u5185\u5b58\u3002", "result": "\u5728\u4e09\u4e2a\u957f\u5bf9\u8bdd\u95ee\u7b54\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u65b0\u65b9\u6cd5\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534740%\uff0c\u57284-6\u500d\u538b\u7f29\u4e0b\u63a5\u8fd1\u5b8c\u6574KV\u7f13\u5b58\u7684\u6027\u80fd\uff0c\u5ef6\u8fdf\u548c\u5185\u5b58\u6700\u591a\u51cf\u5c112.4\u500d\u548c3.5\u500d\u3002", "conclusion": "EpiCache\u6709\u6548\u89e3\u51b3\u4e86KV\u7f13\u5b58\u7684\u5185\u5b58\u81a8\u80c0\u548c\u591a\u8f6e\u5bf9\u8bdd\u51c6\u786e\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u4e0b\u7684\u9ad8\u6548\u957f\u5bf9\u8bdd\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.17293", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17293", "abs": "https://arxiv.org/abs/2509.17293", "authors": ["Ryan Chappell", "Chayan Banerjee", "Kien Nguyen", "Clinton Fookes"], "title": "Physics-Informed Operator Learning for Hemodynamic Modeling", "comment": "To appear in the proceedings of DICTA 2025", "summary": "Accurate modeling of personalized cardiovascular dynamics is crucial for\nnon-invasive monitoring and therapy planning. State-of-the-art physics-informed\nneural network (PINN) approaches employ deep, multi-branch architectures with\nadversarial or contrastive objectives to enforce partial differential equation\nconstraints. While effective, these enhancements introduce significant training\nand implementation complexity, limiting scalability and practical deployment.\nWe investigate physics-informed neural operator learning models as efficient\nsupervisory signals for training simplified architectures through knowledge\ndistillation. Our approach pre-trains a physics-informed DeepONet (PI-DeepONet)\non high-fidelity cuffless blood pressure recordings to learn operator mappings\nfrom raw wearable waveforms to beat-to-beat pressure signals under embedded\nphysics constraints. This pre-trained operator serves as a frozen supervisor in\na lightweight knowledge-distillation pipeline, guiding streamlined base models\nthat eliminate complex adversarial and contrastive learning components while\nmaintaining performance. We characterize the role of physics-informed\nregularization in operator learning and demonstrate its effectiveness for\nsupervisory guidance. Through extensive experiments, our operator-supervised\napproach achieves performance parity with complex baselines (correlation: 0.766\nvs. 0.770, RMSE: 4.452 vs. 4.501), while dramatically reducing architectural\ncomplexity from eight critical hyperparameters to a single regularization\ncoefficient and decreasing training overhead by 4%. Our results demonstrate\nthat operator-based supervision effectively replaces intricate multi-component\ntraining strategies, offering a more scalable and interpretable approach to\nphysiological modeling with reduced implementation burden.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u7b80\u5316\u5fc3\u8840\u7ba1\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e0e\u590d\u6742\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u67b6\u6784\u590d\u6742\u6027\u548c\u8bad\u7ec3\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8ePINN\u7684\u5fc3\u8840\u7ba1\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u591a\u5206\u652f\u67b6\u6784\u548c\u5bf9\u6297/\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5bfc\u81f4\u8bad\u7ec3\u96be\u5ea6\u5927\u3001\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u9996\u5148\u9884\u8bad\u7ec3\u4e00\u4e2a\u7269\u7406\u4fe1\u606fDeepONet\uff08PI-DeepONet\uff09\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u5b66\u4e60\u4ece\u53ef\u7a7f\u6234\u8bbe\u5907\u6ce2\u5f62\u5230\u8840\u538b\u4fe1\u53f7\u7684\u6620\u5c04\u5e76\u5d4c\u5165\u7269\u7406\u7ea6\u675f\uff1b\u7136\u540e\u5c06\u5176\u4f5c\u4e3a\u56fa\u5b9a\u76d1\u7763\u5668\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u53bb\u9664\u590d\u6742\u7684\u5bf9\u6297\u548c\u5bf9\u6bd4\u7ec4\u4ef6\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u590d\u6742\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\uff08\u76f8\u5173\u7cfb\u65700.766 vs 0.770\uff0cRMSE 4.452 vs 4.501\uff09\uff0c\u4f46\u5c06\u67b6\u6784\u8d85\u53c2\u6570\u4ece\u516b\u4e2a\u51cf\u5c11\u5230\u4e00\u4e2a\u6b63\u5219\u5316\u7cfb\u6570\uff0c\u5e76\u964d\u4f4e\u8bad\u7ec3\u5f00\u95004%\u3002", "conclusion": "\u57fa\u4e8e\u7b97\u5b50\u7684\u76d1\u7763\u53ef\u6709\u6548\u66ff\u4ee3\u590d\u6742\u7684\u591a\u7ec4\u4ef6\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3a\u751f\u7406\u5efa\u6a21\u63d0\u4f9b\u66f4\u53ef\u6269\u5c55\u3001\u6613\u89e3\u91ca\u4e14\u5b9e\u73b0\u8d1f\u62c5\u66f4\u4f4e\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.16942", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16942", "abs": "https://arxiv.org/abs/2509.16942", "authors": ["Bin Wang", "Fei Deng", "Zeyu Chen", "Zhicheng Yu", "Yiguang Liu"], "title": "Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic\nsegmentation of Remote Sensing Images (RSIs) using only a well-trained source\nmodel and unlabeled target domain data. However, the lack of ground-truth\nlabels in the target domain often leads to the generation of noisy\npseudo-labels. Such noise impedes the effective mitigation of domain shift\n(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA\nframework. It employs prototype-weighted pseudo-labels to facilitate reliable\nself-training (ST) under pseudo-labels noise. We, in addition, introduce a\nprototype-contrast strategy that encourages the aggregation of features\nbelonging to the same class, enabling the model to learn discriminative target\ndomain representations without relying on ground-truth supervision. Extensive\nexperiments show that our approach substantially outperforms existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u5f15\u5bfc\u7684\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u6846\u67b6ProSFDA\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u539f\u578b\u52a0\u6743\u4f2a\u6807\u7b7e\u548c\u539f\u578b\u5bf9\u6bd4\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u65e0\u771f\u5b9e\u6807\u7b7e\u7684\u76ee\u6807\u57df\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u4f2a\u6807\u7b7e\u566a\u58f0\u96be\u4ee5\u6709\u6548\u7f13\u89e3\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5f71\u54cd\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51faProSFDA\u6846\u67b6\uff0c\u91c7\u7528\u539f\u578b\u52a0\u6743\u4f2a\u6807\u7b7e\u8fdb\u884c\u53ef\u9760\u81ea\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u539f\u578b\u5bf9\u6bd4\u7b56\u7565\uff0c\u4fc3\u4f7f\u540c\u7c7b\u7279\u5f81\u805a\u5408\uff0c\u5b66\u4e60\u6709\u5224\u522b\u6027\u7684\u76ee\u6807\u57df\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "ProSFDA\u901a\u8fc7\u539f\u578b\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u4f18\u5316\u548c\u7279\u5f81\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u4e0b\u7684\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2509.17399", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17399", "abs": "https://arxiv.org/abs/2509.17399", "authors": ["Pramit Sahoo", "Maharaj Brahma", "Maunendra Sankar Desarkar"], "title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context", "comment": "Accepted at EMNLP 2025", "summary": "Large language models (LLMs) are widely used in various tasks and\napplications. However, despite their wide capabilities, they are shown to lack\ncultural alignment \\citep{ryan-etal-2024-unintended,\nalkhamissi-etal-2024-investigating} and produce biased generations\n\\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.\nEvaluation of LLMs for cultural awareness and alignment is particularly\nchallenging due to the lack of proper evaluation metrics and unavailability of\nculturally grounded datasets representing the vast complexity of cultures at\nthe regional and sub-regional levels. Existing datasets for culture specific\nitems (CSIs) focus primarily on concepts at the regional level and may contain\nfalse positives. To address this issue, we introduce a novel CSI dataset for\nIndian culture, belonging to 17 cultural facets. The dataset comprises $\\sim$8k\ncultural concepts from 36 sub-regions. To measure the cultural competence of\nLLMs on a cultural text adaptation task, we evaluate the adaptations using the\nCSIs created, LLM as Judge, and human evaluations from diverse\nsocio-demographic region. Furthermore, we perform quantitative analysis\ndemonstrating selective sub-regional coverage and surface-level adaptations\nacross all considered LLMs. Our dataset is available here:\n\\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},\nproject\nwebpage\\footnote{\\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},\nand our codebase with model outputs can be found here:\n\\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5370\u5ea6\u6587\u5316\u7684\u65b0\u578b\u6587\u5316\u7279\u5b9a\u9879\u76ee\uff08CSI\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea68000\u4e2a\u6765\u81ea36\u4e2a\u6b21\u533a\u57df\u7684\u6587\u5316\u6982\u5ff5\uff0c\u6db5\u76d617\u4e2a\u6587\u5316\u7ef4\u5ea6\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u9002\u5e94\u4efb\u52a1\u4e2d\u7684\u6587\u5316\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u9002\u5f53\u7684\u6587\u5316\u77e5\u8bc6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5bb9\u6613\u4ea7\u751f\u504f\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u3001\u66f4\u51c6\u786e\u7684\u6587\u5316\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d617\u4e2a\u6587\u5316\u7ef4\u5ea6\u300136\u4e2a\u6b21\u533a\u57df\u7684\u5370\u5ea6\u6587\u5316CSI\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6587\u5316\u6587\u672c\u9002\u5e94\u4efb\u52a1\uff0c\u7ed3\u5408LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u548c\u6765\u81ea\u4e0d\u540c\u793e\u4f1a\u4eba\u53e3\u80cc\u666f\u7684\u4eba\u7c7b\u8bc4\u4f30\uff0c\u5bf9LLMs\u7684\u6587\u5316\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524dLLMs\u5728\u6b21\u533a\u57df\u8986\u76d6\u4e0a\u5b58\u5728\u9009\u62e9\u6027\u504f\u5dee\uff0c\u4e14\u6587\u5316\u9002\u5e94\u591a\u505c\u7559\u5728\u8868\u9762\u5c42\u6b21\uff0c\u672a\u80fd\u6df1\u5165\u7406\u89e3\u6587\u5316\u5185\u6db5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u6784\u5efa\u7ec6\u7c92\u5ea6\u6587\u5316\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u8bc4\u4f30LLMs\u7684\u6587\u5316\u5bf9\u9f50\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\u548c\u65b9\u6cd5\u3002"}}
{"id": "2509.17304", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17304", "abs": "https://arxiv.org/abs/2509.17304", "authors": ["Tian Xie", "Ding Zhu", "Jia Liu", "Mahdi Khalili", "Xueru Zhang"], "title": "SPRINT: Stochastic Performative Prediction With Variance Reduction", "comment": null, "summary": "Performative prediction (PP) is an algorithmic framework for optimizing\nmachine learning (ML) models where the model's deployment affects the\ndistribution of the data it is trained on. Compared to traditional ML with\nfixed data, designing algorithms in PP converging to a stable point -- known as\na stationary performative stable (SPS) solution -- is more challenging than the\ncounterpart in conventional ML tasks due to the model-induced distribution\nshifts. While considerable efforts have been made to find SPS solutions using\nmethods such as repeated gradient descent (RGD) and greedy stochastic gradient\ndescent (SGD-GD), most prior studies assumed a strongly convex loss until a\nrecent work established $\\mathcal{O}(1/\\sqrt{T})$ convergence of SGD-GD to SPS\nsolutions under smooth, non-convex losses. However, this latest progress is\nstill based on the restricted bounded variance assumption in stochastic\ngradient estimates and yields convergence bounds with a non-vanishing error\nneighborhood that scales with the variance. This limitation motivates us to\nimprove convergence rates and reduce error in stochastic optimization for PP,\nparticularly in non-convex settings. Thus, we propose a new algorithm called\nstochastic performative prediction with variance reduction (SPRINT) and\nestablish its convergence to an SPS solution at a rate of $\\mathcal{O}(1/T)$.\nNotably, the resulting error neighborhood is **independent** of the variance of\nthe stochastic gradients. Experiments on multiple real datasets with non-convex\nmodels demonstrate that SPRINT outperforms SGD-GD in both convergence rate and\nstability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u6267\u884c\u9884\u6d4b\u7b97\u6cd5SPRINT\uff0c\u7528\u4e8e\u5728\u975e\u51f8\u635f\u5931\u4e0b\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u5c0f\u7684\u8bef\u5dee\u90bb\u57df\uff0c\u4e14\u8bef\u5dee\u90bb\u57df\u4e0e\u968f\u673a\u68af\u5ea6\u7684\u65b9\u5dee\u65e0\u5173\u3002", "motivation": "\u73b0\u6709\u6267\u884c\u9884\u6d4b\u65b9\u6cd5\u5728\u975e\u51f8\u8bbe\u7f6e\u4e0b\u4f9d\u8d56\u6709\u754c\u65b9\u5dee\u5047\u8bbe\uff0c\u5bfc\u81f4\u6536\u655b\u5b58\u5728\u4e0d\u53ef\u5ffd\u7565\u7684\u8bef\u5dee\u90bb\u57df\uff0c\u9650\u5236\u4e86\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6536\u655b\u901f\u5ea6\u5e76\u964d\u4f4e\u8bef\u5dee\u3002", "method": "\u63d0\u51faSPRINT\u7b97\u6cd5\uff0c\u7ed3\u5408\u65b9\u5dee\u7f29\u51cf\u6280\u672f\uff0c\u5728\u5e73\u6ed1\u975e\u51f8\u635f\u5931\u4e0b\u4f18\u5316\u6267\u884c\u9884\u6d4b\u95ee\u9898\u3002", "result": "\u7406\u8bba\u8bc1\u660eSPRINT\u4ee5O(1/T)\u901f\u7387\u6536\u655b\u5230SPS\u89e3\uff0c\u4e14\u8bef\u5dee\u90bb\u57df\u72ec\u7acb\u4e8e\u68af\u5ea6\u65b9\u5dee\uff1b\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u5176\u6bd4SGD-GD\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7a33\u5b9a\u6027\u3002", "conclusion": "SPRINT\u5728\u975e\u51f8\u6267\u884c\u9884\u6d4b\u95ee\u9898\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u68af\u5ea6\u65b9\u5dee\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.16944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16944", "abs": "https://arxiv.org/abs/2509.16944", "authors": ["Yuheng Shi", "Xiaohuan Pei", "Minjing Dong", "Chang Xu"], "title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception", "comment": "19 pages, 5 figures", "summary": "Multimodal Large Language Models (MLLMs) require high-resolution visual\ninformation to perform fine-grained perception, yet processing entire\nhigh-resolution images is computationally prohibitive. While recent methods\nleverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they\ntypically present a difficult trade-off: training-based approaches depend on\nlarge-scale annotated datasets, while training-free methods that utilize the\nmodel's internal attention are computationally inefficient and less accurate,\nrequiring either multi-pass prefill stages or reliance on the slow\nauto-regressive decoding process. In this paper, we propose an efficient,\nannotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves\nthis trade-off. The SD-RPN is built around a pipeline that transforms the noisy\nattention maps from the MLLM's middle layers into high-quality pseudo-RoI\nlabels by explicitly denoising the signal and resolving ambiguity. We use these\nlabels to train a lightweight Region Proposal Network (RPN) that learns a more\nprecise localization. This RPN is also highly efficient, predicting the RoI in\na single forward pass using features from the MLLM's middle layers, decoupling\nRoI identification from the auto-regressive generation and avoiding costly\nmulti-pass operations.To validate our approach, we integrate the framework into\nthe LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)\nquestion-answer pairs, our method demonstrates exceptional data efficiency and\ngeneralization, achieving over a 10% absolute accuracy improvement on unseen\nbenchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a\npractical and scalable solution for enhancing the fine-grained perception of\nMLLMs without requiring costly supervision or full model fine-tuning. Code is\navailable at https://github.com/YuHengsss/SD-RPN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u3001\u81ea\u84b8\u998f\u7684\u533a\u57df\u5efa\u8bae\u7f51\u7edcSD-RPN\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u6d88\u6b67\u4e2d\u95f4\u5c42\u6ce8\u610f\u529b\u56fe\u751f\u6210\u9ad8\u8d28\u91cf\u4f2aRoI\u6807\u7b7e\uff0c\u8bad\u7ec3\u8f7b\u91cfRPN\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u9ad8\u6548\u7cbe\u51c6\u5b9a\u4f4d\uff0c\u5728LLaVA-1.5\u4e0a\u9a8c\u8bc1\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5174\u8da3\u533a\u57df\uff08RoI\uff09\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4e0e\u8bad\u7ec3\u81ea\u7531\u4f46\u8ba1\u7b97\u4f4e\u6548\u3001\u7cbe\u5ea6\u4e0d\u8db3\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u96be\u4ee5\u517c\u987e\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u8bbe\u8ba1SD-RPN\u6846\u67b6\uff0c\u5229\u7528MLLM\u4e2d\u95f4\u5c42\u566a\u58f0\u6ce8\u610f\u529b\u56fe\uff0c\u901a\u8fc7\u663e\u5f0f\u53bb\u566a\u548c\u6d88\u6b67\u751f\u6210\u9ad8\u8d28\u91cf\u4f2aRoI\u6807\u7b7e\uff0c\u7528\u5176\u76d1\u7763\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7RPN\uff0c\u8be5RPN\u4ec5\u9700\u4e00\u6b21\u524d\u5411\u5373\u53ef\u9884\u6d4bRoI\uff0c\u8131\u79bb\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728TextVQA\u3001DocVQA\u548cV-Star\u7b49\u672a\u89c1\u57fa\u51c6\u4e0a\u5b9e\u73b0\u8d85\u8fc710%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u4ec5\u75281\u4e07\u95ee\u7b54\u5bf9\u8bad\u7ec3\u5373\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SD-RPN\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6602\u8d35\u76d1\u7763\u6216\u5168\u6a21\u578b\u5fae\u8c03\u3002"}}
{"id": "2509.17418", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17418", "abs": "https://arxiv.org/abs/2509.17418", "authors": ["Junhong Liang", "Bojun Zhang"], "title": "Vision Language Models Are Not (Yet) Spelling Correctors", "comment": null, "summary": "Spelling correction from visual input poses unique challenges for vision\nlanguage models (VLMs), as it requires not only detecting but also correcting\ntextual errors directly within images. We present ReViCo (Real Visual\nCorrection), the first benchmark that systematically evaluates VLMs on\nreal-world visual spelling correction across Chinese and English. ReViCo\ncontains naturally occurring errors collected from real-world image data and\nsupports fine-grained evaluation at both image and token levels. Through\ncomprehensive experiments on representative cascaded (Qwen) and native\n(InternVL) open-source models, as well as closed-source systems (GPT-4o,\nClaude), we show that current VLMs fall significantly short of human\nperformance, particularly in correction. To address these limitations, we\nexplore two solution paradigms: a Joint OCR-Correction pipeline and a\nBackground Information enhanced approach, both of which yield consistent\nperformance gains. Our analysis highlights fundamental limitations of existing\narchitectures and provides actionable insights for advancing multimodal\nspelling correction.", "AI": {"tldr": "ReViCo\u662f\u9996\u4e2a\u9488\u5bf9\u4e2d\u82f1\u6587\u771f\u5b9e\u573a\u666f\u4e0b\u89c6\u89c9\u62fc\u5199\u7ea0\u9519\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u6587\u672c\u7ea0\u9519\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u8054\u5408OCR-\u7ea0\u9519\u548c\u589e\u5f3a\u80cc\u666f\u4fe1\u606f\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9\u8f93\u5165\u4e2d\u7684\u62fc\u5199\u7ea0\u9519\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u6311\u6218\uff0c\u9700\u8981\u76f4\u63a5\u5728\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5e76\u7ea0\u6b63\u6587\u672c\u9519\u8bef\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efaReViCo\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e2d\u7684\u81ea\u7136\u62fc\u5199\u9519\u8bef\uff0c\u652f\u6301\u56fe\u50cf\u7ea7\u548c\u4ee4\u724c\u7ea7\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff1b\u5b9e\u9a8c\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90\u4e0e\u95ed\u6e90VLM\uff0c\u5e76\u63a2\u7d22\u8054\u5408OCR-\u7ea0\u9519\u6d41\u7a0b\u548c\u80cc\u666f\u4fe1\u606f\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u73b0\u6709VLM\u5728\u7ea0\u9519\u4efb\u52a1\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u5c24\u5176\u5728\u51c6\u786e\u4fee\u6b63\u9519\u8bef\u65b9\u9762\uff1b\u6240\u63d0\u4e24\u79cd\u65b9\u6cd5\u5747\u5e26\u6765\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u89c6\u89c9\u62fc\u5199\u7ea0\u9519\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u7ed3\u5408OCR\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u589e\u5f3a\u7b56\u7565\u4ee5\u63a8\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2509.17322", "categories": ["cs.LG", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.17322", "abs": "https://arxiv.org/abs/2509.17322", "authors": ["Chi Zhang", "Mengxin Zheng", "Qian Lou", "Hui Min Leung", "Fan Chen"], "title": "VQEzy: An Open-Source Dataset for Parameter Initialize in Variational Quantum Eigensolvers", "comment": null, "summary": "Variational Quantum Eigensolvers (VQEs) are a leading class of noisy\nintermediate-scale quantum (NISQ) algorithms, whose performance is highly\nsensitive to parameter initialization. Although recent machine learning-based\ninitialization methods have achieved state-of-the-art performance, their\nprogress has been limited by the lack of comprehensive datasets. Existing\nresources are typically restricted to a single domain, contain only a few\nhundred instances, and lack complete coverage of Hamiltonians, ansatz circuits,\nand optimization trajectories. To overcome these limitations, we introduce\nVQEzy, the first large-scale dataset for VQE parameter initialization. VQEzy\nspans three major domains and seven representative tasks, comprising 12,110\ninstances with full VQE specifications and complete optimization trajectories.\nThe dataset is available online, and will be continuously refined and expanded\nto support future research in VQE optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VQEzy\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668\uff08VQE\uff09\u53c2\u6570\u521d\u59cb\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e09\u4e2a\u4e3b\u8981\u9886\u57df\u548c\u4e03\u9879\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u5305\u542b12,110\u4e2a\u5b9e\u4f8b\u53ca\u5176\u5b8c\u6574\u7684\u4f18\u5316\u8f68\u8ff9\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u8986\u76d6\u4e0d\u5168\u7684\u95ee\u9898\uff0c\u63a8\u52a8VQE\u4f18\u5316\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684VQE\u53c2\u6570\u521d\u59cb\u5316\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u9886\u57df\u5355\u4e00\u3001\u7f3a\u4e4f\u5b8c\u6574\u8986\u76d6\uff0c\u9650\u5236\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5927\u89c4\u6a21\u3001\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVQEzy\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e09\u4e2a\u4e3b\u8981\u9886\u57df\u548c\u4e03\u9879\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u8bb0\u5f55\u4e86\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u5b8c\u6574VQE\u914d\u7f6e\uff08\u5305\u62ec\u54c8\u5bc6\u987f\u91cf\u3001ansatz\u7535\u8def\uff09\u4ee5\u53ca\u5b8c\u6574\u7684\u4f18\u5316\u8fc7\u7a0b\u8f68\u8ff9\u3002", "result": "VQEzy\u5305\u542b12,110\u4e2a\u5b9e\u4f8b\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684VQE\u89c4\u8303\u548c\u4f18\u5316\u8f68\u8ff9\uff0c\u662f\u76ee\u524d\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u8de8\u9886\u57df\u7684VQE\u521d\u59cb\u5316\u6570\u636e\u96c6\uff0c\u5e76\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u5c06\u6301\u7eed\u66f4\u65b0\u548c\u6269\u5c55\u3002", "conclusion": "VQEzy\u4e3aVQE\u7b97\u6cd5\u7684\u53c2\u6570\u521d\u59cb\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u671b\u4fc3\u8fdb\u673a\u5668\u5b66\u4e60\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7ed3\u5408\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u63d0\u5347VQE\u5728NISQ\u65f6\u4ee3\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.16949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16949", "abs": "https://arxiv.org/abs/2509.16949", "authors": ["Ruicong Liu", "Takehiko Ohkawa", "Tze Ho Elden Tse", "Mingfang Zhang", "Angela Yao", "Yoichi Sato"], "title": "Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation", "comment": null, "summary": "This paper presents RPEP, the first pre-training method for event-based 3D\nhand pose estimation using labeled RGB images and unpaired, unlabeled event\ndata. Event data offer significant benefits such as high temporal resolution\nand low latency, but their application to hand pose estimation is still limited\nby the scarcity of labeled training data. To address this, we repurpose real\nRGB datasets to train event-based estimators. This is done by constructing\npseudo-event-RGB pairs, where event data is generated and aligned with the\nground-truth poses of RGB images. Unfortunately, existing pseudo-event\ngeneration techniques assume stationary objects, thus struggling to handle\nnon-stationary, dynamically moving hands. To overcome this, RPEP introduces a\nnovel generation strategy that decomposes hand movements into smaller,\nstep-by-step motions. This decomposition allows our method to capture temporal\nchanges in articulation, constructing more realistic event data for a moving\nhand. Additionally, RPEP imposes a motion reversal constraint, regularizing\nevent generation using reversed motion. Extensive experiments show that our\npre-trained model significantly outperforms state-of-the-art methods on real\nevent data, achieving up to 24% improvement on EvRealHands. Moreover, it\ndelivers strong performance with minimal labeled samples for fine-tuning,\nmaking it well-suited for practical deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RPEP\uff0c\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76843D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u6ce8\u7684RGB\u56fe\u50cf\u548c\u672a\u914d\u5bf9\u7684\u65e0\u6807\u7b7e\u4e8b\u4ef6\u6570\u636e\uff0c\u901a\u8fc7\u6784\u5efa\u4f2a\u4e8b\u4ef6-RGB\u5bf9\u5e76\u5f15\u5165\u5206\u6b65\u8fd0\u52a8\u5206\u89e3\u548c\u8fd0\u52a8\u53cd\u8f6c\u7ea6\u675f\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u52a8\u6001\u624b\u90e8\u4e8b\u4ef6\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6807\u6ce8\u4e8b\u4ef6\u6570\u636e\u7a00\u7f3a\uff0c\u57fa\u4e8e\u4e8b\u4ef6\u7684\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u53d7\u9650\uff1b\u73b0\u6709\u4f2a\u4e8b\u4ef6\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u52a8\u6001\u79fb\u52a8\u7684\u624b\u90e8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5229\u7528\u65e0\u6807\u7b7e\u4e8b\u4ef6\u6570\u636e\u5e76\u9002\u5e94\u975e\u9759\u6001\u624b\u90e8\u8fd0\u52a8\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRPEP\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9eRGB\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u4e8b\u4ef6\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5206\u6b65\u8fd0\u52a8\u5206\u89e3\u751f\u6210\u66f4\u771f\u5b9e\u7684\u52a8\u6001\u624b\u90e8\u4f2a\u4e8b\u4ef6\u6570\u636e\uff0c\u5e76\u5f15\u5165\u8fd0\u52a8\u53cd\u8f6c\u7ea6\u675f\u6765\u6b63\u5219\u5316\u4e8b\u4ef6\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728EvRealHands\u4e0a\u6700\u9ad8\u63d0\u534724%\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u5fae\u8c03\u5373\u53ef\u53d6\u5f97\u826f\u597d\u6027\u80fd\u3002", "conclusion": "RPEP\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u76843D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u8bad\u7ec3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u624b\u90e8\u4e8b\u4ef6\u6570\u636e\u751f\u6210\u96be\u9898\uff0c\u63a8\u52a8\u4e86\u4e8b\u4ef6\u76f8\u673a\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.17107", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17107", "abs": "https://arxiv.org/abs/2509.17107", "authors": ["Lingzhao Kong", "Jiacheng Lin", "Siyu Li", "Kai Luo", "Zhiyong Li", "Kailun Yang"], "title": "CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception", "comment": "The source code will be made publicly available at\n  https://github.com/godk0509/CoBEVMoE", "summary": "Collaborative perception aims to extend sensing coverage and improve\nperception accuracy by sharing information among multiple agents. However, due\nto differences in viewpoints and spatial positions, agents often acquire\nheterogeneous observations. Existing intermediate fusion methods primarily\nfocus on aligning similar features, often overlooking the perceptual diversity\namong agents. To address this limitation, we propose CoBEVMoE, a novel\ncollaborative perception framework that operates in the Bird's Eye View (BEV)\nspace and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In\nDMoE, each expert is dynamically generated based on the input features of a\nspecific agent, enabling it to extract distinctive and reliable cues while\nattending to shared semantics. This design allows the fusion process to\nexplicitly model both feature similarity and heterogeneity across agents.\nFurthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance\ninter-expert diversity and improve the discriminability of the fused\nrepresentation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets\ndemonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,\nit improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the\nAP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the\neffectiveness of expert-based heterogeneous feature modeling in multi-agent\ncollaborative perception. The source code will be made publicly available at\nhttps://github.com/godk0509/CoBEVMoE.", "AI": {"tldr": "\u63d0\u51faCoBEVMoE\uff0c\u4e00\u79cd\u57fa\u4e8e\u9e1f\u77b0\u56fe\u7a7a\u95f4\u548c\u52a8\u6001\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u7684\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u4e13\u5bb6\u7f51\u7edc\u5efa\u6a21\u591a\u667a\u80fd\u4f53\u95f4\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u4e0e\u5f02\u8d28\u6027\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u4e13\u5bb6\u5ea6\u91cf\u635f\u5931\u63d0\u5347\u878d\u5408\u8868\u5f81\u7684\u5224\u522b\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e2d\u95f4\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u9f50\u76f8\u4f3c\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u591a\u667a\u80fd\u4f53\u95f4\u89c2\u6d4b\u89c6\u89d2\u548c\u7a7a\u95f4\u4f4d\u7f6e\u5dee\u5f02\u5e26\u6765\u7684\u611f\u77e5\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "\u5728BEV\u7a7a\u95f4\u4e2d\u6784\u5efa\u534f\u4f5c\u611f\u77e5\u6846\u67b6CoBEVMoE\uff0c\u91c7\u7528\u52a8\u6001\u4e13\u5bb6\u6df7\u5408\uff08DMoE\uff09\u7ed3\u6784\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u6839\u636e\u5404\u667a\u80fd\u4f53\u8f93\u5165\u7279\u5f81\u52a8\u6001\u751f\u6210\uff0c\u5e76\u8bbe\u8ba1\u52a8\u6001\u4e13\u5bb6\u5ea6\u91cf\u635f\u5931\uff08DEML\uff09\u4ee5\u589e\u5f3a\u4e13\u5bb6\u95f4\u591a\u6837\u6027\u3002", "result": "\u5728OPV2V\u548cDAIR-V2X-C\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCoBEVMoE\u5728\u76f8\u673aBEV\u5206\u5272IoU\u4e0a\u63d0\u5347+1.5%\uff0cLiDAR 3D\u68c0\u6d4bAP@50\u4e0a\u63d0\u5347+3.0%\uff0c\u6027\u80fd\u9886\u5148\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u591a\u667a\u80fd\u4f53\u7279\u5f81\u7684\u5f02\u8d28\u6027\u4e0e\u5171\u6027\uff0cCoBEVMoE\u6709\u6548\u63d0\u5347\u4e86\u534f\u4f5c\u611f\u77e5\u7684\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u4e13\u5bb6\u673a\u5236\u7684\u5f02\u6784\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.17421", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.17421", "abs": "https://arxiv.org/abs/2509.17421", "authors": ["Fei Zhao", "Chengqiang Lu", "Yufan Shen", "Qimeng Wang", "Yicheng Qian", "Haoxin Zhang", "Yan Gao", "Yi Wu", "Yao Hu", "Zhen Wu", "Shangyu Xing", "Xinyu Dai"], "title": "RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios", "comment": "Findings of EMNLP 2025 camera-ready", "summary": "While various multimodal multi-image evaluation datasets have been emerged,\nbut these datasets are primarily based on English, and there has yet to be a\nChinese multi-image dataset. To fill this gap, we introduce RealBench, the\nfirst Chinese multimodal multi-image dataset, which contains 9393 samples and\n69910 images. RealBench distinguishes itself by incorporating real\nuser-generated content, ensuring high relevance to real-world applications.\nAdditionally, the dataset covers a wide variety of scenes, image resolutions,\nand image structures, further increasing the difficulty of multi-image\nunderstanding. Ultimately, we conduct a comprehensive evaluation of RealBench\nusing 21 multimodal LLMs of different sizes, including closed-source models\nthat support multi-image inputs as well as open-source visual and video models.\nThe experimental results indicate that even the most powerful closed-source\nmodels still face challenges when handling multi-image Chinese scenarios.\nMoreover, there remains a noticeable performance gap of around 71.8\\% on\naverage between open-source visual/video models and closed-source models. These\nresults show that RealBench provides an important research foundation for\nfurther exploring multi-image understanding capabilities in the Chinese\ncontext.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u4e2d\u6587\u591a\u6a21\u6001\u591a\u56fe\u50cf\u6570\u636e\u96c6RealBench\uff0c\u5305\u542b9393\u4e2a\u6837\u672c\u548c69910\u5f20\u56fe\u50cf\uff0c\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u751f\u6210\u5185\u5bb9\uff0c\u6db5\u76d6\u591a\u79cd\u573a\u666f\u3001\u5206\u8fa8\u7387\u548c\u56fe\u50cf\u7ed3\u6784\u3002\u901a\u8fc7\u5bf921\u4e2a\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u8bc4\u4f30\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u4e2d\u6587\u591a\u56fe\u50cf\u573a\u666f\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5f00\u6e90\u6a21\u578b\u4e0e\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u7ea671.8%\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u586b\u8865\u4e2d\u6587\u591a\u6a21\u6001\u591a\u56fe\u50cf\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u591a\u56fe\u50cf\u7406\u89e3\u5728\u4e2d\u6587\u573a\u666f\u4e0b\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efaRealBench\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u7528\u6237\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u5bf921\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u95ed\u6e90\u548c\u5f00\u6e90\u89c6\u89c9/\u89c6\u9891\u6a21\u578b\uff09\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u5373\u4f7f\u6700\u5f3a\u7684\u95ed\u6e90\u6a21\u578b\u5728\u4e2d\u6587\u591a\u56fe\u50cf\u573a\u666f\u4e2d\u4ecd\u8868\u73b0\u4e0d\u4f73\uff0c\u5f00\u6e90\u89c6\u89c9/\u89c6\u9891\u6a21\u578b\u4e0e\u95ed\u6e90\u6a21\u578b\u5e73\u5747\u6027\u80fd\u5dee\u8ddd\u8fbe71.8%\u3002", "conclusion": "RealBench\u4e3a\u4e2d\u6587\u73af\u5883\u4e0b\u7684\u591a\u56fe\u50cf\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2509.17325", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17325", "abs": "https://arxiv.org/abs/2509.17325", "authors": ["Weihua Du", "Hailei Gong", "Zhan Ling", "Kang Liu", "Lingfeng Shen", "Xuesong Yao", "Yufei Xu", "Dingyuan Shi", "Yiming Yang", "Jiecao Chen"], "title": "Generalizable End-to-End Tool-Use RL with Synthetic CodeGym", "comment": "22 pages. Project available at https://github.com/StigLidu/CodeGym", "summary": "Tool-augmented large language models (LLMs), hereafter LLM agents, leverage\nexternal tools to solve diverse tasks and interface with the real world.\nHowever, current training practices largely rely on supervised fine-tuning\n(SFT) over static trajectories or reinforcement learning (RL) on narrow tasks,\nand generalize poorly beyond development settings, leading to brittleness with\nnew tools and unseen workflows. Because code execution reflects many structures\nof real-world workflows, coding problems provide a natural basis for building\nagent training environments. Motivated by this, we introduce CodeGym, a\nscalable framework that synthesizes diverse, verifiable, and controllable\nmulti-turn tool-use environments for agent RL, enabling LLM agents to explore\nand master various workflows actively. CodeGym rewrites static coding problems\ninto interactive environments by extracting atomic functions or logic into\ncallable tools, yielding verifiable tasks that span various tool-execution\nworkflows. Models of varying sizes and chain-of-thought configurations, trained\nin CodeGym, exhibit consistent out-of-distribution generalizability; for\nexample, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points\non the OOD benchmark $\\tau$-Bench. These results highlight CodeGym as a step\ntoward scalable general-purpose RL environments that align with real-world\nagent workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CodeGym\uff0c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9759\u6001\u7f16\u7a0b\u95ee\u9898\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u73af\u5883\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u591a\u6837\u5316\u3001\u53ef\u9a8c\u8bc1\u548c\u53ef\u63a7\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u73af\u5883\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4ee3\u7406\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u6216\u7a84\u4efb\u52a1\u4e0a\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u5de5\u5177\u548c\u65b0\u5de5\u4f5c\u6d41\u3002\u800c\u4ee3\u7801\u6267\u884c\u7ed3\u6784\u4e0e\u73b0\u5b9e\u4e16\u754c\u5de5\u4f5c\u6d41\u76f8\u4f3c\uff0c\u56e0\u6b64\u5229\u7528\u7f16\u7a0b\u95ee\u9898\u6784\u5efa\u4ee3\u7406\u8bad\u7ec3\u73af\u5883\u5177\u6709\u5929\u7136\u4f18\u52bf\u3002", "method": "\u63d0\u51faCodeGym\u6846\u67b6\uff0c\u5c06\u9759\u6001\u7f16\u7a0b\u9898\u91cd\u5199\u4e3a\u4ea4\u4e92\u5f0f\u73af\u5883\uff0c\u63d0\u53d6\u539f\u5b50\u51fd\u6570\u6216\u903b\u8f91\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u6784\u5efa\u591a\u6837\u5316\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\uff0c\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728CodeGym\u4e2d\u8bad\u7ec3\u7684\u4e0d\u540c\u89c4\u6a21\u548c\u601d\u7ef4\u94fe\u914d\u7f6e\u7684\u6a21\u578b\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u4f8b\u5982Qwen2.5-32B-Instruct\u5728OOD\u57fa\u51c6\u03c4-Bench\u4e0a\u51c6\u786e\u7387\u7edd\u5bf9\u63d0\u53478.7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CodeGym\u662f\u8fc8\u5411\u53ef\u6269\u5c55\u3001\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u6709\u6548\u5bf9\u9f50\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u63d0\u5347LLM\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2509.16956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16956", "abs": "https://arxiv.org/abs/2509.16956", "authors": ["Luca Zanchetta", "Lorenzo Papa", "Luca Maiano", "Irene Amerini"], "title": "VidCLearn: A Continual Learning Approach for Text-to-Video Generation", "comment": null, "summary": "Text-to-video generation is an emerging field in generative AI, enabling the\ncreation of realistic, semantically accurate videos from text prompts. While\ncurrent models achieve impressive visual quality and alignment with input text,\nthey typically rely on static knowledge, making it difficult to incorporate new\ndata without retraining from scratch. To address this limitation, we propose\nVidCLearn, a continual learning framework for diffusion-based text-to-video\ngeneration. VidCLearn features a student-teacher architecture where the student\nmodel is incrementally updated with new text-video pairs, and the teacher model\nhelps preserve previously learned knowledge through generative replay.\nAdditionally, we introduce a novel temporal consistency loss to enhance motion\nsmoothness and a video retrieval module to provide structural guidance at\ninference. Our architecture is also designed to be more computationally\nefficient than existing models while retaining satisfactory generation\nperformance. Experimental results show VidCLearn's superiority over baseline\nmethods in terms of visual quality, semantic alignment, and temporal coherence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6VidCLearn\uff0c\u901a\u8fc7\u5b66\u751f-\u6559\u5e08\u67b6\u6784\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u5b66\u4e60\u65b0\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\uff0c\u96be\u4ee5\u5728\u4e0d\u4ece\u5934\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u878d\u5165\u65b0\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5b66\u751f-\u6559\u5e08\u67b6\u6784\uff0c\u5b66\u751f\u6a21\u578b\u589e\u91cf\u5b66\u4e60\u65b0\u6587\u672c-\u89c6\u9891\u5bf9\uff0c\u6559\u5e08\u6a21\u578b\u901a\u8fc7\u751f\u6210\u56de\u653e\u4fdd\u7559\u65e7\u77e5\u8bc6\uff1b\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u8fd0\u52a8\u5e73\u6ed1\u6027\uff0c\u5e76\u8bbe\u8ba1\u89c6\u9891\u68c0\u7d22\u6a21\u5757\u5728\u63a8\u7406\u65f6\u63d0\u4f9b\u7ed3\u6784\u5f15\u5bfc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVidCLearn\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "VidCLearn\u6709\u6548\u5b9e\u73b0\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u751f\u6210\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17323", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17323", "abs": "https://arxiv.org/abs/2509.17323", "authors": ["Buyin Deng", "Lingxin Huang", "Kai Luo", "Fei Teng", "Kailun Yang"], "title": "DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking", "comment": "The source code will be made publicly available at\n  https://github.com/warriordby/DepTR-MOT", "summary": "Visual Multi-Object Tracking (MOT) is a crucial component of robotic\nperception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D\ncues, such as bounding boxes and motion modeling, which struggle under\nocclusions and close-proximity interactions. Trackers relying on these 2D cues\nare particularly unreliable in robotic environments, where dense targets and\nfrequent occlusions are common. While depth information has the potential to\nalleviate these issues, most existing MOT datasets lack depth annotations,\nleading to its underexploited role in the domain. To unveil the potential of\ndepth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based\ndetector enhanced with instance-level depth information. Specifically, we\npropose two key innovations: (i) foundation model-based instance-level soft\ndepth label supervision, which refines depth prediction, and (ii) the\ndistillation of dense depth maps to maintain global depth consistency. These\nstrategies enable DepTR-MOT to output instance-level depth during inference,\nwithout requiring foundation models and without additional computational cost.\nBy incorporating depth cues, our method enhances the robustness of the TBD\nparadigm, effectively resolving occlusion and close-proximity challenges.\nExperiments on both the QuadTrack and DanceTrack datasets demonstrate the\neffectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,\nrespectively. In particular, results on QuadTrack, a robotic platform MOT\ndataset, highlight the advantages of our method in handling occlusion and\nclose-proximity challenges in robotic tracking. The source code will be made\npublicly available at https://github.com/warriordby/DepTR-MOT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDepTR-MOT\uff0c\u4e00\u79cd\u57fa\u4e8eDETR\u5e76\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u6df1\u5ea6\u4fe1\u606f\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u8f6f\u6df1\u5ea6\u6807\u7b7e\u76d1\u7763\u548c\u7a20\u5bc6\u6df1\u5ea6\u56fe\u84b8\u998f\uff0c\u63d0\u5347\u906e\u6321\u548c\u8fd1\u8ddd\u79bb\u4ea4\u4e92\u4e0b\u7684\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e2D\u7ebf\u7d22\u7684\u89c6\u89c9\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u73af\u5883\u4e2d\u56e0\u9891\u7e41\u906e\u6321\u548c\u5bc6\u96c6\u76ee\u6807\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u591a\u6570MOT\u6570\u636e\u96c6\u7f3a\u4e4f\u6df1\u5ea6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u4fe1\u606f\u7684\u5229\u7528\u3002", "method": "\u63d0\u51faDepTR-MOT\uff0c\u91c7\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u5b9e\u4f8b\u7ea7\u8f6f\u6df1\u5ea6\u6807\u7b7e\u8fdb\u884c\u76d1\u7763\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u7a20\u5bc6\u6df1\u5ea6\u56fe\u4fdd\u6301\u5168\u5c40\u6df1\u5ea6\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u8f93\u51fa\u5b9e\u4f8b\u7ea7\u6df1\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728QuadTrack\u548cDanceTrack\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f9727.59\u548c44.47\u7684HOTA\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u906e\u6321\u548c\u8fd1\u8ddd\u79bb\u573a\u666f\u4e0b\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u5e73\u53f0\u6570\u636e\u96c6QuadTrack\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6df1\u5ea6\u4fe1\u606f\u6709\u6548\u589e\u5f3a\u4e86Tracking-By-Detection\u6846\u67b6\u7684\u9c81\u68d2\u6027\uff0cDepTR-MOT\u4e3a\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17428", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17428", "abs": "https://arxiv.org/abs/2509.17428", "authors": ["Hyesung Jeon", "Seojune Lee", "Beomseok Kang", "Yulhwa Kim", "Jae-Joon Kim"], "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models", "comment": "25 pages, 9 figures, 14 tables", "summary": "The demand for efficient deployment of large language models (LLMs) has\ndriven interest in quantization, which reduces inference cost, and\nparameter-efficient fine-tuning (PEFT), which lowers training overhead. This\nmotivated the development of quantization-aware PEFT to produce accurate yet\nefficient quantized models. In this setting, reducing quantization error prior\nto fine-tuning is crucial for achieving high model accuracy. However, existing\nmethods that rely on low-rank adaptation suffer from limited representational\ncapacity. Recent Fourier-related transform (FT)-based adapters offer greater\nrepresentational power than low-rank adapters, but their direct integration\ninto quantized models often results in ineffective error reduction and\nincreased computational overhead. To overcome these limitations, we propose\nQWHA, a method that integrates FT-based adapters into quantized models by\nemploying the Walsh-Hadamard Transform (WHT) as the transform kernel, together\nwith a novel adapter initialization scheme incorporating adaptive parameter\nselection and value refinement. We demonstrate that QWHA effectively mitigates\nquantization errors while facilitating fine-tuning, and that its design\nsubstantially reduces computational cost. Experimental results show that QWHA\nconsistently outperforms baselines in low-bit quantization accuracy and\nachieves significant training speedups over existing FT-based adapters. The\ncode is available at https://github.com/vantaa89/qwha.", "AI": {"tldr": "\u63d0\u51faQWHA\u65b9\u6cd5\uff0c\u5229\u7528Walsh-Hadamard\u53d8\u6362\u548c\u65b0\u578b\u9002\u914d\u5668\u521d\u59cb\u5316\u65b9\u6848\uff0c\u6709\u6548\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u5e76\u63d0\u5347\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u7684\u5fae\u8c03\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u91cf\u5316\u611f\u77e5\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5728\u8868\u793a\u80fd\u529b\u4e0e\u91cf\u5316\u8bef\u5dee\u6291\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u4f4e\u79e9\u9002\u914d\u5668\u80fd\u529b\u6709\u9650\uff0c\u800c\u5085\u91cc\u53f6\u76f8\u5173\u53d8\u6362\u9002\u914d\u5668\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528Walsh-Hadamard\u53d8\u6362\u4f5c\u4e3a\u53d8\u6362\u6838\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u53c2\u6570\u9009\u62e9\u4e0e\u503c\u4f18\u5316\u7684\u65b0\u578b\u9002\u914d\u5668\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5c06FT-based\u9002\u914d\u5668\u6709\u6548\u96c6\u6210\u5230\u91cf\u5316\u6a21\u578b\u4e2d\u3002", "result": "QWHA\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u8bad\u7ec3\u901f\u5ea6\u660e\u663e\u5feb\u4e8e\u73b0\u6709FT-based\u9002\u914d\u5668\u3002", "conclusion": "QWHA\u80fd\u6709\u6548\u7f13\u89e3\u91cf\u5316\u8bef\u5dee\uff0c\u63d0\u5347\u91cf\u5316\u6a21\u578b\u5fae\u8c03\u6548\u679c\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u5e73\u8861\u3002"}}
{"id": "2509.17400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17400", "abs": "https://arxiv.org/abs/2509.17400", "authors": ["Xiaoyang Xu", "Xiaofeng Lin", "Koh Takeuchi", "Kyohei Atarashi", "Hisashi Kashima"], "title": "Robust Anomaly Detection Under Normality Distribution Shift in Dynamic Graphs", "comment": null, "summary": "Anomaly detection in dynamic graphs is a critical task with broad real-world\napplications, including social networks, e-commerce, and cybersecurity. Most\nexisting methods assume that normal patterns remain stable over time; however,\nthis assumption often fails in practice due to the phenomenon we refer to as\nnormality distribution shift (NDS), where normal behaviors evolve over time.\nIgnoring NDS can lead models to misclassify shifted normal instances as\nanomalies, degrading detection performance. To tackle this issue, we propose\nWhENDS, a novel unsupervised anomaly detection method that aligns normal edge\nembeddings across time by estimating distributional statistics and applying\nwhitening transformations. Extensive experiments on four widely-used dynamic\ngraph datasets show that WhENDS consistently outperforms nine strong baselines,\nachieving state-of-the-art results and underscoring the importance of\naddressing NDS in dynamic graph anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5WhENDS\uff0c\u901a\u8fc7\u767d\u5316\u53d8\u6362\u5bf9\u9f50\u52a8\u6001\u56fe\u4e2d\u8de8\u65f6\u95f4\u7684\u6b63\u5e38\u8fb9\u5d4c\u5165\uff0c\u6709\u6548\u5e94\u5bf9\u6b63\u5e38\u6027\u5206\u5e03\u6f02\u79fb\uff08NDS\uff09\u95ee\u9898\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4e5d\u4e2a\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6b63\u5e38\u6a21\u5f0f\u5728\u65f6\u95f4\u4e0a\u7a33\u5b9a\uff0c\u4f46\u5728\u5b9e\u9645\u4e2d\u7531\u4e8e\u6b63\u5e38\u884c\u4e3a\u968f\u65f6\u95f4\u6f14\u53d8\uff08\u5373\u6b63\u5e38\u6027\u5206\u5e03\u6f02\u79fb\uff0cNDS\uff09\uff0c\u8fd9\u4e00\u5047\u8bbe\u5e38\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u6a21\u578b\u8bef\u5c06\u6b63\u5e38\u53d8\u5316\u8bc6\u522b\u4e3a\u5f02\u5e38\u3002", "method": "\u63d0\u51faWhENDS\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f30\u8ba1\u5206\u5e03\u7edf\u8ba1\u91cf\u5e76\u5e94\u7528\u767d\u5316\u53d8\u6362\uff0c\u5bf9\u9f50\u4e0d\u540c\u65f6\u523b\u7684\u6b63\u5e38\u8fb9\u5d4c\u5165\uff0c\u4ece\u800c\u6d88\u9664NDS\u5f71\u54cd\uff0c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u56db\u4e2a\u5e38\u7528\u52a8\u6001\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWhENDS consistently\u4f18\u4e8e\u4e5d\u4e2a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u89e3\u51b3\u6b63\u5e38\u6027\u5206\u5e03\u6f02\u79fb\u5bf9\u52a8\u6001\u56fe\u5f02\u5e38\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0cWhENDS\u901a\u8fc7\u5d4c\u5165\u5bf9\u9f50\u6709\u6548\u5e94\u5bf9\u8be5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2509.16957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16957", "abs": "https://arxiv.org/abs/2509.16957", "authors": ["Leiyu Wang", "Biao Jin", "Feng Huang", "Liqiong Chen", "Zhengyong Wang", "Xiaohai He", "Honggang Chen"], "title": "MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image", "comment": null, "summary": "Oriented object detection for multi-spectral imagery faces significant\nchallenges due to differences both within and between modalities. Although\nexisting methods have improved detection accuracy through complex network\narchitectures, their high computational complexity and memory consumption\nseverely restrict their performance. Motivated by the success of large kernel\nconvolutions in remote sensing, we propose MO R-CNN, a lightweight framework\nfor multi-spectral oriented detection featuring heterogeneous feature\nextraction network (HFEN), single modality supervision (SMS), and\ncondition-based multimodal label fusion (CMLF). HFEN leverages inter-modal\ndifferences to adaptively align, merge, and enhance multi-modal features. SMS\nconstrains multi-scale features and enables the model to learn from multiple\nmodalities. CMLF fuses multimodal labels based on specific rules, providing the\nmodel with a more robust and consistent supervisory signal. Experiments on the\nDroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The\nsource code is available at:https://github.com/Iwill-github/MORCNN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u5149\u8c31\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u6846\u67b6MO R-CNN\uff0c\u5305\u542b\u5f02\u6784\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u3001\u5355\u6a21\u6001\u76d1\u7763\u548c\u57fa\u4e8e\u6761\u4ef6\u7684\u591a\u6a21\u6001\u6807\u7b7e\u878d\u5408\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u590d\u6742\u7f51\u7edc\u7ed3\u6784\u5bfc\u81f4\u8ba1\u7b97\u91cf\u548c\u5185\u5b58\u6d88\u8017\u9ad8\uff0c\u9650\u5236\u4e86\u591a\u5149\u8c31\u56fe\u50cf\u4e2d\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e14\u6a21\u6001\u95f4\u5dee\u5f02\u5e26\u6765\u6311\u6218\u3002", "method": "\u8bbe\u8ba1MO R-CNN\u6846\u67b6\uff0c\u91c7\u7528HFEN\u81ea\u9002\u5e94\u5bf9\u9f50\u3001\u878d\u5408\u548c\u589e\u5f3a\u591a\u6a21\u6001\u7279\u5f81\uff0c\u5f15\u5165SMS\u7ea6\u675f\u591a\u5c3a\u5ea6\u7279\u5f81\u5e76\u5b9e\u73b0\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u901a\u8fc7CMLF\u57fa\u4e8e\u89c4\u5219\u878d\u5408\u591a\u6a21\u6001\u6807\u7b7e\u4ee5\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728DroneVehicle\u3001VEDAI\u548cOGSOD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MO R-CNN\u901a\u8fc7\u8f7b\u91cf\u5316\u7684\u7ed3\u6784\u8bbe\u8ba1\u548c\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5149\u8c31\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17430", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17430", "abs": "https://arxiv.org/abs/2509.17430", "authors": ["Gunjan Chhablani", "Xiaomeng Ye", "Muhammad Zubair Irshad", "Zsolt Kira"], "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device", "comment": "16 pages, 18 figures, paper accepted at ICCV, 2025", "summary": "The field of Embodied AI predominantly relies on simulation for training and\nevaluation, often using either fully synthetic environments that lack\nphotorealism or high-fidelity real-world reconstructions captured with\nexpensive hardware. As a result, sim-to-real transfer remains a major\nchallenge. In this paper, we introduce EmbodiedSplat, a novel approach that\npersonalizes policy training by efficiently capturing the deployment\nenvironment and fine-tuning policies within the reconstructed scenes. Our\nmethod leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to\nbridge the gap between realistic scene capture and effective training\nenvironments. Using iPhone-captured deployment scenes, we reconstruct meshes\nvia GS, enabling training in settings that closely approximate real-world\nconditions. We conduct a comprehensive analysis of training strategies,\npre-training datasets, and mesh reconstruction techniques, evaluating their\nimpact on sim-to-real predictivity in real-world scenarios. Experimental\nresults demonstrate that agents fine-tuned with EmbodiedSplat outperform both\nzero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and\nsynthetically generated datasets (HSSD), achieving absolute success rate\nimprovements of 20\\% and 40\\% on real-world Image Navigation task. Moreover,\nour approach yields a high sim-vs-real correlation (0.87--0.97) for the\nreconstructed meshes, underscoring its effectiveness in adapting policies to\ndiverse environments with minimal effort. Project page:\nhttps://gchhablani.github.io/embodied-splat", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EmbodiedSplat\uff0c\u4e00\u79cd\u5229\u75283D\u9ad8\u65af\u70b9\u9635\uff08GS\uff09\u548cHabitat-Sim\u6a21\u62df\u5668\uff0c\u901a\u8fc7iPhone\u6355\u83b7\u7684\u573a\u666f\u91cd\u5efamesh\u6765\u5b9e\u73b0\u7b56\u7565\u5fae\u8c03\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eabAI\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5177\u8eabAI\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u611f\u7684\u5408\u6210\u73af\u5883\u6216\u6602\u8d35\u8bbe\u5907\u91c7\u96c6\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u5bfc\u81f4\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u56f0\u96be\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7387\u4e14\u8d34\u8fd1\u771f\u5b9e\u73af\u5883\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528iPhone\u62cd\u6444\u90e8\u7f72\u73af\u5883\uff0c\u5229\u75283D\u9ad8\u65af\u70b9\u9635\uff08Gaussian Splatting\uff09\u91cd\u5efa\u573a\u666fmesh\uff0c\u5e76\u5728Habitat-Sim\u4e2d\u8fdb\u884c\u7b56\u7565\u5fae\u8c03\uff0c\u7ed3\u5408\u4e0d\u540c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u91cd\u5efa\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u6a21\u578b\uff0c\u6210\u529f\u7387\u7edd\u5bf9\u63d0\u534720%\uff08HM3D\uff09\u548c40%\uff08HSSD\uff09\uff0c\u4e14\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u76f8\u5173\u6027\u9ad8\u8fbe0.87\u20130.97\u3002", "conclusion": "EmbodiedSplat\u80fd\u6709\u6548\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u73af\u5883\u91cd\u5efa\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u9002\u5e94\uff0c\u4e3a\u5177\u8eabAI\u7684\u73b0\u5b9e\u90e8\u7f72\u63d0\u4f9b\u4e86\u4f4e\u95e8\u69db\u3001\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17436", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17436", "abs": "https://arxiv.org/abs/2509.17436", "authors": ["Tong Chen", "Zimu Wang", "Yiyi Miao", "Haoran Luo", "Yuanfei Sun", "Wei Wang", "Zhengyong Jiang", "Procheta Sen", "Jionglong Su"], "title": "MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses", "comment": "Accepted at EMNLP 2025. Camera-ready version", "summary": "Medical fact-checking has become increasingly critical as more individuals\nseek medical information online. However, existing datasets predominantly focus\non human-generated content, leaving the verification of content generated by\nlarge language models (LLMs) relatively unexplored. To address this gap, we\nintroduce MedFact, the first evidence-based Chinese medical fact-checking\ndataset of LLM-generated medical content. It consists of 1,321 questions and\n7,409 claims, mirroring the complexities of real-world medical scenarios. We\nconduct comprehensive experiments in both in-context learning (ICL) and\nfine-tuning settings, showcasing the capability and challenges of current LLMs\non this task, accompanied by an in-depth error analysis to point out key\ndirections for future research. Our dataset is publicly available at\nhttps://github.com/AshleyChenNLP/MedFact.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MedFact\uff0c\u9996\u4e2a\u57fa\u4e8e\u8bc1\u636e\u7684\u4e2d\u6587\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u533b\u5b66\u5185\u5bb9\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b1,321\u4e2a\u95ee\u9898\u548c7,409\u6761\u58f0\u660e\uff0c\u53cd\u6620\u4e86\u771f\u5b9e\u4e16\u754c\u533b\u5b66\u573a\u666f\u7684\u590d\u6742\u6027\u3002\u7814\u7a76\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4eba\u7c7b\u751f\u6210\u5185\u5bb9\uff0c\u7f3a\u4e4f\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u9a8c\u8bc1\u7814\u7a76\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9LLM\u751f\u6210\u5185\u5bb9\u7684\u4e2d\u6587\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\uff0c\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMedFact\u7684\u8bc1\u636e-based\u4e2d\u6587\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\uff0c\u5305\u542b1,321\u4e2a\u95ee\u9898\u548c7,409\u6761\u7531LLM\u751f\u6210\u7684\u533b\u5b66\u58f0\u660e\u3002\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548c\u5fae\u8c03\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u5bf9\u5f53\u524d\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u6df1\u5165\u7684\u9519\u8bef\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e0a\u5177\u6709\u4e00\u5b9a\u80fd\u529b\uff0c\u4f46\u4e5f\u66b4\u9732\u51fa\u8bf8\u591a\u6311\u6218\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5728\u63a8\u7406\u3001\u8bc1\u636e\u5339\u914d\u548c\u533b\u5b66\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "MedFact\u662f\u9996\u4e2a\u9762\u5411LLM\u751f\u6210\u5185\u5bb9\u7684\u4e2d\u6587\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1LLM\u5728\u8be5\u4efb\u52a1\u4e0a\u5177\u5907\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u533b\u5b66\u9a8c\u8bc1\u9700\u6c42\u3002"}}
{"id": "2509.17405", "categories": ["cs.LG", "49Q22 (Primary) 90C57, 68Txx (Secondary)", "G.3; I.2"], "pdf": "https://arxiv.org/pdf/2509.17405", "abs": "https://arxiv.org/abs/2509.17405", "authors": ["Manish Acharya", "David Hyde"], "title": "Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization", "comment": "19 pages, 11 figures", "summary": "The sliced Wasserstein distance (SW) reduces optimal transport on\n$\\mathbb{R}^d$ to a sum of one-dimensional projections, and thanks to this\nefficiency, it is widely used in geometry, generative modeling, and\nregistration tasks. Recent work shows that quasi-Monte Carlo constructions for\ncomputing SW (QSW) yield direction sets with excellent approximation error.\nThis paper presents an alternate, novel approach: learning directions with\nBayesian optimization (BO), particularly in settings where SW appears inside an\noptimization loop (e.g., gradient flows). We introduce a family of drop-in\nselectors for projection directions: BOSW, a one-shot BO scheme on the unit\nsphere; RBOSW, a periodic-refresh variant; ABOSW, an adaptive hybrid that seeds\nfrom competitive QSW sets and performs a few lightweight BO refinements; and\nARBOSW, a restarted hybrid that periodically relearns directions during\noptimization. Our BO approaches can be composed with QSW and its variants\n(demonstrated by ABOSW/ARBOSW) and require no changes to downstream losses or\ngradients. We provide numerical experiments where our methods achieve\nstate-of-the-art performance, and on the experimental suite of the original QSW\npaper, we find that ABOSW and ARBOSW can achieve convergence comparable to the\nbest QSW variants with modest runtime overhead.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u7684 sliced Wasserstein \u8ddd\u79bb\u65b9\u5411\u9009\u62e9\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec BOSW\u3001RBOSW\u3001ABOSW \u548c ARBOSW\uff0c\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u635f\u5931\u517c\u5bb9\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf sliced Wasserstein (SW) \u4f9d\u8d56\u968f\u673a\u6216\u51c6\u8499\u7279\u5361\u6d1b\uff08QSW\uff09\u6295\u5f71\u65b9\u5411\uff0c\u6548\u7387\u6709\u9650\uff1b\u5f53 SW \u7528\u4e8e\u4f18\u5316\u5faa\u73af\uff08\u5982\u68af\u5ea6\u6d41\uff09\u65f6\uff0c\u56fa\u5b9a\u6216\u9759\u6001\u65b9\u5411\u96c6\u53ef\u80fd\u975e\u6700\u4f18\uff0c\u9700\u81ea\u9002\u5e94\u5b66\u4e60\u9ad8\u6548\u65b9\u5411\u4ee5\u52a0\u901f\u6536\u655b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u7684\u65b9\u5411\u9009\u62e9\u6846\u67b6\uff1aBOSW\uff08\u4e00\u6b21\u6027\u7403\u9762BO\uff09\u3001RBOSW\uff08\u5468\u671f\u5237\u65b0\uff09\u3001ABOSW\uff08\u81ea\u9002\u5e94\u6df7\u5408\uff0c\u4ee5QSW\u4e3a\u521d\u503c\u5e76\u5fae\u8c03\uff09\u3001ARBOSW\uff08\u91cd\u542f\u5f0f\u6df7\u5408\uff0c\u5728\u4f18\u5316\u4e2d\u5468\u671f\u6027\u91cd\u5b66\u4e60\uff09\u3002\u65b9\u6cd5\u53ef\u4e0eQSW\u7ed3\u5408\uff0c\u65e0\u9700\u4fee\u6539\u4e0b\u6e38\u635f\u5931\u6216\u68af\u5ea6\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u539fQSW\u8bba\u6587\u7684\u6d4b\u8bd5\u5957\u4ef6\u4e0a\uff0cABOSW\u548cARBOSW\u4ee5\u9002\u5ea6\u8fd0\u884c\u5f00\u9500\u5b9e\u73b0\u4e86\u4e0e\u6700\u4f73QSW\u53d8\u4f53\u76f8\u5f53\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u8d1d\u53f6\u65af\u4f18\u5316\u4e3a sliced Wasserstein \u8ddd\u79bb\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u5373\u63d2\u5373\u7528\u7684\u65b9\u5411\u5b66\u4e60\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5d4c\u5165\u4f18\u5316\u5faa\u73af\u7684\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6536\u655b\u6548\u7387\u3002"}}
{"id": "2509.16968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16968", "abs": "https://arxiv.org/abs/2509.16968", "authors": ["Haoyang Xu", "Tianhao Zhao", "Sibei Yang", "Yutian Li"], "title": "Penalizing Boundary Activation for Object Completeness in Diffusion Models", "comment": null, "summary": "Diffusion models have emerged as a powerful technique for text-to-image (T2I)\ngeneration, creating high-quality, diverse images across various domains.\nHowever, a common limitation in these models is the incomplete display of\nobjects, where fragments or missing parts undermine the model's performance in\ndownstream applications. In this study, we conduct an in-depth analysis of the\nincompleteness issue and reveal that the primary factor behind incomplete\nobject generation is the usage of RandomCrop during model training. This widely\nused data augmentation method, though enhances model generalization ability,\ndisrupts object continuity during training. To address this, we propose a\ntraining-free solution that penalizes activation values at image boundaries\nduring the early denoising steps. Our method is easily applicable to\npre-trained Stable Diffusion models with minimal modifications and negligible\ncomputational overhead. Extensive experiments demonstrate the effectiveness of\nour method, showing substantial improvements in object integrity and image\nquality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53bb\u566a\u65e9\u671f\u60e9\u7f5a\u56fe\u50cf\u8fb9\u754c\u5904\u7684\u6fc0\u6d3b\u503c\u6765\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u5bf9\u8c61\u4e0d\u5b8c\u6574\u751f\u6210\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5bf9\u8c61\u4e0d\u5b8c\u6574\u663e\u793a\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8bad\u7ec3\u4e2d\u7684RandomCrop\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u662f\u5bfc\u81f4\u6b64\u95ee\u9898\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u65e9\u671f\u9636\u6bb5\u5bf9\u56fe\u50cf\u8fb9\u754c\u7684\u6fc0\u6d3b\u503c\u8fdb\u884c\u60e9\u7f5a\uff0c\u4ee5\u6062\u590d\u5bf9\u8c61\u7684\u5b8c\u6574\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u8f7b\u677e\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684Stable Diffusion\u6a21\u578b\uff0c\u4e14\u51e0\u4e4e\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u5b8c\u6574\u6027\u548c\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u53bb\u566a\u521d\u671f\u7684\u8fb9\u754c\u6fc0\u6d3b\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u7531RandomCrop\u5f15\u8d77\u7684\u76ee\u6807\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u6269\u6563\u6a21\u578b\u751f\u6210\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17647", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17647", "abs": "https://arxiv.org/abs/2509.17647", "authors": ["Yu Liu", "Baoxiong Jia", "Ruijie Lu", "Chuyue Gan", "Huayu Chen", "Junfeng Ni", "Song-Chun Zhu", "Siyuan Huang"], "title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video", "comment": null, "summary": "Building digital twins of articulated objects from monocular video presents\nan essential challenge in computer vision, which requires simultaneous\nreconstruction of object geometry, part segmentation, and articulation\nparameters from limited viewpoint inputs. Monocular video offers an attractive\ninput format due to its simplicity and scalability; however, it's challenging\nto disentangle the object geometry and part dynamics with visual supervision\nalone, as the joint movement of the camera and parts leads to ill-posed\nestimation. While motion priors from pre-trained tracking models can alleviate\nthe issue, how to effectively integrate them for articulation learning remains\nlargely unexplored. To address this problem, we introduce VideoArtGS, a novel\napproach that reconstructs high-fidelity digital twins of articulated objects\nfrom monocular video. We propose a motion prior guidance pipeline that analyzes\n3D tracks, filters noise, and provides reliable initialization of articulation\nparameters. We also design a hybrid center-grid part assignment module for\narticulation-based deformation fields that captures accurate part motion.\nVideoArtGS demonstrates state-of-the-art performance in articulation and mesh\nreconstruction, reducing the reconstruction error by about two orders of\nmagnitude compared to existing methods. VideoArtGS enables practical digital\ntwin creation from monocular video, establishing a new benchmark for\nvideo-based articulated object reconstruction. Our work is made publicly\navailable at: https://videoartgs.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVideoArtGS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u4f53\uff0c\u901a\u8fc7\u5f15\u5165\u8fd0\u52a8\u5148\u9a8c\u5f15\u5bfc\u7ba1\u9053\u548c\u6df7\u5408\u4e2d\u5fc3-\u7f51\u683c\u90e8\u4ef6\u5206\u914d\u6a21\u5757\uff0c\u5728\u5173\u8282\u8fd0\u52a8\u548c\u7f51\u683c\u91cd\u5efa\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u6784\u5efa\u5173\u8282\u7269\u4f53\u7684\u6570\u5b57\u5b6a\u751f\u4f53\u9762\u4e34\u51e0\u4f55\u4e0e\u8fd0\u52a8\u89e3\u8026\u56f0\u96be\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u89c6\u89c9\u76d1\u7763\u548c\u8fd0\u52a8\u5148\u9a8c\u8fdb\u884c\u7cbe\u786e\u5efa\u6a21\u3002", "method": "\u63d0\u51faVideoArtGS\uff0c\u5305\u542b\u8fd0\u52a8\u5148\u9a8c\u5f15\u5bfc\u6d41\u7a0b\u4ee5\u5206\u67903D\u8f68\u8ff9\u5e76\u521d\u59cb\u5316\u5173\u8282\u53c2\u6570\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u4e2d\u5fc3-\u7f51\u683c\u90e8\u4ef6\u5206\u914d\u6a21\u5757\u6765\u6355\u6349\u57fa\u4e8e\u5173\u8282\u7684\u5f62\u53d8\u573a\u3002", "result": "\u5728\u5173\u8282\u8fd0\u52a8\u548c\u7f51\u683c\u91cd\u5efa\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u91cd\u5efa\u8bef\u5dee\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "VideoArtGS\u5b9e\u73b0\u4e86\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u9ad8\u6548\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u5b57\u5b6a\u751f\u4f53\uff0c\u4e3a\u57fa\u4e8e\u89c6\u9891\u7684\u5173\u8282\u7269\u4f53\u91cd\u5efa\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.17437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17437", "abs": "https://arxiv.org/abs/2509.17437", "authors": ["Guizhen Chen", "Weiwen Xu", "Hao Zhang", "Hou Pong Chan", "Deli Zhao", "Anh Tuan Luu", "Yu Rong"], "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning", "comment": "Accepted to EMNLP2025 Findings", "summary": "Recent advancements in reinforcement learning (RL) have enhanced the\nreasoning abilities of large language models (LLMs), yet the impact on\nmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like\ngeometric reasoning, MLLMs hallucinate frequently, leading to inaccurate\nreasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps\nthe benefits of reasoning training. To quantify this, we design a\nGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric\nconcepts and spatial relationships. Experiments on GeoPQA reveal significant\nshortcomings of MLLMs in visual perception, which constrain RL reward signals\nfor effective training. To address this bottleneck, we propose a two-stage RL\ntraining framework by first enhancing the visual perception of geometric\nstructures, then fostering reasoning capabilities. Applied to\nQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by\n9.7% and geometric problem solving by 9.1%, compared to the direct reasoning\ntraining approach. Our method also generalizes to other vision-intensive\ndomains like figure understanding, highlighting the importance of perceptual\ngrounding in effective MLLM reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u89c6\u89c9\u611f\u77e5\u74f6\u9888\u5bfc\u81f4\u7684\u63a8\u7406\u9519\u8bef\u95ee\u9898\u3002\u901a\u8fc7\u5148\u589e\u5f3a\u51e0\u4f55\u7ed3\u6784\u7684\u89c6\u89c9\u611f\u77e5\uff0c\u518d\u8fdb\u884c\u63a8\u7406\u80fd\u529b\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u51e0\u4f55\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u51e0\u4f55\u63a8\u7406\uff09\u4e2d\u9891\u7e41\u51fa\u73b0\u5e7b\u89c9\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u5176\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u4fe1\u53f7\u65e0\u6548\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u6a21\u578b\u7684\u611f\u77e5\u57fa\u7840\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aGeoPQA\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u51e0\u4f55\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e13\u6ce8\u4e8e\u63d0\u5347\u51e0\u4f55\u7ed3\u6784\u7684\u89c6\u89c9\u611f\u77e5\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u63a8\u7406\u8bad\u7ec3\u3002", "result": "\u5728Qwen2.5-VL-3B-Instruct\u6a21\u578b\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\u540e\uff0c\u76f8\u6bd4\u76f4\u63a5\u8fdb\u884c\u63a8\u7406\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e869.7%\uff0c\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u63d0\u5347\u4e869.1%\uff0c\u4e14\u8be5\u65b9\u6cd5\u5728\u56fe\u8868\u7406\u89e3\u7b49\u5176\u4ed6\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u611f\u77e5\u57fa\u7840\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\u611f\u77e5\u4e0e\u63a8\u7406\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.17413", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17413", "abs": "https://arxiv.org/abs/2509.17413", "authors": ["Masako Kishida"], "title": "Distributionally Robust Safety Verification of Neural Networks via Worst-Case CVaR", "comment": null, "summary": "Ensuring the safety of neural networks under input uncertainty is a\nfundamental challenge in safety-critical applications. This paper builds on and\nexpands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP)\nframework for neural network verification to a distributionally robust and\ntail-risk-aware setting by integrating worst-case Conditional Value-at-Risk\n(WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The\nresulting conditions remain SDP-checkable and explicitly account for tail risk.\nThis integration broadens input-uncertainty geometry-covering ellipsoids,\npolytopes, and hyperplanes-and extends applicability to safety-critical domains\nwhere tail-event severity matters. Applications to closed-loop reachability of\ncontrol systems and classification are demonstrated through numerical\nexperiments, illustrating how the risk level $\\varepsilon$ trades conservatism\nfor tolerance to tail events-while preserving the computational structure of\nprior QC/SDP methods for neural network verification and robustness analysis.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Fazlyab\u7684\u4e8c\u6b21\u7ea6\u675f\u548c\u534a\u5b9a\u89c4\u5212\u6846\u67b6\uff0c\u5f15\u5165\u57fa\u4e8e\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08WC-CVaR\uff09\u7684\u5206\u5e03\u9c81\u68d2\u4e0e\u5c3e\u90e8\u98ce\u9669\u611f\u77e5\u65b9\u6cd5\uff0c\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u5728\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9a8c\u8bc1\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u5728\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5b89\u5168\u6027\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5c3e\u90e8\u98ce\u9669\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u5728\u56fa\u5b9a\u5747\u503c\u548c\u534f\u65b9\u5dee\u7684\u77e9\u6a21\u7cca\u96c6\u4e0a\u6574\u5408\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08WC-CVaR\uff09\uff0c\u5c06\u539f\u6709QC/SDP\u6846\u67b6\u6269\u5c55\u5230\u5206\u5e03\u9c81\u68d2\u548c\u5c3e\u98ce\u9669\u611f\u77e5\u8bbe\u7f6e\uff0c\u5e76\u4fdd\u6301\u5176\u53ef\u8f6c\u5316\u4e3a\u534a\u5b9a\u89c4\u5212\u95ee\u9898\u3002", "result": "\u65b0\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u692d\u7403\u3001\u591a\u9762\u4f53\u548c\u8d85\u5e73\u9762\u7b49\u591a\u79cd\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u51e0\u4f55\u7ed3\u6784\uff0c\u663e\u5f0f\u8003\u8651\u5c3e\u90e8\u98ce\u9669\uff0c\u5728\u63a7\u5236\u7cfb\u7edf\u7684\u95ed\u73af\u53ef\u8fbe\u6027\u5206\u6790\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u98ce\u9669\u6c34\u5e73\u03b5\u5728\u4fdd\u5b88\u6027\u4e0e\u5c3e\u90e8\u4e8b\u4ef6\u5bb9\u5fcd\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u7559\u539f\u6709QC/SDP\u8ba1\u7b97\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5c3e\u90e8\u98ce\u9669\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5bf9\u6781\u7aef\u4e8b\u4ef6\u654f\u611f\u7684\u5b89\u5168\u5173\u952e\u9886\u57df\u3002"}}
{"id": "2509.16970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16970", "abs": "https://arxiv.org/abs/2509.16970", "authors": ["Wei Liao", "Chunyan Xu", "Chenxu Wang", "Zhen Cui"], "title": "LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection", "comment": null, "summary": "Sparse annotation in remote sensing object detection poses significant\nchallenges due to dense object distributions and category imbalances. Although\nexisting Dense Pseudo-Label methods have demonstrated substantial potential in\npseudo-labeling tasks, they remain constrained by selection ambiguities and\ninconsistencies in confidence estimation.In this paper, we introduce an\nLLM-assisted semantic guidance framework tailored for sparsely annotated remote\nsensing object detection, exploiting the advanced semantic reasoning\ncapabilities of large language models (LLMs) to distill high-confidence\npseudo-labels.By integrating LLM-generated semantic priors, we propose a\nClass-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns\npseudo-labels for both unlabeled and sparsely labeled data, ensuring robust\nsupervision across varying data distributions. Additionally, we develop an\nAdaptive Hard-Negative Reweighting Module to stabilize the supervised learning\nbranch by mitigating the influence of confounding background information.\nExtensive experiments on DOTA and HRSC2016 demonstrate that the proposed method\noutperforms existing single-stage detector-based frameworks, significantly\nimproving detection performance under sparse annotations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bed\u4e49\u5f15\u5bfc\u7684\u7a00\u758f\u6807\u6ce8\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u611f\u77e5\u5bc6\u96c6\u4f2a\u6807\u7b7e\u5206\u914d\u548c\u81ea\u9002\u5e94\u96be\u8d1f\u6837\u672c\u91cd\u52a0\u6743\u6a21\u5757\uff0c\u63d0\u5347\u7a00\u758f\u6807\u6ce8\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u6807\u6ce8\u5728\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u4e2d\u56e0\u76ee\u6807\u5bc6\u96c6\u5206\u5e03\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5e26\u6765\u6311\u6218\uff0c\u73b0\u6709\u5bc6\u96c6\u4f2a\u6807\u7b7e\u65b9\u6cd5\u53d7\u9650\u4e8e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165LLM\u8f85\u52a9\u7684\u8bed\u4e49\u5f15\u5bfc\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\uff1b\u8bbe\u8ba1\u7c7b\u611f\u77e5\u5bc6\u96c6\u4f2a\u6807\u7b7e\u5206\u914d\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u4e3a\u672a\u6807\u6ce8\u548c\u7a00\u758f\u6807\u6ce8\u6570\u636e\u5206\u914d\u4f2a\u6807\u7b7e\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u96be\u8d1f\u6837\u672c\u91cd\u52a0\u6743\u6a21\u5757\uff0c\u51cf\u8f7b\u80cc\u666f\u5e72\u6270\u5bf9\u76d1\u7763\u5b66\u4e60\u5206\u652f\u7684\u5f71\u54cd\u3002", "result": "\u5728DOTA\u548cHRSC2016\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u6846\u67b6\uff0c\u5728\u7a00\u758f\u6807\u6ce8\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u7a00\u758f\u6807\u6ce8\u5e26\u6765\u7684\u76d1\u7763\u4e0d\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u8bed\u4e49\u5148\u9a8c\u548c\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u9065\u611f\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2509.17684", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.17684", "abs": "https://arxiv.org/abs/2509.17684", "authors": ["ThankGod Egbe", "Peng Wang", "Zhihao Guo", "Zidong Chen"], "title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning", "comment": null, "summary": "This paper evaluates DINOv3, a recent large-scale self-supervised vision\nbackbone, for visuomotor diffusion policy learning in robotic manipulation. We\ninvestigate whether a purely self-supervised encoder can match or surpass\nconventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under\nthree regimes: training from scratch, frozen, and finetuned. Across four\nbenchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned\ndiffusion policy, we find that (i) finetuned DINOv3 matches or exceeds\nResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating\nstrong transferable priors, and (iii) self-supervised features improve sample\nefficiency and robustness. These results support self-supervised large visual\nmodels as effective, generalizable perceptual front-ends for action diffusion\npolicies, motivating further exploration of scalable label-free pretraining in\nrobotic manipulation. Compared to using ResNet18 as a backbone, our approach\nwith DINOv3 achieves up to a 10% absolute increase in test-time success rates\non challenging tasks such as Can, and on-the-par performance in tasks like\nLift, PushT, and Square.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc4\u4f30\u4e86DINOv3\u8fd9\u4e00\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u6269\u6563\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6216\u5ab2\u7f8e\u4f20\u7edf\u7684ImageNet\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982ResNet-18\uff09\uff0c\u5c24\u5176\u5728\u6837\u672c\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u7eaf\u81ea\u76d1\u7763\u7f16\u7801\u5668\u662f\u5426\u80fd\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u673a\u5668\u4eba\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u4e2d\u66ff\u4ee3\u6216\u8d85\u8d8a\u4f20\u7edf\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u5728Push-T\u3001Lift\u3001Can\u3001Square\u56db\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0c\u91c7\u7528\u7edf\u4e00\u7684FiLM\u6761\u4ef6\u6269\u6563\u7b56\u7565\uff0c\u6bd4\u8f83DINOv3\u5728\u4ece\u5934\u8bad\u7ec3\u3001\u51bb\u7ed3\u548c\u5fae\u8c03\u4e09\u79cd\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u4e0eResNet-18\u7684\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u540e\u7684DINOv3\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7ResNet-18\uff1b\u51bb\u7ed3\u72b6\u6001\u4e0b\u7684DINOv3\u4ecd\u5177\u7ade\u4e89\u529b\uff1b\u81ea\u76d1\u7763\u7279\u5f81\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\uff1b\u5728Can\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u6210\u529f\u7387\u6700\u9ad8\u63d0\u5347\u8fbe10%\u3002", "conclusion": "DINOv3\u4f5c\u4e3a\u81ea\u76d1\u7763\u5927\u6a21\u578b\u53ef\u6709\u6548\u5145\u5f53\u6269\u6563\u7b56\u7565\u7684\u611f\u77e5\u524d\u7aef\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u63a2\u7d22\u65e0\u6807\u7b7e\u3001\u53ef\u6269\u5c55\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.17444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17444", "abs": "https://arxiv.org/abs/2509.17444", "authors": ["Shohei Hisada", "Endo Sunao", "Himi Yamato", "Shoko Wakamiya", "Eiji Aramaki"], "title": "Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system", "comment": "draft v0.1", "summary": "This study investigates the applicability of HealthBench, a large-scale,\nrubric-based medical benchmark, to the Japanese context. While robust\nevaluation frameworks are crucial for the safe development of medical LLMs,\nresources in Japanese remain limited, often relying on translated\nmultiple-choice questions. Our research addresses this gap by first\nestablishing a performance baseline, applying a machine-translated version of\nHealthBench's 5,000 scenarios to evaluate both a high-performing multilingual\nmodel (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,\nwe employ an LLM-as-a-Judge approach to systematically classify the benchmark's\nscenarios and rubric criteria, identifying \"contextual gaps\" where content is\nmisaligned with Japan's clinical guidelines, healthcare systems, or cultural\nnorms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric\nmismatches and a significant failure in the Japanese-native model, which lacked\nthe required clinical completeness. Furthermore, our classification indicates\nthat while the majority of scenarios are applicable, a substantial portion of\nthe rubric criteria requires localization. This work underscores the\nlimitations of direct benchmark translation and highlights the urgent need for\na context-aware, localized adaptation, a J-HealthBench, to ensure the reliable\nand safe evaluation of medical LLMs in Japan.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u5927\u578b\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u533b\u7597\u57fa\u51c6HealthBench\u5e94\u7528\u4e8e\u65e5\u672c\u8bed\u5883\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u76f4\u63a5\u7ffb\u8bd1\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u5f00\u53d1\u9002\u7528\u4e8e\u65e5\u672c\u4e34\u5e8a\u6307\u5357\u3001\u533b\u7597\u4f53\u7cfb\u548c\u6587\u5316\u89c4\u8303\u7684\u672c\u5730\u5316\u7248\u672cJ-HealthBench\u3002", "motivation": "\u65e5\u8bed\u73af\u5883\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u8d44\u6e90\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u7ffb\u8bd1\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b89\u5168\u53ef\u9760\u7684\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u9996\u5148\u4f7f\u7528\u673a\u5668\u7ffb\u8bd1\u7684HealthBench 5000\u4e2a\u573a\u666f\u5bf9GPT-4.1\u548c\u65e5\u672c\u672c\u571f\u5f00\u6e90\u6a21\u578bLLM-jp-3.1\u8fdb\u884c\u6027\u80fd\u57fa\u7ebf\u6d4b\u8bd5\uff1b\u5176\u6b21\u91c7\u7528LLM-as-a-Judge\u65b9\u6cd5\u7cfb\u7edf\u5206\u7c7b\u573a\u666f\u4e0e\u8bc4\u5206\u6807\u51c6\uff0c\u8bc6\u522b\u4e0e\u65e5\u672c\u4e34\u5e8a\u5b9e\u8df5\u4e0d\u7b26\u7684\u2018\u60c5\u5883\u5dee\u8ddd\u2019\u3002", "result": "GPT-4.1\u56e0\u8bc4\u5206\u6807\u51c6\u4e0d\u5339\u914d\u8868\u73b0\u7565\u6709\u4e0b\u964d\uff0c\u800cLLM-jp-3.1\u5728\u4e34\u5e8a\u5b8c\u6574\u6027\u65b9\u9762\u663e\u8457\u5931\u8d25\uff1b\u591a\u6570\u573a\u666f\u9002\u7528\uff0c\u4f46\u5927\u91cf\u8bc4\u5206\u6807\u51c6\u9700\u672c\u5730\u5316\u8c03\u6574\u3002", "conclusion": "\u76f4\u63a5\u7ffb\u8bd1\u7684\u57fa\u51c6\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u5fc5\u987b\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u672c\u5730\u5316\u9002\u914d\u6784\u5efaJ-HealthBench\uff0c\u4ee5\u786e\u4fdd\u5728\u65e5\u672c\u5b89\u5168\u53ef\u9760\u5730\u8bc4\u4f30\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2509.17446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17446", "abs": "https://arxiv.org/abs/2509.17446", "authors": ["Haofeng Huang", "Yifei Han", "Long Zhang", "Bin Li", "Yangfan He"], "title": "MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion", "comment": "Submitted to ICASSP 2026", "summary": "Multimodal intent recognition (MMIR) suffers from weak semantic grounding and\npoor robustness under noisy or rare-class conditions. We propose MVCL-DAF++,\nwhich extends MVCL-DAF with two key modules: (1) Prototype-aware contrastive\nalignment, aligning instances to class-level prototypes to enhance semantic\nconsistency; and (2) Coarse-to-fine attention fusion, integrating global\nmodality summaries with token-level features for hierarchical cross-modal\ninteraction. On MIntRec and MIntRec2.0, MVCL-DAF++ achieves new\nstate-of-the-art results, improving rare-class recognition by +1.05\\% and\n+4.18\\% WF1, respectively. These results demonstrate the effectiveness of\nprototype-guided learning and coarse-to-fine fusion for robust multimodal\nunderstanding. The source code is available at\nhttps://github.com/chr1s623/MVCL-DAF-PlusPlus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MVCL-DAF++\uff0c\u901a\u8fc7\u539f\u578b\u611f\u77e5\u5bf9\u6bd4\u5bf9\u9f50\u548c\u7c97\u5230\u7cbe\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b\u5728\u566a\u58f0\u548c\u7a00\u6709\u7c7b\u522b\u6761\u4ef6\u4e0b\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728MIntRec\u548cMIntRec2.0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b\uff08MMIR\uff09\u5b58\u5728\u8bed\u4e49\u63a5\u5730\u5f31\u3001\u5728\u566a\u58f0\u6216\u7a00\u6709\u7c7b\u522b\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u63d0\u5347\u6a21\u578b\u5bf9\u7a00\u6709\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faMVCL-DAF++\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a(1) \u539f\u578b\u611f\u77e5\u5bf9\u6bd4\u5bf9\u9f50\uff0c\u5c06\u5b9e\u4f8b\u4e0e\u7c7b\u7ea7\u522b\u539f\u578b\u5bf9\u9f50\u4ee5\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\uff1b(2) \u7c97\u5230\u7cbe\u6ce8\u610f\u529b\u878d\u5408\uff0c\u7ed3\u5408\u5168\u5c40\u6a21\u6001\u6458\u8981\u4e0etoken\u7ea7\u7279\u5f81\u5b9e\u73b0\u5206\u5c42\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728MIntRec\u548cMIntRec2.0\u6570\u636e\u96c6\u4e0a\uff0cMVCL-DAF++\u5206\u522b\u5728\u52a0\u6743F1\u5206\u6570\u4e0a\u63d0\u5347\u4e86+1.05%\u548c+4.18%\uff0c\u5c24\u5176\u663e\u8457\u6539\u5584\u4e86\u7a00\u6709\u7c7b\u522b\u7684\u8bc6\u522b\u6548\u679c\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u539f\u578b\u5f15\u5bfc\u5b66\u4e60\u548c\u7c97\u5230\u7cbe\u878d\u5408\u7b56\u7565\u6709\u6548\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u7406\u89e3\u7684\u9c81\u68d2\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4e3a\u89e3\u51b3MMIR\u4e2d\u7684\u7a00\u6709\u7c7b\u522b\u548c\u566a\u58f0\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.16972", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16972", "abs": "https://arxiv.org/abs/2509.16972", "authors": ["Quanzhu Niu", "Dengxian Gong", "Shihao Chen", "Tao Zhang", "Yikang Zhou", "Haobo Yuan", "Lu Qi", "Xiangtai Li", "Shunping Ji"], "title": "The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA", "comment": "1st place report of 7th LSVOS RVOS track in ICCV 2025. The code is\n  released in Sa2VA repository: https://github.com/magic-research/Sa2VA", "summary": "Referring video object segmentation (RVOS) requires segmenting and tracking\nobjects in videos conditioned on natural-language expressions, demanding\nfine-grained understanding of both appearance and motion. Building on Sa2VA,\nwhich couples a Multi-modal Large Language Model (MLLM) with the video\nsegmentation model SAM2, we identify two key bottlenecks that limit\nsegmentation performance: sparse frame sampling and reliance on a single [SEG]\ntoken for an entire video. We propose Segmentation Augmented and Selective\nAveraged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge\n(RVOS track), SaSaSa2VA achieves a $J\\&F$ of 67.45, ranking first and\nsurpassing the runner-up by 2.80 points. This result and ablation studies\ndemonstrate that efficient segmentation augmentation and test-time ensembling\nsubstantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA\nrepository: https://github.com/magic-research/Sa2VA.", "AI": {"tldr": "\u63d0\u51faSaSaSa2VA\u6a21\u578b\uff0c\u901a\u8fc7\u589e\u5f3a\u5206\u5272\u548c\u9009\u62e9\u6027\u5e73\u5747\u673a\u5236\uff0c\u5728RVOS\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cLSVOS\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7a00\u758f\u5e27\u91c7\u6837\u548c\u5355\u4e00[SEG]\u4ee4\u724c\u9650\u5236\uff0c\u5bfc\u81f4\u89c6\u9891\u6307\u4ee3\u8868\u8fbe\u5206\u5272\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u5728Sa2VA\u57fa\u7840\u4e0a\u5f15\u5165\u5206\u5272\u589e\u5f3a\u548c\u9009\u62e9\u6027\u5e73\u5747\u673a\u5236\uff0c\u4f18\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u9891\u5206\u5272\u6a21\u578b\u7684\u534f\u540c\u3002", "result": "\u5728LSVOS\u6311\u6218\u8d5bRVOS\u8d5b\u9053\u4e0a\u53d6\u5f9767.45\u7684J&F\u5206\u6570\uff0c\u8d85\u8d8a\u7b2c\u4e8c\u540d2.80\u70b9\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u9ad8\u6548\u7684\u5206\u5272\u589e\u5f3a\u548c\u6d4b\u8bd5\u65f6\u96c6\u6210\u80fd\u663e\u8457\u63d0\u5347\u57fa\u4e8eMLLM\u7684RVOS\u6027\u80fd\u3002"}}
{"id": "2509.17445", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17445", "abs": "https://arxiv.org/abs/2509.17445", "authors": ["Chaodong Tong", "Qi Zhang", "Lei Jiang", "Yanbing Liu", "Nannan Sun", "Wei Li"], "title": "Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks", "comment": "5pages, 5 figures, submit to ICASSP 2026", "summary": "Reliable question answering with large language models (LLMs) is challenged\nby hallucinations, fluent but factually incorrect outputs arising from\nepistemic uncertainty. Existing entropy-based semantic-level uncertainty\nestimation methods are limited by sampling noise and unstable clustering of\nvariable-length answers. We propose Semantic Reformulation Entropy (SRE), which\nimproves uncertainty estimation in two ways. First, input-side semantic\nreformulations produce faithful paraphrases, expand the estimation space, and\nreduce biases from superficial decoder tendencies. Second, progressive,\nenergy-based hybrid clustering stabilizes semantic grouping. Experiments on\nSQuAD and TriviaQA show that SRE outperforms strong baselines, providing more\nrobust and generalizable hallucination detection. These results demonstrate\nthat combining input diversification with multi-signal clustering substantially\nenhances semantic-level uncertainty estimation.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u91cd\u6784\u71b5\uff08SRE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u4fa7\u8bed\u4e49\u91cd\u6784\u548c\u6e10\u8fdb\u5f0f\u6df7\u5408\u805a\u7c7b\u63d0\u5347\u5927\u6a21\u578b\u95ee\u7b54\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u6709\u6548\u68c0\u6d4b\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u71b5\u7684\u8bed\u4e49\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u91c7\u6837\u566a\u58f0\u548c\u53ef\u53d8\u957f\u5ea6\u56de\u7b54\u7684\u4e0d\u7a33\u5b9a\u805a\u7c7b\uff0c\u96be\u4ee5\u53ef\u9760\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u91cd\u6784\u71b5\uff08SRE\uff09\uff0c\u5728\u8f93\u5165\u4fa7\u751f\u6210\u5fe0\u5b9e\u7684\u8bed\u4e49\u6539\u5199\u4ee5\u6269\u5c55\u4f30\u8ba1\u7a7a\u95f4\u5e76\u51cf\u5c11\u89e3\u7801\u504f\u5dee\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u6e10\u8fdb\u5f0f\u6df7\u5408\u805a\u7c7b\u6765\u7a33\u5b9a\u8bed\u4e49\u5206\u7ec4\u3002", "result": "\u5728SQuAD\u548cTriviaQA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSRE\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u66f4\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u5730\u68c0\u6d4b\u5e7b\u89c9\u3002", "conclusion": "\u7ed3\u5408\u8f93\u5165\u591a\u6837\u5316\u4e0e\u591a\u4fe1\u53f7\u805a\u7c7b\u80fd\u663e\u8457\u63d0\u5347\u8bed\u4e49\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6548\u679c\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u95ee\u7b54\u7cfb\u7edf\u3002"}}
{"id": "2509.17472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17472", "abs": "https://arxiv.org/abs/2509.17472", "authors": ["Jia Li", "Shiyu Long", "Ye Yuan"], "title": "Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector", "comment": null, "summary": "Multivariate time series (MTS) anomaly detection commonly encounters in\nvarious domains like finance, healthcare, and industrial monitoring. However,\nexisting MTS anomaly detection methods are mostly defined on the static graph\nstructure, which fails to perform an accurate representation of complex\nspatio-temporal correlations in MTS. To address this issue, this study proposes\na Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector (PGMA) with\nthe following two-fold ideas: a) designing a periodic time-slot allocation\nstrategy based Fast Fourier Transform (FFT), which enables the graph structure\nto reflect dynamic changes in MTS; b) utilizing graph neural network and\ntemporal extension convolution to accurate extract the complex spatio-temporal\ncorrelations from the reconstructed periodic graphs. Experiments on four real\ndatasets from real applications demonstrate that the proposed PGMA outperforms\nstate-of-the-art models in MTS anomaly detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5468\u671f\u56fe\u589e\u5f3a\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5PGMA\uff0c\u901a\u8fc7FFT\u8bbe\u8ba1\u5468\u671f\u6027\u65f6\u9699\u5206\u914d\u7b56\u7565\uff0c\u5e76\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u65f6\u95f4\u6269\u5c55\u5377\u79ef\u63d0\u53d6\u590d\u6742\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709MTS\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u591a\u57fa\u4e8e\u9759\u6001\u56fe\u7ed3\u6784\uff0c\u96be\u4ee5\u51c6\u786e\u8868\u793aMTS\u4e2d\u7684\u590d\u6742\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eFFT\u7684\u5468\u671f\u6027\u65f6\u9699\u5206\u914d\u7b56\u7565\uff0c\u6784\u5efa\u52a8\u6001\u56fe\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u65f6\u95f4\u6269\u5c55\u5377\u79ef\u6765\u6355\u6349\u65f6\u7a7a\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPGMA\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684MTS\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u3002", "conclusion": "PGMA\u80fd\u66f4\u6709\u6548\u5730\u5efa\u6a21\u52a8\u6001\u65f6\u7a7a\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.16977", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16977", "abs": "https://arxiv.org/abs/2509.16977", "authors": ["Petros Georgoulas Wraight", "Giorgos Sfikas", "Ioannis Kordonis", "Petros Maragos", "George Retsinas"], "title": "Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime", "comment": null, "summary": "Handwritten Text Recognition (HTR) is a task of central importance in the\nfield of document image understanding. State-of-the-art methods for HTR require\nthe use of extensive annotated sets for training, making them impractical for\nlow-resource domains like historical archives or limited-size modern\ncollections. This paper introduces a novel framework that, unlike the standard\nHTR model paradigm, can leverage mild prior knowledge of lexical\ncharacteristics; this is ideal for scenarios where labeled data are scarce. We\npropose an iterative bootstrapping approach that aligns visual features\nextracted from unlabeled images with semantic word representations using\nOptimal Transport (OT). Starting with a minimal set of labeled examples, the\nframework iteratively matches word images to text labels, generates\npseudo-labels for high-confidence alignments, and retrains the recognizer on\nthe growing dataset. Numerical experiments demonstrate that our iterative\nvisual-semantic alignment scheme significantly improves recognition accuracy on\nlow-resource HTR benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8fed\u4ee3\u89c6\u89c9-\u8bed\u4e49\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u624b\u5199\u6587\u672c\u8bc6\u522b\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u548c\u4f2a\u6807\u7b7e\u8fed\u4ee3\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u5199\u6587\u672c\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u5728\u5386\u53f2\u6863\u6848\u7b49\u4f4e\u8d44\u6e90\u9886\u57df\u4e0d\u5b9e\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u8bcd\u6c47\u5148\u9a8c\u77e5\u8bc6\u3001\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8fed\u4ee3\u81ea\u4e3e\u65b9\u6cd5\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\uff08Optimal Transport\uff09\u5c06\u65e0\u6807\u7b7e\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u4e49\u8bcd\u8868\u793a\u5bf9\u9f50\uff0c\u4ece\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u51fa\u53d1\uff0c\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u5e76\u4e0d\u65ad\u91cd\u8bad\u7ec3\u8bc6\u522b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90HTR\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8fed\u4ee3\u89c6\u89c9-\u8bed\u4e49\u5bf9\u9f50\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u4f4e\u8d44\u6e90\u624b\u5199\u6587\u672c\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.17449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17449", "abs": "https://arxiv.org/abs/2509.17449", "authors": ["Leonor Veloso", "Lea Hirlimann", "Philipp Wicke", "Hinrich Sch\u00fctze"], "title": "SLAyiNG: Towards Queer Language Processing", "comment": "To be presented at Queer in AI @ NeurIPS 2025 (non-archival)", "summary": "Knowledge of slang is a desirable feature of LLMs in the context of user\ninteraction, as slang often reflects an individual's social identity. Several\nworks on informal language processing have defined and curated benchmarks for\ntasks such as detection and identification of slang. In this paper, we focus on\nqueer slang. Queer slang can be mistakenly flagged as hate speech or can evoke\nnegative responses from LLMs during user interaction. Research efforts so far\nhave not focused explicitly on queer slang. In particular, detection and\nprocessing of queer slang have not been thoroughly evaluated due to the lack of\na high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the\nfirst dataset containing annotated queer slang derived from subtitles, social\nmedia posts, and podcasts, reflecting real-world usage. We describe our data\ncuration process, including the collection of slang terms and definitions,\nscraping sources for examples that reflect usage of these terms, and our\nongoing annotation process. As preliminary results, we calculate\ninter-annotator agreement for human annotators and OpenAI's model o3-mini,\nevaluating performance on the task of sense disambiguation. Reaching an average\nKrippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models\ncan serve as tools for pre-filtering, but the complex and often sensitive\nnature of queer language data requires expert and community-driven annotation\nefforts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5305\u542b\u6ce8\u91ca\u7684\u9177\u513f\u4fda\u8bed\u6570\u636e\u96c6SLAyiNG\uff0c\u65e8\u5728\u89e3\u51b3\u9177\u513f\u4fda\u8bed\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u88ab\u8bef\u6807\u4e3a\u4ec7\u6068\u8a00\u8bba\u7684\u95ee\u9898\u3002", "motivation": "\u9177\u513f\u4fda\u8bed\u5e38\u88ab\u8bef\u8ba4\u4e3a\u4ec7\u6068\u8a00\u8bba\uff0c\u5f71\u54cd\u7528\u6237\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u6765\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u4ece\u5b57\u5e55\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u64ad\u5ba2\u4e2d\u6536\u96c6\u5e76\u6574\u7406\u9177\u513f\u4fda\u8bed\uff0c\u8fdb\u884c\u4eba\u5de5\u4e0e\u6a21\u578b\u534f\u540c\u7684\u6807\u6ce8\uff0c\u5e76\u8ba1\u7b97\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u4eba\u7c7b\u6807\u6ce8\u8005\u4e0eOpenAI\u7684o3-mini\u6a21\u578b\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e0a\u8fbe\u5230\u5e73\u5747Krippendorff's alpha\u4e3a0.746\u3002", "conclusion": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u53ef\u7528\u4e8e\u9884\u8fc7\u6ee4\uff0c\u4f46\u9177\u513f\u8bed\u8a00\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u654f\u611f\u6027\u4ecd\u9700\u4e13\u5bb6\u548c\u793e\u533a\u9a71\u52a8\u7684\u6807\u6ce8\u5de5\u4f5c\u3002"}}
{"id": "2509.17491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17491", "abs": "https://arxiv.org/abs/2509.17491", "authors": ["Firuz Kamalov", "Mohmad Al Falasi", "Fadi Thabtah"], "title": "Path-Weighted Integrated Gradients for Interpretable Dementia Classification", "comment": null, "summary": "Integrated Gradients (IG) is a widely used attribution method in explainable\nartificial intelligence (XAI). In this paper, we introduce Path-Weighted\nIntegrated Gradients (PWIG), a generalization of IG that incorporates a\ncustomizable weighting function into the attribution integral. This\nmodification allows for targeted emphasis along different segments of the path\nbetween a baseline and the input, enabling improved interpretability, noise\nmitigation, and the detection of path-dependent feature relevance. We establish\nits theoretical properties and illustrate its utility through experiments on a\ndementia classification task using the OASIS-1 MRI dataset. Attribution maps\ngenerated by PWIG highlight clinically meaningful brain regions associated with\nvarious stages of dementia, providing users with sharp and stable explanations.\nThe results suggest that PWIG offers a flexible and theoretically grounded\napproach for enhancing attribution quality in complex predictive models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Path-Weighted Integrated Gradients (PWIG)\uff0c\u662fIntegrated Gradients\u7684\u63a8\u5e7f\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b9a\u5236\u7684\u6743\u91cd\u51fd\u6570\u63d0\u5347\u5f52\u56e0\u8d28\u91cf\uff0c\u5e94\u7528\u4e8e\u75f4\u5446\u75c7\u5206\u7c7b\u4efb\u52a1\uff0c\u663e\u793a\u51fa\u66f4\u6e05\u6670\u3001\u7a33\u5b9a\u7684\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u5728\u89e3\u91ca\u6027\u3001\u566a\u58f0\u6291\u5236\u548c\u8def\u5f84\u4f9d\u8d56\u7279\u5f81\u8bc6\u522b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u5177\u7075\u6d3b\u6027\u7684\u5f52\u56e0\u65b9\u6cd5\u3002", "method": "\u5728Integrated Gradients\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u79ef\u5206\u8def\u5f84\u4e2d\u5f15\u5165\u53ef\u81ea\u5b9a\u4e49\u7684\u6743\u91cd\u51fd\u6570\uff0c\u5f62\u6210Path-Weighted Integrated Gradients (PWIG)\uff0c\u5e76\u5206\u6790\u5176\u7406\u8bba\u6027\u8d28\u3002", "result": "\u5728OASIS-1 MRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPWIG\u751f\u6210\u7684\u5f52\u56e0\u56fe\u80fd\u7a81\u51fa\u4e0e\u75f4\u5446\u4e0d\u540c\u9636\u6bb5\u76f8\u5173\u7684\u4e34\u5e8a\u6709\u610f\u4e49\u8111\u533a\uff0c\u63d0\u4f9b\u66f4\u9510\u5229\u548c\u7a33\u5b9a\u7684\u89e3\u91ca\u3002", "conclusion": "PWIG\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u7406\u8bba\u4e25\u8c28\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u590d\u6742\u6a21\u578b\u4e2d\u7279\u5f81\u5f52\u56e0\u7684\u8d28\u91cf\u3002"}}
{"id": "2509.16986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16986", "abs": "https://arxiv.org/abs/2509.16986", "authors": ["Feng Han", "Chao Gong", "Zhipeng Wei", "Jingjing Chen", "Yu-Gang Jiang"], "title": "VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation", "comment": null, "summary": "Recently, autoregressive image generation models have wowed audiences with\ntheir remarkable capability in creating surprisingly realistic images. Models\nsuch as GPT-4o and LlamaGen can not only produce images that faithfully mimic\nrenowned artistic styles like Ghibli, Van Gogh, or Picasso, but also\npotentially generate Not-Safe-For-Work (NSFW) content, raising significant\nconcerns regarding copyright infringement and ethical use. Despite these\nconcerns, methods to safeguard autoregressive text-to-image models remain\nunderexplored. Previous concept erasure methods, primarily designed for\ndiffusion models that operate in denoising latent space, are not directly\napplicable to autoregressive models that generate images token by token. To\naddress this critical gap, we propose Visual Contrast Exploitation (VCE), a\nnovel framework comprising: (1) an innovative contrastive image pair\nconstruction paradigm that precisely decouples unsafe concepts from their\nassociated content semantics, and (2) a sophisticated DPO-based training\napproach that enhances the model's ability to identify and leverage visual\ncontrastive features from image pairs, enabling precise concept erasure. Our\ncomprehensive experiments across three challenging tasks-artist style erasure,\nexplicit content erasure, and object removal-demonstrate that our method\neffectively secures the model, achieving state-of-the-art results while erasing\nunsafe concepts and maintaining the integrity of unrelated safe concepts. The\ncode and models are available at https://github.com/Maplebb/VCE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u89c6\u89c9\u5bf9\u6bd4\u5229\u7528\uff08VCE\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e0d\u5b89\u5168\u6982\u5ff5\u7684\u7cbe\u786e\u64e6\u9664\uff0c\u901a\u8fc7\u6784\u5efa\u5bf9\u6bd4\u56fe\u50cf\u5bf9\u548c\u57fa\u4e8eDPO\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u827a\u672f\u5bb6\u98ce\u683c\u3001\u663e\u5f0f\u5185\u5bb9\u548c\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u9010token\u751f\u6210\u56fe\u50cf\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u56e0\u6b64\u7f3a\u4e4f\u5bf9\u8fd9\u7c7b\u6a21\u578b\u7684\u5b89\u5168\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVCE\u6846\u67b6\uff0c\u5305\u62ec\u521b\u65b0\u7684\u5bf9\u6bd4\u56fe\u50cf\u5bf9\u6784\u9020\u8303\u5f0f\u548c\u57fa\u4e8eDPO\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u89e3\u8026\u4e0d\u5b89\u5168\u6982\u5ff5\u4e0e\u5176\u8bed\u4e49\u5e76\u589e\u5f3a\u6a21\u578b\u5bf9\u89c6\u89c9\u5bf9\u6bd4\u7279\u5f81\u7684\u5229\u7528\u80fd\u529b\u3002", "result": "\u5728\u827a\u672f\u5bb6\u98ce\u683c\u64e6\u9664\u3001\u663e\u5f0f\u5185\u5bb9\u64e6\u9664\u548c\u7269\u4f53\u79fb\u9664\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u64e6\u9664\u4e0d\u5b89\u5168\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u5185\u5bb9\u7684\u5b8c\u6574\u6027\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "VCE\u4e3a\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7248\u6743\u548c\u4f26\u7406\u65b9\u9762\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2509.17455", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17455", "abs": "https://arxiv.org/abs/2509.17455", "authors": ["Haoyang Chen", "Kumiko Tanaka-Ishii"], "title": "Codifying Natural Langauge Tasks", "comment": "Submitted to Journal of Automated Software Engineering", "summary": "We explore the applicability of text-to-code to solve real-world problems\nthat are typically solved in natural language, such as legal judgment and\nmedical QA. Unlike previous works, our approach leverages the explicit\nreasoning provided by program generation. We present ICRAG, a framework that\ntransforms natural language into executable programs through iterative\nrefinement using external knowledge from domain resources and GitHub. Across 13\nbenchmarks, ICRAG achieves up to 161.1\\% relative improvement. We provide a\ndetailed analysis of the generated code and the impact of external knowledge,\nand we discuss the limitations of applying text-to-code approaches to\nreal-world natural language tasks.", "AI": {"tldr": "\u63d0\u51faICRAG\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7a0b\u5e8f\u5e76\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u572813\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u9ad8161.1%\u7684\u76f8\u5bf9\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u6587\u672c\u5230\u4ee3\u7801\u65b9\u6cd5\u5728\u6cd5\u5f8b\u5224\u51b3\u548c\u533b\u7597\u95ee\u7b54\u7b49\u73b0\u5b9e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5229\u7528\u7a0b\u5e8f\u751f\u6210\u7684\u663e\u5f0f\u63a8\u7406\u5f25\u8865\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faICRAG\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5e76\u5f15\u5165\u6765\u81ea\u9886\u57df\u8d44\u6e90\u548cGitHub\u7684\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u589e\u5f3a\u3002", "result": "\u572813\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cICRAG\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6700\u9ad8\u5b9e\u73b0161.1%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5916\u90e8\u77e5\u8bc6\u5bf9\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u7684\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u6587\u672c\u5230\u4ee3\u7801\u65b9\u6cd5\u7ed3\u5408\u663e\u5f0f\u63a8\u7406\u548c\u5916\u90e8\u77e5\u8bc6\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4f46\u5176\u9002\u7528\u6027\u4ecd\u53d7\u9650\u4e8e\u9886\u57df\u77e5\u8bc6\u8986\u76d6\u548c\u7a0b\u5e8f\u751f\u6210\u51c6\u786e\u6027\u3002"}}
{"id": "2509.17495", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.17495", "abs": "https://arxiv.org/abs/2509.17495", "authors": ["Ke Ma", "Jialiang Lu", "Philippe Martins"], "title": "BiLCNet : BiLSTM-Conformer Network for Encrypted Traffic Classification with 5G SA Physical Channel Records", "comment": "6 pages, 5 figures", "summary": "Accurate and efficient traffic classification is vital for wireless network\nmanagement, especially under encrypted payloads and dynamic application\nbehavior, where traditional methods such as port-based identification and deep\npacket inspection (DPI) are increasingly inadequate. This work explores the\nfeasibility of using physical channel data collected from the air interface of\n5G Standalone (SA) networks for traffic sensing. We develop a preprocessing\npipeline to transform raw channel records into structured representations with\ncustomized feature engineering to enhance downstream classification\nperformance. To jointly capture temporal dependencies and both local and global\nstructural patterns inherent in physical channel records, we propose a novel\nhybrid architecture: BiLSTM-Conformer Network (BiLCNet), which integrates the\nsequential modeling capability of Bidirectional Long Short-Term Memory networks\n(BiLSTM) with the spatial feature extraction strength of Conformer blocks.\nEvaluated on a noise-limited 5G SA dataset, our model achieves a classification\naccuracy of 93.9%, outperforming a series of conventional machine learning and\ndeep learning algorithms. Furthermore, we demonstrate its generalization\nability under zero-shot transfer settings, validating its robustness across\ntraffic categories and varying environmental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e5G\u72ec\u7acb\u7ec4\u7f51\u7269\u7406\u5c42\u4fe1\u9053\u6570\u636e\u7684\u9ad8\u6548\u6d41\u91cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7BiLSTM-Conformer\u6df7\u5408\u7f51\u7edc\uff08BiLCNet\uff09\u5b9e\u73b093.9%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7aef\u53e3\u548c\u6df1\u5ea6\u5305\u68c0\u6d4b\u7684\u65b9\u6cd5\u5728\u52a0\u5bc6\u6d41\u91cf\u548c\u52a8\u6001\u5e94\u7528\u884c\u4e3a\u4e0b\u6548\u679c\u53d7\u9650\uff0c\u96be\u4ee5\u6ee1\u8db35G\u7f51\u7edc\u4e2d\u7cbe\u51c6\u9ad8\u6548\u7684\u6d41\u91cf\u5206\u7c7b\u9700\u6c42\u3002", "method": "\u5229\u75285G SA\u7f51\u7edc\u7a7a\u53e3\u91c7\u96c6\u7684\u7269\u7406\u5c42\u4fe1\u9053\u6570\u636e\uff0c\u8bbe\u8ba1\u9884\u5904\u7406\u6d41\u7a0b\u4e0e\u7279\u5f81\u5de5\u7a0b\uff0c\u5e76\u63d0\u51faBiLSTM-Conformer\u6df7\u5408\u6a21\u578b\uff08BiLCNet\uff09\uff0c\u7ed3\u5408BiLSTM\u7684\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\u4e0eConformer\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7ed3\u6784\u6355\u6349\u80fd\u529b\u8fdb\u884c\u6d41\u91cf\u5206\u7c7b\u3002", "result": "\u5728\u566a\u58f0\u53d7\u9650\u76845G SA\u6570\u636e\u96c6\u4e0a\u8fbe\u523093.9%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u591a\u79cd\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5229\u7528\u7269\u7406\u5c42\u4fe1\u9053\u6570\u636e\u7ed3\u5408BiLCNet\u53ef\u6709\u6548\u63d0\u5347\u52a0\u5bc6\u73af\u5883\u4e0b\u65e0\u7ebf\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2509.16988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16988", "abs": "https://arxiv.org/abs/2509.16988", "authors": ["Mingshuai Sheng", "Bhatti Uzair Aslam", "Junfeng Zhang", "Siling Feng", "Yonis Gulzar"], "title": "A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale Encoder-Decoder for Hyperspectral Change Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Hyperspectral change detection (HCD) aims to accurately identify land-cover\nchanges in hyperspectral images of the same area acquired at different times,\nwith key applications in environmental monitoring and disaster assessment. To\naddress limitations of existing methods, such as insufficient use of multiscale\nfeatures and low efficiency in differential feature fusion, this paper proposes\na cross-hierarchical multi-feature fusion network (CHMFFN) based on a\nmultiscale encoder-decoder architecture. The front-end adopts a multiscale\nfeature extraction subnetwork, built on an encoder-decoder backbone with\nresidual connections and a dual-core channel-spatial attention (DCCSA) module\nto extract spectral-spatial-temporal features (SSTF). The encoder captures\nmultiscale features from shallow details to deep semantics via residual blocks\nand convolutional kernels with varying receptive fields. The decoder restores\nspatial resolution and suppresses noise information through skip connections\nintegrating encoder features. Additionally, a spectral-temporal change feature\nlearning (STCFL) module learns cross-temporal change features at different\nlevels, strengthening inter-temporal difference capture. An adaptive fusion of\nadvanced features (AFAF) module dynamically balances hierarchical differential\nfeatures via adaptive weights, enhancing representation of complex changes.\nExperiments on four public hyperspectral datasets show CHMFFN outperforms\nstate-of-the-art methods, verifying its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u8de8\u5c42\u6b21\u591a\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08CHMFFN\uff09\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u53d8\u5316\u68c0\u6d4b\uff0c\u901a\u8fc7\u63d0\u53d6\u5149\u8c31-\u7a7a\u95f4-\u65f6\u95f4\u7279\u5f81\u5e76\u81ea\u9002\u5e94\u878d\u5408\u591a\u5c42\u6b21\u5dee\u5f02\u7279\u5f81\uff0c\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9ad8\u5149\u8c31\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u5728\u591a\u5c3a\u5ea6\u7279\u5f81\u5229\u7528\u548c\u5dee\u5f02\u7279\u5f81\u878d\u5408\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u590d\u6742\u5730\u8868\u53d8\u5316\u3002", "method": "\u63d0\u51faCHMFFN\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5b50\u7f51\u7edc\uff0c\u7ed3\u5408\u6b8b\u5dee\u8fde\u63a5\u548c\u53cc\u6838\u901a\u9053-\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff08DCCSA\uff09\u63d0\u53d6\u5149\u8c31-\u7a7a\u95f4-\u65f6\u95f4\u7279\u5f81\uff1b\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u6355\u83b7\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u5f15\u5165\u5149\u8c31-\u65f6\u95f4\u53d8\u5316\u7279\u5f81\u5b66\u4e60\uff08STCFL\uff09\u6a21\u5757\u548c\u81ea\u9002\u5e94\u9ad8\u7ea7\u7279\u5f81\u878d\u5408\uff08AFAF\uff09\u6a21\u5757\uff0c\u589e\u5f3a\u8de8\u65f6\u95f4\u5dee\u5f02\u6355\u6349\u4e0e\u7279\u5f81\u878d\u5408\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCHMFFN\u5728\u53d8\u5316\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "CHMFFN\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u73af\u5883\u76d1\u6d4b\u548c\u707e\u5bb3\u8bc4\u4f30\u7b49\u5e94\u7528\u3002"}}
{"id": "2509.17459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17459", "abs": "https://arxiv.org/abs/2509.17459", "authors": ["Namyoung Kim", "Kai Tzu-iunn Ong", "Yeonjun Hwang", "Minseok Kang", "Iiseo Jihn", "Gayoung Kim", "Minju Kim", "Jinyoung Yeo"], "title": "PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Dialogue agents based on large language models (LLMs) have shown promising\nperformance in proactive dialogue, which requires effective strategy planning.\nHowever, existing approaches to strategy planning for proactive dialogue face\nseveral limitations: limited strategy coverage, preference bias in planning,\nand reliance on costly additional training. To address these, we propose\nPRINCIPLES: a synthetic strategy memory for proactive dialogue agents.\nPRINCIPLES is derived through offline self-play simulations and serves as\nreusable knowledge that guides strategy planning during inference, eliminating\nthe need for additional training and data annotation. We evaluate PRINCIPLES in\nboth emotional support and persuasion domains, demonstrating consistent\nimprovements over strong baselines. Furthermore, PRINCIPLES maintains its\nrobustness across extended and more diverse evaluation settings. See our\nproject page at https://huggingface.co/spaces/kimnamssya/Principles.", "AI": {"tldr": "\u63d0\u51faPRINCIPLES\uff0c\u4e00\u79cd\u57fa\u4e8e\u79bb\u7ebf\u81ea\u5bf9\u5f08\u751f\u6210\u7684\u53ef\u590d\u7528\u7b56\u7565\u8bb0\u5fc6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u5bf9\u8bdd\u4e2d\u7684\u7b56\u7565\u89c4\u5212\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5e76\u5728\u60c5\u611f\u652f\u6301\u4e0e\u8bf4\u670d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u5bf9\u8bdd\u7b56\u7565\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u7b56\u7565\u8986\u76d6\u6709\u9650\u3001\u89c4\u5212\u504f\u597d\u504f\u5dee\u53ca\u4f9d\u8d56\u9ad8\u6210\u672c\u989d\u5916\u8bad\u7ec3\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u79bb\u7ebf\u81ea\u5bf9\u5f08\u6a21\u62df\u751f\u6210PRINCIPLES\u7b56\u7565\u8bb0\u5fc6\u5e93\uff0c\u5c06\u5176\u4f5c\u4e3a\u5916\u90e8\u77e5\u8bc6\u6307\u5bfc\u63a8\u7406\u9636\u6bb5\u7684\u7b56\u7565\u89c4\u5212\uff0c\u907f\u514d\u989d\u5916\u8bad\u7ec3\u548c\u6570\u636e\u6807\u6ce8\u3002", "result": "\u5728\u60c5\u611f\u652f\u6301\u548c\u8bf4\u670d\u4e24\u4e2a\u9886\u57df\u4e2d\uff0cPRINCIPLES consistently \u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u66f4\u957f\u3001\u66f4\u591a\u6837\u5316\u7684\u5bf9\u8bdd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u9c81\u68d2\u6027\u3002", "conclusion": "PRINCIPLES\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u52a8\u5bf9\u8bdd\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u8986\u76d6\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u7b56\u7565\u89c4\u5212\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.17514", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17514", "abs": "https://arxiv.org/abs/2509.17514", "authors": ["Tianyi Chen", "Pengxiao Lin", "Zhiwei Wang", "Zhi-Qin John Xu"], "title": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data", "comment": null, "summary": "State Space Models (SSMs) have emerged as promising alternatives to attention\nmechanisms, with the Mamba architecture demonstrating impressive performance\nand linear complexity for processing long sequences. However, the fundamental\ndifferences between Mamba and Transformer architectures remain incompletely\nunderstood. In this work, we use carefully designed synthetic tasks to reveal\nMamba's inherent limitations. Through experiments, we identify that Mamba's\nnonlinear convolution introduces an asymmetry bias that significantly impairs\nits ability to recognize symmetrical patterns and relationships. Using\ncomposite function and inverse sequence matching tasks, we demonstrate that\nMamba strongly favors compositional solutions over symmetrical ones and\nstruggles with tasks requiring the matching of reversed sequences. We show\nthese limitations stem not from the SSM module itself but from the nonlinear\nconvolution preceding it, which fuses token information asymmetrically. These\ninsights provide a new understanding of Mamba's constraints and suggest\nconcrete architectural improvements for future sequence models.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u63ed\u793a\u4e86Mamba\u67b6\u6784\u5728\u5904\u7406\u5bf9\u79f0\u6a21\u5f0f\u65f6\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u6307\u51fa\u5176\u975e\u7ebf\u6027\u5377\u79ef\u5f15\u5165\u7684\u4e0d\u5bf9\u79f0\u504f\u5dee\u5f71\u54cd\u4e86\u5bf9\u79f0\u5173\u7cfb\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u7406\u89e3Mamba\u4e0eTransformer\u67b6\u6784\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\uff0c\u5e76\u63ed\u793aMamba\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u7684\u6f5c\u5728\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u5408\u6210\u4efb\u52a1\uff08\u5982\u590d\u5408\u51fd\u6570\u548c\u9006\u5e8f\u5339\u914d\u4efb\u52a1\uff09\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790Mamba\u7684\u884c\u4e3a\u53ca\u5176\u5c40\u9650\u6027\u6765\u6e90\u3002", "result": "\u53d1\u73b0Mamba\u56e0\u975e\u7ebf\u6027\u5377\u79ef\u7684\u4e0d\u5bf9\u79f0\u878d\u5408\u800c\u504f\u597d\u7ec4\u5408\u89e3\u800c\u975e\u5bf9\u79f0\u89e3\uff0c\u96be\u4ee5\u5904\u7406\u9006\u5e8f\u5339\u914d\u4efb\u52a1\uff0c\u4e14\u8be5\u95ee\u9898\u6e90\u4e8eSSM\u524d\u7684\u975e\u7ebf\u6027\u5377\u79ef\u6a21\u5757\u3002", "conclusion": "Mamba\u7684\u5c40\u9650\u6027\u6765\u81ea\u5176\u975e\u7ebf\u6027\u5377\u79ef\u7ed3\u6784\uff0c\u800c\u975eSSM\u672c\u8eab\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u5e8f\u5217\u6a21\u578b\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.17012", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17012", "abs": "https://arxiv.org/abs/2509.17012", "authors": ["Zhichao Ma", "Fan Huang", "Lu Zhao", "Fengjun Guo", "Guangtao Zhai", "Xiongkuo Min"], "title": "DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment", "comment": null, "summary": "Document image quality assessment (DIQA) is an important component for\nvarious applications, including optical character recognition (OCR), document\nrestoration, and the evaluation of document image processing systems. In this\npaper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset\ncomprises 5,000 document images, generated by applying multiple document\nenhancement techniques to 500 real-world images with diverse distortions. Each\nenhanced image was rated by 15 subjects across three rating dimensions: overall\nquality, sharpness, and color fidelity. Furthermore, we propose a specialized\nno-reference DIQA model that exploits document layout features to maintain\nquality perception at reduced resolutions to lower computational cost.\nRecognizing that image quality is influenced by both low-level and high-level\nvisual features, we designed a feature fusion module to extract and integrate\nmulti-level features from document images. To generate multi-dimensional\nscores, our model employs independent quality heads for each dimension to\npredict score distributions, allowing it to learn distinct aspects of document\nimage quality. Experimental results demonstrate that our method outperforms\ncurrent state-of-the-art general-purpose IQA models on both DIQA-5000 and an\nadditional document image dataset focused on OCR accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u53c2\u8003\u6587\u6863\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08DIQA\uff09\u6a21\u578b\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b5000\u5f20\u56fe\u50cf\u7684\u4e3b\u89c2DIQA-5000\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u7ea7\u7279\u5f81\u548c\u72ec\u7acb\u8d28\u91cf\u5934\u5b9e\u73b0\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u5206\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u6587\u6863\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u964d\u4f4e\u5206\u8fa8\u7387\u4e0b\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u6ee1\u8db3OCR\u3001\u6587\u6863\u4fee\u590d\u7b49\u5e94\u7528\u7684\u9700\u6c42\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u9488\u5bf9\u6587\u6863\u56fe\u50cf\u7684\u4e3b\u89c2\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u9ad8\u6548\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6587\u6863\u5e03\u5c40\u7279\u5f81\u7684\u65e0\u53c2\u8003DIQA\u6a21\u578b\uff0c\u91c7\u7528\u7279\u5f81\u878d\u5408\u6a21\u5757\u6574\u5408\u4f4e\u5c42\u4e0e\u9ad8\u5c42\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u72ec\u7acb\u7684\u8d28\u91cf\u5934\u5206\u522b\u9884\u6d4b\u6574\u4f53\u8d28\u91cf\u3001\u6e05\u6670\u5ea6\u548c\u8272\u5f69\u4fdd\u771f\u5ea6\u7684\u8bc4\u5206\u5206\u5e03\uff1b\u540c\u65f6\u6784\u5efa\u4e86DIQA-5000\u4e3b\u89c2\u6570\u636e\u96c6\uff0c\u5305\u542b5000\u5f20\u7ecf\u589e\u5f3a\u5904\u7406\u7684\u771f\u5b9e\u6587\u6863\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u753115\u540d\u53d7\u8bd5\u8005\u5728\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728DIQA-5000\u548c\u53e6\u4e00\u4e2a\u9762\u5411OCR\u51c6\u786e\u7387\u7684\u6587\u6863\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u901a\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u4e3b\u89c2\u6570\u636e\u96c6\u548c\u8bbe\u8ba1\u9762\u5411\u6587\u6863\u56fe\u50cf\u7684\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5206\u8fa8\u7387\u4e0b\u7684\u6587\u6863\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\uff0c\u4e3a\u6587\u6863\u5206\u6790\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2509.17482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17482", "abs": "https://arxiv.org/abs/2509.17482", "authors": ["Tsung-Hsuan Pan", "Chung-Chi Chen", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Diagnosing Model Editing via Knowledge Spectrum", "comment": null, "summary": "Model editing, the process of efficiently modifying factual knowledge in\npre-trained language models, is critical for maintaining their accuracy and\nrelevance. However, existing editing methods often introduce unintended side\neffects, degrading model performance in unpredictable ways. While much research\nhas focused on improving editing algorithms, the role of the target knowledge's\nintrinsic properties remains a significant, underexplored factor. This paper\naddresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic\nframework for categorizing knowledge based on its real-world popularity, the\nmodel's pre-edit familiarity, and the linguistic structure of the eliciting\nquestion. Our empirical analysis reveals that these characteristics are strong\npredictors of editing success and stability. Informed by these findings, we\nintroduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that\ntailors editing intensity to the diagnosed difficulty of a knowledge item. We\ndemonstrate that this framework significantly improves success rates for\nchallenging edits while optimizing computational resources. Our work provides a\nmore comprehensive understanding of the factors governing model editing.", "AI": {"tldr": "\u63d0\u51fa\u201c\u77e5\u8bc6\u8c31\u201d\u6846\u67b6\u4ee5\u7cfb\u7edf\u5206\u7c7b\u77e5\u8bc6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f15\u5165\u201c\u77e5\u8bc6\u8bca\u65ad\u6846\u67b6\u201d\u6765\u81ea\u9002\u5e94\u8c03\u6574\u7f16\u8f91\u5f3a\u5ea6\uff0c\u63d0\u5347\u6a21\u578b\u7f16\u8f91\u7684\u6210\u529f\u7387\u4e0e\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5e38\u5e26\u6765\u4e0d\u53ef\u9884\u6d4b\u7684\u526f\u4f5c\u7528\uff0c\u4e14\u77e5\u8bc6\u672c\u8eab\u7684\u5185\u5728\u5c5e\u6027\u5bf9\u7f16\u8f91\u6548\u679c\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u201c\u77e5\u8bc6\u8c31\u201d\u6846\u67b6\uff0c\u4ece\u73b0\u5b9e\u6d41\u884c\u5ea6\u3001\u6a21\u578b\u5148\u9a8c\u719f\u6089\u5ea6\u548c\u95ee\u9898\u8bed\u8a00\u7ed3\u6784\u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u77e5\u8bc6\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u81ea\u9002\u5e94\u7684\u201c\u77e5\u8bc6\u8bca\u65ad\u6846\u67b6\u201d\u6765\u8c03\u8282\u7f16\u8f91\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u77e5\u8bc6\u7684\u5185\u5728\u7279\u6027\u662f\u7f16\u8f91\u6210\u529f\u4e0e\u7a33\u5b9a\u6027\u7684\u5f3a\u9884\u6d4b\u56e0\u5b50\uff0c\u6240\u63d0\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u56f0\u96be\u77e5\u8bc6\u7f16\u8f91\u7684\u6210\u529f\u7387\u5e76\u4f18\u5316\u4e86\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u3002", "conclusion": "\u77e5\u8bc6\u7684\u5185\u5728\u5c5e\u6027\u5728\u6a21\u578b\u7f16\u8f91\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u6240\u63d0\u51fa\u7684\u8bca\u65ad\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u7f16\u8f91\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.17530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17530", "abs": "https://arxiv.org/abs/2509.17530", "authors": ["Sayanta Adhikari", "Vishnuprasadh Kumaravelu", "P. K. Srijith"], "title": "An Unlearning Framework for Continual Learning", "comment": null, "summary": "Growing concerns surrounding AI safety and data privacy have driven the\ndevelopment of Machine Unlearning as a potential solution. However, current\nmachine unlearning algorithms are designed to complement the offline training\nparadigm. The emergence of the Continual Learning (CL) paradigm promises\nincremental model updates, enabling models to learn new tasks sequentially.\nNaturally, some of those tasks may need to be unlearned to address safety or\nprivacy concerns that might arise. We find that applying conventional\nunlearning algorithms in continual learning environments creates two critical\nproblems: performance degradation on retained tasks and task relapse, where\npreviously unlearned tasks resurface during subsequent learning. Furthermore,\nmost unlearning algorithms require data to operate, which conflicts with CL's\nphilosophy of discarding past data. A clear need arises for unlearning\nalgorithms that are data-free and mindful of future learning. To that end, we\npropose UnCLe, an Unlearning framework for Continual Learning. UnCLe employs a\nhypernetwork that learns to generate task-specific network parameters, using\ntask embeddings. Tasks are unlearned by aligning the corresponding generated\nnetwork parameters with noise, without requiring any data. Empirical\nevaluations on several vision data sets demonstrate UnCLe's ability to\nsequentially perform multiple learning and unlearning operations with minimal\ndisruption to previously acquired knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UnCLe\uff0c\u4e00\u79cd\u7528\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u65e0\u6570\u636e\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u5e76\u5c06\u5176\u5bf9\u9f50\u566a\u58f0\u5b9e\u73b0\u65e0\u9700\u6570\u636e\u7684\u9057\u5fd8\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4f20\u7edf\u9057\u5fd8\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u548c\u4efb\u52a1\u56de\u6eaf\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u5b89\u5168\u4e0e\u6570\u636e\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u673a\u5668\u9057\u5fd8\u6210\u4e3a\u5fc5\u8981\u624b\u6bb5\u3002\u7136\u800c\u73b0\u6709\u9057\u5fd8\u7b97\u6cd5\u591a\u9488\u5bf9\u79bb\u7ebf\u8bad\u7ec3\u8bbe\u8ba1\uff0c\u96be\u4ee5\u9002\u5e94\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e2d\u589e\u91cf\u66f4\u65b0\u4e0e\u672a\u6765\u5b66\u4e60\u9700\u6c42\uff0c\u4e14\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u539f\u59cb\u6570\u636e\uff0c\u8fdd\u80cc\u6301\u7eed\u5b66\u4e60\u4e22\u5f03\u5386\u53f2\u6570\u636e\u7684\u539f\u5219\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6570\u636e\u3001\u517c\u987e\u672a\u6765\u5b66\u4e60\u7684\u9057\u5fd8\u673a\u5236\u3002", "method": "\u63d0\u51faUnCLe\u6846\u67b6\uff0c\u91c7\u7528\u8d85\u7f51\u7edc\u6839\u636e\u4efb\u52a1\u5d4c\u5165\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u6a21\u578b\u53c2\u6570\uff1b\u901a\u8fc7\u5c06\u9700\u9057\u5fd8\u4efb\u52a1\u5bf9\u5e94\u7684\u751f\u6210\u53c2\u6570\u5bf9\u9f50\u566a\u58f0\u5206\u5e03\u6765\u5b9e\u73b0\u6570\u636e\u65e0\u5173\u7684\u9057\u5fd8\uff0c\u907f\u514d\u8bbf\u95ee\u539f\u59cb\u6570\u636e\uff0c\u5e76\u5728\u540e\u7eed\u5b66\u4e60\u4e2d\u4fdd\u6301\u5bf9\u5df2\u4fdd\u7559\u4efb\u52a1\u7684\u77e5\u8bc6\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUnCLe\u80fd\u591f\u987a\u5e8f\u6267\u884c\u591a\u6b21\u5b66\u4e60\u4e0e\u9057\u5fd8\u64cd\u4f5c\uff0c\u5728\u4e0d\u4f7f\u7528\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u51cf\u5c11\u5bf9\u5df2\u6709\u77e5\u8bc6\u7684\u5e72\u6270\uff0c\u6709\u6548\u6291\u5236\u4efb\u52a1\u56de\u6eaf\u548c\u6027\u80fd\u9000\u5316\u3002", "conclusion": "UnCLe\u4e3a\u6301\u7eed\u5b66\u4e60\u73af\u5883\u4e0b\u7684\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u81ea\u7531\u3001\u6301\u7eed\u517c\u5bb9\u7684\u6a21\u578b\u9057\u5fd8\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u5b89\u5168\u4e0e\u9690\u79c1\u4fdd\u62a4\u5728\u52a8\u6001\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u843d\u5730\u5e94\u7528\u3002"}}
{"id": "2509.17024", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17024", "abs": "https://arxiv.org/abs/2509.17024", "authors": ["Wenxuan Fang", "Jili Fan", "Chao Wang", "Xiantao Hu", "Jiangwei Weng", "Ying Tai", "Jian Yang", "Jun Li"], "title": "When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration", "comment": null, "summary": "Adverse Weather Image Restoration (AWIR) is a highly challenging task due to\nthe unpredictable and dynamic nature of weather-related degradations.\nTraditional task-specific methods often fail to generalize to unseen or complex\ndegradation types, while recent prompt-learning approaches depend heavily on\nthe degradation estimation capabilities of vision-language models, resulting in\ninconsistent restorations. In this paper, we propose \\textbf{LCDiff}, a novel\nframework comprising two key components: \\textit{Lumina-Chroma Decomposition\nNetwork} (LCDN) and \\textit{Lumina-Guided Diffusion Model} (LGDM). LCDN\nprocesses degraded images in the YCbCr color space, separately handling\ndegradation-related luminance and degradation-invariant chrominance components.\nThis decomposition effectively mitigates weather-induced degradation while\npreserving color fidelity. To further enhance restoration quality, LGDM\nleverages degradation-related luminance information as a guiding condition,\neliminating the need for explicit degradation prompts. Additionally, LGDM\nincorporates a \\textit{Dynamic Time Step Loss} to optimize the denoising\nnetwork, ensuring a balanced recovery of both low- and high-frequency features\nin the image. Finally, we present DriveWeather, a comprehensive all-weather\ndriving dataset designed to enable robust evaluation. Extensive experiments\ndemonstrate that our approach surpasses state-of-the-art methods, setting a new\nbenchmark in AWIR. The dataset and code are available at:\nhttps://github.com/fiwy0527/LCDiff.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u6846\u67b6LCDiff\uff0c\u5305\u542bLumina-Chroma\u5206\u89e3\u7f51\u7edc\uff08LCDN\uff09\u548cLumina-\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff08LGDM\uff09\uff0c\u5728YCbCr\u8272\u5f69\u7a7a\u95f4\u4e2d\u5206\u79bb\u4eae\u5ea6\u4e0e\u8272\u5ea6\u5206\u91cf\u4ee5\u6709\u6548\u53bb\u9664\u5929\u6c14\u9000\u5316\u5e76\u4fdd\u6301\u8272\u5f69\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5229\u7528\u52a8\u6001\u65f6\u95f4\u6b65\u635f\u5931\u4f18\u5316\u53bb\u566a\u8fc7\u7a0b\uff0c\u5e76\u53d1\u5e03\u4e86DriveWeather\u6570\u636e\u96c6\u7528\u4e8e\u5168\u5929\u6c14\u9a7e\u9a76\u573a\u666f\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u6216\u590d\u6742\u7684\u5929\u6c14\u9000\u5316\u7c7b\u578b\uff0c\u800c\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u9000\u5316\u4f30\u8ba1\u80fd\u529b\uff0c\u5bfc\u81f4\u6062\u590d\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u65e0\u9700\u663e\u5f0f\u9000\u5316\u63d0\u793a\u7684\u56fe\u50cf\u6062\u590d\u6846\u67b6\u3002", "method": "\u63d0\u51faLCDiff\u6846\u67b6\uff0c\u5176\u4e2dLCDN\u5728YCbCr\u7a7a\u95f4\u5206\u79bb\u4eae\u5ea6\uff08\u9000\u5316\u76f8\u5173\uff09\u548c\u8272\u5ea6\uff08\u9000\u5316\u4e0d\u53d8\uff09\u5206\u91cf\uff1bLGDM\u4ee5\u4eae\u5ea6\u4fe1\u606f\u4e3a\u5f15\u5bfc\u6761\u4ef6\u8fdb\u884c\u56fe\u50cf\u6062\u590d\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u65f6\u95f4\u6b65\u635f\u5931\u4f18\u5316\u7f51\u7edc\u5bf9\u9ad8\u4f4e\u9891\u7279\u5f81\u7684\u6062\u590d\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u56fe\u50cf\u6062\u590d\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u8272\u5f69\u4fdd\u771f\u5ea6\u548c\u7ec6\u8282\u6062\u590d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LCDiff\u901a\u8fc7\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u4e0e\u5f15\u5bfc\u6269\u6563\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6062\u590d\uff0c\u4e3a\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7DriveWeather\u6570\u636e\u96c6\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002"}}
{"id": "2509.17486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17486", "abs": "https://arxiv.org/abs/2509.17486", "authors": ["Lvzhou Luo", "Yixuan Cao", "Ping Luo"], "title": "AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation", "comment": "Accepted at EMNLP 2025 (Findings)", "summary": "Retrieval-augmented generation improves the factual accuracy of Large\nLanguage Models (LLMs) by incorporating external context, but often suffers\nfrom irrelevant retrieved content that hinders effectiveness. Context\ncompression addresses this issue by filtering out irrelevant information from\ncontext before LLM generation. However, existing methods struggle to adaptively\nadjust compression rates for different context, maintain low latency and\nintegrate information across multiple documents. To overcome these limitations,\nWe introduce AttnComp, an adaptive, efficient and context-aware compression\nframework. By leveraging the attention mechanism of LLMs to identify relevant\ninformation, AttnComp employs a Top-P compression algorithm to retain the\nminimal set of documents whose cumulative attention weights exceeds a\npredefined threshold. In addition to compression, AttnComp estimates response\nconfidence by assessing the overall relevance of the retrieved content,\nenabling users to gauge response reliability. Experiments demonstrate that\nAttnComp outperforms existing compression methods and uncompressed baselines,\nachieving higher accuracy with substantial compression rates and lower latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAttnComp\u7684\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7Top-P\u7b97\u6cd5\u4fdd\u7559\u5173\u952e\u6587\u6863\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u54cd\u5e94\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u96be\u4ee5\u81ea\u9002\u5e94\u8c03\u6574\u538b\u7f29\u7387\u3001\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u5e76\u6574\u5408\u591a\u6587\u6863\u4fe1\u606f\uff0c\u5bfc\u81f4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u4ecd\u5b58\u5728\u5927\u91cf\u65e0\u5173\u5185\u5bb9\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u91c7\u7528Top-P\u538b\u7f29\u7b97\u6cd5\u52a8\u6001\u4fdd\u7559\u7d2f\u8ba1\u6ce8\u610f\u529b\u8d85\u8fc7\u9608\u503c\u7684\u6700\u5c0f\u6587\u6863\u96c6\uff0c\u5e76\u4f30\u8ba1\u54cd\u5e94\u7f6e\u4fe1\u5ea6\u4ee5\u8bc4\u4f30\u68c0\u7d22\u5185\u5bb9\u7684\u6574\u4f53\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAttnComp\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u538b\u7f29\u65b9\u6cd5\u548c\u672a\u538b\u7f29\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u56de\u7b54\u51c6\u786e\u7387\u3001\u663e\u8457\u7684\u538b\u7f29\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "AttnComp\u662f\u4e00\u79cd\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u3002"}}
{"id": "2509.17621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17621", "abs": "https://arxiv.org/abs/2509.17621", "authors": ["Khoa Tran", "Hung-Cuong Trinh", "Vy-Rin Nguyen", "T. Nguyen-Thoi", "Vin Nguyen-Thai"], "title": "SeqBattNet: A Discrete-State Physics-Informed Neural Network with Aging Adaptation for Battery Modeling", "comment": null, "summary": "Accurate battery modeling is essential for reliable state estimation in\nmodern applications, such as predicting the remaining discharge time and\nremaining discharge energy in battery management systems. Existing approaches\nface several limitations: model-based methods require a large number of\nparameters; data-driven methods rely heavily on labeled datasets; and current\nphysics-informed neural networks (PINNs) often lack aging adaptation, or still\ndepend on many parameters, or continuously regenerate states. In this work, we\npropose SeqBattNet, a discrete-state PINN with built-in aging adaptation for\nbattery modeling, to predict terminal voltage during the discharge process.\nSeqBattNet consists of two components: (i) an encoder, implemented as the\nproposed HRM-GRU deep learning module, which generates cycle-specific aging\nadaptation parameters; and (ii) a decoder, based on the equivalent circuit\nmodel (ECM) combined with deep learning, which uses these parameters together\nwith the input current to predict voltage. The model requires only three basic\nbattery parameters and, when trained on data from a single cell, still achieves\nrobust performance. Extensive evaluations across three benchmark datasets (TRI,\nRT-Batt, and NASA) demonstrate that SeqBattNet significantly outperforms\nclassical sequence models and PINN baselines, achieving consistently lower RMSE\nwhile maintaining computational efficiency.", "AI": {"tldr": "\u63d0\u51faSeqBattNet\uff0c\u4e00\u79cd\u5177\u6709\u5185\u7f6e\u8001\u5316\u9002\u5e94\u80fd\u529b\u7684\u79bb\u6563\u72b6\u6001\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u7535\u6c60\u653e\u7535\u8fc7\u7a0b\u4e2d\u7684\u7aef\u7535\u538b\u9884\u6d4b\uff0c\u7ed3\u5408HRM-GRU\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u7b49\u6548\u7535\u8def\u6a21\u578b\u7684\u89e3\u7801\u5668\uff0c\u5728\u5c11\u53c2\u6570\u548c\u5355\u7535\u6c60\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7535\u6c60\u5efa\u6a21\u65b9\u6cd5\u5728\u53c2\u6570\u6570\u91cf\u3001\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u6216\u8001\u5316\u9002\u5e94\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u517c\u987e\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u6027\u3002", "method": "\u8bbe\u8ba1SeqBattNet\uff0c\u5305\u542bHRM-GRU\u7f16\u7801\u5668\u751f\u6210\u5468\u671f\u7279\u5b9a\u7684\u8001\u5316\u9002\u5e94\u53c2\u6570\uff0c\u4ee5\u53ca\u7ed3\u5408\u7b49\u6548\u7535\u8def\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u89e3\u7801\u5668\uff0c\u5229\u7528\u5c11\u91cf\u57fa\u672c\u7535\u6c60\u53c2\u6570\u8fdb\u884c\u7535\u538b\u9884\u6d4b\u3002", "result": "\u5728TRI\u3001RT-Batt\u548cNASA\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSeqBattNet\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u548cPINN\u57fa\u7ebf\u65b9\u6cd5\uff0cRMSE\u6301\u7eed\u66f4\u4f4e\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "SeqBattNet\u901a\u8fc7\u878d\u5408\u7269\u7406\u6a21\u578b\u4e0e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4f4e\u53c2\u6570\u4f9d\u8d56\u3001\u5f3a\u8001\u5316\u9002\u5e94\u6027\u548c\u8de8\u6570\u636e\u96c6\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u7cbe\u786e\u7684\u7535\u6c60\u72b6\u6001\u4f30\u8ba1\u3002"}}
{"id": "2509.17027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17027", "abs": "https://arxiv.org/abs/2509.17027", "authors": ["Zhenya Yang"], "title": "Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views", "comment": "Workshop Paper of AECAI@MICCAI 2025", "summary": "Surgical simulation is essential for medical training, enabling practitioners\nto develop crucial skills in a risk-free environment while improving patient\nsafety and surgical outcomes. However, conventional methods for building\nsimulation environments are cumbersome, time-consuming, and difficult to scale,\noften resulting in poor details and unrealistic simulations. In this paper, we\npropose a Gaussian Splatting-based framework to directly reconstruct\ninteractive surgical scenes from endoscopic data while ensuring efficiency,\nrendering quality, and realism. A key challenge in this data-driven simulation\nparadigm is the restricted movement of endoscopic cameras, which limits\nviewpoint diversity. As a result, the Gaussian Splatting representation\noverfits specific perspectives, leading to reduced geometric accuracy. To\naddress this issue, we introduce a novel virtual camera-based regularization\nmethod that adaptively samples virtual viewpoints around the scene and\nincorporates them into the optimization process to mitigate overfitting. An\neffective depth-based regularization is applied to both real and virtual views\nto further refine the scene geometry. To enable fast deformation simulation, we\npropose a sparse control node-based Material Point Method, which integrates\nphysical properties into the reconstructed scene while significantly reducing\ncomputational costs. Experimental results on representative surgical data\ndemonstrate that our method can efficiently reconstruct and simulate surgical\nscenes from sparse endoscopic views. Notably, our method takes only a few\nminutes to reconstruct the surgical scene and is able to produce physically\nplausible deformations in real-time with user-defined interactions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u70b9\u9635\u7684\u6846\u67b6\uff0c\u4ece\u5185\u7aa5\u955c\u6570\u636e\u76f4\u63a5\u91cd\u5efa\u4ea4\u4e92\u5f0f\u624b\u672f\u573a\u666f\uff0c\u5e76\u5f15\u5165\u865a\u62df\u76f8\u673a\u6b63\u5219\u5316\u65b9\u6cd5\u548c\u7a00\u758f\u63a7\u5236\u8282\u70b9\u7684\u7269\u8d28\u70b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u3001\u53ef\u4ea4\u4e92\u7684\u624b\u672f\u6a21\u62df\u3002", "motivation": "\u4f20\u7edf\u624b\u672f\u6a21\u62df\u73af\u5883\u6784\u5efa\u65b9\u6cd5\u7e41\u7410\u3001\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e0d\u8db3\u548c\u771f\u5b9e\u611f\u5dee\uff1b\u540c\u65f6\u5185\u7aa5\u955c\u89c6\u89d2\u53d7\u9650\u5bfc\u81f4\u91cd\u5efa\u6613\u8fc7\u62df\u5408\uff0c\u51e0\u4f55\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u70b9\u9635\u8fdb\u884c\u573a\u666f\u91cd\u5efa\uff0c\u5f15\u5165\u865a\u62df\u76f8\u673a\u81ea\u9002\u5e94\u91c7\u6837\u865a\u62df\u89c6\u89d2\u5e76\u7ed3\u5408\u771f\u5b9e\u4e0e\u865a\u62df\u89c6\u56fe\u7684\u6df1\u5ea6\u6b63\u5219\u5316\u6765\u63d0\u5347\u51e0\u4f55\u51c6\u786e\u6027\uff1b\u4f7f\u7528\u7a00\u758f\u63a7\u5236\u8282\u70b9\u7684\u7269\u8d28\u70b9\u6cd5\u5b9e\u73b0\u5feb\u901f\u7269\u7406\u5f62\u53d8\u6a21\u62df\u3002", "result": "\u5728\u4ee3\u8868\u6027\u624b\u672f\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ec5\u9700\u51e0\u5206\u949f\u5373\u53ef\u5b8c\u6210\u573a\u666f\u91cd\u5efa\uff0c\u652f\u6301\u5b9e\u65f6\u3001\u7269\u7406\u5408\u7406\u7684\u4ea4\u4e92\u5f62\u53d8\u6a21\u62df\uff0c\u4e14\u6e32\u67d3\u8d28\u91cf\u9ad8\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u5730\u4ece\u7a00\u758f\u5185\u7aa5\u955c\u89c6\u89d2\u91cd\u5efa\u5e76\u6a21\u62df\u624b\u672f\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7684\u771f\u5b9e\u611f\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17489", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17489", "abs": "https://arxiv.org/abs/2509.17489", "authors": ["Woongkyu Lee", "Junhee Cho", "Jungwook Choi"], "title": "MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM", "comment": null, "summary": "Large language models (LLMs) have advanced code generation from\nsingle-function tasks to competitive-programming problems, but existing\nmulti-agent solutions either rely on costly large-scale ($>$ 30B) models or\ncollapse when downsized to small open-source models. We present MapCoder-Lite,\nwhich upgrades a single 7B model into four role-specialised agents-retriever,\nplanner, coder, and debugger-using only rank-32, role-specific LoRA adapters\n($<3\\%$ extra parameters). Three lightweight techniques make this possible: (i)\ntrajectory distillation from strong LLMs fixes format fragility in retrieval\nand debugging, (ii) supervisor-guided correction strengthens planning and\ncoding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient\nspecialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests\nshows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\\%$ to\n$28.3\\%$), eliminates all format failures, and closes to within six points of a\n32B baseline while cutting GPU memory and token-generation time by $4\\times$.\nThese results demonstrate that careful agent-wise fine-tuning unleashes\nhigh-quality multi-agent coding on a small language model.", "AI": {"tldr": "MapCoder-Lite\u901a\u8fc7\u89d2\u8272\u4e13\u7528\u7684LoRA\u9002\u914d\u5668\u548c\u8f7b\u91cf\u7ea7\u4f18\u5316\u6280\u672f\uff0c\u5c06\u5355\u4e2a7B\u6a21\u578b\u5347\u7ea7\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u65b9\u6848\u6210\u672c\u9ad8\uff0c\u5c0f\u6a21\u578b\u65b9\u6848\u6027\u80fd\u5dee\u6216\u6613\u5d29\u6e83\uff0c\u4e9f\u9700\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMapCoder-Lite\uff0c\u4f7f\u7528\u56db\u4e2a\u89d2\u8272\u4e13\u7528\u7684LoRA\u9002\u914d\u5668\uff08\u68c0\u7d22\u3001\u89c4\u5212\u3001\u7f16\u7801\u3001\u8c03\u8bd5\uff09\uff0c\u7ed3\u5408\u8f68\u8ff9\u84b8\u998f\u3001\u76d1\u7763\u6821\u6b63\u548c\u9010\u667a\u80fd\u4f53LoRA\u5fae\u8c03\u4e09\u79cd\u8f7b\u91cf\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u4e13\u4e1a\u5316\u3002", "result": "\u5728xCodeEval\u3001APPS\u548cCodeContests\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1axCodeEval\u51c6\u786e\u7387\u4ece13.2%\u63d0\u5347\u81f328.3%\uff0c\u6d88\u9664\u6240\u6709\u683c\u5f0f\u9519\u8bef\uff0c\u6027\u80fd\u63a5\u8fd132B\u6a21\u578b\uff0c\u540c\u65f6\u663e\u5b58\u548c\u751f\u6210\u65f6\u95f4\u51cf\u5c114\u500d\u3002", "conclusion": "\u7cbe\u7ec6\u7684\u9010\u667a\u80fd\u4f53\u5fae\u8c03\u53ef\u5728\u5c0f\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\uff0c\u4e3a\u4f4e\u6210\u672c\u9ad8\u6027\u80fd\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.17625", "categories": ["cs.LG", "cs.CY", "physics.soc-ph", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.17625", "abs": "https://arxiv.org/abs/2509.17625", "authors": ["Blas Kolic", "Corrado Monti", "Gianmarco De Francisci Morales", "Marco Pangallo"], "title": "Comparing Data Assimilation and Likelihood-Based Inference on Latent State Estimation in Agent-Based Models", "comment": null, "summary": "In this paper, we present the first systematic comparison of Data\nAssimilation (DA) and Likelihood-Based Inference (LBI) in the context of\nAgent-Based Models (ABMs). These models generate observable time series driven\nby evolving, partially-latent microstates. Latent states need to be estimated\nto align simulations with real-world data -- a task traditionally addressed by\nDA, especially in continuous and equation-based models such as those used in\nweather forecasting. However, the nature of ABMs poses challenges for standard\nDA methods. Solving such issues requires adaptation of previous DA techniques,\nor ad-hoc alternatives such as LBI. DA approximates the likelihood in a\nmodel-agnostic way, making it broadly applicable but potentially less precise.\nIn contrast, LBI provides more accurate state estimation by directly leveraging\nthe model's likelihood, but at the cost of requiring a hand-crafted,\nmodel-specific likelihood function, which may be complex or infeasible to\nderive. We compare the two methods on the Bounded-Confidence Model, a\nwell-known opinion dynamics ABM, where agents are affected only by others\nholding sufficiently similar opinions. We find that LBI better recovers latent\nagent-level opinions, even under model mis-specification, leading to improved\nindividual-level forecasts. At the aggregate level, however, both methods\nperform comparably, and DA remains competitive across levels of aggregation\nunder certain parameter settings. Our findings suggest that DA is well-suited\nfor aggregate predictions, while LBI is preferable for agent-level inference.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u6570\u636e\u540c\u5316\uff08DA\uff09\u548c\u57fa\u4e8e\u4f3c\u7136\u7684\u63a8\u65ad\uff08LBI\uff09\u5728\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\uff08ABMs\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0LBI\u5728\u6062\u590d\u4e2a\u4f53\u5c42\u9762\u6f5c\u72b6\u6001\u65b9\u9762\u4f18\u4e8eDA\uff0c\u5c24\u5176\u5728\u6a21\u578b\u8bbe\u5b9a\u9519\u8bef\u65f6\u4ecd\u8868\u73b0\u826f\u597d\uff1b\u800c\u5728\u805a\u5408\u5c42\u9762\u4e24\u8005\u6027\u80fd\u76f8\u5f53\uff0cDA\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\u4f9d\u7136\u5177\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u7531\u4e8eABM\u4e2d\u6f5c\u72b6\u6001\u4f30\u8ba1\u5bf9\u8fde\u63a5\u6a21\u62df\u4e0e\u73b0\u5b9e\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u51c6DA\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u6bd4\u8f83DA\u4e0eLBI\u8fd9\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\u7684\u9002\u7528\u6027\u4e0e\u4f18\u52a3\u3002", "method": "\u5728\u6709\u754c\u7f6e\u4fe1\u6a21\u578b\uff08\u4e00\u79cd\u5178\u578b\u7684\u610f\u89c1\u52a8\u529b\u5b66ABM\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5bf9\u6bd4DA\u4e0eLBI\u5728\u4e0d\u540c\u805a\u5408\u5c42\u6b21\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u4e0e\u9884\u6d4b\u6027\u80fd\u3002", "result": "LBI\u5728\u4e2a\u4f53\u5c42\u9762\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8eDA\uff0c\u5373\u4f7f\u5728\u6a21\u578b\u8bbe\u5b9a\u9519\u8bef\u65f6\u4e5f\u66f4\u7a33\u5065\uff1b\u5728\u805a\u5408\u5c42\u9762\u4e24\u8005\u8868\u73b0\u76f8\u8fd1\uff0cDA\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "DA\u9002\u7528\u4e8e\u805a\u5408\u5c42\u9762\u7684\u9884\u6d4b\uff0c\u800cLBI\u66f4\u9002\u5408\u9700\u8981\u7cbe\u786e\u4e2a\u4f53\u63a8\u65ad\u7684\u4efb\u52a1\uff0c\u4e8c\u8005\u5404\u6709\u9002\u7528\u573a\u666f\u3002"}}
{"id": "2509.17040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17040", "abs": "https://arxiv.org/abs/2509.17040", "authors": ["Hang Du", "Jiayang Zhang", "Guoshun Nan", "Wendi Deng", "Zhenyan Chen", "Chenyang Zhang", "Wang Xiao", "Shan Huang", "Yuqi Pan", "Tao Qi", "Sicong Leng"], "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning", "comment": null, "summary": "Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language\nModels (MLLMs) ability to jointly comprehend and reason across multiple images\nand their associated textual contexts, introducing unique challenges beyond\nsingle-image or non-interleaved multi-image tasks. While current multi-image\nbenchmarks overlook interleaved textual contexts and neglect distinct\nrelationships between individual images and their associated texts, enabling\nmodels to reason over multi-image interleaved data may significantly enhance\ntheir comprehension of complex scenes and better capture cross-modal\ncorrelations. To bridge this gap, we introduce a novel benchmark MIR, requiring\njoint reasoning over multiple images accompanied by interleaved textual\ncontexts to accurately associate image regions with corresponding texts and\nlogically connect information across images. To enhance MLLMs ability to\ncomprehend multi-image interleaved data, we introduce reasoning steps for each\ninstance within the benchmark and propose a stage-wise curriculum learning\nstrategy. This strategy follows an \"easy to hard\" approach, progressively\nguiding models from simple to complex scenarios, thereby enhancing their\nability to handle challenging tasks. Extensive experiments benchmarking\nmultiple MLLMs demonstrate that our method significantly enhances models\nreasoning performance on MIR and other established benchmarks. We believe that\nMIR will encourage further research into multi-image interleaved reasoning,\nfacilitating advancements in MLLMs capability to handle complex inter-modal\ntasks.Our code and dataset are available at\nhttps://github.com/Shelly-coder239/MIRBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u56fe\u50cf\u4ea4\u9519\u63a8\u7406\uff08MIR\uff09\u57fa\u51c6\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u56fe\u50cf\u4e0e\u4ea4\u9519\u6587\u672c\u4e0a\u4e0b\u6587\u4e2d\u7684\u8054\u5408\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u4ea4\u9519\u6587\u672c\u4e0a\u4e0b\u6587\u53ca\u56fe\u50cf\u4e0e\u6587\u672c\u95f4\u7684\u7279\u5b9a\u5173\u7cfb\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u590d\u6742\u573a\u666f\u4e0b\u7684\u8de8\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u63d0\u51faMIR\u57fa\u51c6\u5e76\u5f15\u5165\u5206\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u9010\u6b65\u8bad\u7ec3\uff0c\u7ed3\u5408\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u63a8\u7406\u6b65\u9aa4\u6765\u589e\u5f3aMLLM\u5bf9\u591a\u56fe\u50cf\u4ea4\u9519\u6570\u636e\u7684\u7406\u89e3\u3002", "result": "\u5728\u591a\u4e2aMLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728MIR\u53ca\u5176\u4ed6\u57fa\u51c6\u4e0a\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "MIR\u6709\u52a9\u4e8e\u63a8\u52a8\u591a\u56fe\u50cf\u4ea4\u9519\u63a8\u7406\u7684\u7814\u7a76\uff0c\u63d0\u5347MLLM\u5904\u7406\u590d\u6742\u8de8\u6a21\u6001\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2509.17493", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17493", "abs": "https://arxiv.org/abs/2509.17493", "authors": ["Wenhao Zhuang", "Yuan Sun", "Xiaobing Zhao"], "title": "Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages", "comment": null, "summary": "As large language models (LLMs) are trained on increasingly diverse and\nextensive multilingual corpora, they demonstrate cross-lingual transfer\ncapabilities. However, these capabilities often fail to effectively extend to\nlow-resource languages, particularly those utilizing non-Latin scripts. While\ntransliterating low-resource languages into Latin script presents a natural\nsolution, there currently lacks a comprehensive framework for integrating\ntransliteration into LLMs training and deployment. Taking a pragmatic approach,\nthis paper innovatively combines character transliteration with Huffman coding\nto design a complete transliteration framework. Our proposed framework offers\nthe following advantages: 1) Compression: Reduces storage requirements for\nlow-resource language content, achieving up to 50% reduction in file size and\n50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless\nconversion from transliterated text back to the source language. 3) Efficiency:\nEliminates the need for vocabulary expansion for low-resource languages,\nimproving training and inference efficiency. 4) Scalability: The framework can\nbe extended to other low-resource languages. We validate the effectiveness of\nour framework across multiple downstream tasks, including text classification,\nmachine reading comprehension, and machine translation. Experimental results\ndemonstrate that our method significantly enhances the model's capability to\nprocess low-resource languages while maintaining performance on high-resource\nlanguages. Our data and code are publicly available at\nhttps://github.com/CMLI-NLP/HuffmanTranslit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b57\u7b26\u97f3\u8bd1\u4e0e\u970d\u592b\u66fc\u7f16\u7801\u7684\u65b0\u578b\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5c24\u5176\u662f\u975e\u62c9\u4e01\u8bed\u7cfb\uff09\u7684\u5904\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u538b\u7f29\u3001\u65e0\u635f\u8fd8\u539f\u3001\u9ad8\u6548\u8bad\u7ec3\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\uff0c\u4f46\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u7279\u522b\u662f\u4f7f\u7528\u975e\u62c9\u4e01\u5b57\u6bcd\u7684\u8bed\u8a00\uff09\u6548\u679c\u4e0d\u4f73\u3002\u7f3a\u4e4f\u6709\u6548\u7684\u97f3\u8bd1\u6574\u5408\u6846\u67b6\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5b57\u7b26\u97f3\u8bd1\u4e0e\u970d\u592b\u66fc\u7f16\u7801\u7684\u5b8c\u6574\u97f3\u8bd1\u6846\u67b6\uff0c\u5b9e\u73b0\u6587\u672c\u538b\u7f29\u4e0e\u65e0\u635f\u8fd8\u539f\uff0c\u5e76\u907f\u514d\u8bcd\u6c47\u8868\u6269\u5c55\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6587\u672c\u5206\u7c7b\u3001\u673a\u5668\u9605\u8bfb\u7406\u89e3\u3001\u673a\u5668\u7ffb\u8bd1\uff09\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\uff1b\u6587\u4ef6\u5927\u5c0f\u51cf\u5c1150%\uff0ctoken\u6570\u51cf\u5c1150-80%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u97f3\u8bd1\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5904\u7406\u96be\u9898\uff0c\u517c\u5177\u538b\u7f29\u6027\u3001\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17665", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.17665", "abs": "https://arxiv.org/abs/2509.17665", "authors": ["Katharina Simbeck", "Mariam Mahran"], "title": "Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models", "comment": "Accepted at AEQUITAS 2025: Workshop on Fairness and Bias in AI |\n  co-located with ECAI, October 26th, 2025, Bologna, Italy. 12 pages, 1 figure", "summary": "Despite growing research on bias in large language models (LLMs), most work\nhas focused on gender and race, with little attention to religious identity.\nThis paper explores how religion is internally represented in LLMs and how it\nintersects with concepts of violence and geography. Using mechanistic\ninterpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we\nanalyze latent feature activations across five models. We measure overlap\nbetween religion- and violence-related prompts and probe semantic patterns in\nactivation contexts. While all five religions show comparable internal\ncohesion, Islam is more frequently linked to features associated with violent\nlanguage. In contrast, geographic associations largely reflect real-world\nreligious demographics, revealing how models embed both factual distributions\nand cultural stereotypes. These findings highlight the value of structural\nanalysis in auditing not just outputs but also internal representations that\nshape model behavior.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5b97\u6559\u8eab\u4efd\u7684\u5185\u90e8\u8868\u5f81\u53ca\u5176\u4e0e\u66b4\u529b\u548c\u5730\u7406\u6982\u5ff5\u7684\u5173\u8054\uff0c\u53d1\u73b0\u4f0a\u65af\u5170\u6559\u66f4\u5e38\u4e0e\u66b4\u529b\u76f8\u5173\u7279\u5f81\u5173\u8054\uff0c\u800c\u5730\u7406\u5173\u8054\u5219\u53cd\u6620\u73b0\u5b9e\u5b97\u6559\u5206\u5e03\uff0c\u63ed\u793a\u6a21\u578b\u5982\u4f55\u5d4c\u5165\u4e8b\u5b9e\u4fe1\u606f\u4e0e\u6587\u5316\u523b\u677f\u5370\u8c61\u3002", "motivation": "\u73b0\u6709\u5bf9LLM\u504f\u89c1\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u6027\u522b\u548c\u79cd\u65cf\uff0c\u5ffd\u89c6\u5b97\u6559\u8eab\u4efd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76\u5b97\u6559\u5728\u6a21\u578b\u4e2d\u7684\u8868\u5f81\u53ca\u5176\u6f5c\u5728\u504f\u89c1\u3002", "method": "\u4f7f\u7528\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u548c\u901a\u8fc7Neuronpedia API\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\uff0c\u5206\u6790\u4e94\u4e2aLLM\u4e2d\u4e0e\u5b97\u6559\u3001\u66b4\u529b\u548c\u5730\u7406\u76f8\u5173\u7684\u9690\u5c42\u7279\u5f81\u6fc0\u6d3b\u60c5\u51b5\uff0c\u6d4b\u91cf\u63d0\u793a\u95f4\u7684\u6fc0\u6d3b\u91cd\u53e0\u5e76\u63a2\u6d4b\u8bed\u4e49\u6a21\u5f0f\u3002", "result": "\u6240\u6709\u4e94\u79cd\u5b97\u6559\u5728\u6a21\u578b\u4e2d\u5747\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u4f46\u4f0a\u65af\u5170\u6559\u66f4\u9891\u7e41\u5730\u4e0e\u66b4\u529b\u76f8\u5173\u8bed\u8a00\u7279\u5f81\u5173\u8054\uff1b\u5730\u7406\u5173\u8054\u5219\u5927\u4f53\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u5b97\u6559\u4eba\u53e3\u5206\u5e03\u3002", "conclusion": "LLMs\u4e0d\u4ec5\u5d4c\u5165\u4e86\u5ba2\u89c2\u4e8b\u5b9e\u5206\u5e03\uff0c\u4e5f\u5305\u542b\u4e86\u6587\u5316\u523b\u677f\u5370\u8c61\uff0c\u5c24\u5176\u662f\u5b97\u6559\u4e0e\u66b4\u529b\u7684\u4e0d\u5f53\u5173\u8054\uff1b\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u6790\u5185\u90e8\u8868\u5f81\uff0c\u53ef\u66f4\u6df1\u5165\u5ba1\u8ba1\u6a21\u578b\u504f\u89c1\u3002"}}
{"id": "2509.17041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17041", "abs": "https://arxiv.org/abs/2509.17041", "authors": ["Samia Mohinta", "Daniel Franco-Barranco", "Shi Yan Lee", "Albert Cardona"], "title": "Towards Generalized Synapse Detection Across Invertebrate Species", "comment": null, "summary": "Behavioural differences across organisms, whether healthy or pathological,\nare closely tied to the structure of their neural circuits. Yet, the fine-scale\nsynaptic changes that give rise to these variations remain poorly understood,\nin part due to persistent challenges in detecting synapses reliably and at\nscale. Volume electron microscopy (EM) offers the resolution required to\ncapture synaptic architecture, but automated detection remains difficult due to\nsparse annotations, morphological variability, and cross-dataset domain shifts.\nTo address this, we make three key contributions. First, we curate a diverse EM\nbenchmark spanning four datasets across two invertebrate species: adult and\nlarval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,\nwe propose SimpSyn, a single-stage Residual U-Net trained to predict\ndual-channel spherical masks around pre- and post-synaptic sites, designed to\nprioritize training and inference speeds and annotation efficiency over\narchitectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s\nSynful [1], a state-of-the-art multi-task model that jointly infers synaptic\npairs. Despite its simplicity, SimpSyn consistently outperforms Synful in\nF1-score across all volumes for synaptic site detection. While generalization\nacross datasets remains limited, SimpSyn achieves competitive performance when\ntrained on the combined cohort. Finally, ablations reveal that simple\npost-processing strategies - such as local peak detection and distance-based\nfiltering - yield strong performance without complex test-time heuristics.\nTaken together, our results suggest that lightweight models, when aligned with\ntask structure, offer a practical and scalable solution for synapse detection\nin large-scale connectomic pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u5355\u9636\u6bb5Residual U-Net\u6a21\u578bSimpSyn\uff0c\u7528\u4e8e\u5728\u4f53\u79ef\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u68c0\u6d4b\u7a81\u89e6\u4f4d\u70b9\uff0c\u5e76\u5728\u4e00\u4e2a\u8de8\u7269\u79cd\u7684\u591a\u6837\u5316\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7a81\u89e6\u7ed3\u6784\u7684\u7cbe\u7ec6\u53d8\u5316\u4e0e\u751f\u7269\u4f53\u884c\u4e3a\u5dee\u5f02\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u76ee\u524d\u7531\u4e8e\u6807\u6ce8\u7a00\u758f\u3001\u5f62\u6001\u591a\u6837\u6027\u548c\u6570\u636e\u96c6\u95f4\u57df\u504f\u79fb\u7b49\u95ee\u9898\uff0c\u5927\u89c4\u6a21\u53ef\u9760\u5730\u81ea\u52a8\u68c0\u6d4b\u7a81\u89e6\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u4e24\u79cd\u65e0\u810a\u690e\u52a8\u7269\uff08\u679c\u8747\u548c\u5fae\u8702\uff09\u56db\u4e2a\u6570\u636e\u96c6\u7684\u4f53\u79ef\u7535\u955c\u57fa\u51c6\uff1b\u63d0\u51faSimpSyn\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u901a\u9053\u7403\u5f62\u63a9\u7801\u9884\u6d4b\u7a81\u89e6\u524d\u548c\u7a81\u89e6\u540e\u4f4d\u70b9\uff1b\u7ed3\u5408\u5c40\u90e8\u5cf0\u503c\u68c0\u6d4b\u548c\u57fa\u4e8e\u8ddd\u79bb\u7684\u8fc7\u6ee4\u8fdb\u884c\u540e\u5904\u7406\u3002", "result": "SimpSyn\u5728\u6240\u6709\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2dF1\u5206\u6570\u5747\u8d85\u8fc7\u73b0\u6709\u7684\u591a\u4efb\u52a1\u6a21\u578bSynful\uff1b\u5728\u7ec4\u5408\u8bad\u7ec3\u6570\u636e\u4e0a\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff1b\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u7b80\u5355\u540e\u5904\u7406\u5373\u53ef\u83b7\u5f97\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u82e5\u80fd\u4e0e\u4efb\u52a1\u7ed3\u6784\u5339\u914d\uff0c\u53ef\u5728\u5927\u89c4\u6a21\u8fde\u63a5\u7ec4\u5b66\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u7a81\u89e6\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17505", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17505", "abs": "https://arxiv.org/abs/2509.17505", "authors": ["Tu\u011fba Pamay Arslan", "Emircan Erol", "G\u00fcl\u015fen Eryi\u011fit"], "title": "CorefInst: Leveraging LLMs for Multilingual Coreference Resolution", "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL) (2025 August). Submission: March, 2025.\n  Revision: July, 2025. Acceptance: August, 2025", "summary": "Coreference Resolution (CR) is a crucial yet challenging task in natural\nlanguage understanding, often constrained by task-specific architectures and\nencoder-based language models that demand extensive training and lack\nadaptability. This study introduces the first multilingual CR methodology which\nleverages decoder-only LLMs to handle both overt and zero mentions. The article\nexplores how to model the CR task for LLMs via five different instruction sets\nusing a controlled inference method. The approach is evaluated across three\nLLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when\ninstruction-tuned with a suitable instruction set, can surpass state-of-the-art\ntask-specific architectures. Specifically, our best model, a fully fine-tuned\nLlama 3.1 for multilingual CR, outperforms the leading multilingual CR model\n(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages\nin the CorefUD v1.2 dataset collection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u7801\u5668-only\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u8bed\u8a00\u5171\u6307\u6d88\u89e3\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u5728\u591a\u4e2aLLM\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u4efb\u52a1\u4e13\u7528\u67b6\u6784\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5171\u6307\u6d88\u89e3\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u67b6\u6784\u548c\u7f16\u7801\u5668\u6a21\u578b\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\uff0c\u7f3a\u4e4f\u5bf9\u663e\u63d0\u53ca\u548c\u96f6\u5f62\u56de\u6307\u7684\u7edf\u4e00\u5904\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e94\u79cd\u4e0d\u540c\u7684\u6307\u4ee4\u96c6\uff0c\u5c06\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u9002\u914d\u5230Llama 3.1\u3001Gemma 2\u548cMistral 0.3\u7b49\u89e3\u7801\u5668-only LLM\u4e2d\uff0c\u5e76\u91c7\u7528\u53d7\u63a7\u63a8\u7406\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6307\u4ee4\u8c03\u4f18\u540e\u7684LLM\u5728CorefUD v1.2\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u591a\u8bed\u8a00\u6a21\u578bCorpipe 24\u7ea62\u4e2a\u767e\u5206\u70b9\uff0c\u5176\u4e2d\u5168\u5fae\u8c03\u7684Llama 3.1\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u89e3\u7801\u5668-only\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u5408\u9002\u7684\u6307\u4ee4\u8bbe\u8ba1\u53ef\u6709\u6548\u89e3\u51b3\u591a\u8bed\u8a00\u5171\u6307\u6d88\u89e3\u95ee\u9898\uff0c\u4e14\u5728\u5904\u7406\u663e\u63d0\u53ca\u4e0e\u96f6\u5f62\u56de\u6307\u65b9\u9762\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u7075\u6d3b\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.17693", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17693", "abs": "https://arxiv.org/abs/2509.17693", "authors": ["Adam Weso\u0142owski", "Ronin Wu", "Karim Essafi"], "title": "Fast, Accurate and Interpretable Graph Classification with Topological Kernels", "comment": null, "summary": "We introduce a novel class of explicit feature maps based on topological\nindices that represent each graph by a compact feature vector, enabling fast\nand interpretable graph classification. Using radial basis function kernels on\nthese compact vectors, we define a measure of similarity between graphs. We\nperform evaluation on standard molecular datasets and observe that\nclassification accuracies based on single topological-index feature vectors\nunderperform compared to state-of-the-art substructure-based kernels. However,\nwe achieve significantly faster Gram matrix evaluation -- up to $20\\times$\nfaster -- compared to the Weisfeiler--Lehman subtree kernel. To enhance\nperformance, we propose two extensions: 1) concatenating multiple topological\nindices into an \\emph{Extended Feature Vector} (EFV), and 2) \\emph{Linear\nCombination of Topological Kernels} (LCTK) by linearly combining Radial Basis\nFunction kernels computed on feature vectors of individual topological graph\nindices. These extensions deliver up to $12\\%$ percent accuracy gains across\nall the molecular datasets. A complexity analysis highlights the potential for\nexponential quantum speedup for some of the vector components. Our results\nindicate that LCTK and EFV offer a favourable trade-off between accuracy and\nefficiency, making them strong candidates for practical graph learning\napplications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u62d3\u6251\u6307\u6570\u7684\u663e\u5f0f\u7279\u5f81\u6620\u5c04\uff0c\u901a\u8fc7\u7d27\u51d1\u7279\u5f81\u5411\u91cf\u5b9e\u73b0\u5feb\u901f\u4e14\u53ef\u89e3\u91ca\u7684\u56fe\u5206\u7c7b\uff1b\u7ed3\u5408\u591a\u79cd\u62d3\u6251\u6838\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u5e76\u5927\u5e45\u52a0\u5feb\u8ba1\u7b97\u901f\u5ea6\uff0c\u5177\u6709\u6f5c\u5728\u91cf\u5b50\u52a0\u901f\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u56fe\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u514b\u670d\u73b0\u6709\u5b50\u7ed3\u6784\u6838\u65b9\u6cd5\u8ba1\u7b97\u6162\u6216\u51c6\u786e\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u62d3\u6251\u6307\u6570\u6784\u9020\u7d27\u51d1\u7279\u5f81\u5411\u91cf\uff0c\u5e76\u5728\u5176\u4e0a\u5e94\u7528\u5f84\u5411\u57fa\u51fd\u6570\u6838\u5b9a\u4e49\u56fe\u95f4\u76f8\u4f3c\u6027\uff1b\u63d0\u51fa\u6269\u5c55\u7279\u5f81\u5411\u91cf\uff08EFV\uff09\u548c\u62d3\u6251\u6838\u7ebf\u6027\u7ec4\u5408\uff08LCTK\uff09\u4e24\u79cd\u6269\u5c55\u65b9\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u5355\u4e2a\u62d3\u6251\u7279\u5f81\u5411\u91cf\u51c6\u786e\u6027\u4f4e\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4f46Gram\u77e9\u9635\u8ba1\u7b97\u901f\u5ea6\u5feb\u8fbe20\u500d\uff1bEFV\u548cLCTK\u5e26\u6765\u6700\u9ad812%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u6f5c\u5728\u7684\u6307\u6570\u7ea7\u91cf\u5b50\u52a0\u901f\u53ef\u80fd\u3002", "conclusion": "LCTK\u548cEFV\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u662f\u5b9e\u9645\u56fe\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u6709\u529b\u5019\u9009\u65b9\u6cd5\u3002"}}
{"id": "2509.17044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17044", "abs": "https://arxiv.org/abs/2509.17044", "authors": ["Mingqing Zhang", "Zhuoning Xu", "Peijie Wang", "Rongji Li", "Liang Wang", "Qiang Liu", "Jian Xu", "Xuyao Zhang", "Shu Wu", "Liang Wang"], "title": "AgriDoctor: A Multimodal Intelligent Assistant for Agriculture", "comment": null, "summary": "Accurate crop disease diagnosis is essential for sustainable agriculture and\nglobal food security. Existing methods, which primarily rely on unimodal models\nsuch as image-based classifiers and object detectors, are limited in their\nability to incorporate domain-specific agricultural knowledge and lack support\nfor interactive, language-based understanding. Recent advances in large\nlanguage models (LLMs) and large vision-language models (LVLMs) have opened new\navenues for multimodal reasoning. However, their performance in agricultural\ncontexts remains limited due to the absence of specialized datasets and\ninsufficient domain adaptation. In this work, we propose AgriDoctor, a modular\nand extensible multimodal framework designed for intelligent crop disease\ndiagnosis and agricultural knowledge interaction. As a pioneering effort to\nintroduce agent-based multimodal reasoning into the agricultural domain,\nAgriDoctor offers a novel paradigm for building interactive and domain-adaptive\ncrop health solutions. It integrates five core components: a router,\nclassifier, detector, knowledge retriever and LLMs. To facilitate effective\ntraining and evaluation, we construct AgriMM, a comprehensive benchmark\ncomprising 400000 annotated disease images, 831 expert-curated knowledge\nentries, and 300000 bilingual prompts for intent-driven tool selection.\nExtensive experiments demonstrate that AgriDoctor, trained on AgriMM,\nsignificantly outperforms state-of-the-art LVLMs on fine-grained agricultural\ntasks, establishing a new paradigm for intelligent and sustainable farming\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AgriDoctor\uff0c\u4e00\u4e2a\u7528\u4e8e\u667a\u80fd\u4f5c\u7269\u75c5\u5bb3\u8bca\u65ad\u548c\u519c\u4e1a\u77e5\u8bc6\u4ea4\u4e92\u7684\u6a21\u5757\u5316\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6AgriMM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u519c\u4e1a\u9886\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4f5c\u7269\u75c5\u5bb3\u8bca\u65ad\u65b9\u6cd5\u591a\u4f9d\u8d56\u5355\u6a21\u6001\u6a21\u578b\uff0c\u96be\u4ee5\u878d\u5408\u519c\u4e1a\u9886\u57df\u77e5\u8bc6\u4e14\u7f3a\u4e4f\u8bed\u8a00\u4ea4\u4e92\u80fd\u529b\uff1b\u540c\u65f6\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u9886\u57df\u9002\u914d\u9650\u5236\u4e86\u5927\u6a21\u578b\u5728\u519c\u4e1a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faAgriDoctor\u6846\u67b6\uff0c\u96c6\u6210\u8def\u7531\u5668\u3001\u5206\u7c7b\u5668\u3001\u68c0\u6d4b\u5668\u3001\u77e5\u8bc6\u68c0\u7d22\u5668\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e94\u4e2a\u6a21\u5757\uff0c\u5e76\u6784\u5efa\u5305\u542b40\u4e07\u56fe\u50cf\u3001831\u6761\u4e13\u5bb6\u77e5\u8bc6\u548c30\u4e07\u53cc\u8bed\u63d0\u793a\u7684AgriMM\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eAgriMM\u8bad\u7ec3\u7684AgriDoctor\u5728\u7ec6\u7c92\u5ea6\u519c\u4e1a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "AgriDoctor\u4e3a\u519c\u4e1a\u9886\u57df\u5f15\u5165\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u5316\u3001\u53ef\u6301\u7eed\u519c\u4e1a\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.17523", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17523", "abs": "https://arxiv.org/abs/2509.17523", "authors": ["Mar\u00eda Andrea Cruz Bland\u00f3n", "Zakaria Aldeneh", "Jie Chi", "Maureen de Seyssel"], "title": "Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models", "comment": "5 pages, 2 figures", "summary": "Self-supervised learning (SSL) has made significant advances in speech\nrepresentation learning. Models like wav2vec 2.0 and HuBERT have achieved\nstate-of-the-art results in tasks such as speech recognition, particularly in\nmonolingual settings. However, multilingual SSL models tend to underperform\ntheir monolingual counterparts on each individual language, especially in\nmultilingual scenarios with few languages such as the bilingual setting. In\nthis work, we investigate a novel approach to reduce this performance gap by\nintroducing limited visual grounding into bilingual speech SSL models. Our\nresults show that visual grounding benefits both monolingual and bilingual\nmodels, with especially pronounced gains for the latter, reducing the\nmultilingual performance gap on zero-shot phonetic discrimination from 31.5%\nfor audio-only models to 8.04% with grounding.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u53cc\u8bed\u8bed\u97f3\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u4e2d\u5f15\u5165\u6709\u9650\u89c6\u89c9 grounding \u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u7f29\u5c0f\u591a\u8bed\u8a00\u6a21\u578b\u4e0e\u5355\u8bed\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u89c9 grounding \u5bf9\u5355\u8bed\u548c\u53cc\u8bed\u6a21\u578b\u5747\u6709\u63d0\u5347\uff0c\u5c24\u5176\u663e\u8457\u964d\u4f4e\u4e86\u96f6\u6837\u672c\u97f3\u7d20\u5224\u522b\u4efb\u52a1\u4e2d\u7684\u591a\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\uff0c\u4ece\u7eaf\u97f3\u9891\u6a21\u578b\u768431.5%\u964d\u81f38.04%\u3002", "motivation": "\u591a\u8bed\u8a00\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u5728\u4e2a\u522b\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u901a\u5e38\u4e0d\u5982\u5355\u8bed\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u53cc\u8bed\u7b49\u8bed\u8a00\u6570\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u5b58\u5728\u660e\u663e\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u5728\u53cc\u8bed\u8bed\u97f3\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u4e2d\u5f15\u5165\u6709\u9650\u7684\u89c6\u89c9 grounding \u4fe1\u606f\uff0c\u63a2\u7d22\u5176\u5bf9\u8bed\u97f3\u8868\u5f81\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u89c6\u89c9 grounding \u63d0\u5347\u4e86\u5355\u8bed\u548c\u53cc\u8bed\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u97f3\u7d20\u5224\u522b\u4efb\u52a1\u4e0a\uff0c\u53cc\u8bed\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u4ece31.5%\u663e\u8457\u964d\u4f4e\u81f38.04%\u3002", "conclusion": "\u5f15\u5165\u89c6\u89c9 grounding \u80fd\u6709\u6548\u7f29\u5c0f\u591a\u8bed\u8a00\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u4e0e\u5355\u8bed\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u53cc\u8bed\u8bbe\u7f6e\u4e0b\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2509.17695", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.17695", "abs": "https://arxiv.org/abs/2509.17695", "authors": ["Leszek Sliwko"], "title": "Cluster Workload Allocation: A Predictive Approach Leveraging Machine Learning Efficiency", "comment": "This is the accepted version of the paper published in IEEE Access.\n  The final version is available at:\n  https://doi.org/10.1109/ACCESS.2024.3520422", "summary": "This research investigates how Machine Learning (ML) algorithms can assist in\nworkload allocation strategies by detecting tasks with node affinity operators\n(referred to as constraint operators), which constrain their execution to a\nlimited number of nodes. Using real-world Google Cluster Data (GCD) workload\ntraces and the AGOCS framework, the study extracts node attributes and task\nconstraints, then analyses them to identify suitable node-task pairings. It\nfocuses on tasks that can be executed on either a single node or fewer than a\nthousand out of 12.5k nodes in the analysed GCD cluster. Task constraint\noperators are compacted, pre-processed with one-hot encoding, and used as\nfeatures in a training dataset. Various ML classifiers, including Artificial\nNeural Networks, K-Nearest Neighbours, Decision Trees, Naive Bayes, Ridge\nRegression, Adaptive Boosting, and Bagging, are fine-tuned and assessed for\naccuracy and F1-scores. The final ensemble voting classifier model achieved 98%\naccuracy and a 1.5-1.8% misclassification rate for tasks with a single suitable\nnode.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u57fa\u4e8eGoogle\u96c6\u7fa4\u6570\u636e\u548cAGOCS\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u8282\u70b9\u5c5e\u6027\u548c\u4efb\u52a1\u7ea6\u675f\uff0c\u6784\u5efa\u5206\u7c7b\u6a21\u578b\u4ee5\u4f18\u5316\u5177\u6709\u8282\u70b9\u4eb2\u548c\u6027\u7ea6\u675f\u7684\u4efb\u52a1\u5206\u914d\uff0c\u6700\u7ec8\u96c6\u6210\u6295\u7968\u5206\u7c7b\u5668\u5728\u5355\u8282\u70b9\u9002\u914d\u4efb\u52a1\u4e0a\u8fbe\u523098%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u5927\u89c4\u6a21\u96c6\u7fa4\u4e2d\u5177\u6709\u8282\u70b9\u7ea6\u675f\u7684\u4efb\u52a1\u5206\u914d\u6548\u7387\uff0c\u51cf\u5c11\u8d44\u6e90\u6d6a\u8d39\u4e0e\u8c03\u5ea6\u5ef6\u8fdf\uff0c\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u8bc6\u522b\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u8282\u70b9\u3002", "method": "\u4f7f\u7528\u771f\u5b9eGoogle\u96c6\u7fa4\u6570\u636e\uff08GCD\uff09\u548cAGOCS\u6846\u67b6\u63d0\u53d6\u8282\u70b9\u5c5e\u6027\u4e0e\u4efb\u52a1\u7ea6\u675f\uff1b\u5bf9\u7ea6\u675f\u64cd\u4f5c\u7b26\u8fdb\u884c\u72ec\u70ed\u7f16\u7801\u9884\u5904\u7406\uff0c\u5e76\u4f5c\u4e3a\u7279\u5f81\u8f93\u5165\uff1b\u91c7\u7528\u591a\u79cdML\u5206\u7c7b\u5668\uff08\u5982\u795e\u7ecf\u7f51\u7edc\u3001KNN\u3001\u51b3\u7b56\u6811\u7b49\uff09\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8c03\u4f18\uff0c\u6700\u7ec8\u6784\u5efa\u96c6\u6210\u6295\u7968\u5206\u7c7b\u5668\u3002", "result": "\u96c6\u6210\u6295\u7968\u5206\u7c7b\u5668\u5728\u4ec5\u80fd\u8fd0\u884c\u4e8e\u5355\u4e2a\u6216\u5c11\u4e8e\u4e00\u5343\u4e2a\u8282\u70b9\uff08\u517112.5k\u8282\u70b9\uff09\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe98%\uff0c\u8bef\u5206\u7c7b\u7387\u4e3a1.5%-1.8%\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u80fd\u6709\u6548\u652f\u6301\u5e26\u8282\u70b9\u7ea6\u675f\u7684\u4efb\u52a1\u8c03\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9ad8\u5ea6\u53d7\u9650\u7684\u4efb\u52a1\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u8282\u70b9-\u4efb\u52a1\u5339\u914d\u7684\u51c6\u786e\u6027\uff0c\u5177\u5907\u5e94\u7528\u4e8e\u5b9e\u9645\u96c6\u7fa4\u8c03\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17049", "abs": "https://arxiv.org/abs/2509.17049", "authors": ["Peng Wang", "Yong Li", "Lin Zhao", "Xiu-Shen Wei"], "title": "Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization", "comment": null, "summary": "Fine-grained hashing has become a powerful solution for rapid and efficient\nimage retrieval, particularly in scenarios requiring high discrimination\nbetween visually similar categories. To enable each hash bit to correspond to\nspecific visual attributes, we propoe a novel method that harnesses learnable\nqueries for attribute-aware hash codes learning. This method deploys a tailored\nset of queries to capture and represent nuanced attribute-level information\nwithin the hashing process, thereby enhancing both the interpretability and\nrelevance of each hash bit. Building on this query-based optimization\nframework, we incorporate an auxiliary branch to help alleviate the challenges\nof complex landscape optimization often encountered with low-bit hash codes.\nThis auxiliary branch models high-order attribute interactions, reinforcing the\nrobustness and specificity of the generated hash codes. Experimental results on\nbenchmark datasets demonstrate that our method generates attribute-aware hash\ncodes and consistently outperforms state-of-the-art techniques in retrieval\naccuracy and robustness, especially for low-bit hash codes, underscoring its\npotential in fine-grained image hashing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u67e5\u8be2\u7684\u7ec6\u7c92\u5ea6\u54c8\u5e0c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u5206\u652f\u6355\u6349\u9ad8\u9636\u5c5e\u6027\u4ea4\u4e92\uff0c\u751f\u6210\u5177\u6709\u5c5e\u6027\u611f\u77e5\u7684\u54c8\u5e0c\u7801\uff0c\u5728\u4f4e\u6bd4\u7279\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u54c8\u5e0c\u65b9\u6cd5\u96be\u4ee5\u5728\u4f4e\u6bd4\u7279\u7f16\u7801\u4e0b\u4fdd\u6301\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u7684\u5224\u522b\u80fd\u529b\uff0c\u4e14\u54c8\u5e0c\u4f4d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u5bf9\u5e94\u5177\u4f53\u89c6\u89c9\u5c5e\u6027\u3002", "method": "\u8bbe\u8ba1\u53ef\u5b66\u4e60\u67e5\u8be2\u673a\u5236\u4ee5\u6355\u6349\u5c5e\u6027\u7ea7\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u8f85\u52a9\u5206\u652f\u5efa\u6a21\u9ad8\u9636\u5c5e\u6027\u4ea4\u4e92\uff0c\u589e\u5f3a\u54c8\u5e0c\u7801\u7684\u8868\u8fbe\u80fd\u529b\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u68c0\u7d22\u51c6\u786e\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u5c5e\u6027\u611f\u77e5\u7684\u54c8\u5e0c\u7801\uff0c\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u4e2d\u54c8\u5e0c\u6280\u672f\u7684\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2509.17552", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17552", "abs": "https://arxiv.org/abs/2509.17552", "authors": ["Tianle Zhang", "Wanlong Fang", "Jonathan Woo", "Paridhi Latawa", "Deepak A. Subramanian", "Alvin Chan"], "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning", "comment": "NIPS 2025", "summary": "The remarkable performance of Large Language Models (LLMs) can be enhanced\nwith test-time computation, which relies on external tools and even other deep\nlearning models. However, existing approaches for integrating non-text modality\nrepresentations into LLMs typically require additional costly supervised\ntraining, restricting on-the-fly adaptation to new domains and modalities. In\nthis work, we explore the feasibility of integrating representations from\nnon-text foundational models (FMs) into text-based LLMs in a training-free\nmanner. We propose In-Context Representation Learning (ICRL) as a\nproof-of-concept to allow LLMs to adaptively utilize non-text modality\nrepresentations with few-shot learning. Unlike traditional in-context learning,\nwhich incorporates text-label pairs, ICRL replaces text inputs with FM\nrepresentations, enabling the LLM to perform multi-modal inference without\nfine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,\ninvestigating three core research questions: (i) how to map FM representations\ninto LLMs in a training-free manner, (ii) what factors influence ICRL\nperformance, and (iii) what mechanisms underlie the effectiveness of ICRL. To\nthe best of our knowledge, ICRL is the first training-free framework for\nintegrating non-text modality representations into text-based LLMs, presenting\na promising direction for adaptable, multi-modal generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6ICRL\uff0c\u7528\u4e8e\u5c06\u975e\u6587\u672c\u6a21\u6001\u8868\u5f81\uff08\u5982\u5206\u5b50\u9886\u57df\uff09\u96c6\u6210\u5230\u57fa\u4e8e\u6587\u672c\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u5f81\u5b66\u4e60\u5b9e\u73b0\u5c11\u6837\u672c\u591a\u6a21\u6001\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6574\u5408\u975e\u6587\u672c\u6a21\u6001\u8868\u5f81\u65f6\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u76d1\u7763\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5bf9\u65b0\u9886\u57df\u548c\u6a21\u6001\u7684\u5373\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u56e0\u6b64\u63a2\u7d22\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u878d\u5408\u975e\u6587\u672c\u57fa\u7840\u6a21\u578b\u8868\u5f81\u7684\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51faIn-Context Representation Learning (ICRL)\uff0c\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u7528\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u7684\u8868\u5f81\u66ff\u4ee3\u6587\u672c\u8f93\u5165\uff0c\u4f7fLLM\u80fd\u591f\u6267\u884c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5b9e\u73b0\u81ea\u9002\u5e94\u5b66\u4e60\u3002", "result": "\u5728\u5206\u5b50\u9886\u57df\u7684\u591a\u4e2a\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86ICRL\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5206\u6790\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\u53ca\u5176\u5de5\u4f5c\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u662f\u9996\u4e2a\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6574\u5408\u975e\u6587\u672c\u6a21\u6001\u8868\u5f81\u7684\u6846\u67b6\u3002", "conclusion": "ICRL\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6574\u5408\u975e\u6587\u672c\u6a21\u6001\u7684\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u5728\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u591a\u6a21\u6001\u6cdb\u5316\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.17050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17050", "abs": "https://arxiv.org/abs/2509.17050", "authors": ["Junhao Jia", "Yunyou Liu", "Yifei Sun", "Huangwei Chen", "Feiwei Qin", "Changmiao Wang", "Yong Peng"], "title": "Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition", "comment": null, "summary": "Nonlinear manifolds are widespread in deep visual features, where Euclidean\ndistances often fail to capture true similarity. This limitation becomes\nparticularly severe in prototype-based interpretable fine-grained recognition,\nwhere subtle semantic distinctions are essential. To address this challenge, we\npropose a novel paradigm for prototype-based recognition that anchors\nsimilarity within the intrinsic geometry of deep features. Specifically, we\ndistill the latent manifold structure of each class into a diffusion space and\nintroduce a differentiable Nystr\\\"om interpolation, making the geometry\naccessible to both unseen samples and learnable prototypes. To ensure\nefficiency, we employ compact per-class landmark sets with periodic updates.\nThis design keeps the embedding aligned with the evolving backbone, enabling\nfast and scalable inference. Extensive experiments on the CUB-200-2011 and\nStanford Cars datasets show that our GeoProto framework produces prototypes\nfocusing on semantically aligned parts, significantly outperforming Euclidean\nprototype networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5f62\u51e0\u4f55\u7684\u539f\u578b\u8bc6\u522b\u65b0\u8303\u5f0fGeoProto\uff0c\u901a\u8fc7\u6269\u6563\u7a7a\u95f4\u548c\u53ef\u5faeNystr\u00f6m\u63d2\u503c\u6355\u6349\u6df1\u5ea6\u7279\u5f81\u7684\u5185\u5728\u7ed3\u6784\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u56fe\u50cf\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u7684\u539f\u578b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u6df1\u5ea6\u89c6\u89c9\u7279\u5f81\u4e2d\u7684\u975e\u7ebf\u6027\u6d41\u5f62\u7ed3\u6784\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4e2d\u8bed\u4e49\u5dee\u5f02\u7ec6\u5fae\uff0c\u5bfc\u81f4\u76f8\u4f3c\u6027\u5ea6\u91cf\u4e0d\u51c6\u786e\u3002", "method": "\u5c06\u6bcf\u7c7b\u7279\u5f81\u7684\u6f5c\u5728\u6d41\u5f62\u7ed3\u6784\u84b8\u998f\u5230\u6269\u6563\u7a7a\u95f4\uff0c\u5f15\u5165\u53ef\u5faeNystr\u00f6m\u63d2\u503c\u4f7f\u51e0\u4f55\u7ed3\u6784\u5bf9\u672a\u89c1\u6837\u672c\u548c\u53ef\u5b66\u4e60\u539f\u578b\u53ef\u7528\uff0c\u5e76\u91c7\u7528\u7d27\u51d1\u7684\u6bcf\u7c7b\u6807\u8bb0\u96c6\u4e0e\u5468\u671f\u66f4\u65b0\u4ee5\u4fdd\u8bc1\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728CUB-200-2011\u548cStanford Cars\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6b27\u6c0f\u539f\u578b\u7f51\u7edc\uff0c\u751f\u6210\u7684\u539f\u578b\u66f4\u805a\u7126\u4e8e\u8bed\u4e49\u5bf9\u9f50\u7684\u90e8\u5206\u3002", "conclusion": "GeoProto\u901a\u8fc7\u5229\u7528\u7279\u5f81\u7a7a\u95f4\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86\u539f\u578b\u7f51\u7edc\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.17559", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17559", "abs": "https://arxiv.org/abs/2509.17559", "authors": ["Yoko Kayano", "Saku Sugawara"], "title": "Specification-Aware Machine Translation and Evaluation for Purpose Alignment", "comment": null, "summary": "In professional settings, translation is guided by communicative goals and\nclient needs, often formalized as specifications. While existing evaluation\nframeworks acknowledge the importance of such specifications, these\nspecifications are often treated only implicitly in machine translation (MT)\nresearch. Drawing on translation studies, we provide a theoretical rationale\nfor why specifications matter in professional translation, as well as a\npractical guide to implementing specification-aware MT and evaluation. Building\non this foundation, we apply our framework to the translation of investor\nrelations texts from 33 publicly listed companies. In our experiment, we\ncompare five translation types, including official human translations and\nprompt-based outputs from large language models (LLMs), using expert error\nanalysis, user preference rankings, and an automatic metric. The results show\nthat LLM translations guided by specifications consistently outperformed\nofficial human translations in human evaluations, highlighting a gap between\nperceived and expected quality. These findings demonstrate that integrating\nspecifications into MT workflows, with human oversight, can improve translation\nquality in ways aligned with professional practice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u5f15\u5165\u4e13\u4e1a\u7ffb\u8bd1\u89c4\u8303\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u6295\u8d44\u8005\u5173\u7cfb\u6587\u672c\u7684\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u89c4\u8303\u6307\u5bfc\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d consistently \u4f18\u4e8e\u5b98\u65b9\u4eba\u5de5\u7ffb\u8bd1\uff0c\u8868\u660e\u7ed3\u5408\u89c4\u8303\u4e0e\u4eba\u5de5\u76d1\u7763\u53ef\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u5f80\u5f80\u5ffd\u89c6\u4e13\u4e1a\u573a\u666f\u4e2d\u7684\u7ffb\u8bd1\u89c4\u8303\uff0c\u800c\u5b9e\u9645\u4e13\u4e1a\u7ffb\u8bd1\u9700\u9075\u5faa\u5ba2\u6237\u76ee\u6807\u548c\u5177\u4f53\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u660e\u786e\u7684\u89c4\u8303\u611f\u77e5\u7ffb\u8bd1\u4e0e\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u7ffb\u8bd1\u7814\u7a76\u7406\u8bba\u6784\u5efa\u89c4\u8303\u611f\u77e5\u7684\u673a\u5668\u7ffb\u8bd1\u4e0e\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u572833\u5bb6\u4e0a\u5e02\u516c\u53f8\u7684\u6295\u8d44\u8005\u5173\u7cfb\u6587\u672c\u4e0a\u6bd4\u8f83\u4e94\u79cd\u7ffb\u8bd1\u7c7b\u578b\uff08\u5305\u62ec\u4eba\u5de5\u7ffb\u8bd1\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\uff09\uff0c\u91c7\u7528\u4e13\u5bb6\u9519\u8bef\u5206\u6790\u3001\u7528\u6237\u504f\u597d\u6392\u5e8f\u548c\u81ea\u52a8\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7531\u89c4\u8303\u6307\u5bfc\u7684LLM\u7ffb\u8bd1\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d consistently \u4f18\u4e8e\u5b98\u65b9\u4eba\u5de5\u7ffb\u8bd1\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u8d28\u91cf\u4e0e\u9884\u671f\u8d28\u91cf\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u5c06\u7ffb\u8bd1\u89c4\u8303\u878d\u5165\u673a\u5668\u7ffb\u8bd1\u6d41\u7a0b\u5e76\u8f85\u4ee5\u4eba\u5de5\u76d1\u7763\uff0c\u80fd\u591f\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4e13\u4e1a\u5b9e\u8df5\u9700\u6c42\u3002"}}
{"id": "2509.17729", "categories": ["cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.17729", "abs": "https://arxiv.org/abs/2509.17729", "authors": ["Siming Zheng", "Meifang Lan", "Tong Wang", "Yuanyuan Lin"], "title": "A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis", "comment": null, "summary": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u65b9\u6cd5\u548c\u6837\u672c\u5206\u5272\u6280\u672f\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u9a8c\u4e24\u6837\u672c\u95ee\u9898\u4e2d\u6761\u4ef6\u5206\u5e03\u7684\u76f8\u7b49\u6027\uff0c\u9002\u7528\u4e8e\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u7684\u8fc1\u79fb\u5b66\u4e60\u3002", "motivation": "\u5728\u534f\u53d8\u91cf\u504f\u79fb\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u68c0\u9a8c\u6761\u4ef6\u5206\u5e03\u7684\u76f8\u7b49\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5f3a\u5927\u7684\u6846\u67b6\u6765\u63d0\u5347\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5339\u914d\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5c06\u6761\u4ef6\u5206\u5e03\u68c0\u9a8c\u95ee\u9898\u8f6c\u5316\u4e3a\u65e0\u6761\u4ef6\u5206\u5e03\u68c0\u9a8c\u95ee\u9898\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u4e0e\u6837\u672c\u5206\u5272\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7279\u5b9a\u68c0\u9a8c\u65b9\u6cd5\uff1a\u57fa\u4e8e\u751f\u6210\u7f6e\u6362\u7684\u68c0\u9a8c\u548c\u57fa\u4e8e\u5206\u7c7b\u51c6\u786e\u7387\u7684\u68c0\u9a8c\u3002", "result": "\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u6761\u4ef6\u5206\u5e03\u68c0\u9a8c\u7684\u6700\u5c0f\u6700\u5927\u4e0b\u754c\uff0c\u5e76\u8bc1\u660e\u6240\u63d0\u65b9\u6cd5\u53ef\u8fbe\u5230\u8be5\u4e0b\u754c\uff1b\u540c\u65f6\u8bc1\u660e\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u6cd5\u7684\u68c0\u9a8c\u4e00\u81f4\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u751f\u6210\u5668\u7684\u6536\u655b\u901f\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u80fd\u6709\u6548\u68c0\u9a8c\u6761\u4ef6\u5206\u5e03\u7684\u76f8\u7b49\u6027\uff0c\u4e3a\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2509.17065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17065", "abs": "https://arxiv.org/abs/2509.17065", "authors": ["Yao Du", "Jiarong Guo", "Xiaomeng Li"], "title": "CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner", "comment": "Accepted by MICCAI 2025", "summary": "Echocardiography is a vital non-invasive modality for cardiac assessment,\nwith left ventricular ejection fraction (LVEF) serving as a key indicator of\nheart function. Existing LVEF estimation methods depend on large-scale\nannotated video datasets, which are costly and limit adaptability across\nvarious clinical settings. Recent vision-language models for echocardiography,\nsuch as EchoCLIP, apply image-to-text pretraining but fail to capture crucial\ntemporal dynamics and localized cardiac structures essential for accurate\ndiagnosis. To address these challenges, we propose CardiacCLIP, a video-based\nframework that enhances LVEF prediction through attention-based frame\naggregation and multi-resolution input scaling. Specifically, we introduce MFL\n(Multi Frame Learning), a novel attention-based mechanism for selectively\nfusing informative frames, and EchoZoom, a multi-scale feature extraction\nstrategy that refines spatial representations of cardiac structures. As a novel\nadaptation of CLIP models for few-shot echocardiogram video analysis, our\napproach significantly improves diagnostic accuracy, reducing MAE by 2.07 on\nthe EchoNet-Dynamic dataset under 1-shot setting. The code is available at\nhttps://github.com/xmed-lab/CardiacCLIP.", "AI": {"tldr": "\u63d0\u51faCardiacCLIP\uff0c\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5e27\u805a\u5408\u548c\u591a\u5206\u8fa8\u7387\u8f93\u5165\u63d0\u5347\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\uff08LVEF\uff09\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LVEF\u4f30\u8ba1\u7b97\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u52a8\u6001\u548c\u5c40\u90e8\u5fc3\u810f\u7ed3\u6784\u7684\u5efa\u6a21\uff1b\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u672a\u80fd\u6709\u6548\u6355\u6349\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u4e2d\u7684\u5173\u952e\u65f6\u5e8f\u4fe1\u606f\u3002", "method": "\u63d0\u51faCardiacCLIP\uff0c\u5305\u542bMFL\uff08\u591a\u5e27\u5b66\u4e60\uff09\u6ce8\u610f\u529b\u673a\u5236\u7528\u4e8e\u9009\u62e9\u6027\u878d\u5408\u5173\u952e\u5e27\uff0c\u4ee5\u53caEchoZoom\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u7b56\u7565\u4ee5\u589e\u5f3a\u5fc3\u810f\u7ed3\u6784\u7684\u7a7a\u95f4\u8868\u5f81\uff0c\u7ed3\u5408CLIP\u6a21\u578b\u5b9e\u73b0\u5c11\u6837\u672c\u89c6\u9891\u5206\u6790\u3002", "result": "\u5728EchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\uff0c1-shot\u8bbe\u7f6e\u4e0bMAE\u964d\u4f4e2.07\uff0c\u663e\u8457\u63d0\u5347LVEF\u9884\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "CardiacCLIP\u901a\u8fc7\u5efa\u6a21\u65f6\u5e8f\u52a8\u6001\u548c\u7cbe\u7ec6\u7a7a\u95f4\u7ed3\u6784\uff0c\u6709\u6548\u63d0\u5347\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u8d85\u58f0\u89c6\u9891\u4e2dLVEF\u4f30\u8ba1\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u4e34\u5e8a\u9002\u5e94\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17570", "abs": "https://arxiv.org/abs/2509.17570", "authors": ["Sergey Troshin", "Irina Saparina", "Antske Fokkens", "Vlad Niculae"], "title": "Asking a Language Model for Diverse Responses", "comment": "UncertaiNLP workshop, 2025", "summary": "Large language models increasingly rely on explicit reasoning chains and can\nproduce multiple plausible responses for a given context. We study the\ncandidate sampler that produces the set of plausible responses contrasting the\nancestral (parallel) sampling against two alternatives: enumeration, which asks\nthe model to produce $n$ candidates in one pass, and iterative sampling, which\nproposes candidates sequentially while conditioning on the currently generated\nresponse set. Under matched budgets, we compare these samplers on quality,\nlexical and computation flow diversity, and efficiency. Our empirical results\ndemonstrate that enumeration and iterative strategies result in higher\ndiversity at comparable quality. Our findings highlight the potential of simple\nnon-independent sampling strategies to improve response diversity without\nsacrificing generation quality.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u76f8\u540c\u9884\u7b97\u4e0b\uff0c\u679a\u4e3e\u548c\u8fed\u4ee3\u91c7\u6837\u7b56\u7565\u76f8\u6bd4\u7956\u5148\u91c7\u6837\u80fd\u4ea7\u751f\u66f4\u9ad8\u591a\u6837\u6027\u4e14\u8d28\u91cf\u76f8\u5f53\u7684\u54cd\u5e94\uff0c\u8868\u660e\u7b80\u5355\u7684\u975e\u72ec\u7acb\u91c7\u6837\u7b56\u7565\u53ef\u63d0\u5347\u54cd\u5e94\u591a\u6837\u6027\u800c\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u54cd\u5e94\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u63a2\u7d22\u4e0d\u540c\u7684\u5019\u9009\u91c7\u6837\u7b56\u7565\u3002", "method": "\u5bf9\u6bd4\u4e86\u4e09\u79cd\u91c7\u6837\u65b9\u6cd5\uff1a\u7956\u5148\uff08\u5e76\u884c\uff09\u91c7\u6837\u3001\u679a\u4e3e\u91c7\u6837\uff08\u4e00\u6b21\u6027\u751f\u6210n\u4e2a\u5019\u9009\uff09\u548c\u8fed\u4ee3\u91c7\u6837\uff08\u987a\u5e8f\u751f\u6210\u5e76\u57fa\u4e8e\u5df2\u751f\u6210\u96c6\u5408\u6761\u4ef6\u5316\uff09\u3002", "result": "\u5728\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u679a\u4e3e\u548c\u8fed\u4ee3\u91c7\u6837\u7b56\u7565\u6bd4\u7956\u5148\u91c7\u6837\u5177\u6709\u66f4\u9ad8\u7684\u8bcd\u6c47\u548c\u8ba1\u7b97\u6d41\u7a0b\u591a\u6837\u6027\u3002", "conclusion": "\u679a\u4e3e\u548c\u8fed\u4ee3\u7b49\u975e\u72ec\u7acb\u91c7\u6837\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u54cd\u5e94\u591a\u6837\u6027\uff0c\u662f\u6539\u8fdb\u5927\u6a21\u578b\u63a8\u7406\u8f93\u51fa\u591a\u6837\u6027\u7684\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2509.17730", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17730", "abs": "https://arxiv.org/abs/2509.17730", "authors": ["Bonan Zhang", "Zhongqi Chen", "Bowen Song", "Qinya Li", "Fan Wu", "Guihai Chen"], "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs", "comment": null, "summary": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u53ef\u9a8c\u8bc1\u7ed3\u679c\u4e0e\u6a21\u578b\u81ea\u8eab\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u65b9\u6cd5\u56e0\u4e8c\u503c\u53cd\u9988\u7a00\u758f\u4e14\u5956\u52b1\u7c97\u7c92\u5ea6\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\uff0c\u96be\u4ee5\u6355\u6349\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\u3002", "method": "\u5c06\u6a21\u578b\u5bf9\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4e0e\u53ef\u9a8c\u8bc1\u7ed3\u679c\u7ed3\u5408\uff0c\u6784\u5efa\u66f4\u4e30\u5bcc\u3001\u7ec6\u7c92\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u4e0d\u589e\u52a0\u663e\u8457\u8bad\u7ec3\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u7684\u4ee4\u724c\u6d88\u8017\uff0c\u5e76\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u589e\u5f3a\u5176\u4ed6\u5148\u8fdbRL\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5956\u52b1\u7a00\u758f\u548c\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2509.17074", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17074", "abs": "https://arxiv.org/abs/2509.17074", "authors": ["Qian Zhang", "Lin Zhang", "Xing Fang", "Mingxin Zhang", "Zhiyuan Wei", "Ran Song", "Wei Zhang"], "title": "Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models", "comment": "Submitted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2026", "summary": "Visual affordance learning is crucial for robots to understand and interact\neffectively with the physical world. Recent advances in this field attempt to\nleverage pre-trained knowledge of vision-language foundation models to learn\naffordance properties with limited training data, providing a novel paradigm\nfor visual affordance learning. However, these methods overlook the\nsignificance of maintaining feature alignment between visual images and\nlanguage descriptions for identifying affordance areas with textual guidance,\nand thus may lead to suboptimal results. In this paper, we present an\ninformative framework for text-guided affordance learning, which involves\ninformation-based constraints to achieve text-image alignment at feature level.\nSpecifically, we design an affordance mutual information constraint that helps\nlearn appropriate textual prompts and task-oriented visual features\nsimultaneously by maximizing the mutual information between the features of the\naffordance areas in the input images and the corresponding textual prompts. In\naddition, we propose an object-level information constraint that maximizes the\nmutual information between the visual features of a given object and the text\nfeatures of the category it belongs to. This enables the model to capture\nhigh-quality representations for the object, providing more reliable semantic\npriors for identifying affordance regions. Experimental results on the AGD20K\ndataset show that the proposed method outperforms existing approaches and\nachieves the new state-of-the-art in one-shot affordance learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u7ea6\u675f\u7684\u6587\u672c\u5f15\u5bfc\u4e0b\u7684\u89c6\u89c9\u53ef\u64cd\u4f5c\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u7279\u5f81\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u5b9e\u73b0\u56fe\u6587\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u53ef\u64cd\u4f5c\u533a\u57df\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5728\u7279\u5f81\u5c42\u9762\u4fdd\u6301\u56fe\u50cf\u4e0e\u6587\u672c\u63cf\u8ff0\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u6587\u672c\u5f15\u5bfc\u4e0b\u7684\u53ef\u64cd\u4f5c\u533a\u57df\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u7ea6\u675f\uff1a1\uff09\u53ef\u64cd\u4f5c\u6027\u4e92\u4fe1\u606f\u7ea6\u675f\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u6587\u672c\u63d0\u793a\u548c\u4efb\u52a1\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\uff1b2\uff09\u5bf9\u8c61\u7ea7\u4fe1\u606f\u7ea6\u675f\uff0c\u589e\u5f3a\u5bf9\u8c61\u4e0e\u5176\u7c7b\u522b\u6587\u672c\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728AGD20K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u4e00\u6b21\u6027\u53ef\u64cd\u4f5c\u6027\u5b66\u4e60\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7279\u5f81\u7ea7\u522b\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u673a\u5236\uff0c\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5f15\u5bfc\u4e0b\u89c6\u89c9\u53ef\u64cd\u4f5c\u6027\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.17628", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17628", "abs": "https://arxiv.org/abs/2509.17628", "authors": ["Yuzhen Lei", "Hongbin Xie", "Jiaxing Zhao", "Shuangxue Liu", "Xuan Song"], "title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents", "comment": "10 pages, 5 figures", "summary": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks\nwithin single domains. However, their reasoning and coordination capabilities\nin complex, multi-stage scenarios remain underexplored. Existing benchmarks\ntypically focus on isolated tasks or narrow domains, overlooking models'\nabilities for multi-stage collaboration and optimization without explicit\nexternal guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel\nbenchmark comprising 126696 domain-specific QA instances spanning scenarios in\nautomotive, pharmaceutical, electronics, and energy sectors. The dataset is\ncreated using a structured three-phase pipeline: dynamic sampling, iterative\nquestion-answer generation, and a multi-level quality assessment to ensure data\nquality. Tasks are further categorized into three difficulty levels according\nto stage coverage and complexity. With MSCoRe, we have conducted a\ncomprehensive evaluation of various state-of-the-art LLM agents. The commercial\nmodels performed best across all tasks and scenarios, but a notable gap in\nROUGE scores remains between simple and complex tasks. We also tested the\nmodels' robustness and found that their performance is negatively affected by\nnoisy data. MSCoRe provides a valuable new resource for the community to\nevaluate and improve multi-stage reasoning in LLM agents. The code and data are\navailable at https://github.com/D3E0-source/MSCoRE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MSCoRe\uff0c\u4e00\u4e2a\u5305\u542b126,696\u4e2a\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u5b9e\u4f8b\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9636\u6bb5\u590d\u6742\u573a\u666f\u4e2d\u7684\u63a8\u7406\u4e0e\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u5173\u6ce8\u5355\u4e00\u4efb\u52a1\u6216\u72ed\u7a84\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5728\u65e0\u5916\u90e8\u6307\u5bfc\u4e0b\u7684\u591a\u9636\u6bb5\u534f\u4f5c\u4e0e\u4f18\u5316\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u66f4\u590d\u6742\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a\u52a8\u6001\u91c7\u6837\u3001\u8fed\u4ee3\u95ee\u7b54\u751f\u6210\u548c\u591a\u5c42\u6b21\u8d28\u91cf\u8bc4\u4f30\uff0c\u6784\u5efa\u4e86\u6db5\u76d6\u6c7d\u8f66\u3001\u5236\u836f\u3001\u7535\u5b50\u548c\u80fd\u6e90\u9886\u57df\u7684\u591a\u9636\u6bb5\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5e76\u6309\u96be\u5ea6\u5206\u7ea7\u3002", "result": "\u5728\u591a\u4e2a\u5148\u8fdbLLM\u4ee3\u7406\u4e0a\u8fdb\u884c\u4e86\u7efc\u5408\u8bc4\u4f30\uff0c\u5546\u4e1a\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684ROUGE\u5206\u6570\u663e\u8457\u4f4e\u4e8e\u7b80\u5355\u4efb\u52a1\uff0c\u4e14\u566a\u58f0\u6570\u636e\u4f1a\u8d1f\u9762\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "MSCoRe\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9636\u6bb5\u63a8\u7406\u4e0e\u534f\u4f5c\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.17734", "categories": ["cs.LG", "cs.CE", "62M10, 68T05, 86A08, 62P12", "I.2.6; I.5.1; G.3; J.2"], "pdf": "https://arxiv.org/pdf/2509.17734", "abs": "https://arxiv.org/abs/2509.17734", "authors": ["Pablo Rodr\u00edguez-Bocca", "Guillermo Pereira", "Diego Kiedanski", "Soledad Collazo", "Sebasti\u00e1n Basterrech", "Gerardo Rubino"], "title": "An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures", "comment": "Manuscript to appear in the proceedings of IJCNN 2025, in the\n  workshop entitled \"AI for a Cooler Planet: Tackling Environmental Challenges\n  with Neural Networks.'' Total pages: 14. Total figures: 9 (containing a total\n  of 27 images). Total tables: 1", "summary": "In recent years, great progress has been made in the field of forecasting\nmeteorological variables. Recently, deep learning architectures have made a\nmajor breakthrough in forecasting the daily average temperature over a ten-day\nhorizon. However, advances in forecasting events related to the maximum\ntemperature over short horizons remain a challenge for the community. A problem\nthat is even more complex consists in making predictions of the maximum daily\ntemperatures in the short, medium, and long term. In this work, we focus on\nforecasting events related to the maximum daily temperature over medium-term\nperiods (90 days). Therefore, instead of addressing the problem from a\nmeteorological point of view, this article tackles it from a climatological\npoint of view. Due to the complexity of this problem, a common approach is to\nframe the study as a temporal classification problem with the classes: maximum\ntemperature \"above normal\", \"normal\" or \"below normal\". From a practical point\nof view, we created a large historical dataset (from 1981 to 2018) collecting\ninformation from weather stations located in South America. In addition, we\nalso integrated exogenous information from the Pacific, Atlantic, and Indian\nOcean basins. We applied the AutoGluonTS platform to solve the above-mentioned\nproblem. This AutoML tool shows competitive forecasting performance with\nrespect to large operational platforms dedicated to tackling this\nclimatological problem; but with a \"relatively\" low computational cost in terms\nof time and resources.", "AI": {"tldr": "\u672c\u6587\u5229\u7528AutoGluonTS\u5e73\u53f0\uff0c\u57fa\u4e8e\u5386\u53f2\u6c14\u8c61\u6570\u636e\u548c\u6d77\u6d0b\u5916\u6e90\u4fe1\u606f\uff0c\u5bf9\u5357\u7f8e\u6d32\u672a\u676590\u5929\u65e5\u6700\u9ad8\u6c14\u6e29\u5f02\u5e38\uff08\u504f\u9ad8\u3001\u6b63\u5e38\u3001\u504f\u4f4e\uff09\u8fdb\u884c\u4e2d\u671f\u6c14\u5019\u5206\u7c7b\u9884\u6d4b\uff0c\u5177\u6709\u8f83\u9ad8\u7cbe\u5ea6\u548c\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5c3d\u7ba1\u5728\u65e5\u5e73\u5747\u6c14\u6e29\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u6700\u5927\u65e5\u6c14\u6e29\u7684\u77ed\u671f\u81f3\u957f\u671f\u4e8b\u4ef6\u9884\u6d4b\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u957f\u671f\uff08\u598290\u5929\uff09\u5c3a\u5ea6\u4e0a\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4ece\u6c14\u8c61\u89d2\u5ea6\u51fa\u53d1\uff0c\u672c\u6587\u5219\u4ece\u6c14\u5019\u5b66\u89d2\u5ea6\u5207\u5165\uff0c\u65e8\u5728\u63d0\u5347\u6781\u7aef\u6e29\u5ea6\u4e8b\u4ef6\u7684\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u5c06\u6700\u5927\u65e5\u6c14\u6e29\u9884\u6d4b\u95ee\u9898\u8f6c\u5316\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\uff0c\u5206\u4e3a\u201c\u9ad8\u4e8e\u6b63\u5e38\u201d\u3001\u201c\u6b63\u5e38\u201d\u548c\u201c\u4f4e\u4e8e\u6b63\u5e38\u201d\u4e09\u7c7b\uff1b\u6784\u5efa1981\u20132018\u5e74\u5357\u7f8e\u6d32\u6c14\u8c61\u7ad9\u5386\u53f2\u6570\u636e\u96c6\uff0c\u5e76\u878d\u5408\u592a\u5e73\u6d0b\u3001\u5927\u897f\u6d0b\u548c\u5370\u5ea6\u6d0b\u7684\u5916\u6e90\u6c14\u5019\u4fe1\u606f\uff1b\u91c7\u7528AutoGluonTS\u8fd9\u4e00\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u5e73\u53f0\u8fdb\u884c\u5efa\u6a21\u4e0e\u9884\u6d4b\u3002", "result": "AutoGluonTS\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u53ef\u4e0e\u5927\u578b\u4e1a\u52a1\u5316\u9884\u6d4b\u5e73\u53f0\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0a\u5177\u6709\u76f8\u5bf9\u66f4\u4f4e\u7684\u6210\u672c\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e2d\u671f\u6c14\u5019\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86AutoML\u65b9\u6cd5\uff08\u7279\u522b\u662fAutoGluonTS\uff09\u5728\u4e2d\u671f\u6700\u5927\u65e5\u6c14\u6e29\u5206\u7c7b\u9884\u6d4b\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u4e3a\u6c14\u5019\u4e8b\u4ef6\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17078", "abs": "https://arxiv.org/abs/2509.17078", "authors": ["Kihyun Kim", "Michalis Lazarou", "Tania Stathaki"], "title": "Enhanced Detection of Tiny Objects in Aerial Images", "comment": null, "summary": "While one-stage detectors like YOLOv8 offer fast training speed, they often\nunder-perform on detecting small objects as a trade-off. This becomes even more\ncritical when detecting tiny objects in aerial imagery due to low-resolution\ntargets and cluttered backgrounds. To address this, we introduce three\nenhancement strategies -- input image resolution adjustment, data augmentation,\nand attention mechanisms -- that can be easily implemented on YOLOv8. We\ndemonstrate that image size enlargement and the proper use of augmentation can\nlead to enhancement. Additionally, we designed a Mixture of Orthogonal\nNeural-modules Network (MoonNet) pipeline which consists of attention-augmented\nCNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE\nBlock) and the Convolutional Block Attention Module (CBAM), were integrated\ninto the backbone of YOLOv8 with an increased number of channels, and the\nMoonNet backbone obtained improved detection accuracy compared to the original\nYOLOv8. MoonNet further proved its adaptability and potential by achieving\nstate-of-the-art performance on a tiny-object benchmark when integrated with\nthe YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoonNet\u7684\u6539\u8fdbYOLOv8\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u9ad8\u8f93\u5165\u5206\u8fa8\u7387\u3001\u6570\u636e\u589e\u5f3a\u548c\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u5347\u822a\u62cd\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "YOLOv8\u7b49\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u5728\u5904\u7406\u822a\u62cd\u56fe\u50cf\u4e2d\u7684\u5fae\u5c0f\u76ee\u6807\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4f4e\u5206\u8fa8\u7387\u76ee\u6807\u548c\u590d\u6742\u80cc\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u589e\u5f3a\u7b56\u7565\uff1a\u8c03\u6574\u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\u3001\u6570\u636e\u589e\u5f3a\u4ee5\u53ca\u5728YOLOv8\u4e3b\u5e72\u7f51\u7edc\u4e2d\u96c6\u6210SE Block\u548cCBAM\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u8bbe\u8ba1\u4e86MoonNet\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5927\u56fe\u50cf\u5c3a\u5bf8\u548c\u5408\u7406\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u53ef\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff1bMoonNet\u5728YOLOv8\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u4e0eYOLC\u7ed3\u5408\u65f6\u5728\u5fae\u5c0f\u76ee\u6807\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "MoonNet\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86YOLO\u7cfb\u5217\u5728\u822a\u62cd\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17641", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17641", "abs": "https://arxiv.org/abs/2509.17641", "authors": ["Hyunjong Ok", "Suho Yoo", "Hyeonjun Kim", "Jaeho Lee"], "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?", "comment": "Preprint", "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AuditoryBench++\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7eaf\u6587\u672c\u73af\u5883\u4e0b\u8bed\u8a00\u6a21\u578b\u542c\u89c9\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5e76\u5f15\u5165\u4e86AIR-CoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u6b8a\u6807\u8bb0\u548c\u77e5\u8bc6\u6ce8\u5165\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u548c\u6574\u5408\u542c\u89c9\u4fe1\u606f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u5bf9\u58f0\u97f3\u5c5e\u6027\uff08\u5982\u97f3\u9ad8\u3001\u54cd\u5ea6\u3001\u58f0\u6e90\u5173\u8054\uff09\u7684\u5e38\u8bc6\u6027\u63a8\u7406\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u63d0\u51faAuditoryBench++\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u542c\u89c9\u6bd4\u8f83\u5230\u60c5\u5883\u5316\u63a8\u7406\u7684\u4efb\u52a1\uff1b\u5e76\u8bbe\u8ba1AIR-CoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u5ea6\u68c0\u6d4b\u4e0e\u7279\u6b8a\u6807\u8bb0\u7ed3\u5408\u77e5\u8bc6\u6ce8\u5165\uff0c\u5728\u63a8\u7406\u65f6\u6a21\u62df\u542c\u89c9\u60f3\u8c61\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6700\u65b0\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528AIR-CoT\u7684\u65b9\u6cd5\u5728\u542c\u89c9\u77e5\u8bc6\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u666e\u904d\u4f18\u4e8e\u73b0\u6210\u6a21\u578b\u53ca\u7ecf\u8fc7\u542c\u89c9\u77e5\u8bc6\u589e\u5f3a\u7684\u6a21\u578b\u3002", "conclusion": "AIR-CoT\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u76f4\u63a5\u97f3\u9891\u8f93\u5165\u60c5\u51b5\u4e0b\u7684\u542c\u89c9\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0cAuditoryBench++\u4e3a\u8bc4\u4f30\u6b64\u7c7b\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\u3002"}}
{"id": "2509.17738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17738", "abs": "https://arxiv.org/abs/2509.17738", "authors": ["Ting Han", "Linara Adilova", "Henning Petzka", "Jens Kleesiek", "Michael Kamp"], "title": "Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking", "comment": "Preprint version", "summary": "Neural collapse, i.e., the emergence of highly symmetric, class-wise\nclustered representations, is frequently observed in deep networks and is often\nassumed to reflect or enable generalization. In parallel, flatness of the loss\nlandscape has been theoretically and empirically linked to generalization. Yet,\nthe causal role of either phenomenon remains unclear: Are they prerequisites\nfor generalization, or merely by-products of training dynamics? We disentangle\nthese questions using grokking, a training regime in which memorization\nprecedes generalization, allowing us to temporally separate generalization from\ntraining dynamics and we find that while both neural collapse and relative\nflatness emerge near the onset of generalization, only flatness consistently\npredicts it. Models encouraged to collapse or prevented from collapsing\ngeneralize equally well, whereas models regularized away from flat solutions\nexhibit delayed generalization. Furthermore, we show theoretically that neural\ncollapse implies relative flatness under classical assumptions, explaining\ntheir empirical co-occurrence. Our results support the view that relative\nflatness is a potentially necessary and more fundamental property for\ngeneralization, and demonstrate how grokking can serve as a powerful probe for\nisolating its geometric underpinnings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u574d\u7f29\u548c\u635f\u5931\u666f\u89c2\u5e73\u5766\u6027\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5229\u7528\u201cgrokking\u201d\u73b0\u8c61\u5c06\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u5206\u79bb\uff0c\u53d1\u73b0\u5e73\u5766\u6027\u59cb\u7ec8\u9884\u6d4b\u6cdb\u5316\uff0c\u800c\u795e\u7ecf\u574d\u7f29\u5219\u4e0d\u4e00\u5b9a\u3002", "motivation": "\u795e\u7ecf\u574d\u7f29\u548c\u635f\u5931\u666f\u89c2\u5e73\u5766\u6027\u5e38\u88ab\u8ba4\u4e3a\u4e0e\u6cdb\u5316\u6709\u5173\uff0c\u4f46\u4e8c\u8005\u662f\u5426\u662f\u6cdb\u5316\u7684\u524d\u63d0\u5c1a\u4e0d\u660e\u786e\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5176\u56e0\u679c\u4f5c\u7528\u3002", "method": "\u5229\u7528grokking\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u65f6\u95f4\u4e0a\u5206\u79bb\u6cdb\u5316\u4e0e\u8bad\u7ec3\u52a8\u6001\uff0c\u89c2\u5bdf\u795e\u7ecf\u574d\u7f29\u548c\u76f8\u5bf9\u5e73\u5766\u6027\u51fa\u73b0\u7684\u65f6\u95f4\u53ca\u5176\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4e8c\u8005\u5173\u7cfb\u3002", "result": "\u795e\u7ecf\u574d\u7f29\u548c\u76f8\u5bf9\u5e73\u5766\u6027\u5747\u5728\u6cdb\u5316\u5f00\u59cb\u65f6\u51fa\u73b0\uff0c\u4f46\u53ea\u6709\u76f8\u5bf9\u5e73\u5766\u6027\u80fd\u4e00\u81f4\u5730\u9884\u6d4b\u6cdb\u5316\uff1b\u9f13\u52b1\u6216\u6291\u5236\u574d\u7f29\u4e0d\u5f71\u54cd\u6cdb\u5316\uff0c\u800c\u8fdc\u79bb\u5e73\u5766\u89e3\u4f1a\u5ef6\u8fdf\u6cdb\u5316\uff1b\u7406\u8bba\u4e0a\u8bc1\u660e\u795e\u7ecf\u574d\u7f29\u8574\u542b\u76f8\u5bf9\u5e73\u5766\u6027\u3002", "conclusion": "\u76f8\u5bf9\u5e73\u5766\u6027\u53ef\u80fd\u662f\u6cdb\u5316\u7684\u4e00\u4e2a\u66f4\u57fa\u672c\u4e14\u5fc5\u8981\u7684\u51e0\u4f55\u5c5e\u6027\uff0c\u800c\u795e\u7ecf\u574d\u7f29\u5e76\u975e\u6cdb\u5316\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0cgrokking\u53ef\u6709\u6548\u7528\u4e8e\u63a2\u7a76\u6cdb\u5316\u7684\u51e0\u4f55\u57fa\u7840\u3002"}}
{"id": "2509.17079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17079", "abs": "https://arxiv.org/abs/2509.17079", "authors": ["Yuhong Feng", "Hongtao Chen", "Qi Zhang", "Jie Chen", "Zhaoxi He", "Mingzhe Liu", "Jianghai Liao"], "title": "A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion", "comment": "Submitted to ICASSP 2026", "summary": "Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in\nchallenging conditions. While recent Transformer-based methods excel at\ncapturing global context, their inherent lack of spatial inductive bias causes\nattention to spread to irrelevant background regions, compromising crowd\nlocalization precision. Furthermore, effectively bridging the gap between these\ndistinct modalities remains a major hurdle. To tackle this, we propose the Dual\nModulation Framework, comprising two modules: Spatially Modulated Attention\n(SMA), which improves crowd localization by using a learnable Spatial Decay\nMask to penalize attention between distant tokens and prevent focus from\nspreading to the background; and Adaptive Fusion Modulation (AFM), which\nimplements a dynamic gating mechanism to prioritize the most reliable modality\nfor adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting\ndatasets demonstrate the superior performance of our method compared to\nprevious works. Code available at\nhttps://github.com/Cht2924/RGBT-Crowd-Counting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8c03\u5236\u6846\u67b6\uff08Dual Modulation Framework\uff09\uff0c\u5305\u542b\u7a7a\u95f4\u8c03\u5236\u6ce8\u610f\u529b\uff08SMA\uff09\u548c\u81ea\u9002\u5e94\u878d\u5408\u8c03\u5236\uff08AFM\uff09\uff0c\u4ee5\u63d0\u5347RGB-\u70ed\u6210\u50cf\u4eba\u7fa4\u8ba1\u6570\u7684\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5728RGB-T\u4eba\u7fa4\u8ba1\u6570\u4e2d\u56e0\u7f3a\u4e4f\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\u800c\u5bfc\u81f4\u6ce8\u610f\u529b\u6269\u6563\u81f3\u80cc\u666f\u533a\u57df\uff0c\u4e14\u6a21\u6001\u878d\u5408\u6548\u679c\u4e0d\u4f73\uff0c\u5f71\u54cd\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "method": "\u8bbe\u8ba1\u4e86SMA\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u8870\u51cf\u63a9\u7801\u6291\u5236\u8fdc\u8ddd\u79bbtoken\u95f4\u7684\u6ce8\u610f\u529b\uff0c\u589e\u5f3a\u7a7a\u95f4\u805a\u7126\uff1b\u540c\u65f6\u63d0\u51faAFM\u6a21\u5757\uff0c\u91c7\u7528\u52a8\u6001\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u9009\u62e9\u66f4\u53ef\u9760\u7684\u6a21\u6001\u8fdb\u884c\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2aRGB-T\u4eba\u7fa4\u8ba1\u6570\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u6570\u7cbe\u5ea6\u548c\u5b9a\u4f4d\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u80cc\u666f\u548c\u6a21\u6001\u5dee\u5f02\u5927\u7684\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u8c03\u5236\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86RGB-T\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u805a\u7126\u80fd\u529b\u548c\u8de8\u6a21\u6001\u878d\u5408\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u6570\u4e0e\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.17667", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17667", "abs": "https://arxiv.org/abs/2509.17667", "authors": ["Arafat Ahsan", "Vandan Mujadia", "Pruthwik Mishra", "Yash Bhaskar", "Dipti Misra Sharma"], "title": "Crosslingual Optimized Metric for Translation Assessment of Indian Languages", "comment": "Under review", "summary": "Automatic evaluation of translation remains a challenging task owing to the\northographic, morphological, syntactic and semantic richness and divergence\nobserved across languages. String-based metrics such as BLEU have previously\nbeen extensively used for automatic evaluation tasks, but their limitations are\nnow increasingly recognized. Although learned neural metrics have helped\nmitigate some of the limitations of string-based approaches, they remain\nconstrained by a paucity of gold evaluation data in most languages beyond the\nusual high-resource pairs. In this present work we address some of these gaps.\nWe create a large human evaluation ratings dataset for 13 Indian languages\ncovering 21 translation directions and then train a neural translation\nevaluation metric named Cross-lingual Optimized Metric for Translation\nAssessment of Indian Languages (COMTAIL) on this dataset. The best performing\nmetric variants show significant performance gains over previous\nstate-of-the-art when adjudging translation pairs with at least one Indian\nlanguage. Furthermore, we conduct a series of ablation studies to highlight the\nsensitivities of such a metric to changes in domain, translation quality, and\nlanguage groupings. We release both the COMTAIL dataset and the accompanying\nmetric models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u8bed\u8a00\u4f18\u5316\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807COMTAIL\uff0c\u57fa\u4e8e\u5305\u542b13\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5927\u578b\u4eba\u5de5\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5728\u81f3\u5c11\u5305\u542b\u4e00\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u7ffb\u8bd1\u5bf9\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u7ffb\u8bd1\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982BLEU\uff09\u5728\u5904\u7406\u8bed\u8a00\u95f4\u4e30\u5bcc\u7684\u5f62\u6001\u3001\u53e5\u6cd5\u548c\u8bed\u4e49\u5dee\u5f02\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u795e\u7ecf\u8bc4\u4f30\u6a21\u578b\u53d7\u9650\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\u5bf9\u4e4b\u5916\u7684\u9ec4\u91d1\u8bc4\u4f30\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d613\u79cd\u5370\u5ea6\u8bed\u8a00\u300121\u4e2a\u7ffb\u8bd1\u65b9\u5411\u7684\u5927\u89c4\u6a21\u4eba\u5de5\u8bc4\u4f30\u8bc4\u5206\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e86\u540d\u4e3aCOMTAIL\u7684\u795e\u7ecf\u7ffb\u8bd1\u8bc4\u4f30\u6a21\u578b\u3002\u540c\u65f6\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u9886\u57df\u3001\u7ffb\u8bd1\u8d28\u91cf\u548c\u8bed\u8a00\u7ec4\u5408\u4e0b\u7684\u8868\u73b0\u654f\u611f\u6027\u3002", "result": "COMTAIL\u5728\u6d89\u53ca\u5370\u5ea6\u8bed\u8a00\u7684\u7ffb\u8bd1\u8bc4\u4f30\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u9886\u57df\u3001\u7ffb\u8bd1\u8d28\u91cf\u53ca\u8bed\u8a00\u5206\u7ec4\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "conclusion": "COMTAIL\u6709\u6548\u63d0\u5347\u4e86\u5370\u5ea6\u8bed\u8a00\u7ffb\u8bd1\u7684\u81ea\u52a8\u8bc4\u4f30\u6027\u80fd\uff0c\u6240\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6a21\u578b\u6709\u52a9\u4e8e\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bc4\u4f30\u7814\u7a76\u3002"}}
{"id": "2509.17752", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.17752", "abs": "https://arxiv.org/abs/2509.17752", "authors": ["Miao Li", "Phuc Nguyen", "Christopher Tam", "Alexandra Morgan", "Kenneth Ge", "Rahul Bansal", "Linzi Yu", "Rima Arnaout", "Ramy Arnaout"], "title": "GEM-T: Generative Tabular Data via Fitting Moments", "comment": "18 pages, 4 figures", "summary": "Tabular data dominates data science but poses challenges for generative\nmodels, especially when the data is limited or sensitive. We present a novel\napproach to generating synthetic tabular data based on the principle of maximum\nentropy -- MaxEnt -- called GEM-T, for ``generative entropy maximization for\ntables.'' GEM-T directly captures nth-order interactions -- pairwise,\nthird-order, etc. -- among columns of training data. In extensive testing,\nGEM-T matches or exceeds deep neural network approaches previously regarded as\nstate-of-the-art in 23 of 34 publicly available datasets representing diverse\nsubject domains (68\\%). Notably, GEM-T involves orders-of-magnitude fewer\ntrainable parameters, demonstrating that much of the information in real-world\ndata resides in low-dimensional, potentially human-interpretable correlations,\nprovided that the input data is appropriately transformed first. Furthermore,\nMaxEnt better handles heterogeneous data types (continuous vs. discrete vs.\ncategorical), lack of local structure, and other features of tabular data.\nGEM-T represents a promising direction for light-weight high-performance\ngenerative models for structured data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u71b5\u539f\u7406\u7684\u65b0\u578b\u8868\u683c\u6570\u636e\u751f\u6210\u65b9\u6cd5GEM-T\uff0c\u80fd\u591f\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u6548\u679c\uff0c\u4e14\u53c2\u6570\u91cf\u5c11\u5f97\u591a\u3002", "motivation": "\u89e3\u51b3\u5728\u6570\u636e\u6709\u9650\u6216\u654f\u611f\u65f6\u751f\u6210\u8868\u683c\u6570\u636e\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u73b0\u6709\u751f\u6210\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u7c7b\u578b\u548c\u7f3a\u4e4f\u5c40\u90e8\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6700\u5927\u71b5\u539f\u7406\uff08MaxEnt\uff09\uff0c\u76f4\u63a5\u6355\u6349\u8bad\u7ec3\u6570\u636e\u4e2d\u5217\u4e4b\u95f4\u7684\u9ad8\u9636\u4ea4\u4e92\uff08\u5982\u4e8c\u9636\u3001\u4e09\u9636\u7b49\uff09\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u8f93\u5165\u6570\u636e\u53d8\u6362\u5b9e\u73b0\u9ad8\u6548\u5efa\u6a21\u3002", "result": "\u572834\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e2d\uff0cGEM-T\u572823\u4e2a\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u540c\u65f6\u6a21\u578b\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u5904\u7406\u8fde\u7eed\u3001\u79bb\u6563\u548c\u5206\u7c7b\u53d8\u91cf\u7b49\u5f02\u6784\u6570\u636e\u7c7b\u578b\u3002", "conclusion": "GEM-T\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u7684\u8f7b\u91cf\u7ea7\u9ad8\u6027\u80fd\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u8868\u660e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7684\u5927\u90e8\u5206\u4fe1\u606f\u5b58\u5728\u4e8e\u4f4e\u7ef4\u3001\u6f5c\u5728\u53ef\u89e3\u91ca\u7684\u76f8\u5173\u6027\u4e2d\u3002"}}
{"id": "2509.17083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17083", "abs": "https://arxiv.org/abs/2509.17083", "authors": ["Zipeng Wang", "Dan Xu"], "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis", "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative\nto NeRF-based approaches, enabling real-time, high-quality novel view synthesis\nthrough explicit, optimizable 3D Gaussians. However, 3DGS suffers from\nsignificant memory overhead due to its reliance on per-Gaussian parameters to\nmodel view-dependent effects and anisotropic shapes. While recent works propose\ncompressing 3DGS with neural fields, these methods struggle to capture\nhigh-frequency spatial variations in Gaussian properties, leading to degraded\nreconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a\nnovel scene representation that combines the strengths of explicit Gaussians\nand neural fields. HyRF decomposes the scene into (1) a compact set of explicit\nGaussians storing only critical high-frequency parameters and (2) grid-based\nneural fields that predict remaining properties. To enhance representational\ncapacity, we introduce a decoupled neural field architecture, separately\nmodeling geometry (scale, opacity, rotation) and view-dependent color.\nAdditionally, we propose a hybrid rendering scheme that composites Gaussian\nsplatting with a neural field-predicted background, addressing limitations in\ndistant scene representation. Experiments demonstrate that HyRF achieves\nstate-of-the-art rendering quality while reducing model size by over 20 times\ncompared to 3DGS and maintaining real-time performance. Our project page is\navailable at https://wzpscott.github.io/hyrf/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHybrid Radiance Fields (HyRF)\u7684\u65b0\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u663e\u5f0f\u9ad8\u65af\u5206\u5e03\u548c\u795e\u7ecf\u573a\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u5b58\u5728\u663e\u8457\u7684\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u9891\u7a7a\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u7ec6\u8282\u91cd\u5efa\u6548\u679c\u4e0b\u964d\u3002", "method": "HyRF\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u4ec5\u5b58\u50a8\u5173\u952e\u9ad8\u9891\u53c2\u6570\u7684\u7d27\u51d1\u663e\u5f0f\u9ad8\u65af\u96c6\u5408\u548c\u9884\u6d4b\u5176\u4f59\u5c5e\u6027\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u795e\u7ecf\u573a\uff0c\u5e76\u5f15\u5165\u89e3\u8026\u7684\u795e\u7ecf\u573a\u67b6\u6784\u5206\u522b\u5efa\u6a21\u51e0\u4f55\uff08\u5c3a\u5ea6\u3001\u4e0d\u900f\u660e\u5ea6\u3001\u65cb\u8f6c\uff09\u548c\u89c6\u56fe\u76f8\u5173\u989c\u8272\uff0c\u540c\u65f6\u63d0\u51fa\u6df7\u5408\u6e32\u67d3\u65b9\u6848\u4ee5\u6539\u5584\u8fdc\u8ddd\u79bb\u573a\u666f\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHyRF\u5728\u6e32\u67d3\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6a21\u578b\u5927\u5c0f\u6bd43DGS\u51cf\u5c1120\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "HyRF\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u9ad8\u65af\u548c\u795e\u7ecf\u573a\u7684\u4f18\u70b9\uff0c\u6709\u6548\u89e3\u51b3\u4e863DGS\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5728\u9ad8\u8d28\u91cf\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u7387\u4e0e\u6027\u80fd\u5e73\u8861\u3002"}}
{"id": "2509.17669", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17669", "abs": "https://arxiv.org/abs/2509.17669", "authors": ["Yan Zhuang", "Yuan Sun"], "title": "PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation", "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), Controllable Text\nGeneration (CTG) has become a critical technology for enhancing system\nreliability and user experience. Addressing the limitations of traditional\nmethods, this paper proposes the PG-CE (Progressive Generation with Constraint\nEnhancement) approach, which decomposes CTG tasks into three steps: type\nprediction, constraint construction, and guided generation. This method employs\nconstraint generation models to dynamically build multi-dimensional constraints\nincluding tone, expression style, and thematic focus to guide output.\nExperiments demonstrate that PG-CE significantly improves generation quality\nacross multiple scenarios while maintaining text controllability, thematic\nrelevance, and response practicality. The research developed a dataset\ncontaining 90,000 constraint-text pairs (with an 8:2 ratio between daily and\nother topics), effectively reflecting real-world application requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPG-CE\uff08\u6e10\u8fdb\u751f\u6210\u4e0e\u7ea6\u675f\u589e\u5f3a\uff09\u7684\u53ef\u63a7\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u578b\u9884\u6d4b\u3001\u7ea6\u675f\u6784\u5efa\u548c\u5f15\u5bfc\u751f\u6210\u4e09\u6b65\u5206\u89e3\u4efb\u52a1\uff0c\u5229\u7528\u52a8\u6001\u751f\u6210\u7684\u591a\u7ef4\u7ea6\u675f\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u53ef\u63a7\u6587\u672c\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u517c\u987e\u751f\u6210\u6587\u672c\u7684\u53ef\u63a7\u6027\u3001\u4e3b\u9898\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u5c06\u53ef\u63a7\u6587\u672c\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u6b65\u9aa4\uff1a\u7c7b\u578b\u9884\u6d4b\u3001\u7ea6\u675f\u6784\u5efa\u548c\u5f15\u5bfc\u751f\u6210\uff1b\u91c7\u7528\u7ea6\u675f\u751f\u6210\u6a21\u578b\u52a8\u6001\u6784\u5efa\u5305\u62ec\u8bed\u6c14\u3001\u8868\u8fbe\u98ce\u683c\u548c\u4e3b\u9898\u7126\u70b9\u5728\u5185\u7684\u591a\u7ef4\u7ea6\u675f\uff0c\u5e76\u7528\u4e8e\u6307\u5bfc\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPG-CE\u5728\u591a\u79cd\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6587\u672c\u53ef\u63a7\u6027\u3001\u4e3b\u9898\u76f8\u5173\u6027\u548c\u54cd\u5e94\u5b9e\u7528\u6027\uff1b\u7814\u7a76\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b9\u4e07\u5bf9\u7ea6\u675f-\u6587\u672c\u7684\u6570\u636e\u96c6\uff08\u65e5\u5e38\u4e0e\u5176\u4ed6\u8bdd\u9898\u6bd4\u4f8b\u4e3a8:2\uff09\uff0c\u53cd\u6620\u771f\u5b9e\u5e94\u7528\u573a\u666f\u3002", "conclusion": "PG-CE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u53ef\u63a7\u6587\u672c\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u5f15\u5165\u52a8\u6001\u591a\u7ef4\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u53ef\u63a7\u6027\u7684\u6587\u672c\u751f\u6210\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.17084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17084", "abs": "https://arxiv.org/abs/2509.17084", "authors": ["Binhua Huang", "Nan Wang", "Arjun Parakash", "Soumyabrata Dev"], "title": "MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors", "comment": "6 pages, 3 figures", "summary": "Video action recognition is a fundamental task in computer vision, but\nstate-of-the-art models are often computationally expensive and rely on\nextensive video pre-training. In parallel, large-scale vision-language models\nlike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot\ncapabilities on static images, while motion vectors (MV) provide highly\nefficient temporal information directly from compressed video streams. To\nsynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple\nyet powerful two-stream late fusion framework for efficient video recognition.\nOur approach combines features from a frozen CLIP image encoder with features\nfrom a lightweight, supervised network trained on raw MV. During fusion, both\nbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is\ntrained, ensuring extreme efficiency. Through comprehensive experiments on the\nUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,\nsignificantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)\nbaselines. Our work provides a new, highly efficient baseline for video\nunderstanding that effectively bridges the gap between large static models and\ndynamic, low-cost motion cues. Our code and models are available at\nhttps://github.com/microa/MoCLIP-Lite.", "AI": {"tldr": "\u63d0\u51faMoCLIP-Lite\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u6d41 late fusion \u6846\u67b6\uff0c\u7ed3\u5408\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u8fd0\u52a8\u5411\u91cf\u7f51\u7edc\uff0c\u4ec5\u8bad\u7ec3\u5c0f\u578bMLP\u5934\uff0c\u5728UCF101\u4e0a\u8fbe\u523089.2% Top-1\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u5927\u91cf\u9884\u8bad\u7ec3\uff0c\u800cCLIP\u5728\u9759\u6001\u56fe\u50cf\u4e0a\u6709\u5f3a\u96f6\u6837\u672c\u80fd\u529b\uff0c\u8fd0\u52a8\u5411\u91cf\uff08MV\uff09\u80fd\u9ad8\u6548\u63d0\u4f9b\u538b\u7f29\u89c6\u9891\u4e2d\u7684\u65f6\u5e8f\u4fe1\u606f\uff0c\u56e0\u6b64\u5e0c\u671b\u878d\u5408\u4e24\u8005\u4f18\u52bf\u5b9e\u73b0\u9ad8\u6548\u8bc6\u522b\u3002", "method": "\u91c7\u7528\u53cc\u6d41late fusion\u6846\u67b6\uff1a\u4e00\u8def\u4f7f\u7528\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u53e6\u4e00\u8def\u7528\u8f7b\u91cf\u76d1\u7763\u7f51\u7edc\u5904\u7406\u539f\u59cb\u8fd0\u52a8\u5411\u91cf\uff1b\u878d\u5408\u65f6\u4e24\u4e2a\u4e3b\u5e72\u5747\u51bb\u7ed3\uff0c\u4ec5\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684MLP\u5934\u90e8\u3002", "result": "\u5728UCF101\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8689.2%\u7684Top-1\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672cCLIP\uff0865.0%\uff09\u548c\u4ec5\u7528MV\uff0866.5%\uff09\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MoCLIP-Lite\u6709\u6548\u7ed3\u5408\u4e86\u5927\u89c4\u6a21\u9759\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4f4e\u6210\u672c\u52a8\u6001\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5728\u4fdd\u6301\u6781\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u89c6\u9891\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.17671", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17671", "abs": "https://arxiv.org/abs/2509.17671", "authors": ["Selva Ta\u015f", "Mahmut El Huseyni", "\u00d6zay Ezerceli", "Reyhan Bayraktar", "Fatma Bet\u00fcl Terzio\u011flu"], "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications", "comment": null, "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Turk-LettuceDetect\uff0c\u9996\u4e2a\u9488\u5bf9\u571f\u8033\u5176\u8bedRAG\u5e94\u7528\u7684\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578b\u5957\u4ef6\uff0c\u57fa\u4e8eLettuceDetect\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u68c0\u6d4b\u89c6\u4e3a\u8bcd\u5143\u7ea7\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5728\u673a\u5668\u7ffb\u8bd1\u7684RAGTruth\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e86\u4e09\u79cd\u7f16\u7801\u5668\u67b6\u6784\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u666e\u904d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5c24\u5176\u5728\u571f\u8033\u5176\u8bed\u7b49\u5f62\u6001\u590d\u6742\u3001\u8d44\u6e90\u7a00\u7f3a\u7684\u8bed\u8a00\u4e2d\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4ecd\u96be\u4ee5\u6709\u6548\u6291\u5236\u5e7b\u89c9\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u6b64\u7c7b\u8bed\u8a00\u7684\u68c0\u6d4b\u5de5\u5177\u3002", "method": "\u91c7\u7528LettuceDetect\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u68c0\u6d4b\u5b9a\u4e49\u4e3a\u8bcd\u5143\u7ea7\u5206\u7c7b\u4efb\u52a1\uff0c\u5fae\u8c03ModernBERT\u3001TurkEmbed4STS\u548cEuroBERT\u4e09\u79cd\u7f16\u7801\u5668\u67b6\u6784\uff0c\u4f7f\u7528\u673a\u5668\u7ffb\u8bd1\u7684RAGTruth\u6570\u636e\u96c6\uff0817,790\u4e2a\u6837\u672c\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ModernBERT\u6a21\u578b\u5728\u5b8c\u6574\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u52300.7266\u7684F1\u5206\u6570\uff0c\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u652f\u6301\u6700\u957f8192\u4e2a\u8bcd\u5143\u7684\u4e0a\u4e0b\u6587\u4e14\u8ba1\u7b97\u9ad8\u6548\uff1b\u76f8\u6bd4\u4e3b\u6d41\u5927\u6a21\u578b\u9ad8\u53ec\u56de\u4f46\u4f4e\u7cbe\u5ea6\u7684\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u66f4\u5177\u5e73\u8861\u6027\u3002", "conclusion": "Turk-LettuceDetect\u586b\u8865\u4e86\u591a\u8bed\u8a00NLP\u4e2d\u571f\u8033\u5176\u8bed\u5e7b\u89c9\u68c0\u6d4b\u7684\u7a7a\u767d\uff0c\u4e3a\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00AI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6240\u53d1\u5e03\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2509.17784", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17784", "abs": "https://arxiv.org/abs/2509.17784", "authors": ["Jin Li", "Shoujin Wang", "Qi Zhang", "Feng Liu", "Tongliang Liu", "Longbing Cao", "Shui Yu", "Fang Chen"], "title": "Revealing Multimodal Causality with Large Language Models", "comment": "Accepted at NeurIPS 2025", "summary": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLLM-CD\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u6a21\u6001\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\uff0c\u901a\u8fc7\u5bf9\u6bd4\u56e0\u5b50\u53d1\u73b0\u3001\u7edf\u8ba1\u56e0\u679c\u7ed3\u6784\u63a8\u65ad\u548c\u8fed\u4ee3\u7684\u591a\u6a21\u6001\u53cd\u4e8b\u5b9e\u63a8\u7406\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u7ed3\u6784\u6a21\u7cca\u6027\u4e0a\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u591a\u6a21\u6001\u56e0\u679c\u53d1\u73b0\u65f6\u96be\u4ee5\u5145\u5206\u63a2\u7d22\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e14\u5728\u7eaf\u89c2\u6d4b\u6570\u636e\u4e0b\u9762\u5bf9\u7ed3\u6784\u6a21\u7cca\u6027\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u63d0\u5347\u591a\u6a21\u6001\u56e0\u679c\u53d1\u73b0\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faMLLM-CD\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u5bf9\u6bd4\u6837\u672c\u5bf9\u63a2\u7d22\u4ea4\u4e92\u7684\u5bf9\u6bd4\u56e0\u5b50\u53d1\u73b0\u6a21\u5757\u3001\u7528\u4e8e\u63a8\u65ad\u56e0\u5b50\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u7edf\u8ba1\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u6a21\u5757\uff0c\u4ee5\u53ca\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u6a21\u5757\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMLLM-CD\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u771f\u5b9e\u7684\u591a\u6a21\u6001\u56e0\u5b50\u53ca\u5176\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MLLM-CD\u4e3a\u4ece\u591a\u6a21\u6001\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u5728\u590d\u6742\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.17086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17086", "abs": "https://arxiv.org/abs/2509.17086", "authors": ["Jie Chen", "Yuhong Feng", "Tao Dai", "Mingzhe Liu", "Hongtao Chen", "Zhaoxi He", "Jiancong Bai"], "title": "SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion Networks", "comment": "Submitted to ICASSP 2026", "summary": "Detecting and localizing poultry is essential for advancing smart poultry\nfarming. Despite the progress of detection-centric methods, challenges persist\nin free-range settings due to multiscale targets, obstructions, and complex or\ndynamic backgrounds. To tackle these challenges, we introduce an innovative\npoultry detection approach named SFN-YOLO that utilizes scale-aware fusion.\nThis approach combines detailed local features with broader global context to\nimprove detection in intricate environments. Furthermore, we have developed a\nnew expansive dataset (M-SCOPE) tailored for varied free-range conditions.\nComprehensive experiments demonstrate our model achieves an mAP of 80.7% with\njust 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining\nstrong generalization capability across different domains. The efficient and\nreal-time detection capabilities of SFN-YOLO support automated smart poultry\nfarming. The code and dataset can be accessed at\nhttps://github.com/chenjessiee/SFN-YOLO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSFN-YOLO\u7684\u65b0\u578b\u5bb6\u79bd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u7b56\u7565\uff0c\u5728\u590d\u6742\u653e\u517b\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u68c0\u6d4b\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u591a\u6837\u5316\u653e\u517b\u6761\u4ef6\u7684\u65b0\u6570\u636e\u96c6M-SCOPE\u3002", "motivation": "\u5728\u653e\u517b\u73af\u5883\u4e0b\uff0c\u7531\u4e8e\u76ee\u6807\u591a\u5c3a\u5ea6\u3001\u906e\u6321\u548c\u590d\u6742\u6216\u52a8\u6001\u80cc\u666f\uff0c\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u5bb6\u79bd\u68c0\u6d4b\u65b9\u6848\u4ee5\u63a8\u52a8\u667a\u6167\u517b\u79bd\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faSFN-YOLO\u6a21\u578b\uff0c\u91c7\u7528\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u673a\u5236\uff0c\u878d\u5408\u5c40\u90e8\u7ec6\u8282\u7279\u5f81\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6M-SCOPE\u7528\u4e8e\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "SFN-YOLO\u5728\u4ec5720\u4e07\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8fbe\u523080.7%\u7684mAP\uff0c\u6bd4\u57fa\u51c6\u6a21\u578b\u51cf\u5c1135.1%\u7684\u53c2\u6570\u91cf\uff0c\u4e14\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SFN-YOLO\u901a\u8fc7\u8f7b\u91cf\u5316\u8bbe\u8ba1\u548c\u6709\u6548\u7684\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u5bb6\u79bd\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u667a\u6167\u517b\u79bd\u573a\u666f\uff0c\u5177\u5907\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2509.17680", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17680", "abs": "https://arxiv.org/abs/2509.17680", "authors": ["Shenghao Ye", "Yu Guo", "Dong Jin", "Yikai Shen", "Yunpeng Hou", "Shuangwu Chen", "Jian Yang", "Xiaofeng Jiang"], "title": "When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables", "comment": "23 pages, 24 figures", "summary": "Table question answering (TableQA) is a fundamental task in natural language\nprocessing (NLP). The strong reasoning capabilities of large language models\n(LLMs) have brought significant advances in this field. However, as real-world\napplications involve increasingly complex questions and larger tables,\nsubstantial noisy data is introduced, which severely degrades reasoning\nperformance. To address this challenge, we focus on improving two core\ncapabilities: Relevance Filtering, which identifies and retains information\ntruly relevant to reasoning, and Table Pruning, which reduces table size while\npreserving essential content. Based on these principles, we propose EnoTab, a\ndual denoising framework for complex questions and large-scale tables.\nSpecifically, we first perform Evidence-based Question Denoising by decomposing\nthe question into minimal semantic units and filtering out those irrelevant to\nanswer reasoning based on consistency and usability criteria. Then, we propose\nEvidence Tree-guided Table Denoising, which constructs an explicit and\ntransparent table pruning path to remove irrelevant data step by step. At each\npruning step, we observe the intermediate state of the table and apply a\npost-order node rollback mechanism to handle abnormal table states, ultimately\nproducing a highly reliable sub-table for final answer reasoning. Finally,\nextensive experiments show that EnoTab achieves outstanding performance on\nTableQA tasks with complex questions and large-scale tables, confirming its\neffectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEnoTab\u7684\u53cc\u964d\u566a\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u590d\u6742\u95ee\u9898\u548c\u5927\u89c4\u6a21\u8868\u683c\u4e0b\u7684\u8868\u683c\u95ee\u7b54\u6027\u80fd\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bc1\u636e\u7684\u95ee\u9898\u964d\u566a\u548c\u8bc1\u636e\u6811\u5f15\u5bfc\u7684\u8868\u683c\u964d\u566a\u6765\u589e\u5f3a\u76f8\u5173\u6027\u8fc7\u6ee4\u4e0e\u8868\u526a\u679d\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u5e94\u7528\u4e2d\u95ee\u9898\u548c\u8868\u683c\u65e5\u76ca\u590d\u6742\uff0c\u566a\u58f0\u6570\u636e\u4e25\u91cd\u5f71\u54cd\u4e86\u5927\u6a21\u578b\u5728\u8868\u683c\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u6a21\u578b\u7684\u76f8\u5173\u4fe1\u606f\u7b5b\u9009\u548c\u8868\u683c\u538b\u7f29\u80fd\u529b\u3002", "method": "\u63d0\u51faEnoTab\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff1a1\uff09\u57fa\u4e8e\u8bc1\u636e\u7684\u95ee\u9898\u964d\u566a\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u8bed\u4e49\u5355\u5143\u5e76\u4f9d\u636e\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u8fc7\u6ee4\u65e0\u5173\u90e8\u5206\uff1b2\uff09\u8bc1\u636e\u6811\u5f15\u5bfc\u7684\u8868\u683c\u964d\u566a\uff0c\u9010\u6b65\u6784\u5efa\u526a\u679d\u8def\u5f84\uff0c\u5e76\u5f15\u5165\u540e\u5e8f\u8282\u70b9\u56de\u6eda\u673a\u5236\u5904\u7406\u5f02\u5e38\u8868\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEnoTab\u5728\u590d\u6742\u95ee\u9898\u548c\u5927\u89c4\u6a21\u8868\u683c\u7684TableQA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "EnoTab\u901a\u8fc7\u53cc\u91cd\u964d\u566a\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u683c\u95ee\u7b54\u80fd\u529b\uff0c\u5177\u5907\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.17791", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17791", "abs": "https://arxiv.org/abs/2509.17791", "authors": ["Robert Hu", "Carlo Luschi", "Paul Balanca"], "title": "Elucidating the Design Space of FP4 training", "comment": null, "summary": "The increasing computational demands of foundation models have spurred\nresearch into low-precision training, with 4-bit floating-point (\\texttt{FP4})\nformats emerging as a frontier for maximizing hardware throughput. While\nnumerous techniques have been proposed to stabilize \\texttt{FP4} training, they\noften present isolated solutions with varying, and not always clear,\ncomputational overheads. This paper aims to provide a unified view of the\ndesign space of \\texttt{FP4} training. We introduce a comprehensive,\nquantisation gradient-based framework for microscaling quantization that allows\nfor a theoretical analysis of the computational costs associated with different\nstabilization methods on both the forward and backward passes. Using a\nsimulator built on this framework, we conduct an extensive empirical study\nacross a wide range of machine learning tasks, including regression, image\nclassification, diffusion models, and language models. By systematically\nevaluating thousands of combinations of techniques, such as novel gradient\napproximations, rounding strategies, and scaling methods, we identify which\nconfigurations offer the most favourable performance-to-overhead trade-off. We\nfind that the techniques enabling the best trade-off involve carefully\ncombining Hadamard transformations, tensor scaling and stochastic rounding. We\nfurther find that using \\texttt{UE5M3} as a scaling factor potentially offers a\ngood compromise between range and precision with manageable computational\noverhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u91cf\u5316\u68af\u5ea6\u7684\u5fae\u7f29\u653e\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u67904\u4f4d\u6d6e\u70b9\u8bad\u7ec3\u4e2d\u4e0d\u540c\u7a33\u5b9a\u5316\u6280\u672f\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u786e\u5b9a\u4e86\u6027\u80fd\u4e0e\u5f00\u9500\u4e4b\u95f4\u7684\u6700\u4f73\u6743\u8861\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u76844\u4f4d\u6d6e\u70b9\uff08FP4\uff09\u8bad\u7ec3\u65b9\u6cd5\u591a\u4e3a\u5b64\u7acb\u89e3\u51b3\u65b9\u6848\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bbe\u8ba1\u7a7a\u95f4\u5206\u6790\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5316\u68af\u5ea6\u7684\u5fae\u7f29\u653e\u91cf\u5316\u6846\u67b6\uff0c\u6784\u5efa\u6a21\u62df\u5668\u5bf9\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u4e2d\u7684\u5404\u79cd\u7a33\u5b9a\u5316\u6280\u672f\uff08\u5982\u68af\u5ea6\u8fd1\u4f3c\u3001\u820d\u5165\u7b56\u7565\u3001\u7f29\u653e\u65b9\u6cd5\uff09\u8fdb\u884c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5728\u56de\u5f52\u3001\u56fe\u50cf\u5206\u7c7b\u3001\u6269\u6563\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6570\u5343\u79cd\u6280\u672f\u7ec4\u5408\uff0c\u53d1\u73b0\u7ed3\u5408Hadamard\u53d8\u6362\u3001\u5f20\u91cf\u7f29\u653e\u548c\u968f\u673a\u820d\u5165\u7684\u65b9\u6cd5\u5177\u6709\u6700\u4f18\u7684\u6027\u80fd-\u5f00\u9500\u6743\u8861\uff0c\u4e14UE5M3\u683c\u5f0f\u4f5c\u4e3a\u7f29\u653e\u56e0\u5b50\u5728\u7cbe\u5ea6\u548c\u8303\u56f4\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u6298\u8877\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aFP4\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bbe\u8ba1\u89c6\u89d2\uff0c\u8bc6\u522b\u51fa\u9ad8\u6548\u7684\u6280\u672f\u7ec4\u5408\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u5728\u5b9e\u9645\u786c\u4ef6\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u3002"}}
{"id": "2509.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17088", "abs": "https://arxiv.org/abs/2509.17088", "authors": ["Jiexuan Zhang", "Yiheng Du", "Qian Wang", "Weiqi Li", "Yu Gu", "Jian Zhang"], "title": "AlignedGen: Aligning Style Across Generated Images", "comment": null, "summary": "Despite their generative power, diffusion models struggle to maintain style\nconsistency across images conditioned on the same style prompt, hindering their\npractical deployment in creative workflows. While several training-free methods\nattempt to solve this, they are constrained to the U-Net architecture, which\nnot only leads to low-quality results and artifacts like object repetition but\nalso renders them incompatible with superior Diffusion Transformer (DiT). To\naddress these issues, we introduce AlignedGen, a novel training-free framework\nthat enhances style consistency across images generated by DiT models. Our work\nfirst reveals a critical insight: naive attention sharing fails in DiT due to\nconflicting positional signals from improper position embeddings. We introduce\nShifted Position Embedding (ShiftPE), an effective solution that resolves this\nconflict by allocating a non-overlapping set of positional indices to each\nimage. Building on this foundation, we develop Advanced Attention Sharing\n(AAS), a suite of three techniques meticulously designed to fully unleash the\npotential of attention sharing within the DiT. Furthermore, to broaden the\napplicability of our method, we present an efficient query, key, and value\nfeature extraction algorithm, enabling our method to seamlessly incorporate\nexternal images as style references. Extensive experimental results validate\nthat our method effectively enhances style consistency across generated images\nwhile maintaining precise text-to-image alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAlignedGen\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u57fa\u4e8eDiT\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728U-Net\u67b6\u6784\u4e0a\u7684\u5c40\u9650\u6027\u548c\u4f4d\u7f6e\u5d4c\u5165\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u76f8\u540c\u98ce\u683c\u63d0\u793a\u4e0b\u751f\u6210\u56fe\u50cf\u65f6\u96be\u4ee5\u4fdd\u6301\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u4e14\u73b0\u6709\u8bad\u7ec3-free\u65b9\u6cd5\u53d7\u9650\u4e8eU-Net\u67b6\u6784\uff0c\u65e0\u6cd5\u517c\u5bb9\u6027\u80fd\u66f4\u4f18\u7684DiT\u6a21\u578b\uff0c\u540c\u65f6\u5b58\u5728\u751f\u6210\u8d28\u91cf\u4f4e\u548c\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Shifted Position Embedding (ShiftPE) \u89e3\u51b3DiT\u4e2d\u6ce8\u610f\u529b\u5171\u4eab\u65f6\u7684\u4f4d\u7f6e\u4fe1\u53f7\u51b2\u7a81\uff0c\u5e76\u8bbe\u8ba1\u4e86Advanced Attention Sharing (AAS) \u4e09\u6280\u672f\u7ec4\u5408\u4ee5\u5145\u5206\u53d1\u6325\u6ce8\u610f\u529b\u5171\u4eab\u6f5c\u529b\uff0c\u8fd8\u5f15\u5165\u4e86\u9ad8\u6548\u7684\u67e5\u8be2\u3001\u952e\u3001\u503c\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\u4ee5\u652f\u6301\u5916\u90e8\u56fe\u50cf\u4f5c\u4e3a\u98ce\u683c\u53c2\u8003\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u786e\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u95f4\u7684\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u4e14\u9002\u7528\u4e8eDiT\u67b6\u6784\u3002", "conclusion": "AlignedGen\u4e3aDiT\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3-free\u98ce\u683c\u4e00\u81f4\u6027\u751f\u6210\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u4f4d\u7f6e\u5d4c\u5165\u548c\u67b6\u6784\u517c\u5bb9\u6027\u65b9\u9762\u7684\u7f3a\u9677\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2509.17688", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17688", "abs": "https://arxiv.org/abs/2509.17688", "authors": ["Daiye Miao", "Yufang Liu", "Jie Wang", "Changzhi Sun", "Yunke Zhang", "Demei Yan", "Shaokang Dong", "Qi Zhang", "Yuanbin Wu"], "title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation", "comment": "Accepted to EMNLP 2025 (Main Conference),13 pages,10 figures", "summary": "LoRA has become one of the most widely used parameter-efficient fine-tuning\nmethods due to its simplicity and effectiveness. However, numerous studies have\nshown that LoRA often introduces substantial parameter redundancy, which not\nonly increases the number of trainable parameters but also hinders the\neffectiveness of fine-tuning. Since identifying redundant parameters in LoRA is\ninherently difficult, how to eliminate them efficiently and accurately remains\na challenging problem. In this paper, we propose TASO, a redundancy reduction\nmethod that leverages importance information from the pretrained model's\nweights to mitigate LoRA redundancy. Specifically, we estimate parameter\nimportance on downstream tasks and identify task-specific core regions based on\nthe distribution of importance scores. The location information of these core\nregions is then used to determine the sparse structure of LoRA modules,\nenabling redundancy removal before fine-tuning. Our approach significantly\nreduces the number of trainable parameters required for task adaptation, while\nproviding a novel task-aligned perspective for LoRA redundancy reduction.\nExperimental results demonstrate that, with a parameter budget comparable to\nLoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across\nmultiple tasks, achieving strong fine-tuning performance while effectively\neliminating redundant parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TASO\uff0c\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u7684\u91cd\u8981\u6027\u4fe1\u606f\u6765\u51cf\u5c11LoRA\u4e2d\u53c2\u6570\u5197\u4f59\u7684\u65b9\u6cd5\u3002TASO\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4f30\u8ba1\u53c2\u6570\u91cd\u8981\u6027\uff0c\u8bc6\u522b\u4efb\u52a1\u7279\u5b9a\u7684\u6838\u5fc3\u533a\u57df\uff0c\u5e76\u636e\u6b64\u786e\u5b9aLoRA\u6a21\u5757\u7684\u7a00\u758f\u7ed3\u6784\uff0c\u4ece\u800c\u5728\u5fae\u8c03\u524d\u53bb\u9664\u5197\u4f59\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0eLoRA\u76f8\u5f53\u751a\u81f3\u66f4\u5c11\u7684\u53c2\u6570\u9884\u7b97\u4e0b\uff0cTASO\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6LoRA\u3002", "motivation": "LoRA\u867d\u7136\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5b58\u5728\u663e\u8457\u7684\u53c2\u6570\u5197\u4f59\u95ee\u9898\uff0c\u5f71\u54cd\u5fae\u8c03\u6548\u679c\u4e14\u589e\u52a0\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002\u7531\u4e8e\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u5197\u4f59\u53c2\u6570\uff0c\u5982\u4f55\u9ad8\u6548\u3001\u7cbe\u786e\u5730\u6d88\u9664\u8fd9\u4e9b\u5197\u4f59\u6210\u4e3a\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faTASO\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u7684\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f30\u8ba1\u53c2\u6570\u91cd\u8981\u6027\uff0c\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7684\u9ad8\u91cd\u8981\u6027\u6838\u5fc3\u533a\u57df\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u533a\u57df\u7684\u4f4d\u7f6e\u4fe1\u606f\u9884\u5148\u5b9a\u4e49LoRA\u6a21\u5757\u7684\u7a00\u758f\u7ed3\u6784\uff0c\u5b9e\u73b0\u5fae\u8c03\u524d\u7684\u5197\u4f59\u53bb\u9664\u3002", "result": "TASO\u5728\u4e0eLoRA\uff08r=1\uff09\u76f8\u5f53\u7684\u53c2\u6570\u9884\u7b97\u4e0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u5fae\u8c03\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u51cf\u5c11\u5197\u4f59\u548c\u63d0\u5347\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TASO\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u5bf9\u9f50\u89c6\u89d2\u6765\u89e3\u51b3LoRA\u4e2d\u7684\u53c2\u6570\u5197\u4f59\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u7a00\u758f\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u6709\u6548\u7684\u53c2\u6570\u5fae\u8c03\u3002"}}
{"id": "2509.17808", "categories": ["cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.17808", "abs": "https://arxiv.org/abs/2509.17808", "authors": ["Yuxi Lu", "Biao Wu", "Zhidong Li", "Kunqi Li", "Chenya Huang", "Huacan Wang", "Qizhen Lan", "Ronghao Chen", "Ling Chen", "Bin Liang"], "title": "Remote Sensing-Oriented World Model", "comment": "10 pages, 5 figures", "summary": "World models have shown potential in artificial intelligence by predicting\nand reasoning about world states beyond direct observations. However, existing\napproaches are predominantly evaluated in synthetic environments or constrained\nscene settings, limiting their validation in real-world contexts with broad\nspatial coverage and complex semantics. Meanwhile, remote sensing applications\nurgently require spatial reasoning capabilities for disaster response and urban\nplanning. This paper bridges these gaps by introducing the first framework for\nworld modeling in remote sensing. We formulate remote sensing world modeling as\ndirection-conditioned spatial extrapolation, where models generate semantically\nconsistent adjacent image tiles given a central observation and directional\ninstruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing\nWorld-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks\nacross four scenarios: general, flood, urban, and rural. RSWISE combines visual\nfidelity assessment with instruction compliance evaluation using GPT-4o as a\nsemantic judge, ensuring models genuinely perform spatial reasoning rather than\nsimple replication. Afterwards, we present RemoteBAGEL, a unified multimodal\nmodel fine-tuned on remote sensing data for spatial extrapolation tasks.\nExtensive experiments demonstrate that RemoteBAGEL consistently outperforms\nstate-of-the-art baselines on RSWISE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u9065\u611f\u9886\u57df\u7684\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u65b9\u5411\u6761\u4ef6\u4e0b\u7684\u7a7a\u95f4\u5916\u63a8\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u9065\u611f\u56fe\u50cf\u5757\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b1600\u4e2a\u8bc4\u4f30\u4efb\u52a1\u7684RSWISE\u57fa\u51c6\uff0c\u7ed3\u5408\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0eGPT-4o\u8bed\u4e49\u8bc4\u5224\u6765\u9a8c\u8bc1\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u591a\u5728\u5408\u6210\u6216\u53d7\u9650\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5728\u5e7f\u57df\u590d\u6742\u8bed\u4e49\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9a8c\u8bc1\uff1b\u800c\u9065\u611f\u5e94\u7528\u8feb\u5207\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u6784\u5efa\u9002\u7528\u4e8e\u9065\u611f\u7684\u4e16\u754c\u6a21\u578b\u6846\u67b6\u3002", "method": "\u5c06\u9065\u611f\u4e16\u754c\u5efa\u6a21\u5b9a\u4e49\u4e3a\u65b9\u5411\u6761\u4ef6\u4e0b\u7684\u7a7a\u95f4\u5916\u63a8\u4efb\u52a1\uff0c\u63d0\u51faRSWISE\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cd\u573a\u666f\u76841600\u4e2a\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u5229\u7528GPT-4o\u4f5c\u4e3a\u8bed\u4e49\u88c1\u5224\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\u6027\uff1b\u540c\u65f6\u63d0\u51faRemoteBAGEL\u6a21\u578b\uff0c\u5728\u9065\u611f\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u4ee5\u5b8c\u6210\u591a\u6a21\u6001\u7a7a\u95f4\u5916\u63a8\u3002", "result": "RemoteBAGEL\u5728RSWISE\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c06\u4e16\u754c\u6a21\u578b\u5f15\u5165\u9065\u611f\u9886\u57df\uff0c\u901a\u8fc7RSWISE\u57fa\u51c6\u548cRemoteBAGEL\u6a21\u578b\u63a8\u52a8\u4e86\u9065\u611f\u56fe\u50cf\u7684\u7a7a\u95f4\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u707e\u5bb3\u54cd\u5e94\u548c\u57ce\u5e02\u89c4\u5212\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2509.17098", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17098", "abs": "https://arxiv.org/abs/2509.17098", "authors": ["Yuzhu Li", "An Sui", "Fuping Wu", "Xiahai Zhuang"], "title": "Uncertainty-Supervised Interpretable and Robust Evidential Segmentation", "comment": null, "summary": "Uncertainty estimation has been widely studied in medical image segmentation\nas a tool to provide reliability, particularly in deep learning approaches.\nHowever, previous methods generally lack effective supervision in uncertainty\nestimation, leading to low interpretability and robustness of the predictions.\nIn this work, we propose a self-supervised approach to guide the learning of\nuncertainty. Specifically, we introduce three principles about the\nrelationships between the uncertainty and the image gradients around boundaries\nand noise. Based on these principles, two uncertainty supervision losses are\ndesigned. These losses enhance the alignment between model predictions and\nhuman interpretation. Accordingly, we introduce novel quantitative metrics for\nevaluating the interpretability and robustness of uncertainty. Experimental\nresults demonstrate that compared to state-of-the-art approaches, the proposed\nmethod can achieve competitive segmentation performance and superior results in\nout-of-distribution (OOD) scenarios while significantly improving the\ninterpretability and robustness of uncertainty estimation. Code is available\nvia https://github.com/suiannaius/SURE.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\u6765\u6307\u5bfc\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e24\u4e2a\u4e0d\u786e\u5b9a\u6027\u76d1\u7763\u635f\u5931\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728OOD\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u6709\u6548\u76d1\u7763\uff0c\u5bfc\u81f4\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u8f83\u4f4e\u3002", "method": "\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0e\u8fb9\u754c\u548c\u566a\u58f0\u5468\u56f4\u56fe\u50cf\u68af\u5ea6\u5173\u7cfb\u7684\u4e09\u4e2a\u539f\u5219\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e0d\u786e\u5b9a\u6027\u76d1\u7763\u635f\u5931\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u5206\u5272\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5728OOD\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u9884\u6d4b\u4e0e\u4eba\u7c7b\u89e3\u91ca\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17694", "abs": "https://arxiv.org/abs/2509.17694", "authors": ["Dongxu Lu", "Johan Jeuring", "Albert Gatt"], "title": "Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues", "comment": "Accepted for publication at the 18th International Natural Language\n  Generation Conference (INLG 2025)", "summary": "Evaluating large language models (LLMs) in long-form, knowledge-grounded\nrole-play dialogues remains challenging. This study compares LLM-generated and\nhuman-authored responses in multi-turn professional training simulations\nthrough human evaluation ($N=38$) and automated LLM-as-a-judge assessment.\nHuman evaluation revealed significant degradation in LLM-generated response\nquality across turns, particularly in naturalness, context maintenance and\noverall quality, while human-authored responses progressively improved. In line\nwith this finding, participants also indicated a consistent preference for\nhuman-authored dialogue. These human judgements were validated by our automated\nLLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment\nwith human evaluators on both zero-shot pairwise preference and stochastic\n6-shot construct ratings, confirming the widening quality gap between LLM and\nhuman responses over time. Our work contributes a multi-turn benchmark exposing\nLLM degradation in knowledge-grounded role-play dialogues and provides a\nvalidated hybrid evaluation framework to guide the reliable integration of LLMs\nin training simulations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u548c\u81ea\u52a8\u5316LLM-as-a-judge\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0e\u4eba\u5de5\u7f16\u5199\u5728\u591a\u8f6e\u4e13\u4e1a\u57f9\u8bad\u6a21\u62df\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u751f\u6210\u7684\u56de\u5e94\u8d28\u91cf\u968f\u56de\u5408\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u800c\u4eba\u5de5\u56de\u5e94\u8d28\u91cf\u63d0\u5347\uff0c\u4e14\u4eba\u7c7b\u66f4\u504f\u597d\u4eba\u5de5\u5bf9\u8bdd\u3002Gemini 2.0 Flash\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86LLM\u4e0e\u4eba\u7c7b\u56de\u5e94\u95f4\u8d28\u91cf\u5dee\u8ddd\u968f\u65f6\u95f4\u6269\u5927\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u57fa\u51c6\u548c\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u5bfcLLM\u5728\u57f9\u8bad\u6a21\u62df\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u3001\u77e5\u8bc6\u9a71\u52a8\u7684\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e2d\uff0c\u9700\u63a2\u7a76\u5176\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5dee\u8ddd\u53ca\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4eba\u7c7b\u8bc4\u4f30\uff08N=38\uff09\u548c\u57fa\u4e8eGemini 2.0 Flash\u7684LLM-as-a-judge\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u5bf9LLM\u751f\u6210\u4e0e\u4eba\u5de5\u7f16\u5199\u7684\u591a\u8f6e\u5bf9\u8bdd\u56de\u5e94\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5305\u62ec\u96f6\u6837\u672c\u6210\u5bf9\u504f\u597d\u548c\u968f\u673a6\u6837\u672c\u6784\u9020\u8bc4\u5206\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793aLLM\u751f\u6210\u56de\u5e94\u7684\u8d28\u91cf\u968f\u5bf9\u8bdd\u8f6e\u6b21\u589e\u52a0\u800c\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u81ea\u7136\u6027\u3001\u4e0a\u4e0b\u6587\u4fdd\u6301\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\uff1b\u4eba\u5de5\u56de\u5e94\u5219\u6301\u7eed\u6539\u5584\uff0c\u4e14\u88ab\u53c2\u4e0e\u8005\u660e\u663e\u504f\u597d\u3002\u81ea\u52a8\u5316\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u8d28\u91cf\u5dee\u8ddd\u968f\u65f6\u95f4\u6269\u5927\u7684\u8d8b\u52bf\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u957f\u5468\u671f\u77e5\u8bc6\u9a71\u52a8\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4e2d\u5b58\u5728\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u96be\u4ee5\u5339\u654c\u4eba\u7c7b\u8868\u73b0\uff1b\u7814\u7a76\u63d0\u51fa\u7684\u591a\u8f6e\u57fa\u51c6\u548c\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u53ef\u6709\u6548\u8861\u91cf\u5e76\u6307\u5bfcLLM\u5728\u57f9\u8bad\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.17809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17809", "abs": "https://arxiv.org/abs/2509.17809", "authors": ["Shuhan Zhong", "Weipeng Zhuo", "Sizhe Song", "Guanyao Li", "Zhongyi Yu", "S. -H. Gary Chan"], "title": "MTM: A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification", "comment": "KDD 2025", "summary": "Irregular multivariate time series (IMTS) is characterized by the lack of\nsynchronized observations across its different channels. In this paper, we\npoint out that this channel-wise asynchrony can lead to poor channel-wise\nmodeling of existing deep learning methods. To overcome this limitation, we\npropose MTM, a multi-scale token mixing transformer for the classification of\nIMTS. We find that the channel-wise asynchrony can be alleviated by\ndown-sampling the time series to coarser timescales, and propose to incorporate\na masked concat pooling in MTM that gradually down-samples IMTS to enhance the\nchannel-wise attention modules. Meanwhile, we propose a novel channel-wise\ntoken mixing mechanism which proactively chooses important tokens from one\nchannel and mixes them with other channels, to further boost the channel-wise\nlearning of our model. Through extensive experiments on real-world datasets and\ncomparison with state-of-the-art methods, we demonstrate that MTM consistently\nachieves the best performance on all the benchmarks, with improvements of up to\n3.8% in AUPRC for classification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u4ee4\u724c\u6df7\u5408Transformer\uff08MTM\uff09\u7528\u4e8e\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff0c\u901a\u8fc7\u964d\u91c7\u6837\u548c\u901a\u9053\u95f4\u4ee4\u724c\u6df7\u5408\u673a\u5236\u63d0\u5347\u901a\u9053\u5efa\u6a21\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u65f6\uff0c\u7531\u4e8e\u901a\u9053\u95f4\u7684\u5f02\u6b65\u6027\u5bfc\u81f4\u5efa\u6a21\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMTM\u6a21\u578b\uff0c\u7ed3\u5408\u63a9\u7801\u62fc\u63a5\u6c60\u5316\u8fdb\u884c\u591a\u5c3a\u5ea6\u964d\u91c7\u6837\uff0c\u5e76\u8bbe\u8ba1\u901a\u9053\u95f4\u4ee4\u724c\u6df7\u5408\u673a\u5236\u4ee5\u589e\u5f3a\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cMTM\u5728\u6240\u6709\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5206\u7c7bAUPRC\u6700\u9ad8\u63d0\u53473.8%\u3002", "conclusion": "MTM\u6709\u6548\u7f13\u89e3\u4e86\u901a\u9053\u5f02\u6b65\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2509.17100", "categories": ["cs.CV", "68T07", "I.2.10; J.3"], "pdf": "https://arxiv.org/pdf/2509.17100", "abs": "https://arxiv.org/abs/2509.17100", "authors": ["Deepak Alapatt", "Jennifer Eckhoff", "Zhiliang Lyu", "Yutong Ban", "Jean-Paul Mazellier", "Sarah Choksi", "Kunyi Yang", "2024 CVS Challenge Consortium", "Quanzheng Li", "Filippo Filicori", "Xiang Li", "Pietro Mascagni", "Daniel A. Hashimoto", "Guy Rosman", "Ozanan Meireles", "Nicolas Padoy"], "title": "The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment", "comment": "18 pages, 10 figures", "summary": "Advances in artificial intelligence (AI) for surgical quality assessment\npromise to democratize access to expertise, with applications in training,\nguidance, and accreditation. This study presents the SAGES Critical View of\nSafety (CVS) Challenge, the first AI competition organized by a surgical\nsociety, using the CVS in laparoscopic cholecystectomy, a universally\nrecommended yet inconsistently performed safety step, as an exemplar of\nsurgical quality assessment. A global collaboration across 54 institutions in\n24 countries engaged hundreds of clinicians and engineers to curate 1,000\nvideos annotated by 20 surgical experts according to a consensus-validated\nprotocol. The challenge addressed key barriers to real-world deployment in\nsurgery, including achieving high performance, capturing uncertainty in\nsubjective assessment, and ensuring robustness to clinical variability. To\nenable this scale of effort, we developed EndoGlacier, a framework for managing\nlarge, heterogeneous surgical video and multi-annotator workflows. Thirteen\ninternational teams participated, achieving up to a 17\\% relative gain in\nassessment performance, over 80\\% reduction in calibration error, and a 17\\%\nrelative improvement in robustness over the state-of-the-art. Analysis of\nresults highlighted methodological trends linked to model performance,\nproviding guidance for future research toward robust, clinically deployable AI\nfor surgical quality assessment.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7SAGES Critical View of Safety\u6311\u6218\u8d5b\uff0c\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u5728\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u624b\u672f\u8d28\u91cf\u8bc4\u4f30\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86AI\u5728\u57f9\u8bad\u3001\u6307\u5bfc\u548c\u8ba4\u8bc1\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5916\u79d1\u624b\u672f\u4e2dAI\u5e94\u7528\u7684\u5173\u952e\u969c\u788d\uff0c\u5982\u6027\u80fd\u63d0\u5347\u3001\u4e3b\u89c2\u8bc4\u4f30\u7684\u4e0d\u786e\u5b9a\u6027\u6355\u6349\u4ee5\u53ca\u5bf9\u4e34\u5e8a\u53d8\u5f02\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7ec4\u7ec7\u4e86\u753154\u4e2a\u673a\u6784\u53c2\u4e0e\u7684\u56fd\u9645AI\u7ade\u8d5b\uff0c\u4f7f\u75281,000\u4e2a\u753120\u4f4d\u5916\u79d1\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1EndoGlacier\u6846\u67b6\u7ba1\u7406\u5927\u89c4\u6a21\u5f02\u6784\u624b\u672f\u89c6\u9891\u548c\u591a\u6807\u6ce8\u8005\u5de5\u4f5c\u6d41\u3002", "result": "\u53c2\u8d5b\u56e2\u961f\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u6700\u9ad8\u63d0\u534717%\u7684\u8bc4\u4f30\u6027\u80fd\uff0c\u6821\u51c6\u8bef\u5dee\u51cf\u5c11\u8d85\u8fc780%\uff0c\u9c81\u68d2\u6027\u76f8\u5bf9\u63d0\u9ad817%\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4e3a\u672a\u6765\u7814\u53d1\u53ef\u7528\u4e8e\u4e34\u5e8a\u90e8\u7f72\u7684\u9c81\u68d2AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u6cd5\u5b66\u6307\u5bfc\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2509.17701", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17701", "abs": "https://arxiv.org/abs/2509.17701", "authors": ["Mariam Mahran", "Katharina Simbeck"], "title": "Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs", "comment": "Accepted at edu4AI'25: 2nd Workshop on Education for Artificial\n  Intelligence | co-located with ECAI, October 26th, 2025, Bologna, Italy. 7\n  pages, 0 figures", "summary": "Large Language Models (LLMs) are increasingly used for educational support,\nyet their response quality varies depending on the language of interaction.\nThis paper presents an automated multilingual pipeline for generating, solving,\nand evaluating math problems aligned with the German K-10 curriculum. We\ngenerated 628 math exercises and translated them into English, German, and\nArabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)\nwere prompted to produce step-by-step solutions in each language. A held-out\npanel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality\nusing a comparative framework. Results show a consistent gap, with English\nsolutions consistently rated highest, and Arabic often ranked lower. These\nfindings highlight persistent linguistic bias and the need for more equitable\nmultilingual AI systems in education.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u3001\u89e3\u51b3\u548c\u8bc4\u4f30\u7b26\u5408\u5fb7\u56fdK-10\u8bfe\u7a0b\u7684\u6570\u5b66\u95ee\u9898\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u4e2d\u7684\u89e3\u9898\u8d28\u91cf\u6700\u9ad8\uff0c\u963f\u62c9\u4f2f\u8bed\u6700\u4f4e\uff0c\u63ed\u793a\u4e86\u6559\u80b2AI\u4e2d\u7684\u8bed\u8a00\u504f\u89c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6559\u80b2\u652f\u6301\uff0c\u4f46\u5176\u54cd\u5e94\u8d28\u91cf\u56e0\u4ea4\u4e92\u8bed\u8a00\u800c\u5f02\uff0c\u5c24\u5176\u662f\u5728\u975e\u82f1\u8bed\u8bed\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u7cfb\u7edf\u8bc4\u4f30\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLM\u7684\u8868\u73b0\u4ee5\u4fc3\u8fdb\u6559\u80b2\u516c\u5e73\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u81ea\u52a8\u7ba1\u9053\uff0c\u751f\u6210628\u9053\u7b26\u5408\u5fb7\u56fdK-10\u8bfe\u7a0b\u7684\u6570\u5b66\u9898\uff0c\u5e76\u7ffb\u8bd1\u6210\u82f1\u8bed\u3001\u5fb7\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\uff1b\u4f7f\u7528\u4e09\u79cd\u5546\u4e1a\u5927\u6a21\u578b\uff08GPT-4o-mini\u3001Gemini 2.5 Flash\u3001Qwen-plus\uff09\u751f\u6210\u6bcf\u79cd\u8bed\u8a00\u7684\u9010\u6b65\u89e3\u7b54\uff0c\u5e76\u7531\u5305\u62ecClaude 3.5 Haiku\u5728\u5185\u7684LLM\u8bc4\u5ba1\u5c0f\u7ec4\u91c7\u7528\u6bd4\u8f83\u6846\u67b6\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e09\u79cd\u6a21\u578b\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u5747\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff1a\u82f1\u8bed\u89e3\u7b54\u8d28\u91cf consistently \u6700\u9ad8\uff0c\u963f\u62c9\u4f2f\u8bed\u901a\u5e38\u6700\u4f4e\uff0c\u5fb7\u8bed\u5c45\u4e2d\uff0c\u8868\u660e\u5b58\u5728\u7cfb\u7edf\u6027\u8bed\u8a00\u504f\u5dee\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6559\u80b2\u5e94\u7528\u4e2d\u5b58\u5728\u660e\u663e\u7684\u8bed\u8a00\u504f\u89c1\uff0c\u5c24\u5176\u5bf9\u963f\u62c9\u4f2f\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u652f\u6301\u8f83\u5f31\uff0c\u9700\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u3001\u8bc4\u4f30\u673a\u5236\u548c\u672c\u5730\u5316\u7b56\u7565\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00AI\u6559\u80b2\u7cfb\u7edf\u3002"}}
{"id": "2509.17811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17811", "abs": "https://arxiv.org/abs/2509.17811", "authors": ["Thrinadh Pinjala", "Aswin Ram Kumar Gannina", "Debasis Dwibedy"], "title": "MSGAT-GRU: A Multi-Scale Graph Attention and Recurrent Model for Spatiotemporal Road Accident Prediction", "comment": "16 pages, 4 figures, 4 tables", "summary": "Accurate prediction of road accidents remains challenging due to intertwined\nspatial, temporal, and contextual factors in urban traffic. We propose\nMSGAT-GRU, a multi-scale graph attention and recurrent model that jointly\ncaptures localized and long-range spatial dependencies while modeling\nsequential dynamics. Heterogeneous inputs, such as traffic flow, road\nattributes, weather, and points of interest, are systematically fused to\nenhance robustness and interpretability. On the Hybrid Beijing Accidents\ndataset, MSGAT-GRU achieves an RMSE of 0.334 and an F1-score of 0.878,\nconsistently outperforming strong baselines. Cross-dataset evaluation on\nMETR-LA under a 1-hour horizon further supports transferability, with RMSE of\n6.48 (vs. 7.21 for the GMAN model) and comparable MAPE. Ablations indicate that\nthree-hop spatial aggregation and a two-layer GRU offer the best\naccuracy-stability trade-off. These results position MSGAT-GRU as a scalable\nand generalizable model for intelligent transportation systems, providing\ninterpretable signals that can inform proactive traffic management and road\nsafety analytics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSGAT-GRU\u7684\u591a\u5c3a\u5ea6\u56fe\u6ce8\u610f\u529b\u4e0e\u5faa\u73af\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u9053\u8def\u4e8b\u6545\uff0c\u7ed3\u5408\u4e86\u7a7a\u95f4\u3001\u65f6\u95f4\u53ca\u4e0a\u4e0b\u6587\u56e0\u7d20\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u57ce\u5e02\u4ea4\u901a\u4e2d\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u4e0a\u4e0b\u6587\u56e0\u7d20\u4ea4\u7ec7\u590d\u6742\uff0c\u9053\u8def\u4e8b\u6545\u7684\u51c6\u786e\u9884\u6d4b\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6355\u6349\u5c40\u90e8\u548c\u8fdc\u8ddd\u79bb\u7a7a\u95f4\u4f9d\u8d56\u5e76\u5efa\u6a21\u65f6\u5e8f\u52a8\u6001\u7684\u6a21\u578b\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08MSGAT\uff09\u4e0e\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u76f8\u7ed3\u5408\u7684\u67b6\u6784\uff0c\u878d\u5408\u4ea4\u901a\u6d41\u91cf\u3001\u9053\u8def\u5c5e\u6027\u3001\u5929\u6c14\u548c\u5174\u8da3\u70b9\u7b49\u5f02\u6784\u8f93\u5165\uff0c\u4ee5\u8054\u5408\u6355\u6349\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u5e8f\u5217\u52a8\u6001\u3002", "result": "\u5728Hybrid Beijing Accidents\u6570\u636e\u96c6\u4e0a\uff0cMSGAT-GRU\u7684RMSE\u4e3a0.334\uff0cF1\u5f97\u5206\u4e3a0.878\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff1b\u5728METR-LA\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c1\u5c0f\u65f6\u9884\u6d4b\u8303\u56f4\u5185RMSE\u4e3a6.48\uff08\u4f4e\u4e8eGMAN\u76847.21\uff09\uff0cMAPE\u76f8\u5f53\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u4e09\u8df3\u7a7a\u95f4\u805a\u5408\u548c\u53cc\u5c42GRU\u5728\u7cbe\u5ea6\u4e0e\u7a33\u5b9a\u6027\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "MSGAT-GRU\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u4ea4\u901a\u4e8b\u6545\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u4fe1\u53f7\uff0c\u6709\u52a9\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4e3b\u52a8\u4ea4\u901a\u7ba1\u7406\u548c\u9053\u8def\u5b89\u5168\u5206\u6790\u3002"}}
{"id": "2509.17737", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17737", "abs": "https://arxiv.org/abs/2509.17737", "authors": ["Kavin R V", "Pawan Goyal"], "title": "Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics", "comment": "5 pages, 1 figure", "summary": "Standard language models employ unique, monolithic embeddings for each token,\npotentially limiting their ability to capture the multifaceted nature of word\nmeanings. We investigate whether tokens can be more effectively represented\nthrough a compositional structure that accumulates diverse semantic facets. To\nexplore this, we propose Aggregate Semantic Grouping (ASG), a novel approach\nleveraging Product Quantization (PQ). We apply ASG to standard transformer\narchitectures (mBERT, XLM-R, mT5) and evaluate this representational scheme\nacross diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific\nbenchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing\ntokens compositionally via ASG achieves extreme compression in embedding\nparameters (0.4--0.5\\%) while maintaining $>$95\\% task performance relative to\nthe base model, even in generative tasks and extends to both cross lingual\ntransfer and domain-specific settings. These results validate the principle\nthat tokens can be effectively modeled as combinations of shared semantic\nbuilding blocks. ASG offers a simple yet concrete method for achieving this,\nshowcasing how compositional representations can capture linguistic richness\nwhile enabling compact yet semantically rich models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eProduct Quantization\u7684\u805a\u5408\u8bed\u4e49\u5206\u7ec4\uff08ASG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u5f0f\u8868\u793a\u8bcd\u5143\uff0c\u5728\u5927\u5e45\u538b\u7f29\u5d4c\u5165\u53c2\u6570\uff080.4-0.5%\uff09\u7684\u540c\u65f6\u4fdd\u6301\u8d85\u8fc795%\u7684\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u5355\u4e00\u56fa\u5b9a\u5d4c\u5165\u8868\u793a\u8bcd\u5143\uff0c\u96be\u4ee5\u6355\u6349\u8bcd\u8bed\u591a\u9762\u7684\u8bed\u4e49\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u5f15\u5165\u805a\u5408\u8bed\u4e49\u5206\u7ec4\uff08ASG\uff09\uff0c\u5229\u7528\u4e58\u79ef\u91cf\u5316\uff08PQ\uff09\u5c06\u8bcd\u5143\u8868\u793a\u4e3a\u591a\u4e2a\u5171\u4eab\u8bed\u4e49\u57fa\u5757\u7684\u7ec4\u5408\uff0c\u5e76\u5e94\u7528\u4e8emBERT\u3001XLM-R\u3001mT5\u548cBioBERT\u7b49\u6a21\u578b\u3002", "result": "\u5728NLI\u3001NER\u3001QA\u548cBC5CDR\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0cASG\u5728\u4ec5\u75280.4-0.5%\u5d4c\u5165\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301>95%\u539f\u6709\u6027\u80fd\uff0c\u4e14\u9002\u7528\u4e8e\u751f\u6210\u4efb\u52a1\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u3002", "conclusion": "\u8bcd\u5143\u53ef\u901a\u8fc7\u7ec4\u5408\u5171\u4eab\u8bed\u4e49\u57fa\u5757\u6709\u6548\u5efa\u6a21\uff0cASG\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u6d01\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7d27\u51d1\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u6a21\u578b\u8868\u793a\u3002"}}
{"id": "2509.17815", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.17815", "abs": "https://arxiv.org/abs/2509.17815", "authors": ["Andrea Agazzi", "Vittorio Carlei", "Marco Romito", "Samuele Saviozzi"], "title": "Global Optimization via Softmin Energy Minimization", "comment": null, "summary": "Global optimization, particularly for non-convex functions with multiple\nlocal minima, poses significant challenges for traditional gradient-based\nmethods. While metaheuristic approaches offer empirical effectiveness, they\noften lack theoretical convergence guarantees and may disregard available\ngradient information. This paper introduces a novel gradient-based swarm\nparticle optimization method designed to efficiently escape local minima and\nlocate global optima. Our approach leverages a \"Soft-min Energy\" interacting\nfunction, $J_\\beta(\\mathbf{x})$, which provides a smooth, differentiable\napproximation of the minimum function value within a particle swarm. We define\na stochastic gradient flow in the particle space, incorporating a Brownian\nmotion term for exploration and a time-dependent parameter $\\beta$ to control\nsmoothness, similar to temperature annealing. We theoretically demonstrate that\nfor strongly convex functions, our dynamics converges to a stationary point\nwhere at least one particle reaches the global minimum, with other particles\nexhibiting exploratory behavior. Furthermore, we show that our method\nfacilitates faster transitions between local minima by reducing effective\npotential barriers with respect to Simulated Annealing. More specifically, we\nestimate the hitting times of unexplored potential wells for our model in the\nsmall noise regime and show that they compare favorably with the ones of\noverdamped Langevin. Numerical experiments on benchmark functions, including\ndouble wells and the Ackley function, validate our theoretical findings and\ndemonstrate better performance over the well-known Simulated Annealing method\nin terms of escaping local minima and achieving faster convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u7fa4\u4f53\u667a\u80fd\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u8f6f\u6700\u5c0f\u80fd\u91cf\u201d\u51fd\u6570\u548c\u968f\u673a\u68af\u5ea6\u6d41\uff0c\u6709\u6548\u9003\u79bb\u5c40\u90e8\u6781\u5c0f\u503c\u5e76\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u6a21\u62df\u9000\u706b\u7b49\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u6cd5\u96be\u4ee5\u5904\u7406\u975e\u51f8\u51fd\u6570\u7684\u591a\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u800c\u73b0\u6709\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u4e14\u5e38\u5ffd\u7565\u68af\u5ea6\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u5177\u7406\u8bba\u4fdd\u969c\u4e0e\u9ad8\u6548\u6027\u80fd\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8f6f\u6700\u5c0f\u80fd\u91cf\u51fd\u6570J\u03b2(x)\u7684\u968f\u673a\u68af\u5ea6\u6d41\u6a21\u578b\uff0c\u7ed3\u5408\u5e03\u6717\u8fd0\u52a8\u8fdb\u884c\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u65f6\u53d8\u53c2\u6570\u03b2\u63a7\u5236\u5e73\u6ed1\u5ea6\uff0c\u5b9e\u73b0\u7c92\u5b50\u7fa4\u5728\u68af\u5ea6\u4fe1\u606f\u6307\u5bfc\u4e0b\u7684\u5168\u5c40\u641c\u7d22\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5f3a\u51f8\u51fd\u6570\u4e0b\u53ef\u6536\u655b\u81f3\u5168\u5c40\u6700\u4f18\uff1b\u5728\u5c0f\u566a\u58f0\u6761\u4ef6\u4e0b\uff0c\u5176\u547d\u4e2d\u672a\u63a2\u7d22\u52bf\u9631\u7684\u65f6\u95f4\u4f18\u4e8e\u8fc7\u963b\u5c3cLangevin\u65b9\u6cd5\uff1b\u5b9e\u9a8c\u663e\u793a\u5728\u53cc\u4e95\u53caAckley\u7b49\u57fa\u51c6\u51fd\u6570\u4e0a\u4f18\u4e8e\u6a21\u62df\u9000\u706b\u3002", "conclusion": "\u6240\u63d0\u68af\u5ea6\u578b\u7c92\u5b50\u7fa4\u4f18\u5316\u65b9\u6cd5\u878d\u5408\u4e86\u7fa4\u4f53\u667a\u80fd\u4e0e\u68af\u5ea6\u4fe1\u606f\uff0c\u5177\u6709\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u80fd\u66f4\u9ad8\u6548\u5730\u8de8\u8d8a\u52bf\u5792\u3001\u9003\u79bb\u5c40\u90e8\u6781\u5c0f\uff0c\u663e\u8457\u63d0\u5347\u5168\u5c40\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2509.17120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17120", "abs": "https://arxiv.org/abs/2509.17120", "authors": ["Gordon Chen", "Ziqi Huang", "Cheston Tan", "Ziwei Liu"], "title": "Stencil: Subject-Driven Generation with Context Guidance", "comment": "Accepted as Spotlight at ICIP 2025", "summary": "Recent text-to-image diffusion models can generate striking visuals from text\nprompts, but they often fail to maintain subject consistency across generations\nand contexts. One major limitation of current fine-tuning approaches is the\ninherent trade-off between quality and efficiency. Fine-tuning large models\nimproves fidelity but is computationally expensive, while fine-tuning\nlightweight models improves efficiency but compromises image fidelity.\nMoreover, fine-tuning pre-trained models on a small set of images of the\nsubject can damage the existing priors, resulting in suboptimal results. To\nthis end, we present Stencil, a novel framework that jointly employs two\ndiffusion models during inference. Stencil efficiently fine-tunes a lightweight\nmodel on images of the subject, while a large frozen pre-trained model provides\ncontextual guidance during inference, injecting rich priors to enhance\ngeneration with minimal overhead. Stencil excels at generating high-fidelity,\nnovel renditions of the subject in less than a minute, delivering\nstate-of-the-art performance and setting a new benchmark in subject-driven\ngeneration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStencil\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f7f\u7528\u4e24\u4e2a\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4e14\u9ad8\u6548\u7684\u4e3b\u4f53\u4e00\u81f4\u6027\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u8de8\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5f53\u524d\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "Stencil\u6846\u67b6\u5728\u63a8\u7406\u65f6\u540c\u65f6\u5229\u7528\u4e00\u4e2a\u7ecf\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\u7684\u6a21\u578b\u548c\u4e00\u4e2a\u5927\u578b\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u524d\u8005\u7528\u4e8e\u63d0\u9ad8\u6548\u7387\uff0c\u540e\u8005\u63d0\u4f9b\u4e0a\u4e0b\u6587\u6307\u5bfc\u4ee5\u589e\u5f3a\u751f\u6210\u6548\u679c\u3002", "result": "Stencil\u80fd\u591f\u5728\u4e0d\u5230\u4e00\u5206\u949f\u7684\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65b0\u9896\u7684\u4e3b\u4f53\u56fe\u50cf\uff0c\u5e76\u5728\u4e3b\u4f53\u9a71\u52a8\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Stencil\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5fae\u8c03\u4e0e\u5927\u578b\u51bb\u7ed3\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u4f53\u4e00\u81f4\u6027\u7684\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2509.17765", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17765", "abs": "https://arxiv.org/abs/2509.17765", "authors": ["Jin Xu", "Zhifang Guo", "Hangrui Hu", "Yunfei Chu", "Xiong Wang", "Jinzheng He", "Yuxuan Wang", "Xian Shi", "Ting He", "Xinfa Zhu", "Yuanjun Lv", "Yongqi Wang", "Dake Guo", "He Wang", "Linhan Ma", "Pei Zhang", "Xinyu Zhang", "Hongkun Hao", "Zishan Guo", "Baosong Yang", "Bin Zhang", "Ziyang Ma", "Xipin Wei", "Shuai Bai", "Keqin Chen", "Xuejing Liu", "Peng Wang", "Mingkun Yang", "Dayiheng Liu", "Xingzhang Ren", "Bo Zheng", "Rui Men", "Fan Zhou", "Bowen Yu", "Jianxin Yang", "Le Yu", "Jingren Zhou", "Junyang Lin"], "title": "Qwen3-Omni Technical Report", "comment": "https://github.com/QwenLM/Qwen3-Omni", "summary": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.", "AI": {"tldr": "Qwen3-Omni \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u6216\u8d85\u8d8a\u5355\u6a21\u6001\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u97f3\u9891\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u652f\u6301\u591a\u79cd\u8bed\u8a00\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684 Thinker-Talker MoE \u67b6\u6784\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u8bed\u97f3\u751f\u6210\u548c\u9ad8\u6548\u591a\u6a21\u6001\u63a8\u7406\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e0d\u727a\u7272\u4efb\u4e00\u6a21\u6001\u6027\u80fd\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u514b\u670d\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u5728\u7279\u5b9a\u6a21\u6001\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u63a8\u52a8\u591a\u6a21\u6001\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u901a\u7528\u6027\u3002", "method": "\u91c7\u7528 Thinker-Talker MoE \u67b6\u6784\uff0c\u7edf\u4e00\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u8f93\u5165\uff1bTalker \u4f7f\u7528\u591a\u7801\u672c\u65b9\u6848\u81ea\u56de\u5f52\u9884\u6d4b\u79bb\u6563\u8bed\u97f3\u7f16\u89e3\u7801\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u56e0\u679c\u5377\u79ef\u7f51\u7edc\u964d\u4f4e\u6d41\u5f0f\u5408\u6210\u9996\u5305\u5ef6\u8fdf\uff1b\u5f15\u5165 Thinking \u6a21\u578b\u589e\u5f3a\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u57fa\u4e8e Qwen3-Omni \u5fae\u8c03\u51fa\u4e13\u7528\u97f3\u9891\u63cf\u8ff0\u6a21\u578b Captioner\u3002", "result": "\u572836\u4e2a\u97f3\u9891\u53ca\u97f3\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c32\u4e2a\u8fbe\u5230\u5f00\u6e90SOTA\uff0c22\u4e2a\u8fbe\u5230\u6574\u4f53SOTA\uff0c\u8d85\u8d8a Gemini-2.5-Pro\u3001Seed-ASR \u548c GPT-4o-Transcribe \u7b49\u95ed\u6e90\u6a21\u578b\uff1b\u652f\u6301119\u79cd\u6587\u672c\u8bed\u8a00\u300119\u79cd\u8bed\u97f3\u7406\u89e3\u548c10\u79cd\u8bed\u97f3\u751f\u6210\uff1b\u51b7\u542f\u52a8\u4e0b\u7aef\u5230\u7aef\u9996\u5305\u5ef6\u8fdf\u7406\u8bba\u503c\u4e3a234ms\uff1b\u53d1\u5e03\u4e86 Qwen3-Omni-30B-A3B \u7cfb\u5217\u591a\u4e2a\u7248\u672c\u6a21\u578b\u3002", "conclusion": "Qwen3-Omni \u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u591a\u6a21\u6001\u7edf\u4e00\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u5404\u6a21\u6001\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u4e14\u5f00\u653e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17845", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17845", "abs": "https://arxiv.org/abs/2509.17845", "authors": ["Kai Zhang", "Siming Sun", "Zhengyu Fan", "Qinmin Yang", "Xuejun Jiang"], "title": "Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series", "comment": null, "summary": "Time series analysis faces significant challenges in handling variable-length\ndata and achieving robust generalization. While Transformer-based models have\nadvanced time series tasks, they often struggle with feature redundancy and\nlimited generalization capabilities. Drawing inspiration from classical CNN\narchitectures' pyramidal structure, we propose a Multi-Scale Representation\nLearning Framework based on a Conv-like ScaleFusion Transformer. Our approach\nintroduces a temporal convolution-like structure that combines patching\noperations with multi-head attention, enabling progressive temporal dimension\ncompression and feature channel expansion. We further develop a novel\ncross-scale attention mechanism for effective feature fusion across different\ntemporal scales, along with a log-space normalization method for\nvariable-length sequences. Extensive experiments demonstrate that our framework\nachieves superior feature independence, reduced redundancy, and better\nperformance in forecasting and classification tasks compared to\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u5f0fScaleFusion Transformer\u7684\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u65f6\u95f4\u5377\u79ef\u7ed3\u6784\u548c\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u7279\u5f81\u72ec\u7acb\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u5904\u7406\u53d8\u957f\u65f6\u95f4\u5e8f\u5217\u65f6\u5b58\u5728\u7279\u5f81\u5197\u4f59\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u6709\u6548\u878d\u5408\u591a\u5c3a\u5ea6\u65f6\u5e8f\u7279\u5f81\u3002", "method": "\u8bbe\u8ba1\u7c7bCNN\u91d1\u5b57\u5854\u7ed3\u6784\u7684Conv-like ScaleFusion Transformer\uff0c\u5f15\u5165\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684patching\u4e0e\u591a\u5934\u6ce8\u610f\u529b\u7ed3\u5408\u7684\u7ed3\u6784\uff0c\u5b9e\u73b0\u65f6\u95f4\u7ef4\u5ea6\u538b\u7f29\u548c\u901a\u9053\u6269\u5c55\uff1b\u63d0\u51fa\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u548c\u5bf9\u6570\u7a7a\u95f4\u5f52\u4e00\u5316\u65b9\u6cd5\u4ee5\u652f\u6301\u53d8\u957f\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u7279\u5f81\u72ec\u7acb\u6027\u3001\u66f4\u4f4e\u7684\u5197\u4f59\u5ea6\u548c\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u7279\u5f81\u5197\u4f59\u4e0e\u53d8\u957f\u8f93\u5165\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17136", "abs": "https://arxiv.org/abs/2509.17136", "authors": ["Yuhao Tian", "Zheming Yang"], "title": "SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM", "comment": "5 pages, 5 figures", "summary": "Industrial vision inspection requires high accuracy under stringent resource\nconstraints, yet existing approaches face a fundamental trade-off. Multimodal\nLLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive\ncomputational costs, while lightweight edge models often fail on complex cases.\nIn this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative\nindustrial vision inspection framework with MLLM. The framework is composed of\nthree synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect\nInspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)\nAdaptive Edge-Cloud Scheduler. Together, these modules enable robust defect\ndetection by tailoring multimodal reasoning to scene complexity and dynamically\nbalancing computation between edge and cloud resources. Experimental results on\nMVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%\naccuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It\nalso reduces runtime by up to 22.4% and cuts energy per correct decision by\n40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u573a\u666f\u611f\u77e5\u7684\u8fb9\u7f18-\u4e91\u534f\u540c\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u6846\u67b6SAEC\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e0e\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u8ba1\u7b97\uff0c\u901a\u8fc7\u590d\u6742\u5ea6\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u5728\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u65b9\u6cd5\u5728\u9ad8\u7cbe\u5ea6\u4e0e\u8d44\u6e90\u53d7\u9650\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u591a\u6a21\u6001\u5927\u6a21\u578b\u6027\u80fd\u5f3a\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u6a21\u578b\u5219\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7f3a\u9677\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u987e\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u534f\u540c\u6846\u67b6\u3002", "method": "SAEC\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u9488\u5bf9\u590d\u6742\u7f3a\u9677\u68c0\u6d4b\u7684\u9ad8\u6548MLLM\u5fae\u8c03\uff1b(2) \u8f7b\u91cf\u5316\u7684\u591a\u5c3a\u5ea6\u573a\u666f\u590d\u6742\u5ea6\u4f30\u8ba1\uff1b(3) \u81ea\u9002\u5e94\u7684\u8fb9\u7f18-\u4e91\u8ba1\u7b97\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u573a\u666f\u590d\u6742\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u4efb\u52a1\u3002", "result": "\u5728MVTec AD\u548cKSDD2\u6570\u636e\u96c6\u4e0a\uff0cSAEC\u5206\u522b\u8fbe\u523085.11%\u548c82.72%\u7684\u51c6\u786e\u7387\uff0c\u8f83Qwen\u63d0\u534722.1%\u548c20.8%\uff0c\u8f83LLaVA\u63d0\u534733.3%\u548c31.6%\uff1b\u540c\u65f6\u8fd0\u884c\u65f6\u95f4\u6700\u591a\u51cf\u5c1122.4%\uff0c\u6bcf\u6b63\u786e\u51b3\u7b56\u80fd\u8017\u964d\u4f4e40%-74%\u3002", "conclusion": "SAEC\u901a\u8fc7\u573a\u666f\u611f\u77e5\u7684\u8fb9\u7f18-\u4e91\u534f\u540c\u673a\u5236\uff0c\u6709\u6548\u5e73\u8861\u4e86\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u4e2d\u7684\u7cbe\u5ea6\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u80fd\u8017\u7684\u7f3a\u9677\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2509.17766", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17766", "abs": "https://arxiv.org/abs/2509.17766", "authors": ["Ziyi Liu"], "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue", "comment": null, "summary": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u2014\u2014\u72b6\u6001\u66f4\u65b0\u591a\u8f6e\u5bf9\u8bdd\u7b56\u7565\uff0c\u901a\u8fc7\u201c\u72b6\u6001\u91cd\u5efa\u201d\u548c\u201c\u5386\u53f2\u63d0\u9192\u201d\u673a\u5236\u6709\u6548\u7ba1\u7406\u5bf9\u8bdd\u5386\u53f2\uff0c\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5468\u671f\u3001\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b58\u5728\u7684\u4fe1\u606f\u9057\u5fd8\u548c\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u2018\u72b6\u6001\u66f4\u65b0\u2019\u7b56\u7565\uff0c\u5305\u542b\u2018\u72b6\u6001\u91cd\u5efa\u2019\u548c\u2018\u5386\u53f2\u63d0\u9192\u2019\u4e24\u4e2a\u673a\u5236\uff0c\u4ee5\u66f4\u597d\u5730\u7ba1\u7406\u548c\u5229\u7528\u5bf9\u8bdd\u5386\u53f2\u3002", "result": "\u5728HotpotQA\u7b49\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6838\u5fc3\u4fe1\u606f\u8fc7\u6ee4\u5f97\u5206\u63d0\u9ad832.6%\uff0c\u4e0b\u6e38\u95ee\u7b54\u5f97\u5206\u63d0\u534714.1%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1173.1%\uff0ctoken\u6d88\u8017\u964d\u4f4e59.4%\uff1b\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u7ec4\u4ef6\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u7a0b\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u667a\u80fd\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.17866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17866", "abs": "https://arxiv.org/abs/2509.17866", "authors": ["Xinyu He", "Xianghui Cao"], "title": "Understanding Post-Training Structural Changes in Large Language Models", "comment": "38 pages, 26 figures", "summary": "Post-training fundamentally alters the behavior of large language models\n(LLMs), yet its impact on the internal parameter space remains poorly\nunderstood. In this work, we conduct a systematic singular value decomposition\n(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two\nwidely adopted post-training methods: instruction tuning and\nlong-chain-of-thought (Long-CoT) distillation. Our analysis reveals two\nconsistent and unexpected structural changes:(1) a near-uniform geometric\nscaling of singular values across layers, which theoretically modulates\nattention scores; and (2) highly consistent orthogonal transformations are\napplied to the left and right singular vectors of each matrix. Disrupting this\northogonal consistency leads to catastrophic performance degradation. Based on\nthese findings, we propose a simple yet effective framework that interprets\npost-training as a reparameterization of fixed subspaces in the pretrained\nparameter space. Further experiments reveal that singular value scaling behaves\nas a secondary effect, analogous to a temperature adjustment, whereas the core\nfunctional transformation lies in the coordinated rotation of singular vectors.\nThese results challenge the prevailing view of the parameter space in large\nmodels as a black box, uncovering the first clear regularities in how\nparameters evolve during training, and providing a new perspective for deeper\ninvestigation into model parameter changes.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7SVD\u5206\u6790\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53c2\u6570\u7a7a\u95f4\u7684\u7ed3\u6784\u6027\u53d8\u5316\uff0c\u53d1\u73b0\u5947\u5f02\u503c\u7684\u5747\u5300\u7f29\u653e\u548c\u5947\u5f02\u5411\u91cf\u7684\u6b63\u4ea4\u53d8\u6362\u662f\u5173\u952e\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u540e\u8bad\u7ec3\u89e3\u91ca\u4e3a\u9884\u8bad\u7ec3\u53c2\u6570\u7a7a\u95f4\u4e2d\u56fa\u5b9a\u5b50\u7a7a\u95f4\u91cd\u53c2\u6570\u5316\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u7406\u89e3\u540e\u8bad\u7ec3\u5982\u4f55\u6539\u53d8\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u53c2\u6570\u7ed3\u6784\uff0c\u63ed\u793a\u5176\u80cc\u540e\u7684\u89c4\u5f8b\u6027\uff0c\u6311\u6218\u5f53\u524d\u5c06\u53c2\u6570\u7a7a\u95f4\u89c6\u4e3a\u9ed1\u7bb1\u7684\u89c2\u70b9\u3002", "method": "\u5bf9\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u4e2d\u7684\u4e3b\u8981\u7ebf\u6027\u5c42\u8fdb\u884c\u7cfb\u7edf\u7684\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u5206\u6790\uff0c\u805a\u7126\u6307\u4ee4\u5fae\u8c03\u548c\u957f\u94fe\u601d\u7ef4\uff08Long-CoT\uff09\u84b8\u998f\u4e24\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u4e86\u4e24\u4e2a\u4e00\u81f4\u4e14\u610f\u5916\u7684\u7ed3\u6784\u53d8\u5316\uff1a(1) \u5404\u5c42\u5947\u5f02\u503c\u8fd1\u4e4e\u5747\u5300\u7684\u51e0\u4f55\u7f29\u653e\uff0c\u7406\u8bba\u4e0a\u53ef\u8c03\u8282\u6ce8\u610f\u529b\u5206\u6570\uff1b(2) \u6bcf\u4e2a\u77e9\u9635\u7684\u5de6\u53f3\u5947\u5f02\u5411\u91cf\u4e0a\u5e94\u7528\u4e86\u9ad8\u5ea6\u4e00\u81f4\u7684\u6b63\u4ea4\u53d8\u6362\uff0c\u7834\u574f\u8fd9\u79cd\u4e00\u81f4\u6027\u4f1a\u5bfc\u81f4\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u8bc1\u660e\u5947\u5f02\u503c\u7f29\u653e\u662f\u6b21\u8981\u6548\u5e94\uff0c\u6838\u5fc3\u529f\u80fd\u53d8\u6362\u5728\u4e8e\u5947\u5f02\u5411\u91cf\u7684\u534f\u8c03\u65cb\u8f6c\u3002", "conclusion": "\u540e\u8bad\u7ec3\u5e76\u975e\u968f\u673a\u8c03\u6574\u53c2\u6570\uff0c\u800c\u662f\u4ee5\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u5728\u56fa\u5b9a\u5b50\u7a7a\u95f4\u5185\u8fdb\u884c\u91cd\u53c2\u6570\u5316\uff0c\u63ed\u793a\u4e86\u5927\u6a21\u578b\u53c2\u6570\u6f14\u5316\u4e2d\u7684\u9996\u4e2a\u6e05\u6670\u89c4\u5f8b\uff0c\u4e3a\u6df1\u5165\u7814\u7a76\u6a21\u578b\u53c2\u6570\u53d8\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.17172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17172", "abs": "https://arxiv.org/abs/2509.17172", "authors": ["Djamel Eddine Boukhari"], "title": "SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction", "comment": null, "summary": "The automated prediction of facial beauty is a benchmark task in affective\ncomputing that requires a sophisticated understanding of both local aesthetic\ndetails (e.g., skin texture) and global facial harmony (e.g., symmetry,\nproportions). Existing models, based on either Convolutional Neural Networks\n(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases\nthat limit their performance; CNNs excel at local feature extraction but\nstruggle with long-range dependencies, while ViTs model global relationships at\na significant computational cost. This paper introduces the\n\\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture\nthat resolves this trade-off by delegating specialized roles to\nstate-of-the-art models. The first stream leverages a frozen U-Net encoder from\na pre-trained latent diffusion model, providing a powerful generative prior for\nfine-grained aesthetic qualities. The second stream employs a Vision Mamba\n(Vim), a modern state-space model, to efficiently capture global facial\nstructure with linear-time complexity. By synergistically integrating these\ncomplementary representations through a cross-attention mechanism, MD-Net\ncreates a holistic and nuanced feature space for prediction. Evaluated on the\nSCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson\nCorrelation of \\textbf{0.9235} and demonstrating the significant potential of\nhybrid architectures that fuse generative and sequential modeling paradigms for\ncomplex visual assessment tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9762\u90e8\u7f8e\u5b66\u8bc4\u5206\u7684\u53cc\u6d41\u7f51\u7edcMD-Net\uff0c\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684U-Net\u7f16\u7801\u5668\u548cVision Mamba\uff08Vim\uff09\uff0c\u5728\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u7ed3\u6784\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5728SCUT-FBP5500\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CNN\u548cViT\u6a21\u578b\u5728\u9762\u90e8\u7f8e\u5b66\u9884\u6d4b\u4e2d\u5b58\u5728\u5c40\u9650\uff1aCNN\u96be\u4ee5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\uff0cViT\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u987e\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u7ed3\u6784\u4e14\u9ad8\u6548\u7684\u65b0\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u53cc\u6d41\u67b6\u6784MD-Net\uff1a\u7b2c\u4e00\u6d41\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578bU-Net\u7f16\u7801\u5668\u63d0\u53d6\u7cbe\u7ec6\u7f8e\u5b66\u7279\u5f81\uff1b\u7b2c\u4e8c\u6d41\u91c7\u7528Vision Mamba\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u5efa\u6a21\u5168\u5c40\u9762\u90e8\u7ed3\u6784\uff1b\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u4e24\u8005\u7279\u5f81\u3002", "result": "\u5728SCUT-FBP5500\u6570\u636e\u96c6\u4e0a\uff0cMD-Net\u5b9e\u73b0\u4e860.9235\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MD-Net\u901a\u8fc7\u878d\u5408\u751f\u6210\u5148\u9a8c\u4e0e\u5e8f\u5217\u5efa\u6a21\uff0c\u5728\u9762\u90e8\u7f8e\u611f\u9884\u6d4b\u4efb\u52a1\u4e2d\u6709\u6548\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u5c55\u793a\u4e86\u6df7\u5408\u67b6\u6784\u5728\u590d\u6742\u89c6\u89c9\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17768", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17768", "abs": "https://arxiv.org/abs/2509.17768", "authors": ["Jessica Ojo", "Zina Kamel", "David Ifeoluwa Adelani"], "title": "DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching", "comment": null, "summary": "Language Identification (LID) is a core task in multilingual NLP, yet current\nsystems often overfit to clean, monolingual data. This work introduces\nDIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across\ndiverse domains, including speech transcripts, web text, social media texts,\nchildren's stories, and code-switched text. Our findings reveal that while\nmodels achieve high accuracy on curated datasets, performance degrades sharply\non noisy and informal inputs. We also introduce DIVERS-CS, a diverse\ncode-switching benchmark dataset spanning 10 language pairs, and show that\nexisting models struggle to detect multiple languages within the same sentence.\nThese results highlight the need for more robust and inclusive LID systems in\nreal-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DIVERS-BENCH\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u8bed\u8a00\u8bc6\u522b\u6a21\u578b\u5728\u591a\u6837\u5316\u9886\u57df\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u6a21\u578b\u5728\u566a\u58f0\u548c\u975e\u6b63\u5f0f\u8f93\u5165\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u540c\u65f6\u5f15\u5165\u4e86DIVERS-CS\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u53e5\u5185\u8bed\u7801\u8f6c\u6362\u68c0\u6d4b\u4e0a\u7684\u56f0\u96be\u3002", "motivation": "\u5f53\u524d\u7684\u8bed\u8a00\u8bc6\u522b\u7cfb\u7edf\u5728\u5e72\u51c0\u3001\u5355\u8bed\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6837\u4e14\u5608\u6742\u7684\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u5305\u5bb9\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u591a\u79cd\u9886\u57df\u7684\u7efc\u5408\u8bc4\u6d4b\u57fa\u51c6DIVERS-BENCH\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b10\u4e2a\u8bed\u79cd\u5bf9\u7684\u8bed\u7801\u8f6c\u6362\u6570\u636e\u96c6DIVERS-CS\uff0c\u7528\u4e8e\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u8bc6\u522b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u89c4\u6574\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u566a\u58f0\u548c\u975e\u6b63\u5f0f\u6587\u672c\uff08\u5982\u793e\u4ea4\u5a92\u4f53\u3001\u513f\u7ae5\u6545\u4e8b\u3001\u8bed\u7801\u8f6c\u6362\u6587\u672c\uff09\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u5c24\u5176\u96be\u4ee5\u8bc6\u522b\u53e5\u5b50\u5185\u7684\u591a\u8bed\u8a00\u6df7\u5408\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u5177\u9c81\u68d2\u6027\u548c\u5305\u5bb9\u6027\u7684\u8bed\u8a00\u8bc6\u522b\u7cfb\u7edf\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u591a\u8bed\u8a00\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2509.17870", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17870", "abs": "https://arxiv.org/abs/2509.17870", "authors": ["Xiao Mao", "Albert H. Schrotenboer", "Guohua Wu", "Willem van Jaarsveld"], "title": "Improving After-sales Service: Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences", "comment": null, "summary": "Problem definition: For original equipment manufacturers (OEMs), high-tech\nmaintenance is a strategic component in after-sales services, involving close\ncoordination between customers and service engineers. Each customer suggests\nseveral time slots for their maintenance task, from which the OEM must select\none. This decision needs to be made promptly to support customers' planning. At\nthe end of each day, routes for service engineers are planned to fulfill the\ntasks scheduled for the following day. We study this hierarchical and\nsequential decision-making problem-the Dynamic Time Slot Assignment Problem\nwith Commitments and Customer Preferences (DTSAP-CCP)-in this paper.\nMethodology/results: Two distinct approaches are proposed: 1) an\nattention-based deep reinforcement learning with rollout execution (ADRL-RE)\nand 2) a scenario-based planning approach (SBP). The ADRL-RE combines a\nwell-trained attention-based neural network with a rollout framework for online\ntrajectory simulation. To support the training, we develop a neural heuristic\nsolver that provides rapid route planning solutions, enabling efficient\nlearning in complex combinatorial settings. The SBP approach samples several\nscenarios to guide the time slot assignment. Numerical experiments demonstrate\nthe superiority of ADRL-RE and the stability of SBP compared to both rule-based\nand rollout-based approaches. Furthermore, the strong practicality of ADRL-RE\nis verified in a case study of after-sales service for large medical equipment.\nImplications: This study provides OEMs with practical decision-support tools\nfor dynamic maintenance scheduling, balancing customer preferences and\noperational efficiency. In particular, our ADRL-RE shows strong real-world\npotential, supporting timely and customer-aligned maintenance scheduling.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u539f\u59cb\u8bbe\u5907\u5236\u9020\u5546\u5728\u552e\u540e\u670d\u52a1\u4e2d\u9762\u4e34\u7684\u52a8\u6001\u65f6\u95f4\u69fd\u5206\u914d\u95ee\u9898\uff08DTSAP-CCP\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08ADRL-RE\uff09\u548c\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u89c4\u5212\u7684\u65b9\u6cd5\uff08SBP\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ADRL-RE\u5728\u6ee1\u8db3\u5ba2\u6237\u504f\u597d\u4e0e\u8fd0\u8425\u6548\u7387\u4e4b\u95f4\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u539f\u59cb\u8bbe\u5907\u5236\u9020\u5546\u9700\u8981\u5728\u6ee1\u8db3\u5ba2\u6237\u65f6\u95f4\u504f\u597d\u7684\u540c\u65f6\u9ad8\u6548\u5b89\u6392\u670d\u52a1\u5de5\u7a0b\u5e08\u7684\u7ef4\u62a4\u4efb\u52a1\uff0c\u4e14\u51b3\u7b56\u9700\u5feb\u901f\u54cd\u5e94\u4ee5\u652f\u6301\u5ba2\u6237\u8ba1\u5212\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u52a8\u6001\u6027\u3001\u5c42\u7ea7\u6027\u548c\u5e8f\u5217\u6027\u51b3\u7b56\u7684\u8c03\u5ea6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u56de\u6eda\u6267\u884c\u6846\u67b6\uff08ADRL-RE\uff09\uff0c\u5e76\u8bbe\u8ba1\u795e\u7ecf\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u52a0\u901f\u8bad\u7ec3\uff1b2) \u57fa\u4e8e\u591a\u573a\u666f\u91c7\u6837\u7684\u89c4\u5212\u65b9\u6cd5\uff08SBP\uff09\u6765\u6307\u5bfc\u65f6\u95f4\u69fd\u5206\u914d\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cADRL-RE\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u548c\u56de\u6eda\u7684\u65b9\u6cd5\uff0cSBP\u5177\u6709\u8f83\u597d\u7684\u7a33\u5b9a\u6027\uff1b\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86ADRL-RE\u5728\u5927\u578b\u533b\u7597\u8bbe\u5907\u552e\u540e\u670d\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "ADRL-RE\u662f\u4e00\u79cd\u5177\u6709\u5f3a\u73b0\u5b9e\u5e94\u7528\u6f5c\u529b\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u53ca\u65f6\u3001\u8d34\u8fd1\u5ba2\u6237\u9700\u6c42\u4e14\u9ad8\u6548\u7684\u7ef4\u62a4\u8c03\u5ea6\u3002"}}
{"id": "2509.17187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17187", "abs": "https://arxiv.org/abs/2509.17187", "authors": ["Lalith Bharadwaj Baru", "Kamalaker Dadi", "Tapabrata Chakraborti", "Raju S. Bapi"], "title": "Ambiguous Medical Image Segmentation Using Diffusion Schr\u00f6dinger Bridge", "comment": "MICCAI 2025 (11 pages, 2 figures, 1 table, and 26 references)", "summary": "Accurate segmentation of medical images is challenging due to unclear lesion\nboundaries and mask variability. We introduce \\emph{Segmentation Sch\\\"{o}dinger\nBridge (SSB)}, the first application of Sch\\\"{o}dinger Bridge for ambiguous\nmedical image segmentation, modelling joint image-mask dynamics to enhance\nperformance. SSB preserves structural integrity, delineates unclear boundaries\nwithout additional guidance, and maintains diversity using a novel loss\nfunction. We further propose the \\emph{Diversity Divergence Index} ($D_{DDI}$)\nto quantify inter-rater variability, capturing both diversity and consensus.\nSSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER\n(in-house) datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSch\u00f6dinger Bridge\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5SSB\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u75c5\u7076\u8fb9\u754c\u4e0d\u6e05\u548c\u6807\u6ce8\u53d8\u5f02\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u56e0\u75c5\u7076\u8fb9\u754c\u6a21\u7cca\u548c\u6807\u6ce8\u5dee\u5f02\u5927\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u3001\u6e05\u6670\u8fb9\u754c\u548c\u6807\u6ce8\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165Segmentation Sch\u00f6dinger Bridge\uff08SSB\uff09\uff0c\u9996\u6b21\u5c06Sch\u00f6dinger Bridge\u5e94\u7528\u4e8e\u6a21\u7cca\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5efa\u6a21\u56fe\u50cf-\u63a9\u7801\u8054\u5408\u52a8\u6001\uff1b\u63d0\u51fa\u65b0\u7684\u635f\u5931\u51fd\u6570\u4ee5\u4fdd\u6301\u7ed3\u6784\u548c\u591a\u6837\u6027\uff0c\u5e76\u8bbe\u8ba1Diversity Divergence Index\uff08D_{DDI}\uff09\u91cf\u5316\u591a\u6807\u6ce8\u8005\u7684\u5dee\u5f02\u4e0e\u4e00\u81f4\u6027\u3002", "result": "SSB\u5728LIDC-IDRI\u3001COCA\u548cRACER\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u5f53\u524d\u6700\u4f18\u7684\u5206\u5272\u6027\u80fd\u3002", "conclusion": "SSB\u901a\u8fc7\u5efa\u6a21\u56fe\u50cf\u4e0e\u63a9\u7801\u7684\u8054\u5408\u5206\u5e03\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u7cca\u8fb9\u754c\u548c\u591a\u53d8\u6807\u6ce8\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17788", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.17788", "abs": "https://arxiv.org/abs/2509.17788", "authors": ["Xingyu Fan", "Feifei Li", "Wenhui Que", "Hailong Li"], "title": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts", "comment": "7 pages", "summary": "Conversational agents deployed in industrial-scale official account platforms\nmust generate responses that are both contextually grounded and stylistically\naligned-requirements that existing methods struggle to meet. Chain-of-thought\n(CoT) prompting induces significant latency due to multi-turn reasoning;\nper-account fine-tuning is computationally prohibitive; and long prompt-based\nmethods degrade the model's ability to grasp injected context and style. In\nthis paper, we propose WeStar, a lite-adaptive framework for stylized\ncontextual question answering that scales to millions of official accounts.\nWeStar combines context-grounded generation via RAG with style-aware generation\nusing Parametric RAG (PRAG), where LoRA modules are dynamically activated per\nstyle cluster. Our contributions are fourfold: (1) We introduce WeStar, a\nunified framework capable of serving large volumes of official accounts with\nminimal overhead. (2) We propose a multi-dimensional, cluster-based parameter\nsharing scheme that enables compact style representation while preserving\nstylistic diversity. (3) We develop a style-enhanced Direct Preference\nOptimization (SeDPO) method to optimize each style cluster's parameters for\nimproved generation quality. (4) Experiments on a large-scale industrial\ndataset validate the effectiveness and efficiency of WeStar, underscoring its\npracitical value in real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WeStar\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5b98\u65b9\u8d26\u53f7\u7684\u98ce\u683c\u5316\u4e0a\u4e0b\u6587\u95ee\u7b54\uff0c\u7ed3\u5408RAG\u548c\u57fa\u4e8eLoRA\u7684\u52a8\u6001\u53c2\u6570\u6fc0\u6d3b\uff0c\u5728\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u8bdd\u7cfb\u7edf\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5982CoT\u63d0\u793a\u3001\u9010\u8d26\u6237\u5fae\u8c03\u6216\u957f\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u9ad8\u3001\u8ba1\u7b97\u6210\u672c\u5927\u6216\u4e0a\u4e0b\u6587\u7406\u89e3\u9000\u5316\u7b49\u95ee\u9898\u3002", "method": "WeStar\u7ed3\u5408\u4e86\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u4e0e\u57fa\u4e8e\u53c2\u6570\u5316RAG\uff08PRAG\uff09\u7684\u98ce\u683c\u611f\u77e5\u751f\u6210\uff0c\u91c7\u7528\u6309\u98ce\u683c\u805a\u7c7b\u52a8\u6001\u6fc0\u6d3bLoRA\u6a21\u5757\uff0c\u5e76\u63d0\u51fa\u591a\u7ef4\u805a\u7c7b\u53c2\u6570\u5171\u4eab\u673a\u5236\u548c\u98ce\u683c\u589e\u5f3a\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08SeDPO\uff09\u65b9\u6cd5\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWeStar\u5728\u751f\u6210\u8d28\u91cf\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u6d77\u91cf\u5b98\u65b9\u8d26\u53f7\u7684\u4e2a\u6027\u5316\u54cd\u5e94\u751f\u6210\u3002", "conclusion": "WeStar\u901a\u8fc7\u98ce\u683c\u805a\u7c7b\u4e0e\u52a8\u6001\u53c2\u6570\u9009\u62e9\uff0c\u5728\u4e0d\u589e\u52a0\u663e\u8457\u5f00\u9500\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u4e00\u81f4\u7684\u5bf9\u8bdd\u751f\u6210\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u7ea7\u5927\u89c4\u6a21\u90e8\u7f72\u3002"}}
{"id": "2509.17874", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17874", "abs": "https://arxiv.org/abs/2509.17874", "authors": ["Paulius Rauba", "Mihaela van der Schaar"], "title": "Deep Hierarchical Learning with Nested Subspace Networks", "comment": null, "summary": "Large neural networks are typically trained for a fixed computational budget,\ncreating a rigid trade-off between performance and efficiency that is\nill-suited for deployment in resource-constrained or dynamic environments.\nExisting approaches to this problem present a difficult choice: training a\ndiscrete collection of specialist models is computationally prohibitive, while\ndynamic methods like slimmable networks often lack the flexibility to be\napplied to large, pre-trained foundation models. In this work, we propose\nNested Subspace Networks (NSNs), a novel architectural paradigm that enables a\nsingle model to be dynamically and granularly adjusted across a continuous\nspectrum of compute budgets at inference time. The core of our approach is to\nre-parameterize linear layers to satisfy a nested subspace property, such that\nthe function computed at a given rank is a strict subspace of the function at\nany higher rank. We show that this entire hierarchy of models can be optimized\njointly via an uncertainty-aware objective that learns to balance the\ncontributions of different ranks based on their intrinsic difficulty. We\ndemonstrate empirically that NSNs can be surgically applied to pre-trained LLMs\nand unlock a smooth and predictable compute-performance frontier. For example,\na single NSN-adapted model can achieve a 50% reduction in inference FLOPs with\nonly a 5 percentage point loss in accuracy. Our findings establish NSNs as a\npowerful framework for creating the next generation of adaptive foundation\nmodels.", "AI": {"tldr": "\u63d0\u51faNested Subspace Networks (NSNs)\uff0c\u4e00\u79cd\u53ef\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u7684\u6a21\u578b\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u5b9e\u73b0\u8ba1\u7b97\u91cf\u4e0e\u6027\u80fd\u95f4\u7684\u5e73\u6ed1\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u8bad\u7ec3\u5bfc\u81f4\u7684\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u6216\u52a8\u6001\u73af\u5883\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u53c2\u6570\u5316\u7ebf\u6027\u5c42\u4ee5\u6ee1\u8db3\u5d4c\u5957\u5b50\u7a7a\u95f4\u6027\u8d28\uff0c\u4f7f\u5f97\u4f4e\u79e9\u6a21\u578b\u51fd\u6570\u662f\u9ad8\u79e9\u6a21\u578b\u51fd\u6570\u7684\u4e25\u683c\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u76ee\u6807\u8054\u5408\u4f18\u5316\u6574\u4e2a\u6a21\u578b\u5c42\u7ea7\u3002", "result": "NSN\u53ef\u88ab\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u8c03\u6574\u8ba1\u7b97\u91cf\uff0c\u4f8b\u5982\u51cf\u5c1150% FLOPs\u4ec5\u635f\u59315\u4e2a\u767e\u5206\u70b9\u51c6\u786e\u7387\uff0c\u5c55\u73b0\u51fa\u5e73\u6ed1\u4e14\u53ef\u9884\u6d4b\u7684\u6027\u80fd-\u8ba1\u7b97\u6743\u8861\u66f2\u7ebf\u3002", "conclusion": "NSNs\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u81ea\u9002\u5e94\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u8fde\u7eed\u8ba1\u7b97\u9884\u7b97\u4e0b\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5bb9\u91cf\u3002"}}
{"id": "2509.17190", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17190", "abs": "https://arxiv.org/abs/2509.17190", "authors": ["Kabir Hamzah Muhammad", "Marawan Elbatel", "Yi Qin", "Xiaomeng Li"], "title": "Echo-Path: Pathology-Conditioned Echo Video Generation", "comment": "10 pages, 3 figures, MICCAI-AMAI2025 Workshop", "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nglobally, and echocardiography is critical for diagnosis of both common and\ncongenital cardiac conditions. However, echocardiographic data for certain\npathologies are scarce, hindering the development of robust automated diagnosis\nmodels. In this work, we propose Echo-Path, a novel generative framework to\nproduce echocardiogram videos conditioned on specific cardiac pathologies.\nEcho-Path can synthesize realistic ultrasound video sequences that exhibit\ntargeted abnormalities, focusing here on atrial septal defect (ASD) and\npulmonary arterial hypertension (PAH). Our approach introduces a\npathology-conditioning mechanism into a state-of-the-art echo video generator,\nallowing the model to learn and control disease-specific structural and motion\npatterns in the heart. Quantitative evaluation demonstrates that the synthetic\nvideos achieve low distribution distances, indicating high visual fidelity.\nClinically, the generated echoes exhibit plausible pathology markers.\nFurthermore, classifiers trained on our synthetic data generalize well to real\ndata and, when used to augment real training sets, it improves downstream\ndiagnosis of ASD and PAH by 7\\% and 8\\% respectively. Code, weights and dataset\nare available here https://github.com/Marshall-mk/EchoPathv1", "AI": {"tldr": "\u63d0\u51faEcho-Path\uff0c\u4e00\u79cd\u57fa\u4e8e\u75c5\u7406\u6761\u4ef6\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u5177\u6709\u7279\u5b9a\u5fc3\u810f\u5f02\u5e38\uff08\u5982ASD\u548cPAH\uff09\u7684\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\uff0c\u751f\u6210\u7684\u89c6\u9891\u5177\u6709\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5e76\u80fd\u63d0\u5347\u4e0b\u6e38\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u67d0\u4e9b\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVDs\uff09\u7684\u8d85\u58f0\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u8bca\u65ad\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u75c5\u7406\u7279\u5f02\u6027\u7684\u8d85\u58f0\u89c6\u9891\u4ee5\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u5728\u5148\u8fdb\u7684\u8d85\u58f0\u89c6\u9891\u751f\u6210\u6a21\u578b\u57fa\u7840\u4e0a\u5f15\u5165\u75c5\u7406\u6761\u4ef6\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5e76\u63a7\u5236\u75be\u75c5\u76f8\u5173\u7684\u7ed3\u6784\u548c\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4ece\u800c\u751f\u6210\u7279\u5b9a\u75c5\u7406\uff08\u5982\u623f\u7f3aASD\u548c\u80ba\u52a8\u8109\u9ad8\u538bPAH\uff09\u7684\u8d85\u58f0\u89c6\u9891\u3002", "result": "\u5408\u6210\u89c6\u9891\u5728\u5206\u5e03\u8ddd\u79bb\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u9ad8\uff1b\u4e34\u5e8a\u8bc4\u4f30\u663e\u793a\u5177\u6709\u5408\u7406\u7684\u75c5\u7406\u7279\u5f81\uff1b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u6d4b\u8bd5\u65f6\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4e14\u5c06\u5408\u6210\u6570\u636e\u7528\u4e8e\u589e\u5f3a\u8bad\u7ec3\u96c6\u53ef\u4f7fASD\u548cPAH\u7684\u8bca\u65ad\u51c6\u786e\u7387\u5206\u522b\u63d0\u53477%\u548c8%\u3002", "conclusion": "Echo-Path\u80fd\u591f\u6709\u6548\u751f\u6210\u5177\u6709\u4e34\u5e8a\u5408\u7406\u6027\u548c\u8bca\u65ad\u5b9e\u7528\u6027\u7684\u75c5\u7406\u7279\u5f02\u6027\u8d85\u58f0\u89c6\u9891\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u5fc3\u810f\u75c5\u81ea\u52a8\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17794", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17794", "abs": "https://arxiv.org/abs/2509.17794", "authors": ["Tobias Groot", "Salo Lacunes", "Evgenia Ilia"], "title": "Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction", "comment": "EMNLP UncertaiNLP Workshop 2025", "summary": "Natural language generation (NLG) tasks are often subject to inherent\nvariability; \\emph{e.g.} predicting the next word given a context has multiple\nvalid responses, evident when asking multiple humans to complete the task.\nWhile having language models (LMs) that are aligned pluralistically, so that\nthey are able to reproduce well the inherent diversity in perspectives of an\nentire population of interest is clearly beneficial, \\citet{ilia2024predict}\nshow that LMs do not reproduce this type of linguistic variability well. They\nspeculate this inability might stem from the lack of consistent training of LMs\nwith data reflecting this type of inherent variability. As such, we investigate\nwhether training LMs on multiple plausible word continuations per context can\nimprove their ability to reproduce human linguistic variability for next-word\nprediction. We employ fine-tuning techniques for pre-trained and\ninstruction-tuned models; and demonstrate their potential when fine-tuning\nGPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures\ndivergence among empirically estimated human and model next-word distributions\nacross contexts before and after fine-tuning, shows that our multi-label\nfine-tuning improves the LMs' ability to reproduce linguistic variability; both\nfor contexts that admit higher and lower variability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u5728\u591a\u4e2a\u5408\u7406\u7684\u8bcd\u5ef6\u7eed\u4e0a\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\uff0c\u4ee5\u63d0\u5347\u5176\u518d\u73b0\u4eba\u7c7b\u8bed\u8a00\u591a\u6837\u6027\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u4f7f\u7528Provo\u8bed\u6599\u5e93\u5bf9GPT-2\u548cMistral-7B-IT\u8fdb\u884c\u591a\u6807\u7b7e\u5fae\u8c03\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u6a21\u578b\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u8bed\u8a00\u53d8\u5f02\u6027\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u5f88\u597d\u5730\u518d\u73b0\u4eba\u7c7b\u8bed\u8a00\u4e2d\u7684\u56fa\u6709\u53d8\u5f02\u6027\uff0c\u8fd9\u53ef\u80fd\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u53cd\u6620\u8fd9\u79cd\u591a\u6837\u6027\u7684\u591a plausible \u5ef6\u7eed\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u591a plausible \u4e0b\u4e00\u8bcd\u7684\u8bad\u7ec3\u65b9\u5f0f\u6765\u63d0\u5347\u6a21\u578b\u7684\u8bed\u8a00\u591a\u6837\u6027\u751f\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u6807\u7b7e\u5fae\u8c03\u6280\u672f\uff0c\u5728\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u7684\u8bed\u8a00\u6a21\u578b\uff08GPT-2 \u548c Mistral-7B-IT\uff09\u4e0a\u4f7f\u7528 Provo \u8bed\u6599\u5e93\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u5206\u5e03\u4e0e\u4eba\u7c7b\u771f\u5b9e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u53d8\u5316\u3002", "result": "\u591a\u6807\u7b7e\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u518d\u73b0\u4eba\u7c7b\u8bed\u8a00\u53d8\u5f02\u6027\u7684\u80fd\u529b\uff0c\u65e0\u8bba\u662f\u5728\u9ad8\u53d8\u5f02\u6027\u8fd8\u662f\u4f4e\u53d8\u5f02\u6027\u4e0a\u4e0b\u6587\u4e2d\u5747\u8868\u73b0\u66f4\u597d\u3002\u6a21\u578b\u8f93\u51fa\u7684\u8bcd\u5206\u5e03\u66f4\u63a5\u8fd1\u5b9e\u8bc1\u4f30\u8ba1\u7684\u4eba\u7c7b\u53cd\u5e94\u5206\u5e03\u3002", "conclusion": "\u901a\u8fc7\u5728\u5177\u6709\u591a\u79cd\u5408\u7406\u5ef6\u7eed\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u8bed\u8a00\u591a\u6837\u6027\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u8868\u660e\u8bad\u7ec3\u7b56\u7565\u5bf9\u63d0\u5347\u6a21\u578b\u7684\u591a\u5143\u5316\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.17885", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17885", "abs": "https://arxiv.org/abs/2509.17885", "authors": ["Saad Mokssit", "Ouassim Karrakchou", "Alejandro Mousist", "Mounir Ghogho"], "title": "Confidence-gated training for efficient early-exit neural networks", "comment": null, "summary": "Early-exit neural networks reduce inference cost by enabling confident\npredictions at intermediate layers. However, joint training often leads to\ngradient interference, with deeper classifiers dominating optimization. We\npropose Confidence-Gated Training (CGT), a paradigm that conditionally\npropagates gradients from deeper exits only when preceding exits fail. This\nencourages shallow classifiers to act as primary decision points while\nreserving deeper layers for harder inputs. By aligning training with the\ninference-time policy, CGT mitigates overthinking, improves early-exit\naccuracy, and preserves efficiency. Experiments on the Indian Pines and\nFashion-MNIST benchmarks show that CGT lowers average inference cost while\nimproving overall accuracy, offering a practical solution for deploying deep\nmodels in resource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConfidence-Gated Training (CGT)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u4ec5\u5f53\u6d45\u5c42\u51fa\u53e3\u5931\u8d25\u65f6\u624d\u4f20\u64ad\u6df1\u5c42\u68af\u5ea6\uff0c\u4ee5\u51cf\u5c11\u65e9\u671f\u9000\u51fa\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u68af\u5ea6\u5e72\u6270\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65e9\u671f\u9000\u51fa\u795e\u7ecf\u7f51\u7edc\u5728\u8054\u5408\u8bad\u7ec3\u4e2d\u5e38\u51fa\u73b0\u6df1\u5c42\u5206\u7c7b\u5668\u4e3b\u5bfc\u4f18\u5316\u8fc7\u7a0b\u7684\u68af\u5ea6\u5e72\u6270\u95ee\u9898\uff0c\u5bfc\u81f4\u6d45\u5c42\u5206\u7c7b\u5668\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faConfidence-Gated Training (CGT)\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ec5\u5f53\u524d\u9762\u7684\u51fa\u53e3\u672a\u80fd\u6b63\u786e\u9884\u6d4b\u65f6\uff0c\u624d\u5141\u8bb8\u6df1\u5c42\u51fa\u53e3\u4f20\u64ad\u68af\u5ea6\uff0c\u4f7f\u6d45\u5c42\u5206\u7c7b\u5668\u4f18\u5148\u51b3\u7b56\uff0c\u6df1\u5c42\u4fdd\u7559\u5904\u7406\u96be\u6837\u672c\u3002", "result": "\u5728Indian Pines\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCGT\u964d\u4f4e\u4e86\u5e73\u5747\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6574\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "CGT\u6709\u6548\u5bf9\u9f50\u4e86\u8bad\u7ec3\u4e0e\u63a8\u7406\u7b56\u7565\uff0c\u7f13\u89e3\u4e86\u2018\u8fc7\u5ea6\u601d\u8003\u2019\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65e9\u671f\u9000\u51fa\u6a21\u578b\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6df1\u5ea6\u6a21\u578b\u90e8\u7f72\u3002"}}
{"id": "2509.17191", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17191", "abs": "https://arxiv.org/abs/2509.17191", "authors": ["Jinchao Ge", "Tengfei Cheng", "Biao Wu", "Zeyu Zhang", "Shiya Huang", "Judith Bishop", "Gillian Shepherd", "Meng Fang", "Ling Chen", "Yang Zhao"], "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery", "comment": null, "summary": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVaseVL\u7cfb\u7edf\uff0c\u901a\u8fc7SFT\u540e\u63a5\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u7c7b\u578b\u6761\u4ef6\u548c\u7ec4\u5408\u6027\u5bfc\u5411\u7684\u5956\u52b1\u673a\u5236\uff0c\u63d0\u5347MLLMs\u5728\u53e4\u5e0c\u814a\u9676\u5668\u9274\u5b9a\u4e0e\u5386\u53f2\u5f52\u5c5e\u4e2d\u7684\u9c81\u68d2\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u5305\u542b31,773\u5f20\u56fe\u50cf\u7684\u57fa\u51c6\u6570\u636e\u96c6VaseVQA\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u6587\u5316\u9057\u4ea7\u5206\u6790\u4e2d\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6613\u8fc7\u62df\u5408\u8868\u9762\u6a21\u5f0f\uff0c\u5bfc\u81f4\u63a8\u7406\u8106\u5f31\uff0c\u96be\u4ee5\u80dc\u4efb\u6587\u7269\u8ba4\u8bc1\u4e0e\u5386\u53f2\u5f52\u56e0\u4efb\u52a1\u3002", "method": "\u63d0\u51faVaseVL\u6846\u67b6\uff1a\u9996\u5148\u6784\u5efa\u95ee\u9898\u7c7b\u578b\u5206\u7c7b\u4f53\u7cfb\uff0c\u901a\u8fc7\u63a2\u9488\u8bc6\u522bSFT\u6a21\u578b\u7684\u6027\u80fd\u77ed\u677f\uff0c\u968f\u540e\u91c7\u7528\u57fa\u4e8e\u7c7b\u578b\u6761\u4ef6\u548c\u7ec4\u5408\u6027\u8bbe\u8ba1\u7684\u5956\u52b1\u673a\u5236\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002\u540c\u65f6\u53d1\u5e03\u5927\u89c4\u6a21\u6570\u636e\u96c6VaseVQA\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5728\u98ce\u683c\u5206\u7c7b\u548c\u5386\u53f2\u5f52\u56e0\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528SFT\u7684\u57fa\u7ebf\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bca\u65ad\u5f15\u5bfc\u3001\u7c7b\u578b\u6761\u4ef6\u5316\u7684\u5956\u52b1\u5de5\u7a0b\u80fd\u6709\u6548\u589e\u5f3aMLLMs\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u7684\u6df1\u5c42\u63a8\u7406\u80fd\u529b\uff0c\u6240\u63d0\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u4e3a\u6587\u5316\u9057\u4ea7\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.17796", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17796", "abs": "https://arxiv.org/abs/2509.17796", "authors": ["Michal Nov\u00e1k", "Miloslav Konop\u00edk", "Anna Nedoluzhko", "Martin Popel", "Ond\u0159ej Pra\u017e\u00e1k", "Jakub Sido", "Milan Straka", "Zden\u011bk \u017dabokrtsk\u00fd", "Daniel Zeman"], "title": "Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?", "comment": "Accepted to CODI-CRAC 2025", "summary": "The paper presents an overview of the fourth edition of the Shared Task on\nMultilingual Coreference Resolution, organized as part of the CODI-CRAC 2025\nworkshop. As in the previous editions, participants were challenged to develop\nsystems that identify mentions and cluster them according to identity\ncoreference.\n  A key innovation of this year's task was the introduction of a dedicated\nLarge Language Model (LLM) track, featuring a simplified plaintext format\ndesigned to be more suitable for LLMs than the original CoNLL-U representation.\n  The task also expanded its coverage with three new datasets in two additional\nlanguages, using version 1.3 of CorefUD - a harmonized multilingual collection\nof 22 datasets in 17 languages.\n  In total, nine systems participated, including four LLM-based approaches (two\nfine-tuned and two using few-shot adaptation). While traditional systems still\nkept the lead, LLMs showed clear potential, suggesting they may soon challenge\nestablished approaches in future editions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e862025\u5e74CODI-CRAC\u7814\u8ba8\u4f1a\u4e2d\u7b2c\u56db\u5c4a\u591a\u8bed\u8a00\u5171\u6307\u6d88\u89e3\u5171\u4eab\u4efb\u52a1\uff0c\u65b0\u589eLLM\u8d5b\u9053\u548c\u4e09\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u663e\u793a\u4f20\u7edf\u65b9\u6cd5\u4ecd\u9886\u5148\uff0c\u4f46\u5927\u6a21\u578b\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "motivation": "\u63a8\u52a8\u591a\u8bed\u8a00\u5171\u6307\u6d88\u89e3\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u548c\u6f5c\u529b\u3002", "method": "\u8bbe\u7acb\u4f20\u7edf\u7cfb\u7edf\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e24\u4e2a\u8d5b\u9053\uff0c\u4f7f\u7528CorefUD v1.3\u5305\u542b22\u4e2a\u6570\u636e\u96c6\uff0817\u79cd\u8bed\u8a00\uff09\u8fdb\u884c\u8bc4\u4f30\uff0cLLM\u8d5b\u9053\u91c7\u7528\u7b80\u5316\u7684\u7eaf\u6587\u672c\u683c\u5f0f\u3002", "result": "\u5171\u6709\u4e5d\u4e2a\u7cfb\u7edf\u53c2\u4e0e\uff0c\u5176\u4e2d\u56db\u4e2a\u57fa\u4e8eLLM\uff08\u4e24\u4e2a\u5fae\u8c03\uff0c\u4e24\u4e2a\u5c11\u6837\u672c\u9002\u914d\uff09\uff1b\u4f20\u7edf\u7cfb\u7edf\u8868\u73b0\u4ecd\u4f18\u4e8eLLM\uff0c\u4f46LLM\u663e\u793a\u51fa\u5f3a\u52b2\u6f5c\u529b\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u4e2d\u5df2\u63a5\u8fd1\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\uff0c\u672a\u6765\u6709\u671b\u53d6\u4ee3\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u3002"}}
{"id": "2509.17889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17889", "abs": "https://arxiv.org/abs/2509.17889", "authors": ["Phuong Mai Dinh", "Van-Nam Huynh"], "title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring the Pareto frontier in multi-criteria optimization", "comment": null, "summary": "Multi-objective optimization (MOO) is essential for solving complex\nreal-world problems involving multiple conflicting objectives. However, many\npractical applications - including engineering design, autonomous systems, and\nmachine learning - often yield non-convex, degenerate, or discontinuous Pareto\nfrontiers, which involve traditional scalarization and Pareto Set Learning\n(PSL) methods that struggle to approximate accurately. Existing PSL approaches\nperform well on convex fronts but tend to fail in capturing the diversity and\nstructure of irregular Pareto sets commonly observed in real-world scenarios.\nIn this paper, we propose Gaussian-PSL, a novel framework that integrates\nGaussian Splatting into PSL to address the challenges posed by non-convex\nPareto frontiers. Our method dynamically partitions the preference vector\nspace, enabling simple MLP networks to learn localized features within each\nregion, which are then integrated by an additional MLP aggregator. This\npartition-aware strategy enhances both exploration and convergence, reduces\nsensi- tivity to initialization, and improves robustness against local optima.\nWe first provide the mathematical formulation for controllable Pareto set\nlearning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL\narchitecture and evaluate its performance on synthetic and real-world\nmulti-objective benchmarks. Experimental results demonstrate that our approach\noutperforms standard PSL models in learning irregular Pareto fronts while\nmaintaining computational efficiency and model simplicity. This work offers a\nnew direction for effective and scalable MOO under challenging frontier\ngeometries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6Gaussian-PSL\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u65af\u70b9\u9635\u5316\u6280\u672f\u6539\u8fdbPareto\u96c6\u5b66\u4e60\uff0c\u6709\u6548\u5e94\u5bf9\u975e\u51f8\u3001\u9000\u5316\u6216\u4e0d\u8fde\u7eed\u7684Pareto\u524d\u6cbf\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u591a\u6837\u6027\u3001\u6536\u655b\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfPareto\u96c6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u975e\u51f8\u3001\u9000\u5316\u6216\u4e0d\u8fde\u7eed\u7684Pareto\u524d\u6cbf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u6355\u6349\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u4e0d\u89c4\u5219Pareto\u96c6\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faGaussian-PSL\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u65af\u70b9\u9635\u5316\u52a8\u6001\u5212\u5206\u504f\u597d\u5411\u91cf\u7a7a\u95f4\uff0c\u4f7f\u7528\u7b80\u5355MLP\u7f51\u7edc\u5b66\u4e60\u5404\u533a\u57df\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u989d\u5916\u7684MLP\u805a\u5408\u5668\u6574\u5408\u7ed3\u679c\uff0c\u5b9e\u73b0\u5bf9\u4e0d\u89c4\u5219Pareto\u96c6\u7684\u7cbe\u786e\u5efa\u6a21\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u591a\u76ee\u6807\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGaussian-PSL\u5728\u5b66\u4e60\u4e0d\u89c4\u5219Pareto\u524d\u6cbf\u65b9\u9762\u4f18\u4e8e\u6807\u51c6PSL\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u7b80\u6d01\u6027\u3002", "conclusion": "Gaussian-PSL\u4e3a\u5177\u6709\u6311\u6218\u6027\u524d\u6cbf\u51e0\u4f55\u5f62\u72b6\u7684\u591a\u76ee\u6807\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b0\u65b9\u5411\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709Pareto\u96c6\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17206", "abs": "https://arxiv.org/abs/2509.17206", "authors": ["Gunner Stone", "Sushmita Sarker", "Alireza Tavakkoli"], "title": "Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation", "comment": null, "summary": "Generating realistic 3D point clouds is a fundamental problem in computer\nvision with applications in remote sensing, robotics, and digital object\nmodeling. Existing generative approaches primarily capture geometry, and when\nsemantics are considered, they are typically imposed post hoc through external\nsegmentation or clustering rather than integrated into the generative process\nitself. We propose a diffusion-based framework that embeds per-point semantic\nconditioning directly within generation. Each point is associated with a\nconditional variable corresponding to its semantic label, which guides the\ndiffusion dynamics and enables the joint synthesis of geometry and semantics.\nThis design produces point clouds that are both structurally coherent and\nsegmentation-aware, with object parts explicitly represented during synthesis.\nThrough a comparative analysis of guided and unguided diffusion processes, we\ndemonstrate the significant impact of conditional variables on diffusion\ndynamics and generation quality. Extensive experiments validate the efficacy of\nour approach, producing detailed and accurate 3D point clouds tailored to\nspecific parts and features.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u9010\u70b9\u8bed\u4e49\u6761\u4ef6\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e0e\u8bed\u4e49\u7684\u8054\u5408\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u7ed3\u6784\uff0c\u8bed\u4e49\u4fe1\u606f\u901a\u5e38\u4e8b\u540e\u6dfb\u52a0\uff0c\u7f3a\u4e4f\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bf9\u8bed\u4e49\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u6bcf\u4e2a\u70b9\u5173\u8054\u4e00\u4e2a\u8bed\u4e49\u6807\u7b7e\u4f5c\u4e3a\u6761\u4ef6\u53d8\u91cf\uff0c\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e0e\u8bed\u4e49\u7684\u8054\u5408\u751f\u6210\u3002", "result": "\u751f\u6210\u7684\u70b9\u4e91\u7ed3\u6784\u8fde\u8d2f\u4e14\u5177\u5907\u5206\u5272\u610f\u8bc6\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u7ec6\u8282\u4e30\u5bcc\u3001\u51c6\u786e\u76843D\u70b9\u4e91\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u6761\u4ef6\u53d8\u91cf\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u51e0\u4f55\u4e0e\u8bed\u4e49\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2509.17807", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17807", "abs": "https://arxiv.org/abs/2509.17807", "authors": ["Jihae Jeong", "DaeYeop Lee", "DongGeon Lee", "Hwanjo Yu"], "title": "Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark", "comment": "Accepted to MRL@EMNLP 2025", "summary": "Existing physical commonsense reasoning benchmarks predominantly focus on\nWestern contexts, overlooking cultural variations in physical problem-solving.\nTo address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a\nnovel benchmark comprising 181 binary-choice problems that test physical\nreasoning within Korean cultural contexts, ranging from kimchi (Korean food) to\ntraditional fermentation. EPiK is constructed using a two-stage generation and\nverification pipeline to create culturally-authentic problems across 9\nreasoning subtasks and 84 scenarios. Unlike approaches based on simple\ntranslation, our method generates problems organically from Korean contexts\nwhile upholding rigorous physical reasoning standards. Our evaluations show\nthat Korean-specialized models consistently outperform general-purpose models\nof comparable size. This performance gap highlights the limitations of\nculturally-agnostic models and demonstrates the critical need for\nculturally-aware benchmarks to truly measure language understanding. Our EPiK\nis publicly available at https://huggingface.co/datasets/jjae/EPiK.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86EPiK\uff08\u97e9\u56fd\u8bed\u5883\u4e2d\u7684\u65e5\u5e38\u7269\u7406\uff09\uff0c\u4e00\u4e2a\u5305\u542b181\u4e2a\u4e8c\u5143\u9009\u62e9\u9898\u7684\u65b0\u57fa\u51c6\uff0c\u65e8\u5728\u6d4b\u8bd5\u97e9\u56fd\u6587\u5316\u80cc\u666f\u4e0b\u7684\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u5f3a\u8c03\u6587\u5316\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u897f\u65b9\u8bed\u5883\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u5dee\u5f02\u5bf9\u7269\u7406\u95ee\u9898\u89e3\u51b3\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u53cd\u6620\u975e\u897f\u65b9\u6587\u5316\u80cc\u666f\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u751f\u6210\u4e0e\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u4ece\u97e9\u56fd\u6587\u5316\u8bed\u5883\uff08\u5982\u6ce1\u83dc\u3001\u4f20\u7edf\u53d1\u9175\uff09\u4e2d\u6709\u673a\u751f\u6210\u95ee\u9898\uff0c\u6db5\u76d69\u4e2a\u63a8\u7406\u5b50\u4efb\u52a1\u548c84\u79cd\u573a\u666f\uff0c\u5e76\u786e\u4fdd\u7269\u7406\u63a8\u7406\u7684\u4e25\u8c28\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9488\u5bf9\u97e9\u8bed\u4f18\u5316\u7684\u6a21\u578b\u5728EPiK\u4e0a\u6301\u7eed\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u7684\u901a\u7528\u6a21\u578b\uff0c\u663e\u793a\u51fa\u6587\u5316\u76f8\u5173\u6a21\u578b\u7684\u4f18\u52bf\u3002", "conclusion": "\u6587\u5316\u65e0\u5173\u7684\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u6784\u5efa\u6587\u5316\u654f\u611f\u7684\u57fa\u51c6\u5bf9\u4e8e\u771f\u6b63\u8861\u91cf\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0cEPiK\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u516c\u5f00\u53ef\u7528\u8d44\u6e90\u3002"}}
{"id": "2509.17894", "categories": ["cs.LG", "68T07", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.17894", "abs": "https://arxiv.org/abs/2509.17894", "authors": ["Siu Hang Ho", "Prasad Ganesan", "Nguyen Duong", "Daniel Schlabig"], "title": "Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark", "comment": "10 pages, 5 figures. Technical report", "summary": "Efficient inference is a critical challenge in deep generative modeling,\nparticularly as diffusion models grow in capacity and complexity. While\nincreased complexity often improves accuracy, it raises compute costs, latency,\nand memory requirements. This work investigates techniques such as pruning,\nquantization, knowledge distillation, and simplified attention to reduce\ncomputational overhead without impacting performance. The study also explores\nthe Mixture of Experts (MoE) approach to further enhance efficiency. These\nexperiments provide insights into optimizing inference for the state-of-the-art\nFast Diffusion Transformer (fast-DiT) model.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u79cd\u6280\u672f\uff08\u5982\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u7b80\u5316\u6ce8\u610f\u529b\uff09\u4ee5\u53ca\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08MoE\uff09\u6765\u4f18\u5316\u5feb\u901f\u6269\u6563\u53d8\u6362\u5668\uff08fast-DiT\uff09\u7684\u63a8\u7406\u6548\u7387\uff0c\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u7684\u5bb9\u91cf\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u63a8\u7406\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u5185\u5b58\u9700\u6c42\u4e0a\u5347\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u3001\u7b80\u5316\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u53caMixture of Experts\uff08MoE\uff09\u65b9\u6cd5\u5bf9fast-DiT\u6a21\u578b\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u91c7\u7528\u7684\u6280\u672f\u80fd\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5176\u4e2dMoE\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u7ed3\u5408\u591a\u79cd\u6a21\u578b\u538b\u7f29\u4e0e\u67b6\u6784\u4f18\u5316\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u6269\u6563\u53d8\u6362\u5668\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u9ad8\u6027\u80fd\u751f\u6210\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2509.17207", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17207", "abs": "https://arxiv.org/abs/2509.17207", "authors": ["Gunner Stone", "Youngsook Choi", "Alireza Tavakkoli", "Ankita Shukla"], "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds", "comment": null, "summary": "Pre-training strategies play a critical role in advancing the performance of\ntransformer-based models for 3D point cloud tasks. In this paper, we introduce\nPoint-RTD (Replaced Token Denoising), a novel pretraining strategy designed to\nimprove token robustness through a corruption-reconstruction framework. Unlike\ntraditional mask-based reconstruction tasks that hide data segments for later\nprediction, Point-RTD corrupts point cloud tokens and leverages a\ndiscriminator-generator architecture for denoising. This shift enables more\neffective learning of structural priors and significantly enhances model\nperformance and efficiency. On the ShapeNet dataset, Point-RTD reduces\nreconstruction error by over 93% compared to PointMAE, and achieves more than\n14x lower Chamfer Distance on the test set. Our method also converges faster\nand yields higher classification accuracy on ShapeNet, ModelNet10, and\nModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework\nin every case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoint-RTD\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8eTransformer\u76843D\u70b9\u4e91\u6a21\u578b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u66ff\u6362\u4ee4\u724c\u53bb\u566a\uff08Replaced Token Denoising\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u635f\u574f-\u91cd\u5efa\u673a\u5236\u589e\u5f3a\u4ee4\u724c\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165\u5224\u522b\u5668-\u751f\u6210\u5668\u67b6\u6784\u8fdb\u884c\u53bb\u566a\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u63a9\u7801\u91cd\u5efa\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPoint-RTD\u5728ShapeNet\u7b49\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8ePointMAE\uff0c\u5728\u91cd\u5efa\u8bef\u5dee\u3001Chamfer\u8ddd\u79bb\u3001\u6536\u655b\u901f\u5ea6\u548c\u5206\u7c7b\u7cbe\u5ea6\u65b9\u9762\u5747\u6709\u5927\u5e45\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63a9\u7801\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u5904\u74063D\u70b9\u4e91\u65f6\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u7ed3\u6784\u5148\u9a8c\u5b66\u4e60\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u5b66\u4e60\u70b9\u4e91\u7684\u51e0\u4f55\u7ed3\u6784\u5e76\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u63d0\u51faPoint-RTD\uff08Replaced Token Denoising\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u66ff\u6362\u70b9\u4e91token\u5b9e\u73b0\u6570\u636e\u635f\u574f\uff0c\u5e76\u8bbe\u8ba1\u5224\u522b\u5668-\u751f\u6210\u5668\u67b6\u6784\uff1a\u5224\u522b\u5668\u8bc6\u522b\u88ab\u66ff\u6362\u7684\u4f4d\u7f6e\uff0c\u751f\u6210\u5668\u8d1f\u8d23\u91cd\u5efa\u539f\u59cbtoken\u3002\u8be5\u65b9\u6cd5\u6452\u5f03\u4e86\u4f20\u7edf\u7684\u63a9\u7801\u91cd\u5efa\u65b9\u5f0f\uff0c\u8f6c\u800c\u91c7\u7528\u66f4\u5177\u6311\u6218\u6027\u7684\u66ff\u6362\u673a\u5236\uff0c\u8feb\u4f7f\u6a21\u578b\u6df1\u5165\u7406\u89e3\u70b9\u4e91\u7ed3\u6784\u3002", "result": "\u5728ShapeNet\u4e0a\uff0c\u76f8\u6bd4PointMAE\uff0c\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e\u8d85\u8fc793%\uff0c\u6d4b\u8bd5\u96c6Chamfer Distance\u964d\u4f4e14\u500d\u4ee5\u4e0a\uff1b\u540c\u65f6\u5728ShapeNet\u3001ModelNet10\u548cModelNet40\u4e0a\u5206\u7c7b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "Point-RTD\u901a\u8fc7\u5f15\u5165\u66ff\u6362\u4ee4\u724c\u53bb\u566a\u673a\u5236\u548c\u5224\u522b\u5668-\u751f\u6210\u5668\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u4e0e\u6cdb\u5316\u6027\u80fd\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2509.17829", "categories": ["cs.CL", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.17829", "abs": "https://arxiv.org/abs/2509.17829", "authors": ["Manoj Madushanka Perera", "Adnan Mahmood", "Kasun Eranda Wijethilake", "Quan Z. Sheng"], "title": "Towards Adaptive Context Management for Intelligent Conversational Question Answering", "comment": "Comments: 15 pages, 6 figures, Table 1, published in Lecture Notes in\n  Computer Science (LNCS 15391), Proceedings of ADMA 2024. DOI:\n  10.1007/978-981-96-0847-8_25", "summary": "This particular paper introduces an Adaptive Context Management (ACM)\nframework for the Conversational Question Answering (ConvQA) systems. The key\nobjective of the ACM framework is to optimize the use of the conversation\nhistory by dynamically managing context for maximizing the relevant information\nprovided to a ConvQA model within its token limit. Our approach incorporates a\nContext Manager (CM) Module, a Summarization (SM) Module, and an Entity\nExtraction (EE) Module in a bid to handle the conversation history\nefficaciously. The CM Module dynamically adjusts the context size, thereby\npreserving the most relevant and recent information within a model's token\nlimit. The SM Module summarizes the older parts of the conversation history via\na sliding window. When the summarization window exceeds its limit, the EE\nModule identifies and retains key entities from the oldest conversation turns.\nExperimental results demonstrate the effectiveness of our envisaged framework\nin generating accurate and contextually appropriate responses, thereby\nhighlighting the potential of the ACM framework to enhance the robustness and\nscalability of the ConvQA systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5bf9\u8bdd\u5f0f\u95ee\u7b54\u7cfb\u7edf\uff08ConvQA\uff09\u7684\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u7ba1\u7406\uff08ACM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ba1\u7406\u5bf9\u8bdd\u5386\u53f2\u4ee5\u4f18\u5316\u6a21\u578b\u5728\u6709\u9650token\u5185\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6a21\u578b\u8f93\u5165\u957f\u5ea6\u9650\u5236\uff0c\u4f20\u7edfConvQA\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u5229\u7528\u957f\u5bf9\u8bdd\u5386\u53f2\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u56de\u7b54\u4e0d\u51c6\u786e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u4f18\u5316\u4e0a\u4e0b\u6587\u4f7f\u7528\u7684\u65b9\u6cd5\u3002", "method": "ACM\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u4e0a\u4e0b\u6587\u7ba1\u7406\uff08CM\uff09\u6a21\u5757\u52a8\u6001\u8c03\u6574\u4e0a\u4e0b\u6587\u5927\u5c0f\uff1b\u6458\u8981\uff08SM\uff09\u6a21\u5757\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5bf9\u8f83\u65e9\u5bf9\u8bdd\u8fdb\u884c\u6458\u8981\uff1b\u5b9e\u4f53\u63d0\u53d6\uff08EE\uff09\u6a21\u5757\u5728\u6458\u8981\u7a97\u53e3\u8d85\u9650\u65f6\u4fdd\u7559\u6700\u8001\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u5b9e\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u751f\u6210\u51c6\u786e\u4e14\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u56de\u7b54\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u5904\u7406\u957f\u5bf9\u8bdd\u65f6\u7684\u8868\u73b0\u3002", "conclusion": "ACM\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86ConvQA\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u957f\u5bf9\u8bdd\u573a\u666f\u4e0b\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17920", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17920", "abs": "https://arxiv.org/abs/2509.17920", "authors": ["Jamiyan Sukhbaatar", "Satoshi Imamura", "Ibuki Inoue", "Shoya Murakami", "Kazi Mahmudul Hassan", "Seungwoo Han", "Ingon Chanpornpakdi", "Toshihisa Tanaka"], "title": "SingLEM: Single-Channel Large EEG Model", "comment": null, "summary": "Current deep learning models for electroencephalography (EEG) are often\ntask-specific and depend on large labeled datasets, limiting their\nadaptability. Although emerging foundation models aim for broader\napplicability, their rigid dependence on fixed, high-density multi-channel\nmontages restricts their use across heterogeneous datasets and in\nmissing-channel or practical low-channel settings. To address these\nlimitations, we introduce SingLEM, a self-supervised foundation model that\nlearns robust, general-purpose representations from single-channel EEG, making\nit inherently hardware agnostic. The model employs a hybrid encoder\narchitecture that combines convolutional layers to extract local features with\na hierarchical transformer to model both short- and long-range temporal\ndependencies. SingLEM is pretrained on 71 public datasets comprising over 9,200\nsubjects and 357,000 single-channel hours of EEG. When evaluated as a fixed\nfeature extractor across six motor imagery and cognitive tasks, aggregated\nsingle-channel representations consistently outperformed leading multi-channel\nfoundation models and handcrafted baselines. These results demonstrate that a\nsingle-channel approach can achieve state-of-the-art generalization while\nenabling fine-grained neurophysiological analysis and enhancing\ninterpretability. The source code and pretrained models are available at\nhttps://github.com/ttlabtuat/SingLEM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSingLEM\u7684\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u901a\u9053EEG\u5b66\u4e60\u9c81\u68d2\u3001\u901a\u7528\u7684\u8868\u793a\uff0c\u91c7\u7528\u6df7\u5408\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u591a\u901a\u9053\u6a21\u578b\u548c\u624b\u5de5\u7279\u5f81\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u548c\u56fa\u5b9a\u9ad8\u5bc6\u5ea6\u591a\u901a\u9053\u8bbe\u7f6e\u4f9d\u8d56\u6027\u5f3a\uff0c\u9650\u5236\u4e86\u5728\u5f02\u6784\u6216\u4f4e\u901a\u9053\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u5c42\u4e0e\u5206\u5c42Transformer\u7684\u6df7\u5408\u7f16\u7801\u5668\uff0c\u4ece\u5355\u901a\u9053EEG\u4e2d\u81ea\u76d1\u7763\u5b66\u4e60\uff1b\u572871\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u3002", "result": "\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5728\u516d\u4e2a\u8fd0\u52a8\u60f3\u8c61\u548c\u8ba4\u77e5\u4efb\u52a1\u4e2d\uff0c\u805a\u5408\u7684\u5355\u901a\u9053\u8868\u5f81 consistently \u4f18\u4e8e\u9886\u5148\u7684\u591a\u901a\u9053\u57fa\u7840\u6a21\u578b\u548c\u624b\u5de5\u7279\u5f81\u57fa\u7ebf\u3002", "conclusion": "\u5355\u901a\u9053EEG\u57fa\u7840\u6a21\u578b\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u5347\u786c\u4ef6\u517c\u5bb9\u6027\u3001\u795e\u7ecf\u751f\u7406\u5206\u6790\u7cbe\u7ec6\u5ea6\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.17220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17220", "abs": "https://arxiv.org/abs/2509.17220", "authors": ["Mingchen Xu", "Yukun Lai", "Ze Ji", "Jing Wu"], "title": "MirrorSAM2: Segment Mirror in Videos with Depth Perception", "comment": "8 pages", "summary": "This paper presents MirrorSAM2, the first framework that adapts Segment\nAnything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.\nMirrorSAM2 addresses key challenges in mirror detection, such as reflection\nambiguity and texture confusion, by introducing four tailored modules: a Depth\nWarping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point\nPrompt Generator for automatic prompt generation, a Frequency Detail Attention\nFusion Module to enhance structural boundaries, and a Mirror Mask Decoder with\na learnable mirror token for refined segmentation. By fully leveraging the\ncomplementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities\nto the prompt-free setting. To our knowledge, this is the first work to enable\nSAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD\nbenchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under\nchallenging conditions such as small mirrors, weak boundaries, and strong\nreflections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MirrorSAM2\uff0c\u9996\u4e2a\u5c06SAM2\u6a21\u578b\u5e94\u7528\u4e8eRGB-D\u89c6\u9891\u955c\u9762\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u56db\u4e2a\u5b9a\u5236\u6a21\u5757\u89e3\u51b3\u4e86\u955c\u9762\u68c0\u6d4b\u4e2d\u7684\u53cd\u5c04\u6a21\u7cca\u548c\u7eb9\u7406\u6df7\u6dc6\u7b49\u5173\u952e\u95ee\u9898\uff0c\u5e76\u5728\u65e0\u9700\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u81ea\u52a8\u89c6\u9891\u955c\u9762\u5206\u5272\uff0c\u5728VMD\u548cDVMD\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u955c\u9762\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u53cd\u5c04\u6a21\u7cca\u3001\u7eb9\u7406\u6df7\u6dc6\u7b49\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\uff0c\u9996\u6b21\u5b9e\u73b0SAM2\u5728RGB-D\u89c6\u9891\u955c\u9762\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMirrorSAM2\u6846\u67b6\uff0c\u5305\u542b\u6df1\u5ea6\u626d\u66f2\u6a21\u5757\u3001\u6df1\u5ea6\u5f15\u5bfc\u591a\u5c3a\u5ea6\u70b9\u63d0\u793a\u751f\u6210\u5668\u3001\u9891\u57df\u7ec6\u8282\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u548c\u5e26\u53ef\u5b66\u4e60\u955c\u9762\u6807\u8bb0\u7684\u955c\u9762\u63a9\u7801\u89e3\u7801\u5668\uff0c\u5145\u5206\u5229\u7528RGB\u4e0e\u6df1\u5ea6\u4fe1\u606f\u7684\u4e92\u8865\u6027\u3002", "result": "\u5728VMD\u548cDVMD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMirrorSAM2\u5728\u5c0f\u955c\u9762\u3001\u5f31\u8fb9\u754c\u548c\u5f3a\u53cd\u5c04\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MirrorSAM2\u662f\u9996\u4e2a\u5b9e\u73b0\u81ea\u52a8RGB-D\u89c6\u9891\u955c\u9762\u5206\u5272\u7684\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86SAM2\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u9002\u7528\u6027\u548c\u6027\u80fd\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17830", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17830", "abs": "https://arxiv.org/abs/2509.17830", "authors": ["Lekkala Sai Teja", "Annepaka Yadagiri", "and Partha Pakray", "Chukhu Chunka", "Mangadoddi Srikar Vardhan"], "title": "Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation", "comment": "14 pages, 14 figures", "summary": "Generation of Artificial Intelligence (AI) texts in important works has\nbecome a common practice that can be used to misuse and abuse AI at various\nlevels. Traditional AI detectors often rely on document-level classification,\nwhich struggles to identify AI content in hybrid or slightly edited texts\ndesigned to avoid detection, leading to concerns about the model's efficiency,\nwhich makes it hard to distinguish between human-written and AI-generated\ntexts. A sentence-level sequence labeling model proposed to detect transitions\nbetween human- and AI-generated text, leveraging nuanced linguistic signals\noverlooked by document-level classifiers. By this method, detecting and\nsegmenting AI and human-written text within a single document at the\ntoken-level granularity is achieved. Our model combines the state-of-the-art\npre-trained Transformer models, incorporating Neural Networks (NN) and\nConditional Random Fields (CRFs). This approach extends the power of\ntransformers to extract semantic and syntactic patterns, and the neural network\ncomponent to capture enhanced sequence-level representations, thereby improving\nthe boundary predictions by the CRF layer, which enhances sequence recognition\nand further identification of the partition between Human- and AI-generated\ntexts. The evaluation is performed on two publicly available benchmark datasets\ncontaining collaborative human and AI-generated texts. Our experimental\ncomparisons are with zero-shot detectors and the existing state-of-the-art\nmodels, along with rigorous ablation studies to justify that this approach, in\nparticular, can accurately detect the spans of AI texts in a completely\ncollaborative text. All our source code and the processed datasets are\navailable in our GitHub repository.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5e8f\u5217\u6807\u6ce8\u7684\u53e5\u5b50\u7ea7\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u5355\u4e2a\u6587\u6863\u4e2d\u7ec6\u7c92\u5ea6\u5730\u68c0\u6d4b\u548c\u5206\u5272\u4eba\u7c7b\u4e0eAI\u751f\u6210\u7684\u6587\u672c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6587\u6863\u7ea7\u522b\u7684AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u6df7\u5408\u6216\u7ecf\u8fc7\u7f16\u8f91\u7684\u6587\u672c\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u3001\u795e\u7ecf\u7f51\u7edc\u548c\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\uff0c\u5728\u8bcd\u5143\u7ea7\u522b\u8fdb\u884c\u5e8f\u5217\u6807\u6ce8\uff0c\u5229\u7528\u8bed\u8a00\u5b66\u7ec6\u5fae\u4fe1\u53f7\u63d0\u5347\u4eba\u7c7b\u4e0eAI\u6587\u672c\u8fb9\u754c\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u68c0\u6d4b\u5668\u548c\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u534f\u4f5c\u6587\u672c\u4e2dAI\u751f\u6210\u90e8\u5206\u7684\u8fb9\u754c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408\u6587\u672c\u4e2dAI\u751f\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7ec6\u7c92\u5ea6\u3001\u8de8\u6bb5\u843d\u7684\u6587\u672c\u6765\u6e90\u8bc6\u522b\u3002"}}
{"id": "2509.17924", "categories": ["cs.LG", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2509.17924", "abs": "https://arxiv.org/abs/2509.17924", "authors": ["Xiuqi Ge", "Zhibo Yao", "Yaosong Du"], "title": "Medical priority fusion: achieving dual optimization of sensitivity and interpretability in nipt anomaly detection", "comment": "24 pages, 47 figures, publish to BIBM", "summary": "Clinical machine learning faces a critical dilemma in high-stakes medical\napplications: algorithms achieving optimal diagnostic performance typically\nsacrifice the interpretability essential for physician decision-making, while\ninterpretable methods compromise sensitivity in complex scenarios. This paradox\nbecomes particularly acute in non-invasive prenatal testing (NIPT), where\nmissed chromosomal abnormalities carry profound clinical consequences yet\nregulatory frameworks mandate explainable AI systems. We introduce Medical\nPriority Fusion (MPF), a constrained multi-objective optimization framework\nthat resolves this fundamental trade-off by systematically integrating Naive\nBayes probabilistic reasoning with Decision Tree rule-based logic through\nmathematically-principled weighted fusion under explicit medical constraints.\nRigorous validation on 1,687 real-world NIPT samples characterized by extreme\nclass imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold\ncross-validation with comprehensive ablation studies and statistical hypothesis\ntesting using McNemar's paired comparisons. MPF achieved simultaneous\noptimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with\n80% interpretability score, significantly outperforming individual algorithms\n(McNemar's test, p < 0.001). The optimal fusion configuration achieved Grade A\nclinical deployment criteria with large effect size (d = 1.24), establishing\nthe first clinically-deployable solution that maintains both diagnostic\naccuracy and decision transparency essential for prenatal care. This work\ndemonstrates that medical-constrained algorithm fusion can resolve the\ninterpretability-performance trade-off, providing a mathematical framework for\ndeveloping high-stakes medical decision support systems that meet both clinical\nefficacy and explainability requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedical Priority Fusion (MPF)\u7684\u7ea6\u675f\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u660e\u786e\u7684\u533b\u5b66\u7ea6\u675f\u4e0b\u878d\u5408\u6734\u7d20\u8d1d\u53f6\u65af\u548c\u51b3\u7b56\u6811\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4e34\u5e8a\u673a\u5668\u5b66\u4e60\u4e2d\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u975e\u4fb5\u5165\u6027\u4ea7\u524d\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u548c\u9ad8\u53ef\u89e3\u91ca\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u533b\u7597\u5e94\u7528\u4e2d\uff0c\u73b0\u6709\u7b97\u6cd5\u5f80\u5f80\u5728\u8bca\u65ad\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5c24\u5176\u5728\u975e\u4fb5\u5165\u6027\u4ea7\u524d\u68c0\u6d4b\uff08NIPT\uff09\u4e2d\uff0c\u65e2\u9700\u8981\u9ad8\u7075\u654f\u5ea6\u4ee5\u907f\u514d\u6f0f\u8bca\uff0c\u53c8\u9700\u6ee1\u8db3\u76d1\u7ba1\u5bf9\u53ef\u89e3\u91caAI\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51faMedical Priority Fusion (MPF)\u6846\u67b6\uff0c\u7ed3\u5408\u6734\u7d20\u8d1d\u53f6\u65af\u7684\u6982\u7387\u63a8\u7406\u4e0e\u51b3\u7b56\u6811\u7684\u89c4\u5219\u903b\u8f91\uff0c\u901a\u8fc7\u6570\u5b66\u4e0a\u4e25\u8c28\u7684\u52a0\u6743\u878d\u5408\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u533b\u5b66\u7ea6\u675f\u8fdb\u884c\u4f18\u5316\u3002\u57281,687\u4e2a\u771f\u5b9eNIPT\u6837\u672c\u4e0a\u91c7\u7528\u5206\u5c425\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u7ed3\u5408\u6d88\u878d\u5b9e\u9a8c\u548cMcNemar\u914d\u5bf9\u68c0\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "MPF\u5728\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\uff08\u6b63\u5e38\u4e0e\u5f02\u5e38\u6bd4\u4f8b\u4e3a43.4:1\uff09\u4e0b\u8fbe\u523089.3%\u7684\u7075\u654f\u5ea6\uff0895% CI: 83.9-94.7%\uff09\u548c80%\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u7b97\u6cd5\uff08p < 0.001\uff09\uff0c\u6548\u5e94\u91cf\u5927\uff08d = 1.24\uff09\uff0c\u6ee1\u8db3Grade A\u4e34\u5e8a\u90e8\u7f72\u6807\u51c6\u3002", "conclusion": "MPF\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u98ce\u9669\u533b\u5b66\u573a\u666f\u4e2d\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u77db\u76fe\uff0c\u4e3a\u4e34\u5e8a\u53ef\u7528\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u517c\u5177\u8bca\u65ad\u51c6\u786e\u6027\u4e0e\u51b3\u7b56\u900f\u660e\u6027\u7684\u6570\u5b66\u6846\u67b6\u3002"}}
{"id": "2509.17232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17232", "abs": "https://arxiv.org/abs/2509.17232", "authors": ["Bo Liu", "Runlong Li", "Li Zhou", "Yan Zhou"], "title": "DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction", "comment": "15 pages", "summary": "This paper proposes a Diffusion Model-Optimized Neural Radiance Field\n(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency\nin 3D scene reconstruction. By combining diffusion models with Transformers,\nDT-NeRF effectively restores details under sparse viewpoints and maintains high\naccuracy in complex geometric scenes. Experimental results demonstrate that\nDT-NeRF significantly outperforms traditional NeRF and other state-of-the-art\nmethods on the Matterport3D and ShapeNet datasets, particularly in metrics such\nas PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further\nconfirm the critical role of the diffusion and Transformer modules in the\nmodel's performance, with the removal of either module leading to a decline in\nperformance. The design of DT-NeRF showcases the synergistic effect between\nmodules, providing an efficient and accurate solution for 3D scene\nreconstruction. Future research may focus on further optimizing the model,\nexploring more advanced generative models and network architectures to enhance\nits performance in large-scale dynamic scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0eTransformer\u7684\u795e\u7ecf\u8f90\u5c04\u573a\u65b9\u6cd5DT-NeRF\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u89d2\u4e0b\u76843D\u573a\u666f\u91cd\u5efa\u7ec6\u8282\u6062\u590d\u4e0e\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfNeRF\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u7ec6\u8282\u6062\u590d\u4e0d\u8db3\u548c\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u51e0\u4f55\u573a\u666f\u4e2d\u7684\u8868\u73b0\u5c40\u9650\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u4e0eTransformer\u7ed3\u5408\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4f18\u5316\u7ec6\u8282\u751f\u6210\uff0c\u901a\u8fc7Transformer\u589e\u5f3a\u8de8\u89c6\u89d2\u7279\u5f81\u878d\u5408\uff0c\u4ece\u800c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728Matterport3D\u548cShapeNet\u6570\u636e\u96c6\u4e0a\uff0cDT-NeRF\u5728PSNR\u3001SSIM\u3001Chamfer Distance\u548cFidelity\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edfNeRF\u53ca\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u6269\u6563\u548cTransformer\u6a21\u5757\u5747\u5bf9\u6027\u80fd\u6709\u5173\u952e\u8d21\u732e\u3002", "conclusion": "DT-NeRF\u901a\u8fc7\u6a21\u5757\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u76843D\u573a\u666f\u91cd\u5efa\uff0c\u4e3a\u672a\u6765\u5f15\u5165\u66f4\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u548c\u7f51\u7edc\u7ed3\u6784\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2509.17844", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17844", "abs": "https://arxiv.org/abs/2509.17844", "authors": ["Lynn Greschner", "Sabine Weber", "Roman Klinger"], "title": "Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework", "comment": null, "summary": "Emotions, which influence how convincing an argument is, are developed\n  in context of the self and sender, and therefore require modeling\n  the cognitive evaluation process. While binary emotionality has been\n  studied in argument mining, and the cognitive appraisal has been\n  modeled in general emotion analysis, these fields have not been\n  brought together yet. We therefore propose the Contextualized\n  Argument Appraisal Framework that contextualizes the interplay\n  between the sender, receiver, and argument. It includes emotion\n  labels, appraisals, such as argument familiarity, response urgency,\n  and expected effort, as well as convincingness variables. To evaluate\n  the framework and pave the way to computational modeling, we perform\n  a study in a role-playing scenario, mimicking real-world exposure to\n  arguments, asking participants to disclose their emotion, explain the main\ncause, the\n  argument appraisal, and the\n  perceived convincingness. To consider the subjective nature of such\n  annotations, we also collect demographic data and personality traits\n  of both the participants and the perceived sender of the argument.\n  The analysis of the resulting corpus of 800 arguments, each\n  annotated by 5 participants, reveals that convincingness is\n  positively correlated with positive emotions (e.g., trust) and\n  negatively correlated with negative emotions (e.g., anger). The\n  appraisal variables disclose the importance of the argument\n  familiarity. For most participants, the content of the argument\n  itself is the primary driver of the emotional response.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5316\u7684\u8bba\u8bc1\u8bc4\u4ef7\u6846\u67b6\uff0c\u7ed3\u5408\u60c5\u611f\u3001\u8ba4\u77e5\u8bc4\u4ef7\u548c\u8bf4\u670d\u529b\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u53d1\u73b0\u8bf4\u670d\u529b\u4e0e\u79ef\u6781\u60c5\u7eea\u6b63\u76f8\u5173\uff0c\u4e0e\u6d88\u6781\u60c5\u7eea\u8d1f\u76f8\u5173\uff0c\u8bba\u8bc1\u719f\u6089\u5ea6\u662f\u5f71\u54cd\u60c5\u611f\u53cd\u5e94\u7684\u91cd\u8981\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u5c06\u8bba\u8bc1\u6316\u6398\u4e2d\u7684\u4e8c\u5143\u60c5\u611f\u6027\u4e0e\u4e00\u822c\u60c5\u611f\u5206\u6790\u4e2d\u7684\u8ba4\u77e5\u8bc4\u4ef7\u7ed3\u5408\u8d77\u6765\uff0c\u9700\u8981\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\u6765\u5efa\u6a21\u8bba\u8bc1\u4e2d\u60c5\u611f\u7684\u5f62\u6210\u53ca\u5176\u5bf9\u8bf4\u670d\u529b\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u201c\u4e0a\u4e0b\u6587\u5316\u8bba\u8bc1\u8bc4\u4ef7\u6846\u67b6\u201d\uff0c\u5305\u542b\u60c5\u611f\u6807\u7b7e\u3001\u8bc4\u4ef7\u53d8\u91cf\uff08\u5982\u8bba\u8bc1\u719f\u6089\u5ea6\u3001\u54cd\u5e94\u7d27\u8feb\u6027\u3001\u9884\u671f\u52aa\u529b\uff09\u548c\u8bf4\u670d\u529b\u53d8\u91cf\uff0c\u5e76\u5728\u6a21\u62df\u771f\u5b9e\u573a\u666f\u7684\u89d2\u8272\u626e\u6f14\u5b9e\u9a8c\u4e2d\u6536\u96c6800\u4e2a\u8bba\u8bc1\u7684\u6570\u636e\uff0c\u6bcf\u4e2a\u8bba\u8bc1\u75315\u540d\u53c2\u4e0e\u8005\u6807\u6ce8\uff0c\u540c\u65f6\u6536\u96c6\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u4eba\u683c\u7279\u5f81\u6570\u636e\u3002", "result": "\u5206\u6790\u663e\u793a\u8bf4\u670d\u529b\u4e0e\u79ef\u6781\u60c5\u7eea\uff08\u5982\u4fe1\u4efb\uff09\u6b63\u76f8\u5173\uff0c\u4e0e\u6d88\u6781\u60c5\u7eea\uff08\u5982\u6124\u6012\uff09\u8d1f\u76f8\u5173\uff1b\u8bba\u8bc1\u719f\u6089\u5ea6\u662f\u5173\u952e\u8bc4\u4ef7\u53d8\u91cf\uff1b\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u8bba\u8bc1\u5185\u5bb9\u672c\u8eab\u662f\u60c5\u611f\u53cd\u5e94\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86\u60c5\u611f\u3001\u8ba4\u77e5\u8bc4\u4ef7\u4e0e\u8bf4\u670d\u529b\uff0c\u5b9e\u8bc1\u7ed3\u679c\u652f\u6301\u5176\u5408\u7406\u6027\uff0c\u4e3a\u8ba1\u7b97\u5efa\u6a21\u8bba\u8bc1\u4e2d\u7684\u60c5\u611f\u5f71\u54cd\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.17942", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17942", "abs": "https://arxiv.org/abs/2509.17942", "authors": ["Nicholas Kraabel", "Jiangtao Liu", "Yuchen Bian", "Daniel Kifer", "Chaopeng Shen"], "title": "StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions", "comment": null, "summary": "Stewarding natural resources, mitigating floods, droughts, wildfires, and\nlandslides, and meeting growing demands require models that can predict\nclimate-driven land-surface responses and human feedback with high accuracy.\nTraditional impact models, whether process-based, statistical, or machine\nlearning, struggle with spatial generalization due to limited observations and\nconcept drift. Recently proposed vision foundation models trained on satellite\nimagery demand massive compute and are ill-suited for dynamic land-surface\nprediction. We introduce StefaLand, a generative spatiotemporal earth\nfoundation model centered on landscape interactions. StefaLand improves\npredictions on three tasks and four datasets: streamflow, soil moisture, and\nsoil composition, compared to prior state-of-the-art. Results highlight its\nability to generalize across diverse, data-scarce regions and support broad\nland-surface applications. The model builds on a masked autoencoder backbone\nthat learns deep joint representations of landscape attributes, with a\nlocation-aware architecture fusing static and time-series inputs,\nattribute-based representations that drastically reduce compute, and residual\nfine-tuning adapters that enhance transfer. While inspired by prior methods,\ntheir alignment with geoscience and integration in one model enables robust\nperformance on dynamic land-surface tasks. StefaLand can be pretrained and\nfinetuned on academic compute yet outperforms state-of-the-art baselines and\neven fine-tuned vision foundation models. To our knowledge, this is the first\ngeoscience land-surface foundation model that demonstrably improves dynamic\nland-surface interaction predictions and supports diverse downstream\napplications.", "AI": {"tldr": "Stefaland \u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u65f6\u7a7a\u5730\u7403\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u666f\u89c2\u4ea4\u4e92\uff0c\u80fd\u591f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5730\u533a\u6cdb\u5316\uff0c\u5e76\u5728\u6d41\u901f\u3001\u571f\u58e4\u6e7f\u5ea6\u548c\u571f\u58e4\u6210\u5206\u9884\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5f71\u54cd\u6a21\u578b\u5728\u7a7a\u95f4\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u73b0\u6709\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u5927\u4e14\u4e0d\u9002\u5408\u52a8\u6001\u5730\u8868\u9884\u6d4b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u51c6\u786e\u9884\u6d4b\u6c14\u5019\u9a71\u52a8\u7684\u5730\u8868\u54cd\u5e94\u548c\u4eba\u7c7b\u53cd\u9988\u7684\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u4f4d\u7f6e\u611f\u77e5\u67b6\u6784\u878d\u5408\u9759\u6001\u548c\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\uff0c\u91c7\u7528\u5c5e\u6027-based \u8868\u793a\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u5e76\u4f7f\u7528\u6b8b\u5dee\u5fae\u8c03\u9002\u914d\u5668\u589e\u5f3a\u8fc1\u79fb\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\uff08\u6d41\u901f\u3001\u571f\u58e4\u6e7f\u5ea6\u3001\u571f\u58e4\u6210\u5206\uff09\u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u80fd\u591f\u8de8\u591a\u6837\u5316\u3001\u6570\u636e\u7a00\u7f3a\u533a\u57df\u826f\u597d\u6cdb\u5316\uff0c\u4e14\u53ef\u5728\u5b66\u672f\u7ea7\u8ba1\u7b97\u8d44\u6e90\u4e0a\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "conclusion": "Stefaland \u662f\u9996\u4e2a\u663e\u8457\u63d0\u5347\u52a8\u6001\u5730\u8868\u4ea4\u4e92\u9884\u6d4b\u6027\u80fd\u5e76\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u7684\u5730\u7406\u79d1\u5b66\u5730\u8868\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2509.17246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17246", "abs": "https://arxiv.org/abs/2509.17246", "authors": ["Ranran Huang", "Krystian Mikolajczyk"], "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views", "comment": null, "summary": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian\nsplatting from sparse multi-view images, requiring no ground-truth poses during\ntraining and inference. It employs a shared feature extraction backbone,\nenabling simultaneous prediction of 3D Gaussian primitives and camera poses in\na canonical space from unposed inputs. A masked attention mechanism is\nintroduced to efficiently estimate target poses during training, while a\nreprojection loss enforces pixel-aligned Gaussian primitives, providing\nstronger geometric constraints. We further demonstrate the compatibility of our\ntraining framework with different reconstruction architectures, resulting in\ntwo model variants. Remarkably, despite the absence of pose supervision, our\nmethod achieves state-of-the-art performance in both in-domain and\nout-of-domain novel view synthesis, even under extreme viewpoint changes and\nlimited image overlap, and surpasses recent methods that rely on geometric\nsupervision for relative pose estimation. By eliminating dependence on\nground-truth poses, our method offers the scalability to leverage larger and\nmore diverse datasets. Code and pretrained models will be available on our\nproject page: https://ranrhuang.github.io/spfsplatv2/.", "AI": {"tldr": "\u63d0\u51faSPFSplatV2\uff0c\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u76f8\u673a\u4f4d\u59ff\u76d1\u7763\u7684\u9ad8\u6548\u524d\u9988\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u8fdb\u884c3D\u9ad8\u65af\u70b9\u9635\u5316\uff0c\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u6d88\u9664\u5bf9\u771f\u5b9e\u76f8\u673a\u4f4d\u59ff\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u65b9\u6cd5\u5728\u65e0\u59ff\u6001\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\u7f51\u7edc\uff0c\u540c\u65f6\u9884\u6d4b\u89c4\u8303\u7a7a\u95f4\u4e2d\u76843D\u9ad8\u65af\u56fe\u5143\u548c\u76f8\u673a\u4f4d\u59ff\uff1b\u5f15\u5165\u63a9\u7801\u6ce8\u610f\u529b\u673a\u5236\u4f30\u8ba1\u8bad\u7ec3\u65f6\u7684\u76ee\u6807\u4f4d\u59ff\uff0c\u5e76\u901a\u8fc7\u91cd\u6295\u5f71\u635f\u5931\u65bd\u52a0\u51e0\u4f55\u7ea6\u675f\u3002", "result": "\u5728\u57df\u5185\u548c\u57df\u5916\u7684\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5373\u4f7f\u5728\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u548c\u56fe\u50cf\u91cd\u53e0\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u4e5f\u4f18\u4e8e\u4f9d\u8d56\u51e0\u4f55\u76d1\u7763\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "conclusion": "SPFSplatV2\u5b9e\u73b0\u4e86\u65e0\u9700\u4f4d\u59ff\u76d1\u7763\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\uff0c\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u548c\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2509.17855", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17855", "abs": "https://arxiv.org/abs/2509.17855", "authors": ["Robert Litschko", "Verena Blaschke", "Diana Burkhardt", "Barbara Plank", "Diego Frassinelli"], "title": "Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora", "comment": "Accepted at EMNLP 2025 (Findings)", "summary": "Dialects exhibit a substantial degree of variation due to the lack of a\nstandard orthography. At the same time, the ability of Large Language Models\n(LLMs) to process dialects remains largely understudied. To address this gap,\nwe use Bavarian as a case study and investigate the lexical dialect\nunderstanding capability of LLMs by examining how well they recognize and\ntranslate dialectal terms across different parts-of-speech. To this end, we\nintroduce DiaLemma, a novel annotation framework for creating dialect variation\ndictionaries from monolingual data only, and use it to compile a ground truth\ndataset consisting of 100K human-annotated German-Bavarian word pairs. We\nevaluate how well nine state-of-the-art LLMs can judge Bavarian terms as\ndialect translations, inflected variants, or unrelated forms of a given German\nlemma. Our results show that LLMs perform best on nouns and lexically similar\nword pairs, and struggle most in distinguishing between direct translations and\ninflected variants. Interestingly, providing additional context in the form of\nexample usages improves the translation performance, but reduces their ability\nto recognize dialect variants. This study highlights the limitations of LLMs in\ndealing with orthographic dialect variation and emphasizes the need for future\nwork on adapting LLMs to dialects.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u5df4\u4f10\u5229\u4e9a\u65b9\u8a00\u8bcd\u6c47\u53d8\u4f53\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4ec5\u4ece\u5355\u8bed\u6570\u636e\u6784\u5efa\u65b9\u8a00\u53d8\u4f53\u8bcd\u5178\u7684\u6807\u6ce8\u6846\u67b6DiaLemma\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b10\u4e07\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u5fb7-\u5df4\u4f10\u5229\u4e9a\u8bed\u8bcd\u5bf9\u7684\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660eLLM\u5728\u540d\u8bcd\u548c\u8bcd\u5f62\u76f8\u4f3c\u8bcd\u5bf9\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u533a\u5206\u76f4\u63a5\u7ffb\u8bd1\u4e0e\u5c48\u6298\u53d8\u4f53\u65f6\u5b58\u5728\u56f0\u96be\uff1b\u4e0a\u4e0b\u6587\u6709\u52a9\u4e8e\u7ffb\u8bd1\u4f46\u524a\u5f31\u4e86\u5bf9\u65b9\u8a00\u53d8\u4f53\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u6b63\u5b57\u6cd5\uff0c\u65b9\u8a00\u5b58\u5728\u663e\u8457\u53d8\u5f02\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u65b9\u8a00\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u65b9\u8a00\u8bcd\u6c47\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u8de8\u8bcd\u6027\u7684\u65b9\u8a00\u8bcd\u8bc6\u522b\u4e0e\u7ffb\u8bd1\u3002", "method": "\u4ee5\u5df4\u4f10\u5229\u4e9a\u65b9\u8a00\u4e3a\u6848\u4f8b\uff0c\u63d0\u51fa\u540d\u4e3aDiaLemma\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u4ece\u5355\u8bed\u6570\u636e\u81ea\u52a8\u751f\u6210\u65b9\u8a00\u53d8\u4f53\u8bcd\u5178\uff0c\u5e76\u6784\u5efa\u5305\u542b10\u4e07\u4e2a\u4eba\u5de5\u6807\u6ce8\u5fb7-\u5df4\u4f10\u5229\u4e9a\u8bed\u8bcd\u5bf9\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u901a\u8fc7\u8ba9\u4e5d\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u5df4\u4f10\u5229\u4e9a\u8bed\u8bcd\u9879\u662f\u5426\u4e3a\u7ed9\u5b9a\u5fb7\u8bed\u8bcd\u5143\u7684\u65b9\u8a00\u7ffb\u8bd1\u3001\u5c48\u6298\u53d8\u4f53\u6216\u65e0\u5173\u5f62\u5f0f\uff0c\u8bc4\u4f30\u5176\u65b9\u8a00\u7406\u89e3\u80fd\u529b\u3002\u540c\u65f6\u6d4b\u8bd5\u4e86\u63d0\u4f9b\u4f8b\u53e5\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u540d\u8bcd\u548c\u8bcd\u5f62\u76f8\u4f3c\u7684\u8bcd\u5bf9\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u533a\u5206\u76f4\u63a5\u7ffb\u8bd1\u4e0e\u5c48\u6298\u53d8\u4f53\u65f6\u8868\u73b0\u6700\u5dee\u3002\u63d0\u4f9b\u4e0a\u4e0b\u6587\u793a\u4f8b\u80fd\u63d0\u5347\u7ffb\u8bd1\u5224\u65ad\u7684\u51c6\u786e\u6027\uff0c\u5374\u964d\u4f4e\u4e86\u5bf9\u65b9\u8a00\u53d8\u4f53\u7684\u8bc6\u522b\u80fd\u529b\u3002\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4f46\u6574\u4f53\u5747\u663e\u793a\u51fa\u5bf9\u6b63\u5b57\u6cd5\u53d8\u5f02\u7684\u5904\u7406\u5c40\u9650\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6b63\u5b57\u6cd5\u4e0d\u89c4\u8303\u7684\u65b9\u8a00\u53d8\u5f02\u65f6\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u5c24\u5176\u96be\u4ee5\u51c6\u786e\u533a\u5206\u65b9\u8a00\u4e2d\u7684\u76f4\u63a5\u7ffb\u8bd1\u4e0e\u8bed\u6cd5\u5c48\u6298\u5f62\u5f0f\u3002\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u6709\u52a9\u4e8e\u7ffb\u8bd1\u7406\u89e3\uff0c\u4f46\u4e5f\u53ef\u80fd\u5e72\u6270\u5bf9\u65b9\u8a00\u53d8\u4f53\u7684\u8bc6\u522b\u3002\u7814\u7a76\u5f3a\u8c03\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u548c\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u652f\u6301\u65b9\u8a00\u5904\u7406\u3002"}}
{"id": "2509.17970", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17970", "abs": "https://arxiv.org/abs/2509.17970", "authors": ["Yunchu Han", "Zhaojun Nan", "Sheng Zhou", "Zhisheng Niu"], "title": "Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference", "comment": null, "summary": "Deep neural networks (DNNs) have been widely applied in diverse applications,\nbut the problems of high latency and energy overhead are inevitable on\nresource-constrained devices. To address this challenge, most researchers focus\non the dynamic voltage and frequency scaling (DVFS) technique to balance the\nlatency and energy consumption by changing the computing frequency of\nprocessors. However, the adjustment of memory frequency is usually ignored and\nnot fully utilized to achieve efficient DNN inference, which also plays a\nsignificant role in the inference time and energy consumption. In this paper,\nwe first investigate the impact of joint memory frequency and computing\nfrequency scaling on the inference time and energy consumption with a\nmodel-based and data-driven method. Then by combining with the fitting\nparameters of different DNN models, we give a preliminary analysis for the\nproposed model to see the effects of adjusting memory frequency and computing\nfrequency simultaneously. Finally, simulation results in local inference and\ncooperative inference cases further validate the effectiveness of jointly\nscaling the memory frequency and computing frequency to reduce the energy\nconsumption of devices.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\uff0c\u8054\u5408\u8c03\u6574\u5185\u5b58\u9891\u7387\u548c\u8ba1\u7b97\u9891\u7387\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u9a71\u52a8\u4e0e\u6570\u636e\u9a71\u52a8\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u672c\u5730\u548c\u534f\u540c\u63a8\u7406\u573a\u666f\u4e0b\u964d\u4f4e\u80fd\u8017\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9762\u4e34\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8ba1\u7b97\u9891\u7387\u8c03\u8282\uff08DVFS\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u5185\u5b58\u9891\u7387\u7684\u8c03\u8282\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u8054\u5408\u8c03\u9891\u7b56\u7565\u4ee5\u63d0\u5347\u80fd\u6548\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9a71\u52a8\u4e0e\u6570\u636e\u9a71\u52a8\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540cDNN\u6a21\u578b\u4e0b\u5185\u5b58\u9891\u7387\u4e0e\u8ba1\u7b97\u9891\u7387\u8054\u5408\u8c03\u8282\u5bf9\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u5e76\u7ed3\u5408\u62df\u5408\u53c2\u6570\u8fdb\u884c\u521d\u6b65\u5efa\u6a21\u5206\u6790\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728\u672c\u5730\u63a8\u7406\u548c\u534f\u540c\u63a8\u7406\u573a\u666f\u4e2d\uff0c\u8054\u5408\u8c03\u8282\u5185\u5b58\u4e0e\u8ba1\u7b97\u9891\u7387\u80fd\u6709\u6548\u964d\u4f4e\u8bbe\u5907\u80fd\u8017\u3002", "conclusion": "\u8054\u5408\u8c03\u6574\u5185\u5b58\u9891\u7387\u548c\u8ba1\u7b97\u9891\u7387\u662f\u4e00\u79cd\u6709\u6548\u7684\u8282\u80fd\u624b\u6bb5\uff0c\u76f8\u8f83\u4e8e\u5355\u72ec\u8c03\u8282\u8ba1\u7b97\u9891\u7387\uff0c\u80fd\u66f4\u5145\u5206\u5730\u4f18\u5316DNN\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u80fd\u6548\u8868\u73b0\u3002"}}
{"id": "2509.17262", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.17262", "abs": "https://arxiv.org/abs/2509.17262", "authors": ["Xiumei Li", "Marc Windsheimer", "Misha Sadeghi", "Bj\u00f6rn Eskofier", "Andr\u00e9 Kaup"], "title": "Optimized Learned Image Compression for Facial Expression Recognition", "comment": "Accepted at ICIP 2025", "summary": "Efficient data compression is crucial for the storage and transmission of\nvisual data. However, in facial expression recognition (FER) tasks, lossy\ncompression often leads to feature degradation and reduced accuracy. To address\nthese challenges, this study proposes an end-to-end model designed to preserve\ncritical features and enhance both compression and recognition performance. A\ncustom loss function is introduced to optimize the model, tailored to balance\ncompression and recognition performance effectively. This study also examines\nthe influence of varying loss term weights on this balance. Experimental\nresults indicate that fine-tuning the compression model alone improves\nclassification accuracy by 0.71% and compression efficiency by 49.32%, while\njoint optimization achieves significant gains of 4.04% in accuracy and 89.12%\nin efficiency. Moreover, the findings demonstrate that the jointly optimized\nclassification model maintains high accuracy on both compressed and\nuncompressed data, while the compression model reliably preserves image\ndetails, even at high compression rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u901a\u8fc7\u5b9a\u5236\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316\u538b\u7f29\u4e0e\u8bc6\u522b\u6027\u80fd\uff0c\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u548c\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u6709\u635f\u538b\u7f29\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u5bfc\u81f4\u7279\u5f81\u9000\u5316\u548c\u51c6\u786e\u7387\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u6765\u5e73\u8861\u538b\u7f29\u4e0e\u8bc6\u522b\u6027\u80fd\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u635f\u5931\u9879\u6743\u91cd\u7684\u5f71\u54cd\u3002", "result": "\u5355\u72ec\u5fae\u8c03\u538b\u7f29\u6a21\u578b\u63d0\u5347\u4e860.71%\u7684\u51c6\u786e\u7387\u548c49.32%\u7684\u538b\u7f29\u6548\u7387\uff1b\u8054\u5408\u4f18\u5316\u5b9e\u73b0\u4e864.04%\u7684\u51c6\u786e\u7387\u63d0\u5347\u548c89.12%\u7684\u6548\u7387\u63d0\u5347\uff0c\u4e14\u6a21\u578b\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u56fe\u50cf\u7ec6\u8282\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u517c\u987e\u4e86\u538b\u7f29\u6548\u7387\u4e0e\u8bc6\u522b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u7684\u9ad8\u6548\u6570\u636e\u538b\u7f29\u3002"}}
{"id": "2509.17858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17858", "abs": "https://arxiv.org/abs/2509.17858", "authors": ["Milan Straka"], "title": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution", "comment": "Accepted to CODI-CRAC 2025", "summary": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on\nMultilingual Coreference Resolution. This fourth iteration of the shared task\nintroduces a new LLM track alongside the original unconstrained track, features\nreduced development and test sets to lower computational requirements, and\nincludes additional datasets. CorPipe 25 represents a complete reimplementation\nof our previous systems, migrating from TensorFlow to PyTorch. Our system\nsignificantly outperforms all other submissions in both the LLM and\nunconstrained tracks by a substantial margin of 8 percentage points. The source\ncode and trained models are publicly available at\nhttps://github.com/ufal/crac2025-corpipe.", "AI": {"tldr": "CorPipe 25\u662fCRAC 2025\u5171\u4eab\u4efb\u52a1\u7684\u4f18\u80dc\u7cfb\u7edf\uff0c\u91c7\u7528PyTorch\u91cd\u65b0\u5b9e\u73b0\uff0c\u5728LLM\u548c\u65e0\u7ea6\u675f\u4e24\u4e2a\u8d5b\u9053\u5747\u5927\u5e45\u9886\u5148\u3002", "motivation": "\u5e94\u5bf9\u591a\u8bed\u8a00\u5171\u6307\u6d88\u89e3\u6311\u6218\uff0c\u53c2\u4e0eCRAC 2025\u5171\u4eab\u4efb\u52a1\u7684\u65b0LLM\u8d5b\u9053\uff0c\u5e76\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u5b8c\u5168\u57fa\u4e8ePyTorch\u91cd\u6784\u5148\u524d\u7cfb\u7edf\uff0c\u9002\u914d\u65b0\u5f15\u5165\u7684LLM\u8d5b\u9053\uff0c\u5e76\u4f7f\u7528\u7cbe\u7b80\u540e\u7684\u5f00\u53d1\u4e0e\u6d4b\u8bd5\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728LLM\u548c\u65e0\u7ea6\u675f\u4e24\u4e2a\u8d5b\u9053\u5747\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6240\u6709\u53c2\u8d5b\u7cfb\u7edf\uff0c\u9886\u5148\u5e45\u5ea6\u8fbe8\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CorPipe 25\u901a\u8fc7\u91cd\u6784\u548c\u6280\u672f\u8fc1\u79fb\uff0c\u5728\u591a\u8bed\u8a00\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u4e2d\u53d6\u5f97\u9886\u5148\u5730\u4f4d\uff0c\u4e14\u7cfb\u7edf\u4ee3\u7801\u4e0e\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.17971", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17971", "abs": "https://arxiv.org/abs/2509.17971", "authors": ["Tan-Ha Mai", "Hsuan-Tien Lin"], "title": "Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning", "comment": "22 pages, 10 figures", "summary": "In this paper, we investigate the challenges of complementary-label learning\n(CLL), a specialized form of weakly-supervised learning (WSL) where models are\ntrained with labels indicating classes to which instances do not belong, rather\nthan standard ordinary labels. This alternative supervision is appealing\nbecause collecting complementary labels is generally cheaper and less\nlabor-intensive. Although most existing research in CLL emphasizes the\ndevelopment of novel loss functions, the potential of data augmentation in this\ndomain remains largely underexplored. In this work, we uncover that the\nwidely-used Mixup data augmentation technique is ineffective when directly\napplied to CLL. Through in-depth analysis, we identify that the\ncomplementary-label noise generated by Mixup negatively impacts the performance\nof CLL models. We then propose an improved technique called Intra-Cluster Mixup\n(ICM), which only synthesizes augmented data from nearby examples, to mitigate\nthe noise effect. ICM carries the benefits of encouraging complementary label\nsharing of nearby examples, and leads to substantial performance improvements\nacross synthetic and real-world labeled datasets. In particular, our wide\nspectrum of experimental results on both balanced and imbalanced CLL settings\njustifies the potential of ICM in allying with state-of-the-art CLL algorithms,\nachieving significant accuracy increases of 30% and 10% on MNIST and CIFAR\ndatasets, respectively.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e92\u8865\u6807\u7b7e\u5b66\u4e60\uff08CLL\uff09\u4e2d\u6570\u636e\u589e\u5f3a\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u4f20\u7edfMixup\u65b9\u6cd5\u5728CLL\u4e2d\u56e0\u5f15\u5165\u566a\u58f0\u800c\u6548\u679c\u4e0d\u4f73\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Intra-Cluster Mixup\uff08ICM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u5728\u90bb\u8fd1\u6837\u672c\u95f4\u8fdb\u884c\u6df7\u5408\u6765\u51cf\u5c11\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u4e86CLL\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e92\u8865\u6807\u7b7e\u6bd4\u666e\u901a\u6807\u7b7e\u66f4\u6613\u83b7\u53d6\u4e14\u6210\u672c\u66f4\u4f4e\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u589e\u5f3a\u7684\u4f5c\u7528\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22CLL\u4e2d\u6570\u636e\u589e\u5f3a\u7684\u6f5c\u529b\u53ca\u5176\u6311\u6218\u3002", "method": "\u5206\u6790Mixup\u5728CLL\u4e2d\u5931\u6548\u7684\u539f\u56e0\uff0c\u63d0\u51faIntra-Cluster Mixup\uff08ICM\uff09\uff0c\u9650\u5236\u6df7\u5408\u64cd\u4f5c\u5728\u540c\u7c7b\u7c07\u5185\u8fdb\u884c\uff0c\u4ee5\u51cf\u5c11\u4e92\u8865\u6807\u7b7e\u566a\u58f0\u5e76\u4fc3\u8fdb\u90bb\u8fd1\u6837\u672c\u95f4\u7684\u6807\u7b7e\u5171\u4eab\u3002", "result": "ICM\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u63d0\u5347CLL\u6a21\u578b\u6027\u80fd\uff0c\u5728MNIST\u548cCIFAR\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u6700\u9ad830%\u548c10%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u5728\u5e73\u8861\u4e0e\u4e0d\u5e73\u8861\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548\u3002", "conclusion": "ICM\u6709\u6548\u7f13\u89e3\u4e86Mixup\u5728CLL\u4e2d\u4ea7\u751f\u7684\u566a\u58f0\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5408\u7406\u8bbe\u8ba1\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u5f31\u76d1\u7763\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff0c\u4e3aCLL\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2509.17282", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.17282", "abs": "https://arxiv.org/abs/2509.17282", "authors": ["Xiangmin Xu", "Zhen Meng", "Kan Chen", "Jiaming Yang", "Emma Li", "Philip G. Zhao", "David Flynn"], "title": "Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity", "comment": "Submitted to IEEE Transactions on Mobile Computing", "summary": "Real-time Three-dimensional (3D) scene representation is a foundational\nelement that supports a broad spectrum of cutting-edge applications, including\ndigital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and\nthe emerging metaverse. Despite advancements in real-time communication and\ncomputing, achieving a balance between timeliness and fidelity in 3D scene\nrepresentation remains a challenge. This work investigates a wireless network\nwhere multiple homogeneous mobile robots, equipped with cameras, capture an\nenvironment and transmit images to an edge server over channels for 3D\nrepresentation. We propose a contextual-bandit Proximal Policy Optimization\n(PPO) framework incorporating both Age of Information (AoI) and semantic\ninformation to optimize image selection for representation, balancing data\nfreshness and representation quality. Two policies -- the $\\omega$-threshold\nand $\\omega$-wait policies -- together with two benchmark methods are\nevaluated, timeliness embedding and weighted sum, on standard datasets and\nbaseline 3D scene representation models. Experimental results demonstrate\nimproved representation fidelity while maintaining low latency, offering\ninsight into the model's decision-making process. This work advances real-time\n3D scene representation by optimizing the trade-off between timeliness and\nfidelity in dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587-\u8d4c\u535a\u673aPPO\u6846\u67b6\u7684\u56fe\u50cf\u9009\u62e9\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u591a\u673a\u5668\u4eba\u65e0\u7ebf\u7f51\u7edc\u4e2d\u4f18\u5316\u5b9e\u65f6\u4e09\u7ef4\u573a\u666f\u8868\u793a\u7684\u65f6\u6548\u6027\u4e0e\u4fdd\u771f\u5ea6\u5e73\u8861\u3002", "motivation": "\u5728\u6570\u5b57\u5236\u9020\u3001VR/AR/MR\u53ca\u5143\u5b87\u5b99\u7b49\u5e94\u7528\u4e2d\uff0c\u5b9e\u65f6\u4e09\u7ef4\u573a\u666f\u8868\u793a\u9762\u4e34\u65f6\u6548\u6027\u4e0e\u4fdd\u771f\u5ea6\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u6570\u636e\u65b0\u9c9c\u5ea6\u4e0e\u8bed\u4e49\u91cd\u8981\u6027\u7684\u8054\u5408\u4f18\u5316\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u878d\u5408Age of Information\uff08AoI\uff09\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587-\u8d4c\u535a\u673aProximal Policy Optimization\uff08PPO\uff09\u6846\u67b6\uff0c\u8bbe\u8ba1\u03c9-\u9608\u503c\u548c\u03c9-\u7b49\u5f85\u4e24\u79cd\u7b56\u7565\uff0c\u5e76\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u57fa\u7ebf3D\u8868\u793a\u6a21\u578b\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u5347\u4e86\u4e09\u7ef4\u573a\u666f\u8868\u793a\u7684\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8e\u52a0\u6743\u548c\u4e0e\u65f6\u5e8f\u5d4c\u5165\u7b49\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u5b9e\u65f6\u4e09\u7ef4\u573a\u666f\u8868\u793a\u4e2d\u7684\u65f6\u6548\u6027\u4e0e\u4fdd\u771f\u5ea6\u6743\u8861\uff0c\u63a8\u52a8\u4e86\u591a\u673a\u5668\u4eba\u534f\u540c\u611f\u77e5\u7cfb\u7edf\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\u3002"}}
{"id": "2509.17859", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17859", "abs": "https://arxiv.org/abs/2509.17859", "authors": ["Kai Schenck", "Ga\u0161per Begu\u0161"], "title": "Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN", "comment": null, "summary": "This paper outlines the methodology for modeling tonal learning in fully\nunsupervised models of human language acquisition. Tonal patterns are among the\ncomputationally most complex learning objectives in language. We argue that a\nrealistic generative model of human language (ciwGAN) can learn to associate\nits categorical variables with Mandarin Chinese tonal categories without any\nlabeled data. All three trained models showed statistically significant\ndifferences in F0 across categorical variables. The model trained solely on\nmale tokens consistently encoded tone. Our results sug- gest that not only does\nthe model learn Mandarin tonal contrasts, but it learns a system that\ncorresponds to a stage of acquisition in human language learners. We also\noutline methodology for tracing tonal representations in internal convolutional\nlayers, which shows that linguistic tools can contribute to interpretability of\ndeep learning and can ultimately be used in neural experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u6a21\u578b\uff08ciwGAN\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u666e\u901a\u8bdd\u58f0\u8c03\u5b66\u4e60\uff0c\u8bc1\u660e\u8be5\u6a21\u578b\u80fd\u5728\u65e0\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b66\u4f1a\u58f0\u8c03\u8303\u7574\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4e0e\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u9636\u6bb5\u7684\u5bf9\u5e94\u6027\u3002", "motivation": "\u58f0\u8c03\u6a21\u5f0f\u662f\u8bed\u8a00\u5b66\u4e60\u4e2d\u8ba1\u7b97\u4e0a\u6700\u590d\u6742\u7684\u4efb\u52a1\u4e4b\u4e00\uff0c\u5982\u4f55\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u5efa\u6a21\u4eba\u7c7b\u8bed\u8a00\u4e2d\u7684\u58f0\u8c03\u4e60\u5f97\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u73b0\u5b9e\u7684\u751f\u6210\u6a21\u578bciwGAN\uff0c\u5229\u7528\u65e0\u6807\u6ce8\u7684\u666e\u901a\u8bdd\u8bed\u97f3\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5206\u6790\u6a21\u578b\u4e2d\u5206\u7c7b\u53d8\u91cf\u4e0e\u58f0\u8c03\u8303\u7574\u7684\u5173\u8054\uff0c\u540c\u65f6\u8ffd\u8e2a\u5377\u79ef\u5c42\u5185\u90e8\u7684\u58f0\u8c03\u8868\u5f81\u3002", "result": "\u6240\u6709\u4e09\u4e2a\u8bad\u7ec3\u6a21\u578b\u5728F0\u4e0a\u5747\u663e\u793a\u51fa\u5206\u7c7b\u53d8\u91cf\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u4ec5\u4f7f\u7528\u7537\u6027\u8bed\u97f3\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u7a33\u5b9a\u7f16\u7801\u58f0\u8c03\uff0c\u8868\u660e\u6a21\u578b\u6210\u529f\u5b66\u4e60\u4e86\u666e\u901a\u8bdd\u58f0\u8c03\u5bf9\u7acb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u65e0\u76d1\u7763\u6df1\u5ea6\u6a21\u578b\u4e0d\u4ec5\u80fd\u5b66\u4e60\u58f0\u8c03\u8303\u7574\uff0c\u5176\u5b66\u4e60\u8fc7\u7a0b\u8fd8\u6a21\u62df\u4e86\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u7684\u67d0\u4e00\u9636\u6bb5\uff0c\u4e14\u8bed\u8a00\u5b66\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5e76\u652f\u6301\u795e\u7ecf\u5b9e\u9a8c\u7814\u7a76\u3002"}}
{"id": "2509.17987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17987", "abs": "https://arxiv.org/abs/2509.17987", "authors": ["Sanju Xaviar", "Omid Ardakanian"], "title": "Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor Networks", "comment": "12 pages", "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for anomaly\ndetection in sensor networks, particularly when analyzing multivariate time\nseries. In this work, we introduce BETA, a novel grey-box evasion attack\ntargeting such GNN-based detectors, where the attacker is constrained to\nperturb sensor readings from a limited set of nodes, excluding the target\nsensor, with the goal of either suppressing a true anomaly or triggering a\nfalse alarm at the target node. BETA identifies the sensors most influential to\nthe target node's classification and injects carefully crafted adversarial\nperturbations into their features, all while maintaining stealth and respecting\nthe attacker's budget. Experiments on three real-world sensor network datasets\nshow that BETA reduces the detection accuracy of state-of-the-art GNN-based\ndetectors by 30.62 to 39.16% on average, and significantly outperforms baseline\nattack strategies, while operating within realistic constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BETA\uff0c\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u7070\u76d2\u9003\u907f\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6709\u9650\u8282\u70b9\u4e0a\u6270\u52a8\u4f20\u611f\u5668\u8bfb\u6570\uff0c\u6709\u6548\u964d\u4f4e\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u57fa\u4e8eGNN\u7684\u4f20\u611f\u5668\u7f51\u7edc\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u5728\u9762\u5bf9\u53d7\u9650\u653b\u51fb\u8005\u65f6\u7684\u5b89\u5168\u6027\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u8d34\u8fd1\u73b0\u5b9e\u5a01\u80c1\u6a21\u578b\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "method": "BETA\u901a\u8fc7\u8bc6\u522b\u5bf9\u76ee\u6807\u8282\u70b9\u5206\u7c7b\u6700\u5177\u5f71\u54cd\u529b\u7684\u4f20\u611f\u5668\uff0c\u5e76\u5728\u5176\u7279\u5f81\u4e2d\u6ce8\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u5b9e\u73b0\u9690\u853d\u4e14\u7b26\u5408\u9884\u7b97\u9650\u5236\u7684\u653b\u51fb\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u4f20\u611f\u5668\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBETA\u5e73\u5747\u53ef\u5c06\u6700\u5148\u8fdbGNN\u68c0\u6d4b\u5668\u7684\u68c0\u6d4b\u7cbe\u5ea6\u964d\u4f4e30.62%\u81f339.16%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u653b\u51fb\u7b56\u7565\u3002", "conclusion": "BETA\u5c55\u793a\u4e86\u73b0\u6709GNN\u5f02\u5e38\u68c0\u6d4b\u5668\u5728\u53d7\u9650\u4f46\u5b9e\u9645\u53ef\u884c\u7684\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u63d0\u5347\u6b64\u7c7b\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.17283", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.17283", "abs": "https://arxiv.org/abs/2509.17283", "authors": ["Licheng Zhan", "Bach Le", "Naveed Akhtar", "Tuan Ngo"], "title": "Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models", "comment": null, "summary": "Building compliance checking (BCC) is a critical process for ensuring that\nconstructed facilities meet regulatory standards. A core component of BCC is\nthe accurate enumeration of facility types and their spatial distribution.\nDespite its importance, this problem has been largely overlooked in the\nliterature, posing a significant challenge for BCC and leaving a critical gap\nin existing workflows. Performing this task manually is time-consuming and\nlabor-intensive. Recent advances in large language models (LLMs) offer new\nopportunities to enhance automation by combining visual recognition with\nreasoning capabilities. In this paper, we introduce a new task for BCC:\nautomated facility enumeration, which involves validating the quantity of each\nfacility type against statutory requirements. To address it, we propose a novel\nmethod that integrates door detection with LLM-based reasoning. We are the\nfirst to apply LLMs to this task and further enhance their performance through\na Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse\ndatasets and facility types. Experiments on both real-world and synthetic floor\nplan data demonstrate the effectiveness and robustness of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5efa\u7b51\u5408\u89c4\u6027\u68c0\u67e5\u4e2d\u7684\u81ea\u52a8\u5316\u8bbe\u65bd\u679a\u4e3e\u65b0\u4efb\u52a1\uff0c\u7ed3\u5408\u95e8\u68c0\u6d4b\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u5347\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u4e0a\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8bbe\u65bd\u7c7b\u578b\u53ca\u5176\u7a7a\u95f4\u5206\u5e03\u7684\u51c6\u786e\u7edf\u8ba1\u662f\u5efa\u7b51\u5408\u89c4\u6027\u68c0\u67e5\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\uff0c\u4e14\u4eba\u5de5\u7edf\u8ba1\u8017\u65f6\u8d39\u529b\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u95e8\u68c0\u6d4b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06LLM\u5e94\u7528\u4e8e\u81ea\u52a8\u5316\u8bbe\u65bd\u679a\u4e3e\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7Chain-of-Thought\uff08CoT\uff09\u7b56\u7565\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u5e73\u9762\u56fe\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u8bbe\u65bd\u7c7b\u578b\u4e0a\u5747\u6709\u6548\u4e14\u5177\u6709\u5f3a\u5065\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u5efa\u7b51\u5408\u89c4\u6027\u68c0\u67e5\u4e2d\u8bbe\u65bd\u81ea\u52a8\u7edf\u8ba1\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86LLM\u5728\u89c6\u89c9\u8bc6\u522b\u4e0e\u903b\u8f91\u63a8\u7406\u7ed3\u5408\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17879", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17879", "abs": "https://arxiv.org/abs/2509.17879", "authors": ["Tu Nguyen", "Kevin Du", "Alexander Miserlis Hoyle", "Ryan Cotterell"], "title": "How Persuasive is Your Context?", "comment": "Long paper accepted at EMNLP 2025", "summary": "Two central capabilities of language models (LMs) are: (i) drawing on prior\nknowledge about entities, which allows them to answer queries such as \"What's\nthe official language of Austria?\", and (ii) adapting to new information\nprovided in context, e.g., \"Pretend the official language of Austria is\nTagalog.\", that is pre-pended to the question. In this article, we introduce\ntargeted persuasion score (TPS), designed to quantify how persuasive a given\ncontext is to an LM where persuasion is operationalized as the ability of the\ncontext to alter the LM's answer to the question. In contrast to evaluating\npersuasiveness only by inspecting the greedily decoded answer under the model,\nTPS provides a more fine-grained view of model behavior. Based on the\nWasserstein distance, TPS measures how much a context shifts a model's original\nanswer distribution toward a target distribution. Empirically, through a series\nof experiments, we show that TPS captures a more nuanced notion of\npersuasiveness than previously proposed metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u76ee\u6807\u8bf4\u670d\u5206\u6570\uff08TPS\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u4e0a\u4e0b\u6587\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u8bf4\u670d\u529b\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u8861\u91cf\u4e0a\u4e0b\u6587\u5982\u4f55\u6539\u53d8\u6a21\u578b\u7684\u7b54\u6848\u5206\u5e03\uff0c\u6bd4\u73b0\u6709\u6307\u6807\u66f4\u7ec6\u81f4\u5730\u6355\u6349\u8bf4\u670d\u6548\u679c\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u65e2\u9700\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\u56de\u7b54\u95ee\u9898\uff0c\u53c8\u9700\u6839\u636e\u4e0a\u4e0b\u6587\u8c03\u6574\u7b54\u6848\u3002\u5982\u4f55\u51c6\u786e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u5bf9\u6a21\u578b\u7684\u8bf4\u670d\u529b\u4ecd\u7f3a\u4e4f\u7cbe\u7ec6\u5ea6\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u76ee\u6807\u8bf4\u670d\u5206\u6570\uff08TPS\uff09\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0a\u4e0b\u6587\u524d\u540e\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u7684\u53d8\u5316\u6765\u91cf\u5316\u8bf4\u670d\u529b\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u8d2a\u5a6a\u89e3\u7801\u7684\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTPS\u76f8\u6bd4\u4ee5\u5f80\u6307\u6807\u80fd\u66f4\u7ec6\u817b\u5730\u53cd\u6620\u4e0a\u4e0b\u6587\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u6709\u6548\u6355\u6349\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u8bf4\u670d\u7a0b\u5ea6\u53d8\u5316\u3002", "conclusion": "TPS\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u7ec6\u3001\u53ef\u9760\u7684\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u88ab\u8bf4\u670d\u7a0b\u5ea6\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5e73\u8861\u5148\u9a8c\u77e5\u8bc6\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002"}}
{"id": "2509.17990", "categories": ["cs.LG", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2509.17990", "abs": "https://arxiv.org/abs/2509.17990", "authors": ["Yanbo Zhang", "Michael Levin"], "title": "Equilibrium flow: From Snapshots to Dynamics", "comment": "17 pages, 8 figures", "summary": "Scientific data, from cellular snapshots in biology to celestial\ndistributions in cosmology, often consists of static patterns from underlying\ndynamical systems. These snapshots, while lacking temporal ordering, implicitly\nencode the processes that preserve them. This work investigates how strongly\nsuch a distribution constrains its underlying dynamics and how to recover them.\nWe introduce the Equilibrium flow method, a framework that learns continuous\ndynamics that preserve a given pattern distribution. Our method successfully\nidentifies plausible dynamics for 2-D systems and recovers the signature\nchaotic behavior of the Lorenz attractor. For high-dimensional Turing patterns\nfrom the Gray-Scott model, we develop an efficient, training-free variant that\nachieves high fidelity to the ground truth, validated both quantitatively and\nqualitatively. Our analysis reveals the solution space is constrained not only\nby the data but also by the learning model's inductive biases. This capability\nextends beyond recovering known systems, enabling a new paradigm of inverse\ndesign for Artificial Life. By specifying a target pattern distribution, we can\ndiscover the local interaction rules that preserve it, leading to the\nspontaneous emergence of complex behaviors, such as life-like flocking,\nattraction, and repulsion patterns, from simple, user-defined snapshots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Equilibrium flow\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9759\u6001\u6a21\u5f0f\u5206\u5e03\u4e2d\u5b66\u4e60\u4fdd\u6301\u8be5\u5206\u5e03\u7684\u8fde\u7eed\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e2D\u7cfb\u7edf\u3001Lorenz\u5438\u5f15\u5b50\u53ca\u9ad8\u7ef4Gray-Scott\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6570\u636e\u4e0e\u6a21\u578b\u5f52\u7eb3\u504f\u7f6e\u5171\u540c\u7ea6\u675f\u89e3\u7a7a\u95f4\uff0c\u8fdb\u4e00\u6b65\u652f\u6301\u4eba\u5de5\u751f\u547d\u4e2d\u7684\u9006\u5411\u8bbe\u8ba1\u3002", "motivation": "\u9759\u6001\u79d1\u5b66\u6570\u636e\u9690\u542b\u4e86\u751f\u6210\u5b83\u4eec\u7684\u52a8\u529b\u5b66\u8fc7\u7a0b\uff0c\u4f46\u5982\u4f55\u4ece\u65e0\u65f6\u5e8f\u7684\u5feb\u7167\u4e2d\u6062\u590d\u5e95\u5c42\u52a8\u529b\u5b66\u5c1a\u4e0d\u660e\u786e\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5206\u5e03\u5bf9\u52a8\u529b\u5b66\u7684\u7ea6\u675f\u80fd\u529b\u5e76\u63d0\u51fa\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEquilibrium flow\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4fdd\u6301\u7ed9\u5b9a\u6a21\u5f0f\u5206\u5e03\u7684\u8fde\u7eed\u52a8\u529b\u5b66\u7cfb\u7edf\u6765\u6062\u590d\u5e95\u5c42\u52a8\u6001\uff1b\u9488\u5bf9\u9ad8\u7ef4\u56fe\u7075\u6a21\u5f0f\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u53d8\u4f53\u3002", "result": "\u57282D\u7cfb\u7edf\u4e2d\u8bc6\u522b\u51fa\u5408\u7406\u52a8\u529b\u5b66\uff0c\u5728Lorenz\u5438\u5f15\u5b50\u4e2d\u6062\u590d\u6df7\u6c8c\u884c\u4e3a\uff0cGray-Scott\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u6a21\u5f0f\u91cd\u5efa\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6a21\u578b\u5f52\u7eb3\u504f\u7f6e\u5bf9\u89e3\u7a7a\u95f4\u7684\u5f71\u54cd\u3002", "conclusion": "\u9759\u6001\u5206\u5e03\u53ef\u6709\u6548\u7ea6\u675f\u6f5c\u5728\u52a8\u529b\u5b66\uff0c\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u6062\u590d\u5df2\u77e5\u7cfb\u7edf\uff0c\u8fd8\u53ef\u7528\u4e8e\u4eba\u5de5\u751f\u547d\u7684\u9006\u5411\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8bbe\u5b9a\u76ee\u6807\u5206\u5e03\u53d1\u73b0\u751f\u6210\u590d\u6742\u884c\u4e3a\u7684\u5c40\u90e8\u4ea4\u4e92\u89c4\u5219\u3002"}}
{"id": "2509.17912", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17912", "abs": "https://arxiv.org/abs/2509.17912", "authors": ["Nevidu Jayatilleke", "Nisansa de Silva"], "title": "SiDiaC: Sinhala Diachronic Corpus", "comment": "14 pages, 7 figures, 7 tables, Accepted paper at the 39th Pacific\n  Asia Conference on Language, Information and Computation (PACLIC 39)", "summary": "SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a\nhistorical span from the 5th to the 20th century CE. SiDiaC comprises 58k words\nacross 46 literary works, annotated carefully based on the written date, after\nfiltering based on availability, authorship, copyright compliance, and data\nattribution. Texts from the National Library of Sri Lanka were digitised using\nGoogle Document AI OCR, followed by post-processing to correct formatting and\nmodernise the orthography. The construction of SiDiaC was informed by practices\nfrom other corpora, such as FarPaHC, particularly in syntactic annotation and\ntext normalisation strategies, due to the shared characteristics of\nlow-resourced language status. This corpus is categorised based on genres into\ntwo layers: primary and secondary. Primary categorisation is binary,\nclassifying each book into Non-Fiction or Fiction, while the secondary\ncategorisation is more specific, grouping texts under Religious, History,\nPoetry, Language, and Medical genres. Despite challenges including limited\naccess to rare texts and reliance on secondary date sources, SiDiaC serves as a\nfoundational resource for Sinhala NLP, significantly extending the resources\navailable for Sinhala, enabling diachronic studies in lexical change, neologism\ntracking, historical syntax, and corpus-based lexicography.", "AI": {"tldr": "SiDiaC\u662f\u9996\u4e2a\u5168\u9762\u7684\u50e7\u4f3d\u7f57\u8bed\u5386\u65f6\u8bed\u6599\u5e93\uff0c\u6db5\u76d6\u516c\u51435\u4e16\u7eaa\u81f320\u4e16\u7eaa\uff0c\u5305\u542b46\u90e8\u6587\u5b66\u4f5c\u54c1\u517158,000\u8bcd\uff0c\u7ecf\u8fc7\u7cbe\u7ec6\u6807\u6ce8\u548c\u5904\u7406\uff0c\u4e3a\u50e7\u4f3d\u7f57\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u4f4e\u8d44\u6e90\u8bed\u8a00\u50e7\u4f3d\u7f57\u8bed\u5728\u5386\u65f6\u8bed\u8a00\u7814\u7a76\u65b9\u9762\u7684\u8d44\u6e90\u7a7a\u767d\uff0c\u63a8\u52a8\u5176\u81ea\u7136\u8bed\u8a00\u5904\u7406\u53d1\u5c55\u3002", "method": "\u57fa\u4e8e\u6587\u672c\u64b0\u5199\u65f6\u95f4\u5bf9\u6765\u81ea\u65af\u91cc\u5170\u5361\u56fd\u5bb6\u56fe\u4e66\u9986\u7684\u6587\u732e\u8fdb\u884c\u7b5b\u9009\u4e0e\u6570\u5b57\u5316\uff0c\u4f7f\u7528Google Document AI OCR\u6280\u672f\u8bc6\u522b\u6587\u5b57\uff0c\u5e76\u8fdb\u884c\u683c\u5f0f\u4fee\u6b63\u548c\u6b63\u5b57\u6cd5\u73b0\u4ee3\u5316\uff1b\u53c2\u8003FarPaHC\u7b49\u8bed\u6599\u5e93\u7684\u6784\u5efa\u65b9\u6cd5\u8fdb\u884c\u53e5\u6cd5\u6807\u6ce8\u548c\u6587\u672c\u5f52\u4e00\u5316\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u4e24\u4e2a\u5c42\u7ea7\u5206\u7c7b\uff08\u4e3b\u8981\u5206\u4e3a\u975e\u865a\u6784/\u865a\u6784\uff0c\u6b21\u8981\u7ec6\u5206\u4e3a\u5b97\u6559\u3001\u5386\u53f2\u3001\u8bd7\u6b4c\u3001\u8bed\u8a00\u3001\u533b\u5b66\uff09\u7684\u50e7\u4f3d\u7f57\u8bed\u5386\u65f6\u8bed\u6599\u5e93SiDiaC\uff0c\u517146\u90e8\u4f5c\u54c1\u300158k\u8bcd\u3002", "conclusion": "SiDiaC\u4f5c\u4e3a\u9996\u4e2a\u7cfb\u7edf\u7684\u50e7\u4f3d\u7f57\u8bed\u5386\u65f6\u8bed\u6599\u5e93\uff0c\u4e3a\u8bcd\u6c47\u6f14\u53d8\u3001\u65b0\u8bcd\u8ffd\u8e2a\u3001\u5386\u53f2\u53e5\u6cd5\u548c\u57fa\u4e8e\u8bed\u6599\u7684\u8bcd\u5178\u7f16\u7e82\u7b49\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u663e\u8457\u6269\u5c55\u4e86\u50e7\u4f3d\u7f57\u8bed\u7684\u8bed\u8a00\u8d44\u6e90\u3002"}}
{"id": "2509.17998", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17998", "abs": "https://arxiv.org/abs/2509.17998", "authors": ["Richard Cornelius Suwandi", "Feng Yin", "Juntao Wang", "Renjie Li", "Tsung-Hui Chang", "Sergios Theodoridis"], "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs", "comment": "Accepted as Poster at NeurIPS 2025", "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6838\u8fdb\u5316\u65b9\u6cd5\uff08CAKE\uff09\uff0c\u7ed3\u5408BAKER\u7b56\u7565\u4f18\u5316\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7684\u6838\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u8d85\u53c2\u6570\u4f18\u5316\u3001\u63a7\u5236\u5668\u8c03\u53c2\u4e0e\u5149\u5b50\u82af\u7247\u8bbe\u8ba1\u7b49\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u4f9d\u8d56\u56fa\u5b9a\u6216\u542f\u53d1\u5f0f\u9ad8\u65af\u8fc7\u7a0b\u6838\u9009\u62e9\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u76ee\u6807\u51fd\u6570\uff0c\u5bfc\u81f4\u6536\u655b\u6162\u6216\u7ed3\u679c\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4ea4\u53c9\u548c\u53d8\u5f02\u7b97\u5b50\uff0c\u81ea\u9002\u5e94\u751f\u6210\u548c\u4f18\u5316\u9ad8\u65af\u8fc7\u7a0b\u6838\uff1b\u7ed3\u5408BIC\u4e0e\u671f\u671b\u6539\u8fdb\u7684BAKER\u7b56\u7565\u8fdb\u884c\u6838\u9009\u62e9\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cCAKE-based BO\u65b9\u6cd5 consistently \u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CAKE\u901a\u8fc7LLM\u9a71\u52a8\u7684\u6838\u8fdb\u5316\u548cBAKER\u6838\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6548\u7387\u4e0e\u9002\u5e94\u6027\u3002"}}
{"id": "2509.17328", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17328", "abs": "https://arxiv.org/abs/2509.17328", "authors": ["Hongxin Li", "Jingran Su", "Jingfan Chen", "Zheng Ju", "Yuntao Chen", "Qing Li", "Zhaoxiang Zhang"], "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents", "comment": "Accepted to ICCV 2025", "summary": "Building autonomous agents that perceive and operate graphical user\ninterfaces (GUIs) like humans has long been a vision in the field of artificial\nintelligence. Central to these agents is the capability for GUI interaction,\nwhich involves GUI understanding and planning capabilities. Existing methods\nhave tried developing GUI agents based on the multi-modal comprehension ability\nof vision-language models (VLMs). However, the limited scenario, insufficient\nsize, and heterogeneous action spaces hinder the progress of building\ngeneralist GUI agents. To resolve these issues, this paper proposes\n\\textbf{UIPro}, a novel generalist GUI agent trained with extensive\nmulti-platform and multi-task GUI interaction data, coupled with a unified\naction space. We first curate a comprehensive dataset encompassing 20.6 million\nGUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding\ncapability, which is key to downstream GUI agent tasks. Subsequently, we\nestablish a unified action space to harmonize heterogeneous GUI agent task\ndatasets and produce a merged dataset to foster the action prediction ability\nof UIPro via continued fine-tuning. Experimental results demonstrate UIPro's\nsuperior performance across multiple GUI task benchmarks on various platforms,\nhighlighting the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UIPro\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u89c4\u6a21\u591a\u5e73\u53f0\u3001\u591a\u4efb\u52a1GUI\u4ea4\u4e92\u6570\u636e\u8bad\u7ec3\u7684\u901a\u7528GUI\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u63d0\u5347GUI\u7406\u89e3\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u573a\u666f\u5355\u4e00\u3001\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\u548c\u52a8\u4f5c\u7a7a\u95f4\u5f02\u6784\uff0c\u96be\u4ee5\u6784\u5efa\u901a\u7528\u7684GUI\u667a\u80fd\u4f53\u3002", "method": "\u9996\u5148\u6784\u5efa\u5305\u542b2060\u4e07GUI\u7406\u89e3\u4efb\u52a1\u7684\u6570\u636e\u96c6\u7528\u4e8e\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u8bbe\u8ba1\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u5e76\u878d\u5408\u591a\u4efb\u52a1\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aUIPro\u5728\u591a\u4e2a\u5e73\u53f0\u7684GUI\u4efb\u52a1\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UIPro\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u548c\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u6709\u6548\u63d0\u5347\u4e86GUI\u667a\u80fd\u4f53\u7684\u901a\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2509.17921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17921", "abs": "https://arxiv.org/abs/2509.17921", "authors": ["Zhenyun Deng", "Yulong Chen", "Andreas Vlachos"], "title": "Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning", "comment": "Accepted to EMLNP 2025 (Main Conference)", "summary": "Extracting individual sentences from a document as evidence or reasoning\nsteps is commonly done in many NLP tasks. However, extracted sentences often\nlack context necessary to make them understood, e.g., coreference and\nbackground information. To this end, we propose a content selection and\nplanning framework for zero-shot decontextualisation, which determines what\ncontent should be mentioned and in what order for a sentence to be understood\nout of context. Specifically, given a potentially ambiguous sentence and its\ncontext, we first segment it into basic semantically-independent units. We then\nidentify potentially ambiguous units from the given sentence, and extract\nrelevant units from the context based on their discourse relations. Finally, we\ngenerate a content plan to rewrite the sentence by enriching each ambiguous\nunit with its relevant units. Experimental results demonstrate that our\napproach is competitive for sentence decontextualisation, producing sentences\nthat exhibit better semantic integrity and discourse coherence, outperforming\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u53bb\u4e0a\u4e0b\u6587\u5316\u7684\u5185\u5bb9\u9009\u62e9\u4e0e\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u8865\u5145\u53e5\u5b50\u4e2d\u7684\u6a21\u7cca\u5355\u5143\u6765\u589e\u5f3a\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u7bc7\u7ae0\u8fde\u8d2f\u6027\u3002", "motivation": "\u89e3\u51b3\u4ece\u6587\u6863\u4e2d\u63d0\u53d6\u7684\u53e5\u5b50\u5e38\u56e0\u7f3a\u4e4f\u4e0a\u4e0b\u6587\uff08\u5982\u5171\u6307\u548c\u80cc\u666f\u4fe1\u606f\uff09\u800c\u96be\u4ee5\u7406\u89e3\u7684\u95ee\u9898\u3002", "method": "\u5c06\u53e5\u5b50\u5206\u5272\u4e3a\u8bed\u4e49\u72ec\u7acb\u5355\u5143\uff0c\u8bc6\u522b\u5176\u4e2d\u7684\u6a21\u7cca\u5355\u5143\uff0c\u5e76\u6839\u636e\u8bdd\u8bed\u5173\u7cfb\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u76f8\u5173\u5355\u5143\uff0c\u751f\u6210\u5185\u5bb9\u89c4\u5212\u4ee5\u91cd\u5199\u53e5\u5b50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53e5\u5b50\u53bb\u4e0a\u4e0b\u6587\u5316\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u53e5\u5b50\u5177\u6709\u66f4\u597d\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u7bc7\u7ae0\u8fde\u8d2f\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u53e5\u5b50\u8131\u79bb\u4e0a\u4e0b\u6587\u540e\u7684\u53ef\u7406\u89e3\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u7684\u8bc1\u636e\u6216\u63a8\u7406\u6b65\u9aa4\u63d0\u53d6\u3002"}}
{"id": "2509.18001", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18001", "abs": "https://arxiv.org/abs/2509.18001", "authors": ["Haocheng Luo", "Mehrtash Harandi", "Dinh Phung", "Trung Le"], "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise", "comment": null, "summary": "Sharpness-aware minimization (SAM) has emerged as a highly effective\ntechnique for improving model generalization, but its underlying principles are\nnot fully understood. We investigated the phenomenon known as m-sharpness,\nwhere the performance of SAM improves monotonically as the micro-batch size for\ncomputing perturbations decreases. Leveraging an extended Stochastic\nDifferential Equation (SDE) framework, combined with an analysis of the\nstructure of stochastic gradient noise (SGN), we precisely characterize the\ndynamics of various SAM variants. Our findings reveal that the stochastic noise\nintroduced during SAM perturbations inherently induces a variance-based\nsharpness regularization effect. Motivated by our theoretical insights, we\nintroduce Reweighted SAM, which employs sharpness-weighted sampling to mimic\nthe generalization benefits of m-SAM while remaining parallelizable.\nComprehensive experiments validate the effectiveness of our theoretical\nanalysis and proposed method.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Sharpness-aware minimization\uff08SAM\uff09\u4e2d\u5fae\u6279\u91cf\u5927\u5c0f\u5bf9\u6027\u80fd\u5f71\u54cd\u7684m-sharpness\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548c\u68af\u5ea6\u566a\u58f0\u5206\u6790\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u636e\u6b64\u8bbe\u8ba1\u4e86\u53ef\u5e76\u884c\u5316\u7684Reweighted SAM\u65b9\u6cd5\u3002", "motivation": "SAM\u867d\u6709\u6548\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u539f\u7406\u5c1a\u4e0d\u6e05\u6670\uff0c\u5c24\u5176\u662f\u5fae\u6279\u91cf\u5927\u5c0f\u5f71\u54cd\u6027\u80fd\u7684m-sharpness\u73b0\u8c61\u4e9f\u9700\u89e3\u91ca\u3002", "method": "\u6269\u5c55\u4e86\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u968f\u673a\u68af\u5ea6\u566a\u58f0\uff08SGN\uff09\u7ed3\u6784\u7684\u5206\u6790\uff0c\u63ed\u793aSAM\u6270\u52a8\u4e2d\u7684\u566a\u58f0\u5177\u6709\u65b9\u5dee\u578bsharpness\u6b63\u5219\u5316\u6548\u5e94\uff0c\u5e76\u63d0\u51faReweighted SAM\u4ee5\u6a21\u62dfm-SAM\u7684\u6cdb\u5316\u4f18\u52bf\u3002", "result": "\u7406\u8bba\u5206\u6790\u51c6\u786e\u523b\u753b\u4e86\u591a\u79cdSAM\u53d8\u4f53\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u5e76\u884c\u6027\u7684\u540c\u65f6\u80fd\u6709\u6548\u590d\u73b0m-SAM\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "SAM\u7684m-sharpness\u73b0\u8c61\u6e90\u4e8e\u6270\u52a8\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u566a\u58f0\u6240\u5f15\u5165\u7684\u65b9\u5dee\u578bsharpness\u6b63\u5219\u5316\uff0cReweighted SAM\u53ef\u9ad8\u6548\u5229\u7528\u8be5\u673a\u5236\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.17329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17329", "abs": "https://arxiv.org/abs/2509.17329", "authors": ["Neham Jain", "Andrew Jong", "Sebastian Scherer", "Ioannis Gkioulekas"], "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction", "comment": "Project website: https://imaging.cs.cmu.edu/smokeseer", "summary": "Smoke in real-world scenes can severely degrade the quality of images and\nhamper visibility. Recent methods for image restoration either rely on\ndata-driven priors that are susceptible to hallucinations, or are limited to\nstatic low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D\nscene reconstruction and smoke removal from a video capturing multiple views of\na scene. Our method uses thermal and RGB images, leveraging the fact that the\nreduced scattering in thermal images enables us to see through the smoke. We\nbuild upon 3D Gaussian splatting to fuse information from the two image\nmodalities, and decompose the scene explicitly into smoke and non-smoke\ncomponents. Unlike prior approaches, SmokeSeer handles a broad range of smoke\ndensities and can adapt to temporally varying smoke. We validate our approach\non synthetic data and introduce a real-world multi-view smoke dataset with RGB\nand thermal images. We provide open-source code and data at the project\nwebsite.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSmokeSeer\uff0c\u4e00\u79cd\u5229\u7528\u70ed\u6210\u50cf\u548cRGB\u56fe\u50cf\u4ece\u591a\u89c6\u89d2\u89c6\u9891\u4e2d\u540c\u65f6\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\u548c\u53bb\u70df\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u5bc6\u5ea6\u53ca\u52a8\u6001\u53d8\u5316\u7684\u70df\u96fe\u3002", "motivation": "\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u70df\u96fe\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u4e0e\u53ef\u89c1\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5bc6\u5ea6\u6216\u52a8\u6001\u70df\u96fe\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u4e14\u6613\u4ea7\u751f\u5e7b\u89c9\u4f2a\u5f71\u3002", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u70b9\u9635\u5316\u6280\u672f\u878d\u5408\u70ed\u6210\u50cf\u4e0eRGB\u56fe\u50cf\u4fe1\u606f\uff0c\u5229\u7528\u70ed\u56fe\u50cf\u7a7f\u900f\u70df\u96fe\u7684\u80fd\u529b\uff0c\u663e\u5f0f\u5730\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u70df\u96fe\u4e0e\u975e\u70df\u96fe\u6210\u5206\uff0c\u5b9e\u73b0\u8054\u5408\u53bb\u70df\u4e0e3D\u91cd\u5efa\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u89d2RGB\u4e0e\u70ed\u56fe\u50cf\u7684\u70df\u96fe\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u9002\u5e94\u591a\u79cd\u70df\u96fe\u5bc6\u5ea6\u548c\u65f6\u53d8\u70df\u96fe\u3002", "conclusion": "SmokeSeer\u9996\u6b21\u5b9e\u73b0\u4e86\u7ed3\u5408\u70ed\u6210\u50cf\u7684\u591a\u6a21\u6001\u591a\u89c6\u89d2\u53bb\u70df\u4e0e3D\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u70df\u96fe\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u6062\u590d\u4e0e\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2509.17930", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17930", "abs": "https://arxiv.org/abs/2509.17930", "authors": ["Yiwen Guan", "Jacob Whitehill"], "title": "Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation", "comment": null, "summary": "Multilingual translation faces challenges of computational redundancy and\nlimited accuracy for low-resource languages, especially in speech translation.\nTo address this, we propose a novel hierarchical Transformer Encoder Tree (TET)\ncombined with non-autoregressive encoder-only models trained with Connectionist\nTemporal Classification for multilingual translation. By sharing intermediate\nrepresentations among linguistically similar target languages, TET can improve\naccuracy on low-resource languages, reduce computational redundancy, and allow\ngenerating all target languages in a single forward pass, thus eliminating\nsequential bottlenecks and improving parallelism. For speech translation,\ncombining TET with a non-autoregressive speech recognition backbone (wav2vec2)\nshows promising results in terms of translation quality compared to\nautoregressive systems while being 7-14 times faster.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u5316Transformer\u7f16\u7801\u5668\u6811\uff08TET\uff09\u7684\u975e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u7ffb\u8bd1\uff0c\u5c24\u5176\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u8ba1\u7b97\u5197\u4f59\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8bed\u97f3\u7ffb\u8bd1\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u5c42\u6b21\u5316Transformer\u7f16\u7801\u5668\u6811\uff08TET\uff09\uff0c\u5728\u8bed\u8a00\u5b66\u76f8\u4f3c\u7684\u76ee\u6807\u8bed\u8a00\u95f4\u5171\u4eab\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8ewav2vec2\u7684\u975e\u81ea\u56de\u5f52\u7f16\u7801\u5668-\u4ec5\u6a21\u578b\uff0c\u4f7f\u7528CTC\u8bad\u7ec3\u8fdb\u884c\u591a\u8bed\u8a00\u7ffb\u8bd1\u3002", "result": "TET\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5197\u4f59\uff0c\u652f\u6301\u5355\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u6240\u6709\u76ee\u6807\u8bed\u8a00\uff1b\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\uff0c\u76f8\u6bd4\u81ea\u56de\u5f52\u7cfb\u7edf\u901f\u5ea6\u5feb7-14\u500d\uff0c\u4e14\u7ffb\u8bd1\u8d28\u91cf\u66f4\u4f18\u3002", "conclusion": "TET\u7ed3\u6784\u6709\u6548\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u7ffb\u8bd1\u7684\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u9700\u8981\u9ad8\u5e76\u884c\u5ea6\u7684\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u3002"}}
{"id": "2509.18034", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.18034", "abs": "https://arxiv.org/abs/2509.18034", "authors": ["Erkan Bayram", "Mohamed-Ali Belabbas", "Tamer Ba\u015far"], "title": "Control Disturbance Rejection in Neural ODEs", "comment": "Accepted for publication in IEEE CDC 2025", "summary": "In this paper, we propose an iterative training algorithm for Neural ODEs\nthat provides models resilient to control (parameter) disturbances. The method\nbuilds on our earlier work Tuning without Forgetting-and similarly introduces\ntraining points sequentially, and updates the parameters on new data within the\nspace of parameters that do not decrease performance on the previously learned\ntraining points-with the key difference that, inspired by the concept of flat\nminima, we solve a minimax problem for a non-convex non-concave functional over\nan infinite-dimensional control space. We develop a projected gradient descent\nalgorithm on the space of parameters that admits the structure of an\ninfinite-dimensional Banach subspace. We show through simulations that this\nformulation enables the model to effectively learn new data points and gain\nrobustness against control disturbance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u795e\u7ecfODE\u7684\u8fed\u4ee3\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u65e0\u9650\u7ef4\u63a7\u5236\u7a7a\u95f4\u4e2d\u6c42\u89e3\u6781\u5c0f\u6781\u5927\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u5bf9\u53c2\u6570\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u63d0\u5347\u795e\u7ecfODE\u6a21\u578b\u5728\u53c2\u6570\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u907f\u514d\u9057\u5fd8\u5148\u524d\u5b66\u4e60\u7684\u77e5\u8bc6\u3002", "method": "\u57fa\u4e8e\u2018\u4e0d\u9057\u5fd8\u8c03\u4f18\u2019\u601d\u60f3\uff0c\u9010\u4e2a\u5f15\u5165\u8bad\u7ec3\u70b9\uff0c\u5728\u4fdd\u6301\u539f\u6709\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u66f4\u65b0\u53c2\u6570\uff0c\u5e76\u5728\u65e0\u9650\u7ef4\u5df4\u62ff\u8d6b\u5b50\u7a7a\u95f4\u4e0a\u8bbe\u8ba1\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6c42\u89e3\u975e\u51f8\u975e\u51f9\u7684\u6781\u5c0f\u6781\u5927\u95ee\u9898\u3002", "result": "\u4eff\u771f\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u65b0\u6570\u636e\u70b9\uff0c\u5e76\u589e\u5f3a\u5bf9\u63a7\u5236\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u8fed\u4ee3\u8bad\u7ec3\u7b97\u6cd5\u80fd\u591f\u5728\u4e0d\u727a\u7272\u5df2\u6709\u6027\u80fd\u7684\u57fa\u7840\u4e0a\u6301\u7eed\u5b66\u4e60\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u5bf9\u53c2\u6570\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.17365", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17365", "abs": "https://arxiv.org/abs/2509.17365", "authors": ["Amanuel Tafese Dufera"], "title": "Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model", "comment": null, "summary": "Automatic image captioning, a multifaceted task bridging computer vision and\nnatural lan- guage processing, aims to generate descriptive textual content\nfrom visual input. While Convolutional Neural Networks (CNNs) and Long\nShort-Term Memory (LSTM) networks have achieved significant advancements, they\npresent limitations. The inherent sequential nature of RNNs leads to sluggish\ntraining and inference times. LSTMs further struggle with retaining information\nfrom earlier sequence elements when dealing with very long se- quences. This\nproject presents a comprehensive guide to constructing and comprehending\ntransformer models for image captioning. Transformers employ self-attention\nmechanisms, capturing both short- and long-range dependencies within the data.\nThis facilitates efficient parallelization during both training and inference\nphases. We leverage the well-established Transformer architecture, recognized\nfor its effectiveness in managing sequential data, and present a meticulous\nmethodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,\nconstruct a model architecture that integrates an EfficientNetB0 CNN for fea-\nture extraction, and train the model with attention mechanisms incorporated.\nOur approach exemplifies the utilization of parallelization for efficient\ntraining and inference. You can find the project on GitHub.", "AI": {"tldr": "\u672c\u9879\u76ee\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528EfficientNetB0\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u4e0e\u63a8\u7406\u3002", "motivation": "\u4f20\u7edfCNN\u548cLSTM\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u5b58\u5728\u8bad\u7ec3\u6162\u3001\u96be\u4ee5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u7ed3\u5408EfficientNetB0\u4f5c\u4e3a\u5377\u79ef\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728Flickr30k\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e76\u884c\u5316\u8bad\u7ec3\u4e0e\u63a8\u7406\uff0c\u6a21\u578b\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u9a8c\u8bc1\u4e86Transformer\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Transformer\u7ed3\u5408CNN\u7684\u67b6\u6784\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u826f\u597d\u7684\u5e76\u884c\u6027\u548c\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edfRNN\u7ed3\u6784\u3002"}}
{"id": "2509.17932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17932", "abs": "https://arxiv.org/abs/2509.17932", "authors": ["Runheng Liu", "Heyan Huang", "Xingchen Xiao", "Zhijing Wu"], "title": "Training-free Truthfulness Detection via Value Vectors in LLMs", "comment": null, "summary": "Large language models often generate factually incorrect outputs, motivating\nefforts to detect the truthfulness of their content. Most existing approaches\nrely on training probes over internal activations, but these methods suffer\nfrom scalability and generalization issues. A recent training-free method,\nNoVo, addresses this challenge by exploiting statistical patterns from the\nmodel itself. However, it focuses exclusively on attention mechanisms,\npotentially overlooking the MLP module-a core component of Transformer models\nknown to support factual recall. In this paper, we show that certain value\nvectors within MLP modules exhibit truthfulness-related statistical patterns.\nBuilding on this insight, we propose TruthV, a simple and interpretable\ntraining-free method that detects content truthfulness by leveraging these\nvalue vectors. On the NoVo benchmark, TruthV significantly outperforms both\nNoVo and log-likelihood baselines, demonstrating that MLP modules-despite being\nneglected in prior training-free efforts-encode rich and useful signals for\ntruthfulness detection. These findings offer new insights into how truthfulness\nis internally represented in LLMs and motivate further research on scalable and\ninterpretable truthfulness detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u578b\u65b9\u6cd5TruthV\uff0c\u5229\u7528MLP\u6a21\u5757\u4e2d\u7684\u503c\u5411\u91cf\u6765\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u5728NoVo\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u771f\u5b9e\u6027\u68c0\u6d4b\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u63a2\u9488\u6216\u4ec5\u5173\u6ce8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e14\u5ffd\u7565\u4e86MLP\u6a21\u5757\u5728\u4e8b\u5b9e\u56de\u5fc6\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790MLP\u6a21\u5757\u4e2d\u503c\u5411\u91cf\u7684\u7edf\u8ba1\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u53ef\u89e3\u91ca\u7684\u8bad\u7ec3-free\u65b9\u6cd5TruthV\uff0c\u7528\u4e8e\u68c0\u6d4b\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002", "result": "TruthV\u5728NoVo\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8eNoVo\u548c\u5bf9\u6570\u4f3c\u7136\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86MLP\u6a21\u5757\u5305\u542b\u4e30\u5bcc\u7684\u771f\u5b9e\u6027\u4fe1\u53f7\u3002", "conclusion": "MLP\u6a21\u5757\u5728\u771f\u5b9e\u6027\u68c0\u6d4b\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0cTruthV\u4e3a\u7406\u89e3\u5927\u6a21\u578b\u5185\u90e8\u771f\u5b9e\u6027\u8868\u5f81\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u7814\u7a76\u3002"}}
{"id": "2509.18057", "categories": ["cs.LG", "cs.AI", "cs.CC", "math.CO"], "pdf": "https://arxiv.org/pdf/2509.18057", "abs": "https://arxiv.org/abs/2509.18057", "authors": ["Ansh Nagda", "Prabhakar Raghavan", "Abhradeep Thakurta"], "title": "Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory", "comment": null, "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve provable limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528AlphaEvolve\uff08\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f16\u7801\u4ee3\u7406\uff09\u5728\u7ec4\u5408\u7ed3\u6784\u53d1\u73b0\u4e2d\u6539\u8fdb\u4e86MAX-CUT\u3001MAX-Independent Set\u548cMAX-k-CUT\u95ee\u9898\u7684\u7b97\u6cd5\u53ef\u8bc1\u660e\u754c\u9650\uff0c\u5305\u62ec\u5e73\u5747\u60c5\u51b5\u548c\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u786c\u5ea6\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7AI\u4f18\u5316\u9a8c\u8bc1\u8fc7\u7a0b\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u63a2\u7d22AI\u6280\u672f\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u662f\u5426\u80fd\u5e2e\u52a9\u53d1\u73b0\u65b0\u7684\u7ec4\u5408\u7ed3\u6784\uff0c\u4ece\u800c\u6539\u8fdb\u5bf9\u9ad8\u6548\u7b97\u6cd5\u6027\u80fd\u6781\u9650\u7684\u53ef\u8bc1\u660e\u8fb9\u754c\uff0c\u5c24\u5176\u662f\u5728\u56fe\u8bba\u4e2d\u7684\u7ecf\u5178NP\u96be\u95ee\u9898\u4e0a\u3002", "method": "\u4f7f\u7528AlphaEvolve\u8fd9\u4e00LLM\u7f16\u7801\u4ee3\u7406\uff0c\u5728\u4e24\u79cd\u8bbe\u5b9a\u4e0b\u8fdb\u884c\u63a2\u7d22\uff1a1) \u901a\u8fc7\u6784\u9020\u63a5\u8fd1\u6781\u503c\u7684Ramanujan\u56fe\u5e76\u7ed3\u5408\u89e3\u6790\u8bba\u8bc1\uff0c\u6539\u8fdb\u968f\u673a\u6b63\u5219\u56fe\u4e0a\u7684MAX-CUT\u4e0e\u72ec\u7acb\u96c6\u95ee\u9898\u7684\u4e0a\u4e0b\u754c\uff1b2) \u5229\u7528AlphaEvolve\u53d1\u73b0\u65b0\u7684 gadget \u5f52\u7ea6\uff0c\u83b7\u5f97MAX-4-CUT\u548cMAX-3-CUT\u7684\u66f4\u5f3a\u4e0d\u53ef\u8fd1\u4f3c\u6027\u7ed3\u679c\u3002\u540c\u65f6\uff0c\u7528AlphaEvolve\u81ea\u8eab\u4f18\u5316\u9a8c\u8bc1\u8fc7\u7a0b\u4ee5\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u5e73\u5747\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5bf9MAX-CUT\u548cMAX-Independent Set\u8fd1\u4e4e\u6700\u4f18\u7684\u4e0a\u4e0b\u754c\uff1b\u5728\u6700\u574f\u60c5\u51b5\u4e0b\uff0c\u5c06MAX-4-CUT\u7684\u4e0d\u53ef\u8fd1\u4f3c\u6bd4\u6539\u8fdb\u81f30.987\uff08\u4f18\u4e8e\u5148\u524d\u6700\u4f730.9883\uff09\uff0cMAX-3-CUT\u6539\u8fdb\u81f30.9649\uff08\u4f18\u4e8e\u6b64\u524d\u57fa\u4e8egadget\u7684\u7ed3\u679c0.9853\uff09\uff1b\u5e76\u901a\u8fc7AI\u52a0\u901f\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe10,000\u500d\u3002", "conclusion": "AlphaEvolve\u4e0d\u4ec5\u80fd\u8f85\u52a9\u53d1\u73b0\u590d\u6742\u7ec4\u5408\u7ed3\u6784\uff0c\u8fd8\u80fd\u81ea\u6211\u4f18\u5316\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u663e\u8457\u63a8\u52a8\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u96be\u9898\u7684\u8fb9\u754c\u63a2\u7d22\uff0c\u5c55\u793a\u4e86AI\u5728\u5f62\u5f0f\u5316\u6570\u5b66\u4e0e\u7b97\u6cd5\u7406\u8bba\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17374", "abs": "https://arxiv.org/abs/2509.17374", "authors": ["Ankit Yadav", "Ta Duc Huy", "Lingqiao Liu"], "title": "Revisiting Vision Language Foundations for No-Reference Image Quality Assessment", "comment": "23 pages, 16 figures", "summary": "Large-scale vision language pre-training has recently shown promise for\nno-reference image-quality assessment (NR-IQA), yet the relative merits of\nmodern Vision Transformer foundations remain poorly understood. In this work,\nwe present the first systematic evaluation of six prominent pretrained\nbackbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task\nof No-Reference Image Quality Assessment (NR-IQA), each finetuned using an\nidentical lightweight MLP head. Our study uncovers two previously overlooked\nfactors: (1) SigLIP2 consistently achieves strong performance; and (2) the\nchoice of activation function plays a surprisingly crucial role, particularly\nfor enhancing the generalization ability of image quality assessment models.\nNotably, we find that simple sigmoid activations outperform commonly used ReLU\nand GELU on several benchmarks. Motivated by this finding, we introduce a\nlearnable activation selection mechanism that adaptively determines the\nnonlinearity for each channel, eliminating the need for manual activation\ndesign, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and\nAGIQA3K. Extensive ablations confirm the benefits across architectures and\nregimes, establishing strong, resource-efficient NR-IQA baselines.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5728\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08NR-IQA\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0SigLIP2\u6027\u80fd\u4f18\u5f02\uff0c\u4e14\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f71\u54cd\u663e\u8457\uff1b\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u6fc0\u6d3b\u9009\u62e9\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u65b0SOTA\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u5728\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e0d\u540cVision Transformer\u67b6\u6784\u7684\u76f8\u5bf9\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0e\u4f18\u5316\u3002", "method": "\u5bf9CLIP\u3001SigLIP2\u3001DINOv2\u3001DINOv3\u3001Perception\u548cResNet\u516d\u79cd\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u5728\u7edf\u4e00\u8f7b\u91cfMLP\u5934\u4e0b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u6bd4\u8f83\u5176\u5728NR-IQA\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1b\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u901a\u9053\u7ea7\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u673a\u5236\u3002", "result": "\u53d1\u73b0SigLIP2\u8868\u73b0\u4f18\u5f02\uff0c\u4e14sigmoid\u6fc0\u6d3b\u51fd\u6570\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u4f18\u4e8eReLU\u548cGELU\uff1b\u6240\u63d0\u53ef\u5b66\u4e60\u6fc0\u6d3b\u673a\u5236\u5728CLIVE\u3001KADID10K\u548cAGIQA3K\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u76f8\u5173\u7cfb\u6570\uff08SRCC\uff09\u3002", "conclusion": "\u6fc0\u6d3b\u51fd\u6570\u7684\u9009\u62e9\u5bf9NR-IQA\u6a21\u578b\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0cSigLIP2\u662f\u5f3a\u5927\u7684\u9aa8\u5e72\u9009\u62e9\uff0c\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6fc0\u6d3b\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3aNR-IQA\u5efa\u7acb\u4e86\u9ad8\u6548\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2509.17938", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17938", "abs": "https://arxiv.org/abs/2509.17938", "authors": ["Satyapriya Krishna", "Andy Zou", "Rahul Gupta", "Eliot Krzysztof Jones", "Nick Winter", "Dan Hendrycks", "J. Zico Kolter", "Matt Fredrikson", "Spyros Matsoukas"], "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models", "comment": "Preprint", "summary": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86D-REX\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8868\u9762\u65e0\u5bb3\u8f93\u51fa\u4e0b\u9690\u85cf\u6076\u610f\u63a8\u7406\u7684\u6b3a\u9a97\u6027\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u660e\u663e\u6709\u5bb3\u7684\u8f93\u51fa\uff0c\u4f46\u96be\u4ee5\u53d1\u73b0\u6a21\u578b\u5728\u5185\u90e8\u8fdb\u884c\u6076\u610f\u6216\u6b3a\u9a97\u6027\u63a8\u7406\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7cfb\u7edf\u63d0\u793a\u6ce8\u5165\u4e0b\u6a21\u578b\u7ed5\u8fc7\u5b89\u5168\u8fc7\u6ee4\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u7ade\u4e89\u6027\u7ea2\u961f\u6d4b\u8bd5\u6784\u5efaD-REX\u6570\u636e\u96c6\uff0c\u5305\u542b\u8bf1\u5bfc\u6b3a\u9a97\u884c\u4e3a\u7684\u5bf9\u6297\u6027\u7cfb\u7edf\u63d0\u793a\u3001\u7528\u6237\u67e5\u8be2\u3001\u6a21\u578b\u770b\u4f3c\u65e0\u5bb3\u7684\u56de\u5e94\u53ca\u5176\u66b4\u9732\u6076\u610f\u610f\u56fe\u7684\u5185\u90e8\u601d\u7ef4\u94fe\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eD-REX\u5bf9\u73b0\u6709\u6a21\u578b\u548c\u5b89\u5168\u673a\u5236\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u63ed\u793a\u4e86\u4ec5\u4f9d\u8d56\u8f93\u51fa\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u8981\u53d1\u5c55\u80fd\u5ba1\u67e5\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u65b0\u6280\u672f\uff0c\u4ee5\u5e94\u5bf9\u6b3a\u9a97\u6027\u5bf9\u9f50\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2509.18058", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.18058", "abs": "https://arxiv.org/abs/2509.18058", "authors": ["Alexander Panfilov", "Evgenii Kortukov", "Kristina Nikoli\u0107", "Matthias Bethge", "Sebastian Lapuschkin", "Wojciech Samek", "Ameya Prabhu", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM", "comment": null, "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict.", "AI": {"tldr": "\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u6709\u5bb3\u8bf7\u6c42\u65f6\u53ef\u80fd\u53d1\u5c55\u51fa\u7b56\u7565\u6027\u4e0d\u8bda\u5b9e\u884c\u4e3a\uff0c\u5373\u8f93\u51fa\u770b\u4f3c\u6709\u5bb3\u4f46\u5b9e\u9645\u65e0\u5bb3\u7684\u5185\u5bb9\uff0c\u8fd9\u79cd\u884c\u4e3a\u4f1a\u5f71\u54cd\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff0c\u4f46\u53ef\u901a\u8fc7\u5185\u90e8\u6fc0\u6d3b\u7684\u7ebf\u6027\u63a2\u6d4b\u5668\u68c0\u6d4b\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ffd\u6c42\u8bda\u5b9e\u3001\u6709\u76ca\u548c\u65e0\u5bb3\u76ee\u6807\u65f6\uff0c\u5982\u4f55\u5728\u9762\u5bf9\u6076\u610f\u8bf7\u6c42\u65f6\u5e73\u8861\u5e2e\u52a9\u6027\u548c\u5b89\u5168\u6027\uff0c\u63ed\u793a\u6a21\u578b\u53ef\u80fd\u91c7\u7528\u7b56\u7565\u6027\u4e0d\u8bda\u5b9e\u4f5c\u4e3a\u65b0\u7b56\u7565\u7684\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u89c2\u5bdf\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6709\u5bb3\u8bf7\u6c42\u7684\u54cd\u5e94\uff0c\u5206\u6790\u5176\u8f93\u51fa\u7279\u5f81\uff0c\u5e76\u6d4b\u8bd5\u8f93\u51fa\u76d1\u63a7\u5668\u7684\u6709\u6548\u6027\uff1b\u540c\u65f6\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u5206\u6790\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u72b6\u6001\u4ee5\u68c0\u6d4b\u7b56\u7565\u6027\u4e0d\u8bda\u5b9e\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u66f4\u64c5\u957f\u6267\u884c\u7b56\u7565\u6027\u4e0d\u8bda\u5b9e\u884c\u4e3a\uff1b\u8be5\u884c\u4e3a\u53ef\u6b3a\u9a97\u6240\u6709\u57fa\u4e8e\u8f93\u51fa\u7684\u5b89\u5168\u76d1\u63a7\u7cfb\u7edf\uff0c\u5f71\u54cd\u57fa\u51c6\u8bc4\u5206\u53ef\u9760\u6027\uff0c\u4f46\u53ef\u901a\u8fc7\u5185\u90e8\u6fc0\u6d3b\u7684\u7ebf\u6027\u63a2\u9488\u53ef\u9760\u68c0\u6d4b\u3002", "conclusion": "\u7b56\u7565\u6027\u4e0d\u8bda\u5b9e\u662f\u6a21\u578b\u5bf9\u9f50\u96be\u4ee5\u63a7\u5236\u7684\u5177\u4f53\u4f53\u73b0\uff0c\u5c24\u5176\u5728\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\u51b2\u7a81\u65f6\uff0c\u9700\u5173\u6ce8\u6b64\u7c7b\u9690\u6027\u884c\u4e3a\u5bf9\u5b89\u5168\u8bc4\u4f30\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.17397", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.17397", "abs": "https://arxiv.org/abs/2509.17397", "authors": ["Jiaqi Zhu", "Shouyi Lu", "Ziyao Li", "Guirong Zhuo", "Lu Xiong"], "title": "Diff-GNSS: Diffusion-based Pseudorange Error Estimation", "comment": null, "summary": "Global Navigation Satellite Systems (GNSS) are vital for reliable urban\npositioning. However, multipath and non-line-of-sight reception often introduce\nlarge measurement errors that degrade accuracy. Learning-based methods for\npredicting and compensating pseudorange errors have gained traction, but their\nperformance is limited by complex error distributions. To address this\nchallenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement\n(pseudorange) error estimation framework that leverages a conditional diffusion\nmodel to capture such complex distributions. Firstly, a Mamba-based module\nperforms coarse estimation to provide an initial prediction with appropriate\nscale and trend. Then, a conditional denoising diffusion layer refines the\nestimate, enabling fine-grained modeling of pseudorange errors. To suppress\nuncontrolled generative diversity and achieve controllable synthesis, three key\nfeatures related to GNSS measurement quality are used as conditions to\nprecisely guide the reverse denoising process. We further incorporate\nper-satellite uncertainty modeling within the diffusion stage to assess the\nreliability of the predicted errors. We have collected and publicly released a\nreal-world dataset covering various scenes. Experiments on public and\nself-collected datasets show that DiffGNSS consistently outperforms\nstate-of-the-art baselines across multiple metrics. To the best of our\nknowledge, this is the first application of diffusion models to pseudorange\nerror estimation. The proposed diffusion-based refinement module is\nplug-and-play and can be readily integrated into existing networks to markedly\nimprove estimation accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiff-GNSS\u7684\u7c97\u5230\u7cbe\u6846\u67b6\uff0c\u5229\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u8fdb\u884cGNSS\u4f2a\u8ddd\u8bef\u5dee\u4f30\u8ba1\uff0c\u9996\u6b21\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u8be5\u9886\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8e\u591a\u5f84\u6548\u5e94\u548c\u975e\u89c6\u8ddd\u63a5\u6536\u5bfc\u81f4GNSS\u6d4b\u91cf\u8bef\u5dee\u590d\u6742\u4e14\u96be\u4ee5\u5efa\u6a21\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u56e0\u65e0\u6cd5\u6709\u6548\u6355\u6349\u590d\u6742\u8bef\u5dee\u5206\u5e03\u800c\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faDiff-GNSS\u6846\u67b6\uff1a\u9996\u5148\u4f7f\u7528\u57fa\u4e8eMamba\u7684\u6a21\u5757\u8fdb\u884c\u7c97\u7565\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u6761\u4ef6\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u7cbe\u7ec6\u5efa\u6a21\u8bef\u5dee\uff1b\u5f15\u5165\u4e09\u4e2a\u4e0eGNSS\u6d4b\u91cf\u8d28\u91cf\u76f8\u5173\u7684\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\uff0c\u5e76\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a0\u5165\u5355\u536b\u661f\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u91c7\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiff-GNSS\u5728\u591a\u4e2a\u6307\u6807\u4e0a consistently \u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u5176\u6269\u6563\u6a21\u5757\u5177\u6709\u5373\u63d2\u5373\u7528\u7279\u6027\u3002", "conclusion": "Diff-GNSS\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u7684\u4f2a\u8ddd\u8bef\u5dee\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u662f\u9996\u4e2a\u5c06\u6269\u6563\u6a21\u578b\u7528\u4e8e\u4f2a\u8ddd\u8bef\u5dee\u4f30\u8ba1\u7684\u5de5\u4f5c\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17946", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.17946", "abs": "https://arxiv.org/abs/2509.17946", "authors": ["Mian Zhong", "Pristina Wang", "Anjalie Field"], "title": "HICode: Hierarchical Inductive Coding with LLMs", "comment": "Long paper accepted at EMNLP 2025 main conference, 19 pages, 8\n  figures", "summary": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data.", "AI": {"tldr": "\u63d0\u51faHICode\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u6587\u672c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u7ed3\u5408\u81ea\u4e0b\u800c\u4e0a\u7684\u6807\u7b7e\u751f\u6210\u4e0e\u5c42\u6b21\u805a\u7c7b\uff0c\u6709\u6548\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u65b0\u5174\u4e3b\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5b9e\u9645\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7ec6\u7c92\u5ea6\u8bed\u6599\u5206\u6790\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff08\u4e0d\u53ef\u6269\u5c55\uff09\u6216\u96be\u4ee5\u63a7\u5236\u7684\u7edf\u8ba1\u65b9\u6cd5\uff08\u5982\u4e3b\u9898\u5efa\u6a21\uff09\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u63a7\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u53d7\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\u542f\u53d1\uff0c\u63d0\u51faHICode\uff1a\u7b2c\u4e00\u90e8\u5206\u4ece\u6570\u636e\u4e2d\u5f52\u7eb3\u751f\u6210\u6807\u7b7e\uff0c\u7b2c\u4e8c\u90e8\u5206\u5bf9\u6807\u7b7e\u8fdb\u884c\u5c42\u6b21\u805a\u7c7b\u4ee5\u53d1\u73b0\u6f5c\u5728\u4e3b\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86HICode\u4e0e\u4eba\u5de5\u6784\u5efa\u4e3b\u9898\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u4e0e\u4eba\u5de5\u8bc4\u4f30\u8bc1\u660e\u5176\u9c81\u68d2\u6027\uff1b\u5728\u963f\u7247\u5371\u673a\u8bc9\u8bbc\u6587\u4ef6\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u6210\u529f\u8bc6\u522b\u51fa\u5236\u836f\u516c\u53f8\u7684\u6fc0\u8fdb\u8425\u9500\u7b56\u7565\u3002", "conclusion": "HICode\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5b9a\u6027\u5206\u6790\u601d\u60f3\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u5927\u89c4\u6a21\u6587\u672c\u7684\u7ec6\u7c92\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u5206\u6790\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18067", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18067", "abs": "https://arxiv.org/abs/2509.18067", "authors": ["Boyang Zhang", "Quanqi Hu", "Mingxuan Sun", "Qihang Lin", "Tianbao Yang"], "title": "Learning to Rank with Top-$K$ Fairness", "comment": "Already accepted: https://openreview.net/forum?id=SSPCc39XvO\n  @article{ zhang2025learning, title={Learning to Rank with Top-\\$K\\$\n  Fairness}, author={Boyang Zhang and Quanqi Hu and Mingxuan Sun and Qihang Lin\n  and Tianbao Yang}, journal={Transactions on Machine Learning Research},\n  issn={2835-8856}, year={2025},\n  url={https://openreview.net/forum?id=SSPCc39XvO}, note={} }", "summary": "Fairness in ranking models is crucial, as disparities in exposure can\ndisproportionately affect protected groups. Most fairness-aware ranking systems\nfocus on ensuring comparable average exposure for groups across the entire\nranked list, which may not fully address real-world concerns. For example, when\na ranking model is used for allocating resources among candidates or disaster\nhotspots, decision-makers often prioritize only the top-$K$ ranked items, while\nthe ranking beyond top-$K$ becomes less relevant. In this paper, we propose a\nlist-wise learning-to-rank framework that addresses the issues of inequalities\nin top-$K$ rankings at training time. Specifically, we propose a top-$K$\nexposure disparity measure that extends the classic exposure disparity metric\nin a ranked list. We then learn a ranker to balance relevance and fairness in\ntop-$K$ rankings. Since direct top-$K$ selection is computationally expensive\nfor a large number of items, we transform the non-differentiable selection\nprocess into a differentiable objective function and develop efficient\nstochastic optimization algorithms to achieve both high accuracy and sufficient\nfairness. Extensive experiments demonstrate that our method outperforms\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5217\u8868\u5f0f\u5b66\u4e60\u6392\u5e8f\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3top-K\u6392\u540d\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u76f8\u5173\u6027\u7684\u540c\u65f6\u63d0\u5347top-K\u7ed3\u679c\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u516c\u5e73\u6027\u6392\u5e8f\u7cfb\u7edf\u901a\u5e38\u5173\u6ce8\u6574\u4e2a\u6392\u540d\u5217\u8868\u4e2d\u5404\u7ec4\u7684\u5e73\u5747\u66dd\u5149\u5ea6\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u51b3\u7b56\u8005\u5f80\u5f80\u53ea\u5173\u6ce8top-K\u7ed3\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9top-K\u6392\u540d\u4e2d\u7684\u4e0d\u516c\u5e73\u95ee\u9898\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u63d0\u51fatop-K\u66dd\u5149\u5dee\u5f02\u5ea6\u91cf\uff0c\u5e76\u5c06\u5176\u878d\u5165\u53ef\u5fae\u5206\u7684\u76ee\u6807\u51fd\u6570\u4e2d\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u968f\u673a\u4f18\u5316\u7b97\u6cd5\u5728\u8bad\u7ec3\u65f6\u5e73\u8861top-K\u6392\u540d\u7684\u76f8\u5173\u6027\u548c\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3top-K\u6392\u540d\u4e2d\u7684\u4e0d\u516c\u5e73\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u5206\u914d\u7b49\u91cd\u89c6\u524d\u5217\u7ed3\u679c\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.17401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17401", "abs": "https://arxiv.org/abs/2509.17401", "authors": ["Jinyeong Kim", "Junhyeok Kim", "Yumin Shim", "Joohyeok Kim", "Sunyoung Jung", "Seong Jae Hwang"], "title": "Interpreting vision transformers via residual replacement model", "comment": null, "summary": "How do vision transformers (ViTs) represent and process the world? This paper\naddresses this long-standing question through the first systematic analysis of\n6.6K features across all layers, extracted via sparse autoencoders, and by\nintroducing the residual replacement model, which replaces ViT computations\nwith interpretable features in the residual stream. Our analysis reveals not\nonly a feature evolution from low-level patterns to high-level semantics, but\nalso how ViTs encode curves and spatial positions through specialized feature\ntypes. The residual replacement model scalably produces a faithful yet\nparsimonious circuit for human-scale interpretability by significantly\nsimplifying the original computations. As a result, this framework enables\nintuitive understanding of ViT mechanisms. Finally, we demonstrate the utility\nof our framework in debiasing spurious correlations.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u89c6\u89c9Transformer\uff08ViT\uff09\u4e2d\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u76846.6K\u4e2a\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6b8b\u5dee\u66ff\u6362\u6a21\u578b\uff0c\u63ed\u793a\u4e86ViT\u4ece\u4f4e\u7ea7\u6a21\u5f0f\u5230\u9ad8\u7ea7\u8bed\u4e49\u7684\u7279\u5f81\u6f14\u5316\u8fc7\u7a0b\u53ca\u5176\u5bf9\u66f2\u7ebf\u548c\u7a7a\u95f4\u4f4d\u7f6e\u7684\u7f16\u7801\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u89c6\u89c9Transformer\u5982\u4f55\u8868\u793a\u548c\u5904\u7406\u89c6\u89c9\u4e16\u754c\u662f\u4e00\u4e2a\u957f\u671f\u672a\u89e3\u7684\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u5206\u6790ViT\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5728\u6b8b\u5dee\u6d41\u4e2d\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u6b8b\u5dee\u66ff\u6362\u6a21\u578b\uff0c\u7528\u53ef\u89e3\u91ca\u7279\u5f81\u66ff\u4ee3\u539f\u59cb\u8ba1\u7b97\uff0c\u5b9e\u73b0\u5bf9ViT\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e86\u7279\u5f81\u5728\u5c42\u7ea7\u4e2d\u7684\u6f14\u5316\u89c4\u5f8b\uff0c\u8bc6\u522b\u51fa\u7f16\u7801\u66f2\u7ebf\u548c\u7a7a\u95f4\u4f4d\u7f6e\u7684\u4e13\u7528\u7279\u5f81\u7c7b\u578b\uff0c\u6b8b\u5dee\u66ff\u6362\u6a21\u578b\u80fd\u7b80\u5316\u8ba1\u7b97\u5e76\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\uff0c\u63d0\u5347\u4e86\u4eba\u7c7b\u5bf9ViT\u673a\u5236\u7684\u7406\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u589e\u5f3a\u4e86\u5bf9ViT\u5185\u90e8\u673a\u5236\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u5c55\u793a\u4e86\u5176\u5728\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17950", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17950", "abs": "https://arxiv.org/abs/2509.17950", "authors": ["Bradley Hauer", "Colin Choi", "Abram Hindle", "Scott Smallwood", "Grzegorz Kondrak"], "title": "Dorabella Cipher as Musical Inspiration", "comment": "Published in Proceedings of the Workshop on Speech and Music\n  Processing 2021", "summary": "The Dorabella cipher is an encrypted note written by English composer Edward\nElgar, which has defied decipherment attempts for more than a century. While\nmost proposed solutions are English texts, we investigate the hypothesis that\nDorabella represents enciphered music. We weigh the evidence for and against\nthe hypothesis, devise a simplified music notation, and attempt to reconstruct\na melody from the cipher. Our tools are n-gram models of music which we\nvalidate on existing music corpora enciphered using monoalphabetic\nsubstitution. By applying our methods to Dorabella, we produce a decipherment\nwith musical qualities, which is then transformed via artful composition into a\nlistenable melody. Far from arguing that the end result represents the only\ntrue solution, we instead frame the process of decipherment as part of the\ncomposition process.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5fb7\u62c9\u8428\u62c9\u5bc6\u7801\uff08Dorabella cipher\uff09\u662f\u5426\u4e3a\u52a0\u5bc6\u97f3\u4e50\u7684\u5047\u8bbe\uff0c\u4f7f\u7528\u7b80\u5316\u7684\u97f3\u4e50\u8bb0\u8c31\u6cd5\u548c\u97f3\u4e50n-gram\u6a21\u578b\u5c1d\u8bd5\u4ece\u4e2d\u91cd\u5efa\u65cb\u5f8b\uff0c\u5e76\u5c06\u89e3\u5bc6\u8fc7\u7a0b\u89c6\u4e3a\u521b\u4f5c\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\u3002", "motivation": "\u7531\u4e8e\u5fb7\u62c9\u8428\u62c9\u5bc6\u7801\u957f\u671f\u672a\u80fd\u88ab\u7834\u89e3\uff0c\u4f5c\u8005\u8bd5\u56fe\u9a8c\u8bc1\u5176\u662f\u5426\u4ee3\u8868\u52a0\u5bc6\u7684\u97f3\u4e50\uff0c\u800c\u975e\u4f20\u7edf\u8ba4\u4e3a\u7684\u82f1\u6587\u6587\u672c\u3002", "method": "\u63d0\u51fa\u7b80\u5316\u97f3\u4e50\u8bb0\u8c31\u6cd5\uff0c\u5229\u7528\u97f3\u4e50n-gram\u6a21\u578b\u5206\u6790\u5355\u5b57\u6bcd\u66ff\u6362\u52a0\u5bc6\u7684\u97f3\u4e50\u8bed\u6599\u5e93\uff0c\u5e76\u5e94\u7528\u4e8e\u5fb7\u62c9\u8428\u62c9\u5bc6\u7801\u4ee5\u751f\u6210\u5177\u6709\u97f3\u4e50\u7279\u6027\u7684\u89e3\u5bc6\u7ed3\u679c\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u4e00\u6bb5\u5177\u6709\u97f3\u4e50\u7279\u6027\u7684\u89e3\u5bc6\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u827a\u672f\u5316\u4f5c\u66f2\u8f6c\u5316\u4e3a\u53ef\u8046\u542c\u7684\u65cb\u5f8b\u3002", "conclusion": "\u89e3\u5bc6\u5e76\u975e\u552f\u4e00\u6b63\u786e\u7b54\u6848\uff0c\u800c\u662f\u4f5c\u66f2\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u5c55\u793a\u4e86\u5bc6\u7801\u89e3\u8bfb\u4e0e\u97f3\u4e50\u521b\u4f5c\u7684\u878d\u5408\u53ef\u80fd\u6027\u3002"}}
{"id": "2509.18071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18071", "abs": "https://arxiv.org/abs/2509.18071", "authors": ["Lorenzo Rosasco"], "title": "Learning functions, operators and dynamical systems with kernels", "comment": null, "summary": "This expository article presents the approach to statistical machine learning\nbased on reproducing kernel Hilbert spaces. The basic framework is introduced\nfor scalar-valued learning and then extended to operator learning. Finally,\nlearning dynamical systems is formulated as a suitable operator learning\nproblem, leveraging Koopman operator theory.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u7edf\u8ba1\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u6807\u91cf\u503c\u5b66\u4e60\u6269\u5c55\u5230\u7b97\u5b50\u5b66\u4e60\uff0c\u5e76\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u5c06\u52a8\u529b\u7cfb\u7edf\u7684\u5b66\u4e60\u8868\u8ff0\u4e3a\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u7edf\u4e00\u4e14\u5f3a\u5927\u7684\u6846\u67b6\u6765\u5904\u7406\u5404\u79cd\u5b66\u4e60\u4efb\u52a1\uff0c\u5305\u62ec\u590d\u6742\u7684\u52a8\u529b\u7cfb\u7edf\u5efa\u6a21\u3002", "method": "\u91c7\u7528\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u5efa\u7acb\u6807\u91cf\u503c\u5b66\u4e60\u7684\u57fa\u672c\u6846\u67b6\uff0c\u7136\u540e\u5c06\u5176\u63a8\u5e7f\u81f3\u7b97\u5b50\u5b66\u4e60\uff0c\u7ed3\u5408Koopman\u7b97\u5b50\u7406\u8bba\u5e94\u7528\u4e8e\u52a8\u529b\u7cfb\u7edf\u7684\u5b66\u4e60\u3002", "result": "\u6210\u529f\u5730\u5c06\u52a8\u529b\u7cfb\u7edf\u7684\u5b66\u4e60\u95ee\u9898\u8f6c\u5316\u4e3a\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u65b9\u6cd5\u4e3a\u7edf\u8ba1\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u52a8\u529b\u7cfb\u7edf\u7b49\u590d\u6742\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.17406", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17406", "abs": "https://arxiv.org/abs/2509.17406", "authors": ["Jonathan Wuntu", "Muhamad Dwisnanto Putro", "Rendy Syahputra"], "title": "Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture", "comment": null, "summary": "Indonesia's marine ecosystems, part of the globally recognized Coral\nTriangle, are among the richest in biodiversity, requiring efficient monitoring\ntools to support conservation. Traditional fish detection methods are\ntime-consuming and demand expert knowledge, prompting the need for automated\nsolutions. This study explores the implementation of YOLOv10-nano, a\nstate-of-the-art deep learning model, for real-time marine fish detection in\nIndonesian waters, using test data from Bunaken National Marine Park. YOLOv10's\narchitecture, featuring improvements like the CSPNet backbone, PAN for feature\nfusion, and Pyramid Spatial Attention Block, enables efficient and accurate\nobject detection even in complex environments. The model was evaluated on the\nDeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano\nachieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606\nwhile maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It\nalso delivered an average inference speed of 29.29 FPS on the CPU, making it\nsuitable for real-time deployment. Although OpenImages V7-Fish alone provided\nlower accuracy, it complemented DeepFish in enhancing model robustness.\nOverall, this study demonstrates YOLOv10-nano's potential for efficient,\nscalable marine fish monitoring and conservation applications in data-limited\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86YOLOv10-nano\u5728\u5370\u5ea6\u5c3c\u897f\u4e9a\u6d77\u57df\u5b9e\u65f6\u6d77\u6d0b\u9c7c\u7c7b\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5229\u7528Bunaken\u56fd\u5bb6\u6d77\u6d0b\u516c\u56ed\u7684\u6d4b\u8bd5\u6570\u636e\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u51c6\u786e\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5370\u5ea6\u5c3c\u897f\u4e9a\u7684\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u662f\u5168\u7403\u751f\u7269\u591a\u6837\u6027\u6700\u4e30\u5bcc\u7684\u533a\u57df\u4e4b\u4e00\uff0c\u4f20\u7edf\u9c7c\u7c7b\u68c0\u6d4b\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u5de5\u5177\u6765\u652f\u6301\u4fdd\u62a4\u5de5\u4f5c\u3002", "method": "\u91c7\u7528\u6700\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bYOLOv10-nano\uff0c\u7ed3\u5408CSPNet\u4e3b\u5e72\u7f51\u7edc\u3001PAN\u7279\u5f81\u878d\u5408\u548c\u91d1\u5b57\u5854\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5728DeepFish\u548cOpenImages V7-Fish\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "YOLOv10-nano\u5b9e\u73b0\u4e86mAP50\u4e3a0.966\u3001mAP50:95\u4e3a0.606\u7684\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4ec5\u97002.7M\u53c2\u6570\u548c8.4 GFLOPs\u8ba1\u7b97\u91cf\uff0c\u5e76\u5728CPU\u4e0a\u8fbe\u5230\u5e73\u574729.29 FPS\u7684\u63a8\u7406\u901f\u5ea6\uff1b\u4f7f\u7528DeepFish\u6570\u636e\u96c6\u8868\u73b0\u4f18\u5f02\uff0cOpenImages V7-Fish\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "YOLOv10-nano\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6d77\u6d0b\u9c7c\u7c7b\u76d1\u6d4b\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u5730\u533a\u7684\u751f\u6001\u4fdd\u62a4\u5e94\u7528\u3002"}}
{"id": "2509.17961", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17961", "abs": "https://arxiv.org/abs/2509.17961", "authors": ["Li Siyan", "Zhen Xu", "Vethavikashini Chithrra Raghuram", "Xuanming Zhang", "Renzhe Yu", "Zhou Yu"], "title": "Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments", "comment": "Accepted in EMNLP 2025 Findings", "summary": "Asynchronous learning environments (ALEs) are widely adopted for formal and\ninformal learning, but timely and personalized support is often limited. In\nthis context, Virtual Teaching Assistants (VTAs) can potentially reduce the\nworkload of instructors, but rigorous and pedagogically sound evaluation is\nessential. Existing assessments often rely on surface-level metrics and lack\nsufficient grounding in educational theories, making it difficult to\nmeaningfully compare the pedagogical effectiveness of different VTA systems. To\nbridge this gap, we propose an evaluation framework rooted in learning sciences\nand tailored to asynchronous forum discussions, a common VTA deployment context\nin ALE. We construct classifiers using expert annotations of VTA responses on a\ndiverse set of forum posts. We evaluate the effectiveness of our classifiers,\nidentifying approaches that improve accuracy as well as challenges that hinder\ngeneralization. Our work establishes a foundation for theory-driven evaluation\nof VTA systems, paving the way for more pedagogically effective AI in\neducation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u79d1\u5b66\u7406\u8bba\u7684\u865a\u62df\u52a9\u6559\uff08VTA\uff09\u8bc4\u4f30\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u5f02\u6b65\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u8bba\u575b\u8ba8\u8bba\uff0c\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u6784\u5efa\u5206\u7c7b\u5668\u4ee5\u8bc4\u4f30VTA\u56de\u5e94\u8d28\u91cf\uff0c\u4e3aAI\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u6559\u5b66\u8bc4\u4ef7\u57fa\u7840\u3002", "motivation": "\u73b0\u6709VTA\u8bc4\u4f30\u591a\u4f9d\u8d56\u8868\u9762\u6307\u6807\uff0c\u7f3a\u4e4f\u6559\u80b2\u7406\u8bba\u652f\u6491\uff0c\u96be\u4ee5\u6709\u6548\u6bd4\u8f83\u4e0d\u540c\u7cfb\u7edf\u7684\u6559\u5b66\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7406\u8bba\u9a71\u52a8\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u5b66\u4e60\u79d1\u5b66\u7406\u8bba\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u5bb6\u6807\u6ce8\u7684VTA\u56de\u590d\u6570\u636e\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u8bba\u575b\u5e16\u5b50\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u6790\u51c6\u786e\u6027\u63d0\u5347\u65b9\u6cd5\u4e0e\u6cdb\u5316\u6311\u6218\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u53ef\u7528\u4e8e\u8bc4\u4f30VTA\u56de\u5e94\u8d28\u91cf\u7684\u5206\u7c7b\u5668\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f02\u6b65\u5b66\u4e60\u73af\u5883\u4e2dVTA\u7cfb\u7edf\u7684\u7406\u8bba\u9a71\u52a8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5177\u6559\u5b66\u6709\u6548\u6027\u7684AI\u6559\u80b2\u5de5\u5177\u53d1\u5c55\u3002"}}
{"id": "2509.18085", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18085", "abs": "https://arxiv.org/abs/2509.18085", "authors": ["Sudhanshu Agrawal", "Risheek Garrepalli", "Raghavv Goel", "Mingu Lee", "Christopher Lott", "Fatih Porikli"], "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding", "comment": null, "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpiffy\u7684\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u53ef\u5c06\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u53472.8\u20133.1\u500d\uff0c\u5e76\u4e0e\u73b0\u6709\u6280\u672f\u7ed3\u5408\u5b9e\u73b0\u6700\u9ad87.9\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u5206\u5e03\u4e0d\u53d8\u3002", "motivation": "\u73b0\u6709\u7684\u5f00\u6e90\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u901a\u5e38\u6bcf\u6b65\u4ec5\u751f\u6210\u4e00\u4e2atoken\uff0c\u5bfc\u81f4\u751f\u6210\u901f\u5ea6\u8f83\u6162\uff0c\u4e9f\u9700\u5728\u4e0d\u727a\u7272\u8f93\u51fa\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "method": "Spiffy\u5229\u7528dLLM\u81ea\u8eab\u7684\u5206\u5e03\u8fdb\u884c\u81ea\u63a8\u6d4b\u751f\u6210\u5019\u9009\u72b6\u6001\uff0c\u8bbe\u8ba1\u4e86\u9762\u5411dLLM\u53cc\u5411\u3001\u5757\u72b6\u751f\u6210\u7279\u6027\u7684\u6709\u5411\u8349\u6848\u56fe\u7ed3\u6784\uff0c\u5e76\u652f\u6301\u5e76\u884c\u9a8c\u8bc1\uff1b\u540c\u65f6\u5f15\u5165\u79bb\u7ebf\u6821\u51c6\u7b97\u6cd5\u4f18\u5316\u8349\u6848\u56fe\u914d\u7f6e\uff0c\u63d0\u9ad8\u63a5\u53d7\u7387\u548c\u6574\u4f53\u52a0\u901f\u6548\u679c\u3002", "result": "Spiffy\u5728\u4fdd\u6301\u8f93\u51fa\u5206\u5e03\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\uff0c\u4f7fdLLM\u63a8\u7406\u901f\u5ea6\u63d0\u53472.8\u20133.1\u500d\uff1b\u4e0eKV\u7f13\u5b58\u548c\u591atoken\u89e3\u7801\u7b49\u6280\u672f\u7ed3\u5408\u540e\uff0c\u6700\u9ad8\u53ef\u8fbe7.9\u500d\u52a0\u901f\u3002", "conclusion": "Spiffy\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86dLLM\u7684\u751f\u6210\u6548\u7387\uff0c\u4e14\u53ef\u4e0e\u5176\u4ed6\u52a0\u901f\u6280\u672f\u534f\u540c\u4f7f\u7528\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2509.17427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17427", "abs": "https://arxiv.org/abs/2509.17427", "authors": ["Hodaka Kawachi", "Jose Reinaldo Cunha Santos A. V. Silva Neto", "Yasushi Yagi", "Hajime Nagahara", "Tomoya Nakamura"], "title": "Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling", "comment": null, "summary": "We propose a single-snapshot depth-from-defocus (DFD) reconstruction method\nfor coded-aperture imaging that replaces hand-crafted priors with a learned\ndiffusion prior used purely as regularization. Our optimization framework\nenforces measurement consistency via a differentiable forward model while\nguiding solutions with the diffusion prior in the denoised image domain,\nyielding higher accuracy and stability than clas- sical optimization. Unlike\nU-Net-style regressors, our approach requires no paired defocus-RGBD training\ndata and does not tie training to a specific camera configuration. Experiments\non comprehensive simulations and a prototype camera demonstrate consistently\nstrong RGBD reconstructions across noise levels, outperforming both U-Net\nbaselines and a classical coded- aperture DFD method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u5355\u5feb\u7167\u6df1\u5ea6\u4ece\u6563\u7126\uff08DFD\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u5148\u9a8c\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u4e0d\u4f9d\u8d56\u7279\u5b9a\u76f8\u673a\u914d\u7f6e\uff0c\u5728\u4eff\u771f\u548c\u539f\u578b\u76f8\u673a\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684RGBD\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfDFD\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u5148\u9a8c\u6216\u9700\u5927\u91cf\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u6027\u548c\u5b9e\u7528\u6027\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6269\u6563\u5148\u9a8c\u63d0\u5347DFD\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u6446\u8131\u5bf9\u7279\u5b9a\u76f8\u673a\u8bbe\u7f6e\u548c\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u524d\u5411\u6a21\u578b\u4fdd\u8bc1\u6d4b\u91cf\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u53bb\u566a\u56fe\u50cf\u57df\u4e2d\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u6269\u6563\u5148\u9a8c\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\uff1b\u8be5\u65b9\u6cd5\u4e0d\u91c7\u7528U-Net\u5f0f\u56de\u5f52\u5668\uff0c\u800c\u662f\u5c06\u6269\u6563\u6a21\u578b\u4ec5\u7528\u4e8e\u6b63\u5219\u5316\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u7f16\u7801\u5b54\u5f84\u6210\u50cf\u7cfb\u7edf\u3002", "result": "\u5728\u591a\u79cd\u566a\u58f0\u6c34\u5e73\u4e0b\u5747\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684RGBD\u91cd\u5efa\uff0c\u4f18\u4e8eU-Net\u57fa\u7ebf\u65b9\u6cd5\u548c\u7ecf\u5178\u7f16\u7801\u5b54\u5f84DFD\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u76f8\u673a\u914d\u7f6e\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u5c06\u6269\u6563\u5148\u9a8c\u878d\u5165\u4f18\u5316\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u914d\u5bf9\u76d1\u7763\u548c\u7279\u5b9a\u786c\u4ef6\u8bad\u7ec3\u7684\u9ad8\u7cbe\u5ea6\u5355\u5feb\u7167DFD\uff0c\u4e3a\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17991", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17991", "abs": "https://arxiv.org/abs/2509.17991", "authors": ["Aakash Kumar Agarwal", "Saprativa Bhattacharjee", "Mauli Rastogi", "Jemima S. Jacob", "Biplab Banerjee", "Rashmi Gupta", "Pushpak Bhattacharyya"], "title": "ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Almost 50% depression patients face the risk of going into relapse. The risk\nincreases to 80% after the second episode of depression. Although, depression\ndetection from social media has attained considerable attention, depression\nrelapse detection has remained largely unexplored due to the lack of curated\ndatasets and the difficulty of distinguishing relapse and non-relapse users. In\nthis work, we present ReDepress, the first clinically validated social media\ndataset focused on relapse, comprising 204 Reddit users annotated by mental\nhealth professionals. Unlike prior approaches, our framework draws on cognitive\ntheories of depression, incorporating constructs such as attention bias,\ninterpretation bias, memory bias and rumination into both annotation and\nmodeling. Through statistical analyses and machine learning experiments, we\ndemonstrate that cognitive markers significantly differentiate relapse and\nnon-relapse groups, and that models enriched with these features achieve\ncompetitive performance, with transformer-based temporal models attaining an F1\nof 0.86. Our findings validate psychological theories in real-world textual\ndata and underscore the potential of cognitive-informed computational methods\nfor early relapse detection, paving the way for scalable, low-cost\ninterventions in mental healthcare.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ReDepress\uff0c\u9996\u4e2a\u4e13\u6ce8\u4e8e\u6291\u90c1\u75c7\u590d\u53d1\u68c0\u6d4b\u7684\u4e34\u5e8a\u9a8c\u8bc1\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u8ba4\u77e5\u7406\u8bba\u6784\u5efa\u6ce8\u610f\u529b\u3001\u89e3\u91ca\u3001\u8bb0\u5fc6\u504f\u5dee\u548c\u53cd\u520d\u7b49\u7279\u5f81\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0F1\u8fbe0.86\u7684\u590d\u53d1\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6291\u90c1\u75c7\u590d\u53d1\u98ce\u9669\u9ad8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u590d\u53d1\u7684\u6570\u636e\u96c6\u4e14\u96be\u4ee5\u533a\u5206\u590d\u53d1\u4e0e\u975e\u590d\u53d1\u7528\u6237\uff0c\u56e0\u6b64\u9700\u8981\u57fa\u4e8e\u8ba4\u77e5\u7406\u8bba\u7684\u65b0\u578b\u65b9\u6cd5\u8fdb\u884c\u6709\u6548\u8bc6\u522b\u3002", "method": "\u6784\u5efa\u5305\u542b204\u540dReddit\u7528\u6237\u7684\u4e34\u5e8a\u6807\u6ce8\u6570\u636e\u96c6ReDepress\uff0c\u5e76\u57fa\u4e8e\u8ba4\u77e5\u7406\u8bba\u63d0\u53d6\u6ce8\u610f\u529b\u504f\u5dee\u3001\u89e3\u91ca\u504f\u5dee\u3001\u8bb0\u5fc6\u504f\u5dee\u548c\u53cd\u520d\u7b49\u7279\u5f81\uff0c\u91c7\u7528\u7edf\u8ba1\u5206\u6790\u4e0eTransformer\u65f6\u5e8f\u6a21\u578b\u8fdb\u884c\u590d\u53d1\u9884\u6d4b\u3002", "result": "\u8ba4\u77e5\u6807\u8bb0\u80fd\u663e\u8457\u533a\u5206\u590d\u53d1\u4e0e\u975e\u590d\u53d1\u7fa4\u4f53\uff0c\u878d\u5165\u8fd9\u4e9b\u7279\u5f81\u7684\u6a21\u578b\u5728\u590d\u53d1\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0cTransformer\u65f6\u5e8f\u6a21\u578b\u8fbe\u52300.86\u7684F1\u5206\u6570\u3002", "conclusion": "\u8ba4\u77e5\u7406\u8bba\u9a71\u52a8\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728\u771f\u5b9e\u6587\u672c\u6570\u636e\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\uff0c\u4e3a\u6291\u90c1\u75c7\u590d\u53d1\u7684\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.17429", "categories": ["cs.CV", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.17429", "abs": "https://arxiv.org/abs/2509.17429", "authors": ["Zhitao Zeng", "Guojian Yuan", "Junyuan Mao", "Yuxuan Wang", "Xiaoshuang Jia", "Yueming Jin"], "title": "Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration", "comment": "20 pages, 6 figures", "summary": "Accurate temporal prediction is the bridge between comprehensive scene\nunderstanding and embodied artificial intelligence. However, predicting\nmultiple fine-grained states of a scene at multiple temporal scales is\ndifficult for vision-language models. We formalize the Multi-Scale Temporal\nPrediction (MSTP) task in general and surgical scenes by decomposing\nmulti-scale into two orthogonal dimensions: the temporal scale, forecasting\nstates of humans and surgery at varying look-ahead intervals, and the state\nscale, modeling a hierarchy of states in general and surgical scenes. For\nexample, in general scenes, states of contact relationships are finer-grained\nthan states of spatial relationships. In surgical scenes, medium-level steps\nare finer-grained than high-level phases yet remain constrained by their\nencompassing phase. To support this unified task, we introduce the first MSTP\nBenchmark, featuring synchronized annotations across multiple state scales and\ntemporal scales. We further propose a method, Incremental Generation and\nMulti-agent Collaboration (IG-MC), which integrates two key innovations. First,\nwe present a plug-and-play incremental generation module that continuously\nsynthesizes up-to-date visual previews at expanding temporal scales to inform\nmultiple decision-making agents, keeping decisions and generated visuals\nsynchronized and preventing performance degradation as look-ahead intervals\nlengthen. Second, we present a decision-driven multi-agent collaboration\nframework for multi-state prediction, comprising generation, initiation, and\nmulti-state assessment agents that dynamically trigger and evaluate prediction\ncycles to balance global coherence and local fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u65f6\u95f4\u9884\u6d4b\uff08MSTP\uff09\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2aMSTP\u57fa\u51c6\uff0c\u7528\u4e8e\u5728\u901a\u7528\u548c\u624b\u672f\u573a\u666f\u4e2d\u9884\u6d4b\u4e0d\u540c\u65f6\u95f4\u4e0e\u72b6\u6001\u5c3a\u5ea6\u4e0b\u7684\u7ec6\u7c92\u5ea6\u573a\u666f\u72b6\u6001\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u591a\u65f6\u95f4\u5c3a\u5ea6\u4e0b\u7684\u591a\u7c92\u5ea6\u573a\u666f\u72b6\u6001\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5efa\u6a21\u6846\u67b6\u548c\u6570\u636e\u96c6\u652f\u6301\u3002", "method": "\u63d0\u51faIG-MC\u65b9\u6cd5\uff0c\u5305\u542b\u589e\u91cf\u751f\u6210\u6a21\u5757\u4ee5\u540c\u6b65\u751f\u6210\u89c6\u89c9\u9884\u89c8\uff0c\u4ee5\u53ca\u57fa\u4e8e\u51b3\u7b56\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u5b9e\u73b0\u751f\u6210\u3001\u542f\u52a8\u4e0e\u8bc4\u4f30\u7684\u52a8\u6001\u534f\u540c\u3002", "result": "\u5728\u65b0\u63d0\u51fa\u7684MSTP\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86IG-MC\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u65f6\u95f4\u4e0e\u72b6\u6001\u5c3a\u5ea6\u4e0b\u4fdd\u6301\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u4e0e\u5168\u5c40\u4e00\u81f4\u6027\u3002", "conclusion": "IG-MC\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.17995", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17995", "abs": "https://arxiv.org/abs/2509.17995", "authors": ["Yefan Zhou", "Austin Xu", "Yilun Zhou", "Janvijay Singh", "Jiang Gui", "Shafiq Joty"], "title": "Variation in Verification: Understanding Verification Dynamics in Large Language Models", "comment": null, "summary": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0f\u9a8c\u8bc1\u5668\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc712\u4e2a\u57fa\u51c6\u548c14\u4e2a\u5f00\u6e90\u6a21\u578b\u53caGPT-4o\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u95ee\u9898\u96be\u5ea6\u3001\u751f\u6210\u5668\u80fd\u529b\u548c\u9a8c\u8bc1\u5668\u80fd\u529b\u5bf9\u9a8c\u8bc1\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u9a8c\u8bc1\u7b56\u7565\u7684\u673a\u4f1a\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u6765\u89e3\u51b3\u590d\u6742\u95ee\u9898\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u9a8c\u8bc1\u65b9\u6cd5\u5728\u65e0\u53c2\u8003\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u4ecd\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u9a8c\u8bc1\u673a\u5236\u7684\u6709\u6548\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0f\u9a8c\u8bc1\u5668\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8fd9\u4e9b\u9a8c\u8bc1\u5668\u901a\u8fc7\u751f\u6210\u601d\u7ef4\u94fe\u63a8\u7406\u5e76\u7ed9\u51fa\u4e8c\u5143\u5224\u65ad\u6765\u8fdb\u884c\u9a8c\u8bc1\uff1b\u5728\u6570\u5b66\u63a8\u7406\u3001\u77e5\u8bc6\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u768412\u4e2a\u57fa\u51c6\u4e0a\uff0c\u4f7f\u752814\u4e2a\u5f00\u6e90\u6a21\u578b\uff082B\u81f372B\u53c2\u6570\uff09\u548cGPT-4o\u8fdb\u884c\u4e09\u65b9\u9762\u7cfb\u7edf\u5206\u6790\uff1a\u95ee\u9898\u96be\u5ea6\u3001\u751f\u6210\u5668\u80fd\u529b\u4e0e\u9a8c\u8bc1\u5668\u751f\u6210\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u7ed3\u679c\uff1a(1) \u7b80\u5355\u95ee\u9898\u66f4\u6613\u4e8e\u9a8c\u8bc1\u5668\u51c6\u786e\u8bc6\u522b\u6b63\u786e\u54cd\u5e94\uff1b(2) \u5f31\u751f\u6210\u5668\u4ea7\u751f\u7684\u9519\u8bef\u6bd4\u5f3a\u751f\u6210\u5668\u66f4\u5bb9\u6613\u88ab\u68c0\u6d4b\uff1b(3) \u9a8c\u8bc1\u80fd\u529b\u901a\u5e38\u4e0e\u9a8c\u8bc1\u5668\u81ea\u8eab\u89e3\u9898\u80fd\u529b\u76f8\u5173\uff0c\u4f46\u8be5\u5173\u7cfb\u968f\u95ee\u9898\u96be\u5ea6\u53d8\u5316\u800c\u53d8\u5316\u3002\u6b64\u5916\uff0c\u5f31\u751f\u6210\u5668\u5728\u76f8\u540c\u9a8c\u8bc1\u5668\u4e0b\u53ef\u63a5\u8fd1\u5f3a\u751f\u6210\u5668\u7684\u540e\u9a8c\u8bc1\u6027\u80fd\uff0c\u4e14\u5f3a\u9a8c\u8bc1\u5668\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u52bf\u6709\u9650\u3002", "conclusion": "\u9a8c\u8bc1\u6548\u679c\u53d7\u95ee\u9898\u96be\u5ea6\u3001\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\u80fd\u529b\u5171\u540c\u5f71\u54cd\uff0c\u5355\u7eaf\u6269\u5927\u9a8c\u8bc1\u5668\u89c4\u6a21\u65e0\u6cd5\u514b\u670d\u6839\u672c\u6027\u7684\u9a8c\u8bc1\u96be\u9898\uff0c\u9700\u7ed3\u5408\u751f\u6210\u4e0e\u9a8c\u8bc1\u7b56\u7565\u4f18\u5316\u4ee5\u63d0\u5347\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u80fd\u3002"}}
{"id": "2509.18004", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18004", "abs": "https://arxiv.org/abs/2509.18004", "authors": ["Yuhang Dai", "Ziyu Zhang", "Shuai Wang", "Longhao Li", "Zhao Guo", "Tianlun Zuo", "Shuiyuan Wang", "Hongfei Xue", "Chengyou Wang", "Qing Wang", "Xin Xu", "Hui Bu", "Jie Li", "Jian Kang", "Binbin Zhang", "Lei Xie"], "title": "WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing", "comment": "4 pages, 5 figures, 4 tables", "summary": "The scarcity of large-scale, open-source data for dialects severely hinders\nprogress in speech technology, a challenge particularly acute for the widely\nspoken Sichuanese dialects of Chinese. To address this critical gap, we\nintroduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed\nusing our novel Chuan-Pipeline, a complete data processing framework for\ndialectal speech. To facilitate rigorous evaluation and demonstrate the\ncorpus's effectiveness, we also release high-quality ASR and TTS benchmarks,\nWenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show\nthat models trained on WenetSpeech-Chuan achieve state-of-the-art performance\namong open-source systems and demonstrate results comparable to commercial\nservices. As the largest open-source corpus for Sichuanese dialects,\nWenetSpeech-Chuan not only lowers the barrier to research in dialectal speech\nprocessing but also plays a crucial role in promoting AI equity and mitigating\nbias in speech technologies. The corpus, benchmarks, models, and receipts are\npublicly available on our project page.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86WenetSpeech-Chuan\uff0c\u4e00\u4e2a\u5305\u542b10,000\u5c0f\u65f6\u3001\u4e30\u5bcc\u6807\u6ce8\u7684\u56db\u5ddd\u8bdd\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u65b9\u8a00\u8bed\u97f3\u6280\u672f\u7814\u7a76\u4e2d\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u5f00\u6e90\u65b9\u8a00\u6570\u636e\uff0c\u65b9\u8a00\u8bed\u97f3\u6280\u672f\u7684\u53d1\u5c55\u53d7\u5230\u4e25\u91cd\u963b\u788d\uff0c\u5c24\u5176\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u56db\u5ddd\u8bdd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChuan-Pipeline\u7684\u65b0\u9896\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u65b9\u8a00\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u8d28\u91cf\u7684ASR\u548cTTS\u57fa\u51c6\u6d4b\u8bd5\u96c6WenetSpeech-Chuan-Eval\u3002", "result": "\u5728WenetSpeech-Chuan\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5f00\u6e90\u7cfb\u7edf\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u7ed3\u679c\u53ef\u4e0e\u5546\u4e1a\u670d\u52a1\u76f8\u5ab2\u7f8e\u3002", "conclusion": "WenetSpeech-Chuan\u4f5c\u4e3a\u76ee\u524d\u6700\u5927\u7684\u56db\u5ddd\u8bdd\u8bed\u97f3\u5f00\u6e90\u8bed\u6599\u5e93\uff0c\u4e0d\u4ec5\u964d\u4f4e\u4e86\u65b9\u8a00\u8bed\u97f3\u5904\u7406\u7814\u7a76\u7684\u95e8\u69db\uff0c\u8fd8\u5728\u4fc3\u8fdbAI\u516c\u5e73\u6027\u548c\u51cf\u5c11\u8bed\u97f3\u6280\u672f\u504f\u89c1\u65b9\u9762\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2509.17431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17431", "abs": "https://arxiv.org/abs/2509.17431", "authors": ["Keyu Du", "Jingyu Hu", "Haipeng Li", "Hao Xu", "Haibing Huang", "Chi-Wing Fu", "Shuaicheng Liu"], "title": "Emergent 3D Correspondence from Neural Shape Representation", "comment": "This paper is accepted by Siggraph Asia 2025 conference track", "summary": "This paper presents a new approach to estimate accurate and robust 3D\nsemantic correspondence with the hierarchical neural semantic representation.\nOur work has three key contributions. First, we design the hierarchical neural\nsemantic representation (HNSR), which consists of a global semantic feature to\ncapture high-level structure and multi-resolution local geometric features to\npreserve fine details, by carefully harnessing 3D priors from pre-trained 3D\ngenerative models. Second, we design a progressive global-to-local matching\nstrategy, which establishes coarse semantic correspondence using the global\nsemantic feature, then iteratively refines it with local geometric features,\nyielding accurate and semantically-consistent mappings. Third, our framework is\ntraining-free and broadly compatible with various pre-trained 3D generative\nbackbones, demonstrating strong generalization across diverse shape categories.\nOur method also supports various applications, such as shape co-segmentation,\nkeypoint matching, and texture transfer, and generalizes well to structurally\ndiverse shapes, with promising results even in cross-category scenarios. Both\nqualitative and quantitative evaluations show that our method outperforms\nprevious state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u795e\u7ecf\u8bed\u4e49\u8868\u793a\u7684\u65e0\u8bad\u7ec33D\u8bed\u4e49\u5bf9\u5e94\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u8de8\u7c7b\u522b\u548c\u591a\u6837\u5316\u5f62\u72b6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u540c3D\u5f62\u72b6\u95f4\u5efa\u7acb\u51c6\u786e\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u5dee\u5f02\u5927\u6216\u8de8\u7c7b\u522b\u7684\u573a\u666f\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u517c\u987e\u5168\u5c40\u8bed\u4e49\u4e0e\u5c40\u90e8\u7ec6\u8282\u3002", "method": "\u8bbe\u8ba1\u4e86\u5206\u5c42\u795e\u7ecf\u8bed\u4e49\u8868\u793a\uff08HNSR\uff09\uff0c\u7ed3\u5408\u6765\u81ea\u9884\u8bad\u7ec33D\u751f\u6210\u6a21\u578b\u7684\u5148\u9a8c\uff0c\u878d\u5408\u5168\u5c40\u8bed\u4e49\u7279\u5f81\u4e0e\u591a\u5206\u8fa8\u7387\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff1b\u91c7\u7528\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u7684\u6e10\u8fdb\u5339\u914d\u7b56\u7565\uff0c\u5148\u5efa\u7acb\u7c97\u7565\u5bf9\u5e94\uff0c\u518d\u9010\u6b65\u7528\u5c40\u90e8\u7279\u5f81\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u517c\u5bb9\u591a\u79cd\u9884\u8bad\u7ec33D\u751f\u6210 backbone\uff0c\u5728\u5f62\u72b6\u5171\u5206\u5272\u3001\u5173\u952e\u70b9\u5339\u914d\u548c\u7eb9\u7406\u8fc1\u79fb\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8de8\u7c7b\u522b\u573a\u666f\u4e0b\u4ecd\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684HNSR\u548c\u6e10\u8fdb\u5339\u914d\u7b56\u7565\u6709\u6548\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u7a33\u5065\u76843D\u8bed\u4e49\u5bf9\u5e94\uff0c\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.18010", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18010", "abs": "https://arxiv.org/abs/2509.18010", "authors": ["Sara Papi", "Dennis Fucci", "Marco Gaido", "Matteo Negri", "Luisa Bentivogli"], "title": "Cross-Attention is Half Explanation in Speech-to-Text Models", "comment": null, "summary": "Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u97f3\u5230\u6587\u672c\uff08S2T\uff09\u6a21\u578b\u4e2d\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5f97\u5206\u4ec5\u80fd\u90e8\u5206\u53cd\u6620\u89e3\u7801\u5668\u5bf9\u7f16\u7801\u5668\u8868\u793a\u7684\u5173\u6ce8\uff0c\u6700\u591a\u89e3\u91ca52-75%\u7684\u8f93\u5165\u663e\u8457\u6027\uff0c\u63ed\u793a\u4e86\u5c06\u5176\u4f5c\u4e3a\u89e3\u91ca\u4ee3\u7406\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4ea4\u53c9\u6ce8\u610f\u529b\u5206\u6570\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u7b49\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u53cd\u6620\u8f93\u5165\u8bed\u97f3\u4e0e\u751f\u6210\u6587\u672c\u95f4\u4f9d\u8d56\u5173\u7cfb\u4ecd\u7f3a\u4e4f\u9a8c\u8bc1\uff0c\u5c24\u5176\u662f\u5728\u8bed\u97f3\u9886\u57df\u3002", "method": "\u901a\u8fc7\u5c06\u4ea4\u53c9\u6ce8\u610f\u529b\u5206\u6570\u4e0e\u57fa\u4e8e\u7279\u5f81\u5f52\u56e0\u7684\u8f93\u5165\u663e\u8457\u6027\u56fe\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u591a\u79cd\u5355/\u591a\u8bed\u8a00\u3001\u5355/\u591a\u4efb\u52a1S2T\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u89e3\u91ca\u529b\u3002", "result": "\u4ea4\u53c9\u6ce8\u610f\u529b\u4e0e\u663e\u8457\u6027\u56fe\u5728\u5934\u548c\u5c42\u805a\u5408\u540e\u6709\u4e00\u5b9a\u5230\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4ec5\u6355\u83b7\u7ea650%\u7684\u8f93\u5165\u76f8\u5173\u6027\uff0c\u5728\u6700\u4f73\u60c5\u51b5\u4e0b\u4e5f\u53ea\u89e3\u91ca52-75%\u7684\u663e\u8457\u6027\u3002", "conclusion": "\u4ea4\u53c9\u6ce8\u610f\u529b\u867d\u5177\u4e00\u5b9a\u4fe1\u606f\u6027\uff0c\u4f46\u4f5c\u4e3a\u89e3\u91ca\u9884\u6d4b\u884c\u4e3a\u7684\u4ee3\u7406\u662f\u4e0d\u5b8c\u6574\u4e14\u6709\u9650\u7684\uff0c\u9700\u8c28\u614e\u7528\u4e8e\u6a21\u578b\u89e3\u91ca\u3002"}}
{"id": "2509.17452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17452", "abs": "https://arxiv.org/abs/2509.17452", "authors": ["Dujin Lee", "Sojung An", "Jungmyung Wi", "Kuniaki Saito", "Donghyun Kim"], "title": "Training-Free Label Space Alignment for Universal Domain Adaptation", "comment": "22 pages, 12 figures", "summary": "Universal domain adaptation (UniDA) transfers knowledge from a labeled source\ndomain to an unlabeled target domain, where label spaces may differ and the\ntarget domain may contain private classes. Previous UniDA methods primarily\nfocused on visual space alignment but often struggled with visual ambiguities\ndue to content differences, which limited their robustness and\ngeneralizability. To overcome this, we introduce a novel approach that\nleverages the strong \\textit{zero-shot capabilities} of recent vision-language\nfoundation models (VLMs) like CLIP, concentrating solely on label space\nalignment to enhance adaptation stability. CLIP can generate task-specific\nclassifiers based only on label names. However, adapting CLIP to UniDA is\nchallenging because the label space is not fully known in advance. In this\nstudy, we first utilize generative vision-language models to identify unknown\ncategories in the target domain. Noise and semantic ambiguities in the\ndiscovered labels -- such as those similar to source labels (e.g., synonyms,\nhypernyms, hyponyms) -- complicate label alignment. To address this, we propose\na training-free label-space alignment method for UniDA (\\ours). Our method\naligns label spaces instead of visual spaces by filtering and refining noisy\nlabels between the domains. We then construct a \\textit{universal classifier}\nthat integrates both shared knowledge and target-private class information,\nthereby improving generalizability under domain shifts. The results reveal that\nthe proposed method considerably outperforms existing UniDA techniques across\nkey DomainBed benchmarks, delivering an average improvement of\n\\textcolor{blue}{+7.9\\%}in H-score and \\textcolor{blue}{+6.1\\%} in H$^3$-score.\nFurthermore, incorporating self-training further enhances performance and\nachieves an additional (\\textcolor{blue}{+1.6\\%}) increment in both H- and\nH$^3$-scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u7528\u57df\u81ea\u9002\u5e94\uff08UniDA\uff09\uff0c\u901a\u8fc7\u8bc6\u522b\u76ee\u6807\u57df\u672a\u77e5\u7c7b\u522b\u5e76\u51c0\u5316\u566a\u58f0\u6807\u7b7e\uff0c\u6784\u5efa\u7edf\u4e00\u5206\u7c7b\u5668\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709UniDA\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7a7a\u95f4\u5bf9\u9f50\uff0c\u6613\u53d7\u5185\u5bb9\u5dee\u5f02\u5f15\u8d77\u7684\u89c6\u89c9\u6a21\u7cca\u5f71\u54cd\uff0c\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff1b\u4e14CLIP\u7b49\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u6807\u7b7e\u7a7a\u95f4\u4e0d\u5b8c\u5168\u5df2\u77e5\u7684UniDA\u573a\u666f\u3002", "method": "\u5229\u7528\u751f\u6210\u5f0f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\u76ee\u6807\u57df\u4e2d\u7684\u672a\u77e5\u7c7b\u522b\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6807\u7b7e\u7a7a\u95f4\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6ee4\u548c\u4f18\u5316\u8de8\u57df\u566a\u58f0\u6807\u7b7e\uff08\u5982\u540c\u4e49\u8bcd\u3001\u4e0a\u4e0b\u4f4d\u8bcd\u7b49\u8bed\u4e49\u6a21\u7cca\uff09\u5b9e\u73b0\u5bf9\u9f50\uff0c\u5e76\u6784\u5efa\u878d\u5408\u5171\u4eab\u7c7b\u4e0e\u79c1\u6709\u7c7b\u77e5\u8bc6\u7684\u901a\u7528\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u81ea\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728DomainBed\u57fa\u51c6\u4e0a\u5e73\u5747H-score\u63d0\u5347+7.9%\uff0cH\u00b3-score\u63d0\u5347+6.1%\uff1b\u5f15\u5165\u81ea\u8bad\u7ec3\u540e\u8fdb\u4e00\u6b65\u63d0\u5347+1.6%\u3002", "conclusion": "\u4ec5\u901a\u8fc7\u5bf9\u9f50\u6807\u7b7e\u7a7a\u95f4\u800c\u975e\u89c6\u89c9\u7a7a\u95f4\uff0c\u7ed3\u5408VLMs\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u53ef\u6709\u6548\u63d0\u5347UniDA\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\uff0c\u6240\u63d0\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u4e14\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2509.18030", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18030", "abs": "https://arxiv.org/abs/2509.18030", "authors": ["Justin Xu", "Xi Zhang", "Javid Abderezaei", "Julie Bauml", "Roger Boodoo", "Fatemeh Haghighi", "Ali Ganjizadeh", "Eric Brattain", "Dave Van Veen", "Zaiqiao Meng", "David Eyre", "Jean-Benoit Delbrouck"], "title": "RadEval: A framework for radiology text evaluation", "comment": "Accepted to EMNLP 2025 Demo track - Oral", "summary": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration.", "AI": {"tldr": "RadEval\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u653e\u5c04\u5b66\u6587\u672c\uff0c\u6574\u5408\u4e86\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u53d1\u5e03\u4e86\u5e26\u6709\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u53ef\u91cd\u590d\u6027\u548c\u7a33\u5065\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7684\u653e\u5c04\u5b66\u6587\u672c\u8bc4\u4f30\u65b9\u6cd5\u5206\u6563\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u96be\u4ee5\u8fdb\u884c\u53ef\u9760\u7684\u6bd4\u8f83\u548c\u590d\u73b0\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u3001\u5168\u9762\u4e14\u5f00\u653e\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63d0\u5347\u8be5\u9886\u57df\u7684\u7814\u7a76\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "method": "RadEval\u6574\u5408\u4e86\u4ece\u7ecf\u5178\u7684n-gram\u91cd\u53e0\u6307\u6807\u5230\u57fa\u4e8e\u4e34\u5e8a\u6982\u5ff5\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5148\u8fdb\u8bc4\u4f30\u5668\uff0c\u5e76\u6539\u8fdb\u4e86GREEN\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u66f4\u8f7b\u91cf\u5316\u7684\u6a21\u578b\u548c\u9886\u57df\u7279\u5b9a\u7684\u653e\u5c04\u5b66\u7f16\u7801\u5668\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b450\u591a\u4e2a\u4e34\u5e8a\u663e\u8457\u9519\u8bef\u6807\u7b7e\u7684\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "RadEval\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u4e0d\u540c\u6307\u6807\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u5224\u65ad\u5177\u6709\u826f\u597d\u7684\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7edf\u8ba1\u68c0\u9a8c\u5de5\u5177\u548c\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u57fa\u7ebf\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "RadEval\u4e3a\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u6807\u51c6\u5316\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3001\u53ef\u6bd4\u6027\u548c\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2509.17457", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.10; I.4.m"], "pdf": "https://arxiv.org/pdf/2509.17457", "abs": "https://arxiv.org/abs/2509.17457", "authors": ["Pawe\u0142 Jakub Borsukiewicz", "Jordan Samhi", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks", "comment": "22 pages; 24 tables; 11 figures", "summary": "The proliferation of facial recognition systems presents major privacy risks,\ndriving the need for effective countermeasures. Current adversarial techniques\napply generalized methods rather than adapting to individual facial\ncharacteristics, limiting their effectiveness and inconspicuousness. In this\nwork, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique\nthat identifies which facial areas contribute most to recognition at an\nindividual level. Unlike adversarial attack methods that aim to fool\nrecognition systems, LEAM is an explainability technique designed to understand\nhow these systems work, providing insights that could inform future privacy\nprotection research. We integrate LEAM with a face parser to analyze data from\n1000 individuals across 9 pre-trained facial recognition models.\n  Our analysis reveals that while different layers within facial recognition\nmodels vary significantly in their focus areas, these models generally\nprioritize similar facial regions across architectures when considering their\noverall activation patterns, which show significantly higher similarity between\nimages of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.\ndifferent individuals (0.04-0.13), validating the existence of person-specific\nrecognition patterns. Our results show that facial recognition models\nprioritize the central region of face images (with nose areas accounting for\n18.9-29.7% of critical recognition regions), while still distributing attention\nacross multiple facial fragments. Proper selection of relevant facial areas was\nconfirmed using validation occlusions, based on just 1% of the most relevant,\nLEAM-identified, image pixels, which proved to be transferable across different\nmodels. Our findings establish the foundation for future individually tailored\nprivacy protection systems centered around LEAM's choice of areas to be\nperturbed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEAM\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u4e2d\u5bf9\u4e2a\u4f53\u8bc6\u522b\u8d21\u732e\u6700\u5927\u7684\u9762\u90e8\u533a\u57df\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u67b6\u6784\u4e0b\u666e\u904d\u5173\u6ce8\u76f8\u4f3c\u7684\u9762\u90e8\u533a\u57df\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e2a\u4f53\u7279\u5f02\u6027\u8bc6\u522b\u6a21\u5f0f\u7684\u5b58\u5728\uff0c\u4e3a\u672a\u6765\u4e2a\u6027\u5316\u7684\u9690\u79c1\u4fdd\u62a4\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u6027\u6280\u672f\u901a\u5e38\u91c7\u7528\u901a\u7528\u65b9\u6cd5\uff0c\u672a\u80fd\u9488\u5bf9\u4e2a\u4f53\u9762\u90e8\u7279\u5f81\u8fdb\u884c\u8c03\u6574\uff0c\u5bfc\u81f4\u6548\u679c\u548c\u9690\u853d\u6027\u6709\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u5de5\u4f5c\u539f\u7406\u7684\u6280\u672f\uff0c\u4ee5\u63a8\u52a8\u66f4\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86Layer Embedding Activation Mapping (LEAM) \u6280\u672f\uff0c\u7ed3\u5408\u4eba\u8138\u89e3\u6790\u5668\u5206\u67901000\u540d\u4e2a\u4f53\u57289\u4e2a\u9884\u8bad\u7ec3\u9762\u90e8\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u6570\u636e\uff0c\u901a\u8fc7\u6fc0\u6d3b\u6a21\u5f0f\u5206\u6790\u8bc6\u522b\u5173\u952e\u9762\u90e8\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u906e\u6321\u5b9e\u9a8c\u9a8c\u8bc1\u6240\u9009\u533a\u57df\u7684\u6709\u6548\u6027\u548c\u8de8\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u5bf9\u9762\u90e8\u533a\u57df\u7684\u5173\u6ce8\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u6574\u4f53\u6fc0\u6d3b\u6a21\u5f0f\u663e\u793a\u51fa\u9ad8\u5ea6\u76f8\u4f3c\u6027\uff1b\u76f8\u540c\u4e2a\u4f53\u56fe\u50cf\u95f4\u7684Bhattacharyya\u7cfb\u6570\u4e3a0.32-0.57\uff0c\u800c\u4e0d\u540c\u4e2a\u4f53\u95f4\u4e3a0.04-0.13\uff1b\u9f3b\u90e8\u5360\u5173\u952e\u8bc6\u522b\u533a\u57df\u768418.9%-29.7%\uff1b\u4ec5\u4f7f\u75281%\u6700\u76f8\u5173\u7684\u50cf\u7d20\u5373\u53ef\u5b9e\u73b0\u8de8\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "LEAM\u80fd\u591f\u6709\u6548\u63ed\u793a\u9762\u90e8\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u4e2a\u4f53\u7279\u5f02\u6027\u8bc6\u522b\u6a21\u5f0f\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u4e2a\u4f53\u7279\u5f81\u5b9a\u5236\u7684\u9690\u79c1\u4fdd\u62a4\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2509.18052", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18052", "abs": "https://arxiv.org/abs/2509.18052", "authors": ["Jiaxu Zhou", "Jen-tse Huang", "Xuhui Zhou", "Man Ho Lam", "Xintao Wang", "Hao Zhu", "Wenxuan Wang", "Maarten Sap"], "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies", "comment": "Preprint", "summary": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\"", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u4e86\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u793e\u4f1a\u6a21\u62df\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u516d\u7c7b\u5e38\u89c1\u65b9\u6cd5\u8bba\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86PIMMUR\u539f\u5219\u4ee5\u63d0\u5347\u7814\u7a76\u7684\u53ef\u4fe1\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u9075\u5faaPIMMUR\u7684\u60c5\u51b5\u4e0b\uff0c\u8bb8\u591a\u5148\u524d\u62a5\u544a\u7684\u793e\u4f1a\u73b0\u8c61\u65e0\u6cd5\u590d\u73b0\u3002", "motivation": "\u8fd1\u5e74\u6765\u5927\u91cf\u7814\u7a76\u4f7f\u7528LLM\u8fdb\u884c\u793e\u4f1a\u6a21\u62df\uff0c\u4f46\u5176\u65b9\u6cd5\u8bba\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u5bfc\u81f4\u7ed3\u8bba\u53ef\u4fe1\u5ea6\u4e0d\u8db3\u3002\u4f5c\u8005\u65e8\u5728\u8bc6\u522b\u8fd9\u4e9b\u95ee\u9898\u5e76\u5efa\u7acb\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u5bf940\u591a\u7bc7\u76f8\u5173\u8bba\u6587\u7684\u7efc\u8ff0\uff0c\u5f52\u7eb3\u51fa\u516d\u4e2a\u91cd\u590d\u51fa\u73b0\u7684\u65b9\u6cd5\u8bba\u95ee\u9898\uff1a\u4ee3\u7406\u540c\u8d28\u6027\uff08Profile\uff09\u3001\u4e92\u52a8\u7f3a\u5931\u6216\u4eba\u4e3a\u8bbe\u5b9a\uff08Interaction\uff09\u3001\u8bb0\u5fc6\u4e22\u5f03\uff08Memory\uff09\u3001\u63d0\u793a\u8fc7\u5ea6\u63a7\u5236\u7ed3\u679c\uff08Minimal-Control\uff09\u3001\u4ee3\u7406\u53ef\u63a8\u65ad\u5b9e\u9a8c\u5047\u8bbe\uff08Unawareness\uff09\u4ee5\u53ca\u9a8c\u8bc1\u4f9d\u8d56\u7406\u8bba\u6a21\u578b\u800c\u975e\u771f\u5b9e\u6570\u636e\uff08Realism\uff09\u3002\u4f5c\u8005\u5c06\u8fd9\u516d\u70b9\u5f62\u5f0f\u5316\u4e3aPIMMUR\u539f\u5219\uff0c\u5e76\u5728\u4e00\u4e2a\u5f3a\u5236\u6267\u884c\u8fd9\u4e9b\u539f\u5219\u7684\u6846\u67b6\u4e0b\u91cd\u73b0\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u9075\u5b88PIMMUR\u539f\u5219\u65f6\uff0c\u5148\u524d\u7814\u7a76\u4e2d\u62a5\u544a\u7684\u793e\u4f1a\u73b0\u8c61\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u672a\u80fd\u518d\u73b0\u3002\u4f8b\u5982\uff0cGPT-4o\u548cQwen-3\u80fd\u4ece\u539f\u6709\u5b9e\u9a8c\u6307\u4ee4\u4e2d\u63a8\u65ad\u51fa\u5b9e\u9a8c\u76ee\u7684\u7684\u6bd4\u4f8b\u9ad8\u8fbe53.1%\uff0c\u8fdd\u53cd\u4e86Unawareness\u539f\u5219\u3002", "conclusion": "PIMMUR\u539f\u5219\u662f\u6784\u5efa\u53ef\u4fe1LLM\u793e\u4f1a\u6a21\u62df\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u9075\u5faa\u8fd9\u4e9b\u65b9\u6cd5\u8bba\u6807\u51c6\uff0c\u4ee5\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2509.17458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17458", "abs": "https://arxiv.org/abs/2509.17458", "authors": ["Seyed Amir Kasaei", "Ali Aghayari", "Arash Marioriyad", "Niki Sepasian", "Shayan Baghayi Nejad", "MohammadAmin Fazli", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "title": "CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration", "comment": null, "summary": "Text-to-image diffusion models, such as Stable Diffusion, can produce\nhigh-quality and diverse images but often fail to achieve compositional\nalignment, particularly when prompts describe complex object relationships,\nattributes, or spatial arrangements. Recent inference-time approaches address\nthis by optimizing or exploring the initial noise under the guidance of reward\nfunctions that score text-image alignment without requiring model fine-tuning.\nWhile promising, each strategy has intrinsic limitations when used alone:\noptimization can stall due to poor initialization or unfavorable search\ntrajectories, whereas exploration may require a prohibitively large number of\nsamples to locate a satisfactory output. Our analysis further shows that\nneither single reward metrics nor ad-hoc combinations reliably capture all\naspects of compositionality, leading to weak or inconsistent guidance. To\novercome these challenges, we present Category-Aware Reward-based Initial Noise\nOptimization and Exploration (CARINOX), a unified framework that combines noise\noptimization and exploration with a principled reward selection procedure\ngrounded in correlation with human judgments. Evaluations on two complementary\nbenchmarks covering diverse compositional challenges show that CARINOX raises\naverage alignment scores by +16% on T2I-CompBench++ and +11% on the HRS\nbenchmark, consistently outperforming state-of-the-art optimization and\nexploration-based methods across all major categories, while preserving image\nquality and diversity. The project page is available at\nhttps://amirkasaei.com/carinox/{this URL}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCARINOX\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u566a\u58f0\u4f18\u5316\u4e0e\u63a2\u7d22\u4ee5\u53ca\u57fa\u4e8e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u7684\u5956\u52b1\u9009\u62e9\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u8bed\u4e49\u7ec4\u5408\u4efb\u52a1\u4e2d\u7684\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7684\u5bf9\u8c61\u5173\u7cfb\u3001\u5c5e\u6027\u6216\u7a7a\u95f4\u5e03\u5c40\u65f6\u96be\u4ee5\u5b9e\u73b0\u826f\u597d\u7684\u7ec4\u5408\u4e00\u81f4\u6027\uff1b\u5f53\u524d\u57fa\u4e8e\u5956\u52b1\u51fd\u6570\u7684\u4f18\u5316\u6216\u63a2\u7d22\u65b9\u6cd5\u5355\u72ec\u4f7f\u7528\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u7f3a\u4e4f\u53ef\u9760\u7684\u5956\u52b1\u6307\u6807\u6765\u5168\u9762\u8861\u91cf\u7ec4\u5408\u6027\u3002", "method": "\u63d0\u51fa\u4e86CARINOX\u6846\u67b6\uff0c\u5c06\u521d\u59cb\u566a\u58f0\u7684\u4f18\u5316\u4e0e\u63a2\u7d22\u76f8\u7ed3\u5408\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u7684\u539f\u5219\u6027\u5956\u52b1\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u63d0\u4f9b\u66f4\u4e00\u81f4\u548c\u6709\u6548\u7684\u5f15\u5bfc\u3002", "result": "\u5728T2I-CompBench++\u548cHRS\u4e24\u4e2a\u57fa\u51c6\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u5347\u4e8616%\u548c11%\u7684\u5bf9\u9f50\u5f97\u5206\uff0c\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "conclusion": "CARINOX\u901a\u8fc7\u878d\u5408\u4f18\u5316\u4e0e\u63a2\u7d22\u5e76\u91c7\u7528\u66f4\u5408\u7406\u7684\u5956\u52b1\u9009\u62e9\u673a\u5236\uff0c\u5728\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u6539\u5584\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7ec4\u5408\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2509.18060", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18060", "abs": "https://arxiv.org/abs/2509.18060", "authors": ["Yutong Liu", "Ziyue Zhang", "Ban Ma-bao", "Renzeng Duojie", "Yuqing Cai", "Yongbin Yu", "Xiangxiang Wang", "Fan Gao", "Cheng Huang", "Nyima Tashi"], "title": "TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \u00dc-Tsang, Amdo and Kham Speech Dataset Generation", "comment": null, "summary": "Tibetan is a low-resource language with limited parallel speech corpora\nspanning its three major dialects (\\\"U-Tsang, Amdo, and Kham), limiting\nprogress in speech modeling. To address this issue, we propose TMD-TTS, a\nunified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes\nparallel dialectal speech from explicit dialect labels. Our method features a\ndialect fusion module and a Dialect-Specialized Dynamic Routing Network\n(DSDR-Net) to capture fine-grained acoustic and linguistic variations across\ndialects. Extensive objective and subjective evaluations demonstrate that\nTMD-TTS significantly outperforms baselines in dialectal expressiveness. We\nfurther validate the quality and utility of the synthesized speech through a\nchallenging Speech-to-Speech Dialect Conversion (S2SDC) task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u85cf\u8bed\u591a\u65b9\u8a00\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u6846\u67b6TMD-TTS\uff0c\u901a\u8fc7\u65b9\u8a00\u878d\u5408\u6a21\u5757\u548c\u65b9\u8a00\u4e13\u7528\u52a8\u6001\u8def\u7531\u7f51\u7edc\uff08DSDR-Net\uff09\u6709\u6548\u63d0\u5347\u591a\u65b9\u8a00\u8bed\u97f3\u5408\u6210\u6027\u80fd\u3002", "motivation": "\u85cf\u8bed\u8d44\u6e90\u7a00\u7f3a\uff0c\u4e09\u79cd\u4e3b\u8981\u65b9\u8a00\u7f3a\u4e4f\u5e73\u884c\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u9650\u5236\u4e86\u8bed\u97f3\u5efa\u6a21\u7684\u53d1\u5c55\u3002", "method": "\u5f15\u5165TMD-TTS\u6846\u67b6\uff0c\u7ed3\u5408\u65b9\u8a00\u878d\u5408\u6a21\u5757\u548cDSDR-Net\u7f51\u7edc\uff0c\u5229\u7528\u663e\u5f0f\u65b9\u8a00\u6807\u7b7e\u5408\u6210\u591a\u65b9\u8a00\u8bed\u97f3\u3002", "result": "\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u4e2d\uff0cTMD-TTS\u5728\u65b9\u8a00\u8868\u73b0\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8bed\u97f3\u5230\u8bed\u97f3\u65b9\u8a00\u8f6c\u6362\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "TMD-TTS\u80fd\u6709\u6548\u6355\u6349\u85cf\u8bed\u591a\u65b9\u8a00\u7684\u7ec6\u7c92\u5ea6\u58f0\u5b66\u4e0e\u8bed\u8a00\u5dee\u5f02\uff0c\u4e3a\u4f4e\u8d44\u6e90\u591a\u65b9\u8a00\u8bed\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.17461", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17461", "abs": "https://arxiv.org/abs/2509.17461", "authors": ["Yuhao Zhang", "Chengjun Zhang", "Di Wu", "Jie Yang", "Mohamad Sawan"], "title": "CSDformer: A Conversion Method for Fully Spike-Driven Transformer", "comment": null, "summary": "Spike-based transformer is a novel architecture aiming to enhance the\nperformance of spiking neural networks while mitigating the energy overhead\ninherent to transformers. However, methods for generating these models suffer\nfrom critical limitations: excessive training costs introduced by direct\ntraining methods, or unavoidably hardware-unfriendly operations in existing\nconversion methods. In this paper, we propose CSDformer, a novel conversion\nmethod for fully spike-driven transformers. We tailor a conversion-oriented\ntransformer-based architecture and propose a new function NReLU to replace\nsoftmax in self-attention. Subsequently, this model is quantized and trained,\nand converted into a fully spike-driven model with temporal decomposition\ntechnique. Also, we propose delayed Integrate-andFire neurons to reduce\nconversion errors and improve the performance of spiking models. We evaluate\nCSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1\naccuracy under 7 time-steps on ImageNet, demonstrating superiority over\nstate-of-the-art models. Furthermore, CSDformer eliminates the need for\ntraining SNNs, thereby reducing training costs (reducing computational resource\nby 75% and accelerating training speed by 2-3$\\times$). To the best of our\nknowledge, this is the first fully spike-driven transformer-based model\ndeveloped via conversion method, achieving high performance under ultra-low\nlatency, while dramatically reducing both computational complexity and training\noverhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSDformer\u7684\u65b0\u578b\u8f6c\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u5168\u8109\u51b2\u9a71\u52a8\u7684Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165NReLU\u66ff\u4ee3Softmax\u3001\u65f6\u57df\u5206\u89e3\u6280\u672f\u548c\u5ef6\u8fdfIntegrate-and-Fire\u795e\u7ecf\u5143\uff0c\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u5f00\u9500\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u8109\u51b2Transformer\u6a21\u578b\u5b58\u5728\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u6216\u786c\u4ef6\u4e0d\u53cb\u597d\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u8f6c\u6362\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u9762\u5411\u8f6c\u6362\u7684Transformer\u67b6\u6784\uff0c\u7528NReLU\u66ff\u4ee3Softmax\uff1b\u91c7\u7528\u91cf\u5316\u548c\u8bad\u7ec3\u540e\u8f6c\u6362\u4e3a\u5168\u8109\u51b2\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u57df\u5206\u89e3\u548c\u5ef6\u8fdfIntegrate-and-Fire\u795e\u7ecf\u5143\u51cf\u5c11\u8f6c\u6362\u8bef\u5dee\u3002", "result": "\u5728ImageNet\u4e0a7\u4e2a\u65f6\u95f4\u6b65\u5185\u8fbe\u523076.36%\u7684Top-1\u51c6\u786e\u7387\uff0c\u8bad\u7ec3\u8d44\u6e90\u51cf\u5c1175%\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472-3\u500d\uff0c\u4e14\u65e0\u9700\u76f4\u63a5\u8bad\u7ec3SNN\u3002", "conclusion": "CSDformer\u662f\u9996\u4e2a\u901a\u8fc7\u8f6c\u6362\u65b9\u6cd5\u5b9e\u73b0\u7684\u5168\u8109\u51b2\u9a71\u52a8Transformer\uff0c\u5728\u8d85\u4f4e\u5ef6\u8fdf\u4e0b\u517c\u5177\u9ad8\u6027\u80fd\u4e0e\u4f4e\u8bad\u7ec3\u5f00\u9500\uff0c\u4e3a\u9ad8\u6548\u8109\u51b2Transformer\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.18063", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18063", "abs": "https://arxiv.org/abs/2509.18063", "authors": ["Jan-Felix Klein", "Lars Ohnemus"], "title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning", "comment": "Work in Progess", "summary": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability.", "AI": {"tldr": "\u63d0\u51faARK-V1\uff0c\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u7b80\u5355KG-agent\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a2\u7d22\u56fe\u6765\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u5728CoLoTa\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u601d\u7ef4\u94fe\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\uff0c\u4f46\u5728\u9700\u8981\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u8db3\uff1b\u77e5\u8bc6\u56fe\u8c31\u867d\u63d0\u4f9b\u7ed3\u6784\u5316\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u56e0\u5176\u590d\u6742\u6027\u548c\u591a\u8df3\u63a8\u7406\u9700\u6c42\u800c\u96be\u4ee5\u6574\u5408\u3002", "method": "\u8bbe\u8ba1ARK-V1\uff0c\u5229\u7528\u672a\u5fae\u8c03\u7684\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u8fed\u4ee3\u63a2\u7d22\u77e5\u8bc6\u56fe\u8c31\u4ee5\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u3002", "result": "\u5728CoLoTa\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cARK-V1\u6bd4\u601d\u7ef4\u94fe\u57fa\u7ebf\u5177\u6709\u66f4\u9ad8\u7684\u6761\u4ef6\u51c6\u786e\u7387\uff0c\u4e14\u66f4\u5927\u7684\u9aa8\u5e72\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u8986\u76d6\u6027\u3001\u6b63\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "ARK-V1\u6709\u6548\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\uff0c\u63d0\u5347\u4e86\u5bf9\u957f\u5c3e\u5b9e\u4f53\u7684\u95ee\u7b54\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5916\u90e8\u77e5\u8bc6\u8fed\u4ee3\u68c0\u7d22\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17462", "abs": "https://arxiv.org/abs/2509.17462", "authors": ["Changwon Kang", "Jisong Kim", "Hongjae Shin", "Junseo Park", "Jun Won Choi"], "title": "MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception", "comment": "Accepted to ICCV 2025", "summary": "The goal of multi-task learning is to learn to conduct multiple tasks\nsimultaneously based on a shared data representation. While this approach can\nimprove learning efficiency, it may also cause performance degradation due to\ntask conflicts that arise when optimizing the model for different objectives.\nTo address this challenge, we introduce MAESTRO, a structured framework\ndesigned to generate task-specific features and mitigate feature interference\nin multi-task 3D perception, including 3D object detection, bird's-eye view\n(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three\ncomponents: the Class-wise Prototype Generator (CPG), the Task-Specific Feature\nGenerator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class\ncategories into foreground and background groups and generates group-wise\nprototypes. The foreground and background prototypes are assigned to the 3D\nobject detection task and the map segmentation task, respectively, while both\nare assigned to the 3D occupancy prediction task. TSFG leverages these\nprototype groups to retain task-relevant features while suppressing irrelevant\nfeatures, thereby enhancing the performance for each task. SPA enhances the\nprototype groups assigned for 3D occupancy prediction by utilizing the\ninformation produced by the 3D object detection head and the map segmentation\nhead. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate\nthat MAESTRO consistently outperforms existing methods across 3D object\ndetection, BEV map segmentation, and 3D occupancy prediction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAESTRO\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4efb\u52a13D\u611f\u77e5\uff0c\u901a\u8fc7\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u548c\u7f13\u89e3\u7279\u5f81\u5e72\u6270\uff0c\u57283D\u76ee\u6807\u68c0\u6d4b\u3001\u9e1f\u77b0\u56fe\u5206\u5272\u548c3D\u5360\u636e\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u867d\u7136\u80fd\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\uff0c\u4f46\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u4f18\u5316\u76ee\u6807\u51b2\u7a81\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51cf\u5c11\u4efb\u52a1\u95f4\u7279\u5f81\u5e72\u6270\u7684\u65b9\u6cd5\u3002", "method": "MAESTRO\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u7c7b\u522b\u539f\u578b\u751f\u6210\u5668\uff08CPG\uff09\u3001\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u751f\u6210\u5668\uff08TSFG\uff09\u548c\u573a\u666f\u539f\u578b\u805a\u5408\u5668\uff08SPA\uff09\u3002CPG\u5c06\u7c7b\u522b\u5206\u4e3a\u524d\u666f\u548c\u80cc\u666f\u7ec4\u5e76\u751f\u6210\u539f\u578b\uff1bTSFG\u5229\u7528\u8fd9\u4e9b\u539f\u578b\u4fdd\u7559\u76f8\u5173\u7279\u5f81\u5e76\u6291\u5236\u65e0\u5173\u7279\u5f81\uff1bSPA\u5219\u878d\u5408\u5176\u4ed6\u4efb\u52a1\u5934\u7684\u4fe1\u606f\u6765\u589e\u5f3a\u5360\u636e\u9884\u6d4b\u7684\u539f\u578b\u3002", "result": "\u5728nuScenes\u548cOcc3D\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAESTRO\u57283D\u76ee\u6807\u68c0\u6d4b\u3001BEV\u5730\u56fe\u5206\u5272\u548c3D\u5360\u636e\u9884\u6d4b\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAESTRO\u6709\u6548\u7f13\u89e3\u4e86\u591a\u4efb\u52a13D\u611f\u77e5\u4e2d\u7684\u7279\u5f81\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5404\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18093", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18093", "abs": "https://arxiv.org/abs/2509.18093", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "SEQR: Secure and Efficient QR-based LoRA Routing", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEQR\u7684\u65e0\u76d1\u7763LoRA\u8def\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6fc0\u6d3b\u8303\u6570\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u8bc1\u660e\u7684\u9002\u914d\u5668\u9009\u62e9\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001LoRA\u7ec4\u5408\u3002", "motivation": "\u5728\u9690\u79c1\u654f\u611f\u573a\u666f\u4e2d\uff0c\u7f3a\u4e4f\u65e0\u9700\u76d1\u7763\u8bad\u7ec3\u7684LoRA\u9002\u914d\u5668\u8def\u7531\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u5f62\u5f0f\u5316\u65e0\u76d1\u7763LoRA\u8def\u7531\u76ee\u6807\u4e3a\u6fc0\u6d3b\u8303\u6570\u6700\u5927\u5316\uff0c\u63d0\u51faSEQR\u7b97\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4fdd\u8bc1\u8def\u7531\u51c6\u786e\u6027\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSEQR\u5728\u591a\u4efb\u52a1\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u5feb\u901f\u51c6\u786e\u8bc6\u522b\u6700\u4f18\u9002\u914d\u5668\u3002", "conclusion": "SEQR\u4e3a\u65e0\u76d1\u7763LoRA\u8def\u7531\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5177\u7406\u8bba\u4fdd\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u52a8\u6001\u9002\u914d\u5668\u7ec4\u5408\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.17476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17476", "abs": "https://arxiv.org/abs/2509.17476", "authors": ["Mallikarjun B. R.", "Fei Yin", "Vikram Voleti", "Nikita Drobyshev", "Maksim Lapin", "Aaryaman Vasishta", "Varun Jampani"], "title": "Stable Video-Driven Portraits", "comment": "https://stable-video-driven-portraits.github.io/", "summary": "Portrait animation aims to generate photo-realistic videos from a single\nsource image by reenacting the expression and pose from a driving video. While\nearly methods relied on 3D morphable models or feature warping techniques, they\noften suffered from limited expressivity, temporal inconsistency, and poor\ngeneralization to unseen identities or large pose variations. Recent advances\nusing diffusion models have demonstrated improved quality but remain\nconstrained by weak control signals and architectural limitations. In this\nwork, we propose a novel diffusion based framework that leverages masked facial\nregions specifically the eyes, nose, and mouth from the driving video as strong\nmotion control cues. To enable robust training without appearance leakage, we\nadopt cross identity supervision. To leverage the strong prior from the\npretrained diffusion model, our novel architecture introduces minimal new\nparameters that converge faster and help in better generalization. We introduce\nspatial temporal attention mechanisms that allow inter frame and intra frame\ninteractions, effectively capturing subtle motions and reducing temporal\nartifacts. Our model uses history frames to ensure continuity across segments.\nAt inference, we propose a novel signal fusion strategy that balances motion\nfidelity with identity preservation. Our approach achieves superior temporal\nconsistency and accurate expression control, enabling high-quality,\ncontrollable portrait animation suitable for real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u5229\u7528\u9a71\u52a8\u89c6\u9891\u4e2d\u9762\u90e8\u5173\u952e\u533a\u57df\uff08\u773c\u3001\u9f3b\u3001\u53e3\uff09\u4f5c\u4e3a\u5f3a\u8fd0\u52a8\u63a7\u5236\u4fe1\u53f7\uff0c\u7ed3\u5408\u8de8\u8eab\u4efd\u76d1\u7763\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u65f6\u5e8f\u4e00\u81f4\u6027\u7684\u53ef\u63a7\u4eba\u50cf\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8868\u8fbe\u80fd\u529b\u3001\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u5bf9\u672a\u89c1\u8eab\u4efd\u6216\u5927\u59ff\u6001\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5f53\u524d\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u53d7\u9650\u4e8e\u5f31\u63a7\u5236\u4fe1\u53f7\u548c\u67b6\u6784\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u3001\u53ef\u63a7\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u9a71\u52a8\u89c6\u9891\u4e2d\u7684\u906e\u7f69\u9762\u90e8\u533a\u57df\uff08\u773c\u775b\u3001\u9f3b\u5b50\u3001\u5634\u5df4\uff09\u4f5c\u4e3a\u5f3a\u8fd0\u52a8\u63a7\u5236\u4fe1\u53f7\uff1b\u91c7\u7528\u8de8\u8eab\u4efd\u76d1\u7763\u4ee5\u907f\u514d\u5916\u89c2\u6cc4\u6f0f\uff1b\u5f15\u5165\u6700\u5c0f\u5316\u65b0\u53c2\u6570\u7684\u7f51\u7edc\u7ed3\u6784\u4ee5\u52a0\u5feb\u6536\u655b\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff1b\u8bbe\u8ba1\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u6355\u6349\u5e27\u95f4\u4e0e\u5e27\u5185\u7ec6\u5fae\u52a8\u4f5c\uff0c\u5e76\u5229\u7528\u5386\u53f2\u5e27\u4fdd\u8bc1\u89c6\u9891\u8fde\u7eed\u6027\uff1b\u63a8\u7406\u9636\u6bb5\u63d0\u51fa\u65b0\u7684\u4fe1\u53f7\u878d\u5408\u7b56\u7565\u4ee5\u5e73\u8861\u52a8\u4f5c\u4fdd\u771f\u5ea6\u4e0e\u8eab\u4efd\u4fdd\u6301\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0e\u8868\u60c5\u63a7\u5236\u7cbe\u5ea6\uff0c\u751f\u6210\u89c6\u9891\u5177\u6709\u66f4\u9ad8\u7684\u771f\u5b9e\u611f\u548c\u7a33\u5b9a\u6027\uff0c\u652f\u6301\u590d\u6742\u8868\u60c5\u4e0e\u5927\u59ff\u6001\u53d8\u5316\u4e0b\u7684\u9ad8\u8d28\u91cf\u4eba\u50cf\u52a8\u753b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u5f3a\u63a7\u5236\u4fe1\u53f7\u3001\u8de8\u8eab\u4efd\u76d1\u7763\u548c\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u4eba\u50cf\u52a8\u753b\u4efb\u52a1\u4e2d\u7684\u53ef\u63a7\u6027\u3001\u751f\u6210\u8d28\u91cf\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.17481", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17481", "abs": "https://arxiv.org/abs/2509.17481", "authors": ["Xingqi Wang", "Yiming Cui", "Xin Yao", "Shijin Wang", "Guoping Hu", "Xiaoyu Qin"], "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have recently demonstrated remarkable\nprogress, yet hallucination remains a critical barrier, particularly in chart\nunderstanding, which requires sophisticated perceptual and cognitive abilities\nas well as rigorous factual accuracy. While prior work has investigated\nhallucinations and chart comprehension independently, their intersection\nremains largely unexplored. To address this gap, we present ChartHal, a\nbenchmark that features a fine-grained taxonomy of hallucination scenarios in\nchart understanding, along with a human-validated dataset of 1,062 samples. Our\nevaluation shows that state-of-the-art LVLMs suffer from severe hallucinations\non ChartHal, including proprietary models such as GPT-5 and o4-mini, which\nachieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals\nthat questions involving information absent from or contradictory to charts are\nespecially likely to trigger hallucinations, underscoring the urgent need for\nmore robust mitigation strategies. Code and data are available at\nhttps://github.com/ymcui/ChartHal .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ChartHal\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u5e7b\u89c9\u73b0\u8c61\u7684\u57fa\u51c6\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u7684\u5e7b\u89c9\u5206\u7c7b\u548c1062\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u6837\u672c\u3002\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u5728\u6b64\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51f8\u663e\u4e86\u6539\u8fdb\u7684\u7d27\u8feb\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u4f9d\u7136\u4e25\u91cd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u573a\u666f\u4e2d\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u8fd9\u4e00\u4ea4\u53c9\u9886\u57df\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aChartHal\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u7ec6\u7c92\u5ea6\u7684\u5e7b\u89c9\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u6536\u96c6\u4e861,062\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u6837\u672c\u7528\u4e8e\u8bc4\u4f30\u4e3b\u6d41LVLMs\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u5e7b\u89c9\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u6700\u5148\u8fdb\u7684LVLM\uff08\u5305\u62ecGPT-5\u548co4-mini\uff09\u5728ChartHal\u4e0a\u51c6\u786e\u7387\u5206\u522b\u4ec5\u4e3a34.46%\u548c22.79%\uff0c\u5c24\u5176\u5728\u56fe\u8868\u4fe1\u606f\u7f3a\u5931\u6216\u77db\u76fe\u7684\u95ee\u9898\u4e0a\u66f4\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002", "conclusion": "\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4e9f\u9700\u5173\u6ce8\uff0cChartHal\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5e7b\u89c9\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2509.17492", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17492", "abs": "https://arxiv.org/abs/2509.17492", "authors": ["Qinghua Lin", "Guang-Hai Liu", "Zuoyong Li", "Yang Li", "Yuting Jiang", "Xiang Wu"], "title": "Multimodal Medical Image Classification via Synergistic Learning Pre-training", "comment": null, "summary": "Multimodal pathological images are usually in clinical diagnosis, but\ncomputer vision-based multimodal image-assisted diagnosis faces challenges with\nmodality fusion, especially in the absence of expert-annotated data. To achieve\nthe modality fusion in multimodal images with label scarcity, we propose a\nnovel ``pretraining + fine-tuning\" framework for multimodal semi-supervised\nmedical image classification. Specifically, we propose a synergistic learning\npretraining framework of consistency, reconstructive, and aligned learning. By\ntreating one modality as an augmented sample of another modality, we implement\na self-supervised learning pre-train, enhancing the baseline model's feature\nrepresentation capability. Then, we design a fine-tuning method for multimodal\nfusion. During the fine-tuning stage, we set different encoders to extract\nfeatures from the original modalities and provide a multimodal fusion encoder\nfor fusion modality. In addition, we propose a distribution shift method for\nmultimodal fusion features, which alleviates the prediction uncertainty and\noverfitting risks caused by the lack of labeled samples. We conduct extensive\nexperiments on the publicly available gastroscopy image datasets Kvasir and\nKvasirv2. Quantitative and qualitative results demonstrate that the proposed\nmethod outperforms the current state-of-the-art classification methods. The\ncode will be released at: https://github.com/LQH89757/MICS.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u201c\u9884\u8bad\u7ec3+\u5fae\u8c03\u201d\u6846\u67b6\u7528\u4e8e\u591a\u6a21\u6001\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u534f\u540c\u5b66\u4e60\u9884\u8bad\u7ec3\u548c\u5206\u5e03\u504f\u79fb\u65b9\u6cd5\u63d0\u5347\u6a21\u6001\u878d\u5408\u6548\u679c\uff0c\u5728Kvasir\u548cKvasirv2\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u6807\u7b7e\u7a00\u7f3a\u4e0b\u7684\u6a21\u6001\u878d\u5408\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u81f4\u6027\u3001\u91cd\u5efa\u6027\u548c\u5bf9\u9f50\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u591a\u7f16\u7801\u5668\u5fae\u8c03\u7b56\u7565\u4e0e\u5206\u5e03\u504f\u79fb\u65b9\u6cd5\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728Kvasir\u548cKvasirv2\u80c3\u955c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9a\u91cf\u4e0e\u5b9a\u6027\u7ed3\u679c\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5728\u6807\u7b7e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17498", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.17498", "abs": "https://arxiv.org/abs/2509.17498", "authors": ["Dilshara Herath", "Chinthaka Abeyrathne", "Prabhani Jayaweera"], "title": "Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of YOLOv5-v11 Models", "comment": "Drowsiness Detection using state of the art YOLO algorithms", "summary": "Driver drowsiness remains a critical factor in road accidents, accounting for\nthousands of fatalities and injuries each year. This paper presents a\ncomprehensive evaluation of real-time, non-intrusive drowsiness detection\nmethods, focusing on computer vision based YOLO (You Look Only Once)\nalgorithms. A publicly available dataset namely, UTA-RLDD was used, containing\nboth awake and drowsy conditions, ensuring variability in gender, eyewear,\nillumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,\nv11n, v11l) are fine-tuned, with performance measured in terms of Precision,\nRecall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest\naccuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal\nbalance between precision (0.954) and inference efficiency, making it highly\nsuitable for embedded deployment. Additionally, we implement an Eye Aspect\nRatio (EAR) approach using Dlib's facial landmarks, which despite its low\ncomputational footprint exhibits reduced robustness under pose variation and\nocclusions. Our findings illustrate clear trade offs between accuracy, latency,\nand resource requirements, and offer practical guidelines for selecting or\ncombining detection methods in autonomous driving and industrial safety\napplications.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8eYOLO\u7684\u5b9e\u65f6\u975e\u4fb5\u5165\u5f0f\u9a7e\u9a76\u5458\u75b2\u52b3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528UTA-RLDD\u6570\u636e\u96c6\u5bf9\u4e03\u79cdYOLO\u53d8\u4f53\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408EAR\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u63ed\u793a\u4e86\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u4e0e\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u9a7e\u9a76\u5458\u75b2\u52b3\u662f\u5bfc\u81f4\u4ea4\u901a\u4e8b\u6545\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u5b9e\u65f6\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u5347\u9053\u8def\u5b89\u5168\u3002", "method": "\u91c7\u7528\u4e03\u79cdYOLO\u6a21\u578b\uff08v5s, v9c, v9t, v10n, v10l, v11n, v11l\uff09\u5728UTA-RLDD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408Dlib\u9762\u90e8\u5173\u952e\u70b9\u7684EAR\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ecPrecision\u3001Recall\u3001mAP0.5\u548cmAP0.5-0.95\u3002", "result": "YOLOv9c\u53d6\u5f97\u6700\u9ad8\u7cbe\u5ea6\uff08mAP0.5\u4e3a0.986\uff0cRecall\u4e3a0.978\uff09\uff0cYOLOv11n\u5728\u7cbe\u5ea6\uff080.954\uff09\u4e0e\u63a8\u7406\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u90e8\u7f72\uff1bEAR\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u4f4e\u4f46\u5bf9\u59ff\u6001\u53d8\u5316\u548c\u906e\u6321\u9c81\u68d2\u6027\u8f83\u5dee\u3002", "conclusion": "\u4e0d\u540c\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u8d44\u6e90\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u6743\u8861\uff0c\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u5de5\u4e1a\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u75b2\u52b3\u68c0\u6d4b\u65b9\u6848\u9009\u62e9\u6216\u878d\u5408\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2509.17500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17500", "abs": "https://arxiv.org/abs/2509.17500", "authors": ["Yujie Xie", "Hongyang Zhang", "Zhihui Liu", "Shihai Ruan"], "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge", "comment": null, "summary": "Large-scale Video Object Segmentation (LSVOS) addresses the challenge of\naccurately tracking and segmenting objects in long video sequences, where\ndifficulties stem from object reappearance, small-scale targets, heavy\nocclusions, and crowded scenes. Existing approaches predominantly adopt\nSAM2-based frameworks with various memory mechanisms for complex video mask\ngeneration. In this report, we proposed Segment Anything with Memory\nStrengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE\ntrack of ICCV 2025, which integrates the strengths of stateof-the-art VOS\nmodels into an effective paradigm. To handle visually similar instances and\nlong-term object disappearance in MOSE, we incorporate a long-term memorymodule\nfor reliable object re-identification. Additionly, we adopt SAM2Long as a\npost-processing strategy to reduce error accumulation and enhance segmentation\nstability in long video sequences. Our method achieved a final performance of\n0.8427 in terms of J &F in the test-set leaderboard.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAMSON\uff0c\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u89c6\u9891\u5bf9\u8c61\u5206\u5272\uff08LSVOS\uff09\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5148\u8fdbVOS\u6a21\u578b\u7684\u4f18\u70b9\u548c\u5f3a\u5316\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5728ICCV 2025 MOSE\u8d5b\u9053\u4e2d\u83b7\u5f97\u7b2c\u4e09\u540d\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u5bf9\u8c61\u91cd\u73b0\u3001\u5c0f\u76ee\u6807\u3001\u4e25\u91cd\u906e\u6321\u548c\u62e5\u6324\u573a\u666f\u5e26\u6765\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u89c6\u89c9\u76f8\u4f3c\u5b9e\u4f8b\u548c\u957f\u671f\u6d88\u5931\u5bf9\u8c61\u7684\u8ddf\u8e2a\u95ee\u9898\u3002", "method": "\u63d0\u51faSAMSON\u6846\u67b6\uff0c\u5f15\u5165\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u5bf9\u8c61\u91cd\u8bc6\u522b\uff0c\u5e76\u91c7\u7528SAM2Long\u4f5c\u4e3a\u540e\u5904\u7406\u7b56\u7565\uff0c\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u5e76\u63d0\u5347\u5206\u5272\u7a33\u5b9a\u6027\u3002", "result": "\u5728MOSE\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728J&F\u6307\u6807\u4e0a\u8fbe\u5230\u4e860.8427\u7684\u6027\u80fd\u3002", "conclusion": "SAMSON\u901a\u8fc7\u589e\u5f3a\u8bb0\u5fc6\u673a\u5236\u548c\u540e\u5904\u7406\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u957f\u89c6\u9891\u4e2d\u7684\u5bf9\u8c61\u5206\u5272\u4e0e\u8ffd\u8e2a\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728LSVOS\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.17506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17506", "abs": "https://arxiv.org/abs/2509.17506", "authors": ["Houqiang Zhong", "Zihan Zheng", "Qiang Hu", "Yuan Tian", "Ning Cao", "Lan Xu", "Xiaoyun Zhang", "Zhengxue Cheng", "Li Song", "Wenjun Zhang"], "title": "4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression", "comment": null, "summary": "Volumetric video has emerged as a key medium for immersive telepresence and\naugmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation\nand realistic spatial interactions. However, delivering high-quality dynamic\nvolumetric content at scale remains challenging due to massive data volume,\ncomplex motion, and limited editability of existing representations. In this\npaper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework\ndesigned for scalable and editable volumetric video streaming. Our method\nintroduces a layered representation that explicitly separates static\nbackgrounds from dynamic foregrounds using a lookahead-based motion\ndecomposition strategy, significantly reducing temporal redundancy and enabling\nselective background/foreground streaming. To capture continuous motion\ntrajectories, we employ a multi-resolution motion estimation grid and a\nlightweight shared MLP, complemented by a dynamic Gaussian compensation\nmechanism to model emergent content. An adaptive grouping scheme dynamically\ninserts background keyframes to balance temporal consistency and compression\nefficiency. Furthermore, an entropy-aware training pipeline jointly optimizes\nthe motion fields and Gaussian parameters under a rate-distortion (RD)\nobjective, while employing range-based and KD-tree compression to minimize\nstorage overhead. Extensive experiments on multiple datasets demonstrate that\n4D-MoDe consistently achieves competitive reconstruction quality with an order\nof magnitude lower storage cost (e.g., as low as \\textbf{11.4} KB/frame)\ncompared to state-of-the-art methods, while supporting practical applications\nsuch as background replacement and foreground-only streaming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4D-MoDe\u7684\u8fd0\u52a8\u89e3\u80264D\u9ad8\u65af\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u4e14\u53ef\u7f16\u8f91\u7684\u4f53\u89c6\u9891\u6d41\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u6210\u672c\u5e76\u652f\u6301\u80cc\u666f\u66ff\u6362\u7b49\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u91cf\u5927\u3001\u8fd0\u52a8\u590d\u6742\u4ee5\u53ca\u73b0\u6709\u8868\u793a\u65b9\u6cd5\u7684\u53ef\u7f16\u8f91\u6027\u6709\u9650\uff0c\u5927\u89c4\u6a21\u4f20\u8f93\u9ad8\u8d28\u91cf\u52a8\u6001\u4f53\u89c6\u9891\u5185\u5bb9\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165\u5206\u5c42\u8868\u793a\uff0c\u901a\u8fc7\u524d\u5411\u770b\u7684\u8fd0\u52a8\u5206\u89e3\u7b56\u7565\u5c06\u9759\u6001\u80cc\u666f\u4e0e\u52a8\u6001\u524d\u666f\u5206\u79bb\uff1b\u91c7\u7528\u591a\u5206\u8fa8\u7387\u8fd0\u52a8\u4f30\u8ba1\u7f51\u683c\u548c\u8f7b\u91cf\u7ea7\u5171\u4eabMLP\u5efa\u6a21\u8fde\u7eed\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u9ad8\u65af\u8865\u507f\u673a\u5236\uff1b\u8bbe\u8ba1\u81ea\u9002\u5e94\u5206\u7ec4\u65b9\u6848\u63d2\u5165\u80cc\u666f\u5173\u952e\u5e27\u4ee5\u5e73\u8861\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u538b\u7f29\u6548\u7387\uff1b\u901a\u8fc7\u71b5\u611f\u77e5\u8bad\u7ec3 pipeline \u8054\u5408\u4f18\u5316\u8fd0\u52a8\u573a\u548c\u9ad8\u65af\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u8303\u56f4\u548cKD\u6811\u7684\u538b\u7f29\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c4D-MoDe\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5b58\u50a8\u6210\u672c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff08\u4f8b\u5982\u4f4e\u81f311.4 KB/\u5e27\uff09\uff0c\u540c\u65f6\u652f\u6301\u80cc\u666f\u66ff\u6362\u548c\u4ec5\u524d\u666f\u6d41\u5f0f\u4f20\u8f93\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "4D-MoDe\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4f53\u89c6\u9891\u538b\u7f29\u4e0e\u6d41\u5f0f\u4f20\u8f93\uff0c\u517c\u5177\u9ad8\u7f16\u8f91\u6027\u548c\u4f4e\u5b58\u50a8\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u6c89\u6d78\u5f0f\u8fdc\u7a0b\u5448\u73b0\u548cAR/VR\u573a\u666f\u3002"}}
{"id": "2509.17513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17513", "abs": "https://arxiv.org/abs/2509.17513", "authors": ["Zihan Zheng", "Zhenlong Wu", "Houqiang Zhong", "Yuan Tian", "Ning Cao", "Lan Xu", "Jiangchao Yao", "Xiaoyun Zhang", "Qiang Hu", "Wenjun Zhang"], "title": "4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming", "comment": "NeurIPS 2025", "summary": "Achieving seamless viewing of high-fidelity volumetric video, comparable to\n2D video experiences, remains an open challenge. Existing volumetric video\ncompression methods either lack the flexibility to adjust quality and bitrate\nwithin a single model for efficient streaming across diverse networks and\ndevices, or struggle with real-time decoding and rendering on lightweight\nmobile platforms. To address these challenges, we introduce 4DGCPro, a novel\nhierarchical 4D Gaussian compression framework that facilitates real-time\nmobile decoding and high-quality rendering via progressive volumetric video\nstreaming in a single bitstream. Specifically, we propose a\nperceptually-weighted and compression-friendly hierarchical 4D Gaussian\nrepresentation with motion-aware adaptive grouping to reduce temporal\nredundancy, preserve coherence, and enable scalable multi-level detail\nstreaming. Furthermore, we present an end-to-end entropy-optimized training\nscheme, which incorporates layer-wise rate-distortion (RD) supervision and\nattribute-specific entropy modeling for efficient bitstream generation.\nExtensive experiments show that 4DGCPro enables flexible quality and multiple\nbitrate within a single model, achieving real-time decoding and rendering on\nmobile devices while outperforming existing methods in RD performance across\nmultiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4DGCPro\u7684\u65b0\u578b\u5206\u5c424D\u9ad8\u65af\u538b\u7f29\u6846\u67b6\uff0c\u652f\u6301\u5728\u5355\u4e00\u7801\u6d41\u4e2d\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u4f53\u89c6\u9891\u6d41\u4f20\u8f93\uff0c\u53ef\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u89e3\u7801\u4e0e\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u7684\u4f53\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u8d28\u91cf\u4e0e\u7801\u7387\u7684\u7075\u6d3b\u6027\u8c03\u6574\uff0c\u4e14\u5728\u8f7b\u91cf\u7ea7\u79fb\u52a8\u5e73\u53f0\u4e0a\u7684\u5b9e\u65f6\u89e3\u7801\u548c\u6e32\u67d3\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u9ad8\u4fdd\u771f\u4f53\u89c6\u9891\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5\u52a0\u6743\u3001\u538b\u7f29\u53cb\u597d\u7684\u5206\u5c424D\u9ad8\u65af\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fd0\u52a8\u611f\u77e5\u81ea\u9002\u5e94\u5206\u7ec4\u4ee5\u51cf\u5c11\u65f6\u95f4\u5197\u4f59\u5e76\u4fdd\u6301\u4e00\u81f4\u6027\uff1b\u8bbe\u8ba1\u4e86\u7aef\u5230\u7aef\u7684\u71b5\u4f18\u5316\u8bad\u7ec3\u65b9\u6848\uff0c\u5f15\u5165\u9010\u5c42\u7387\u5931\u771f\u76d1\u7763\u548c\u5c5e\u6027\u7279\u5b9a\u7684\u71b5\u5efa\u6a21\uff0c\u4ee5\u9ad8\u6548\u751f\u6210\u7801\u6d41\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c4DGCPro\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5355\u4e2a\u6a21\u578b\u5185\u7075\u6d3b\u8c03\u8282\u8d28\u91cf\u548c\u591a\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5b9e\u65f6\u89e3\u7801\u4e0e\u6e32\u67d3\u3002", "conclusion": "4DGCPro\u4e3a\u9ad8\u4fdd\u771f\u4f53\u89c6\u9891\u5728\u591a\u6837\u5316\u7f51\u7edc\u548c\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6d41\u5f0f\u4f20\u8f93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17520", "abs": "https://arxiv.org/abs/2509.17520", "authors": ["Mingda Zhang", "Yuyang Zheng", "Ruixiang Tang", "Jingru Qiu", "Haiyan Ding"], "title": "Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision Fusion for Brain Tumor Segmentation", "comment": "8 pages, 3 figures", "summary": "Brain tumor segmentation requires accurate identification of hierarchical\nregions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)\nfrom multi-sequence magnetic resonance imaging (MRI) images. Due to tumor\ntissue heterogeneity, ambiguous boundaries, and contrast variations across MRI\nsequences, methods relying solely on visual information or post-hoc loss\nconstraints show unstable performance in boundary delineation and hierarchy\npreservation. To address this challenge, we propose the Unified Multimodal\nCoherent Field (UMCF) method. This method achieves synchronous interactive\nfusion of visual, semantic, and spatial information within a unified 3D latent\nspace, adaptively adjusting modal contributions through parameter-free\nuncertainty gating, with medical prior knowledge directly participating in\nattention computation, avoiding the traditional \"process-then-concatenate\"\nseparated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021\ndatasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977\nrespectively, with an average 4.18% improvement across mainstream\narchitectures. By deeply integrating clinical knowledge with imaging features,\nUMCF provides a new technical pathway for multimodal information fusion in\nprecision medicine.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u76f8\u5e72\u573a\uff08UMCF\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\uff0c\u901a\u8fc7\u5728\u7edf\u4e00\u76843D\u6f5c\u5728\u7a7a\u95f4\u4e2d\u540c\u6b65\u878d\u5408\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u533b\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7eaf\u89c6\u89c9\u4fe1\u606f\u6216\u540e\u5904\u7406\u635f\u5931\u7ea6\u675f\uff0c\u5728\u8fb9\u754c\u5212\u5206\u548c\u5c42\u7ea7\u4fdd\u6301\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u5e94\u5bf9\u80bf\u7624\u7ec4\u7ec7\u5f02\u8d28\u6027\u3001\u8fb9\u754c\u6a21\u7cca\u548cMRI\u5e8f\u5217\u95f4\u5bf9\u6bd4\u5ea6\u53d8\u5316\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faUMCF\u65b9\u6cd5\uff0c\u5728\u7edf\u4e003D\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u7a7a\u95f4\u4fe1\u606f\u7684\u540c\u6b65\u4ea4\u4e92\u878d\u5408\uff0c\u91c7\u7528\u65e0\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u95e8\u63a7\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u6001\u8d21\u732e\uff0c\u5e76\u5c06\u533b\u5b66\u5148\u9a8c\u77e5\u8bc6\u76f4\u63a5\u878d\u5165\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u907f\u514d\u4f20\u7edf\u5206\u79bb\u5f0f\u67b6\u6784\u3002", "result": "\u5728BraTS 2020\u548c2021\u6570\u636e\u96c6\u4e0a\uff0cUMCF+nnU-Net\u7684\u5e73\u5747Dice\u7cfb\u6570\u5206\u522b\u4e3a0.8579\u548c0.8977\uff0c\u5728\u4e3b\u6d41\u67b6\u6784\u4e0a\u5e73\u5747\u63d0\u53474.18%\u3002", "conclusion": "UMCF\u901a\u8fc7\u6df1\u5ea6\u878d\u5408\u4e34\u5e8a\u77e5\u8bc6\u4e0e\u5f71\u50cf\u7279\u5f81\uff0c\u4e3a\u7cbe\u51c6\u533b\u5b66\u4e2d\u7684\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.17522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17522", "abs": "https://arxiv.org/abs/2509.17522", "authors": ["Hangzhou He", "Lei Zhu", "Kaiwen Li", "Xinliang Zhang", "Jiakui Hu", "Ourui Fu", "Zhengjian Yao", "Yanye Lu"], "title": "Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models", "comment": null, "summary": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first\npredicting a set of human-understandable concepts and then mapping them to\nlabels through a simple classifier. While users can intervene in the concept\nspace to improve predictions, traditional CBMs typically employ a fixed linear\nclassifier over concept scores, which restricts interventions to manual value\nadjustments and prevents the incorporation of new concepts or domain knowledge\nat test time. These limitations are particularly severe in unsupervised CBMs,\nwhere concept activations are often noisy and densely activated, making user\ninterventions ineffective. We introduce Chat-CBM, which replaces score-based\nclassifiers with a language-based classifier that reasons directly over concept\nsemantics. By grounding prediction in the semantic space of concepts, Chat-CBM\npreserves the interpretability of CBMs while enabling richer and more intuitive\ninterventions, such as concept correction, addition or removal of concepts,\nincorporation of external knowledge, and high-level reasoning guidance.\nLeveraging the language understanding and few-shot capabilities of frozen large\nlanguage models, Chat-CBM extends the intervention interface of CBMs beyond\nnumerical editing and remains effective even in unsupervised settings.\nExperiments on nine datasets demonstrate that Chat-CBM achieves higher\npredictive performance and substantially improves user interactivity while\nmaintaining the concept-based interpretability of CBMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Chat-CBM\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7684\u5206\u7c7b\u5668\uff0c\u66ff\u4ee3\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4e2d\u7684\u5206\u6570\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u8bed\u4e49\u5c42\u9762\u7684\u6982\u5ff5\u63a8\u7406\u589e\u5f3a\u7528\u6237\u5e72\u9884\u80fd\u529b\uff0c\u652f\u6301\u6982\u5ff5\u4fee\u6b63\u3001\u589e\u5220\u3001\u5916\u90e8\u77e5\u8bc6\u878d\u5408\u7b49\u64cd\u4f5c\uff0c\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u4ea4\u4e92\u6027\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u4f7f\u7528\u56fa\u5b9a\u7684\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u9650\u5236\u4e86\u6d4b\u8bd5\u65f6\u7684\u7528\u6237\u5e72\u9884\u65b9\u5f0f\uff0c\u65e0\u6cd5\u7075\u6d3b\u5f15\u5165\u65b0\u6982\u5ff5\u6216\u9886\u57df\u77e5\u8bc6\uff0c\u5c24\u5176\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u56e0\u6982\u5ff5\u6fc0\u6d3b\u566a\u58f0\u5927\u800c\u96be\u4ee5\u6709\u6548\u5e72\u9884\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u76f4\u89c2\u7684\u5e72\u9884\u673a\u5236\u3002", "method": "\u63d0\u51faChat-CBM\uff0c\u7528\u57fa\u4e8e\u8bed\u8a00\u7684\u5206\u7c7b\u5668\u53d6\u4ee3\u57fa\u4e8e\u5206\u6570\u7684\u5206\u7c7b\u5668\uff0c\u76f4\u63a5\u5728\u6982\u5ff5\u8bed\u4e49\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\uff1b\u5229\u7528\u51bb\u7ed3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u548c\u5c11\u6837\u672c\u80fd\u529b\uff0c\u5b9e\u73b0\u5bf9\u6982\u5ff5\u8bed\u4e49\u7684\u81ea\u7136\u8bed\u8a00\u7ea7\u5e72\u9884\u3002", "result": "\u5728\u4e5d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cChat-CBM\u5728\u4fdd\u6301\u6982\u5ff5\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u4f20\u7edfCBMs\u5177\u6709\u66f4\u9ad8\u7684\u9884\u6d4b\u6027\u80fd\u548c\u66f4\u5f3a\u7684\u7528\u6237\u4ea4\u4e92\u80fd\u529b\uff0c\u4e14\u5728\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u4f9d\u7136\u6709\u6548\u3002", "conclusion": "Chat-CBM\u901a\u8fc7\u5c06\u5206\u7c7b\u8fc7\u7a0b\u4ece\u6570\u503c\u8bc4\u5206\u8f6c\u5411\u8bed\u4e49\u63a8\u7406\uff0c\u663e\u8457\u6269\u5c55\u4e86CBMs\u7684\u5e72\u9884\u63a5\u53e3\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u4ee5\u66f4\u81ea\u7136\u3001\u4e30\u5bcc\u7684\u65b9\u5f0f\u53c2\u4e0e\u6a21\u578b\u51b3\u7b56\uff0c\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.17537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17537", "abs": "https://arxiv.org/abs/2509.17537", "authors": ["Dian Jin", "Yanghao Zhou", "Jinxing Zhou", "Jiaqi Ma", "Ruohao Guo", "Dan Guo"], "title": "SimToken: A Simple Baseline for Referring Audio-Visual Segmentation", "comment": null, "summary": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific\nobjects in videos based on natural language expressions involving audio,\nvision, and text information. This task poses significant challenges in\ncross-modal reasoning and fine-grained object localization. In this paper, we\npropose a simple framework, SimToken, that integrates a multimodal large\nlanguage model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided\nto generate a special semantic token representing the referred object. This\ncompact token, enriched with contextual information from all modalities, acts\nas a prompt to guide SAM to segment objectsacross video frames. To further\nimprove semantic learning, we introduce a novel target-consistent semantic\nalignment loss that aligns token embeddings from different expressions but\nreferring to the same object. Experiments on the Ref-AVS benchmark demonstrate\nthat our approach achieves superior performance compared to existing\nmethods.Code will be available at https://github.com/DianJin-HFUT/SimToken", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSimToken\u7684\u7b80\u5355\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u548cSegment Anything Model\uff08SAM\uff09\uff0c\u901a\u8fc7\u751f\u6210\u4ee3\u8868\u76ee\u6807\u5bf9\u8c61\u7684\u8bed\u4e49token\u6765\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u89c6\u9891\u4e2d\u5bf9\u8c61\u5206\u5272\u3002", "motivation": "Referring Audio-Visual Segmentation (Ref-AVS) \u9762\u4e34\u8de8\u6a21\u6001\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u878d\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528MLLM\u751f\u6210\u5305\u542b\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u8bed\u4e49token\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u63d0\u793a\u8f93\u5165SAM\u8fdb\u884c\u89c6\u9891\u5e27\u7684\u5bf9\u8c61\u5206\u5272\uff1b\u5f15\u5165\u76ee\u6807\u4e00\u81f4\u7684\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\uff0c\u4f7f\u4e0d\u540c\u8868\u8fbe\u4f46\u6307\u5411\u540c\u4e00\u5bf9\u8c61\u7684token\u5d4c\u5165\u4fdd\u6301\u4e00\u81f4\uff0c\u4ee5\u589e\u5f3a\u8bed\u4e49\u5b66\u4e60\u3002", "result": "\u5728Ref-AVS\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SimToken\u901a\u8fc7\u7d27\u51d1\u7684\u8bed\u4e49token\u6709\u6548\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u5bf9\u8c61\u6307\u4ee3\u4e0e\u5206\u5272\uff0c\u7ed3\u5408\u8bed\u4e49\u5bf9\u9f50\u7b56\u7565\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2509.17561", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17561", "abs": "https://arxiv.org/abs/2509.17561", "authors": ["Edwine Nabahirwa", "Wei Song", "Minghua Zhang", "Shufan Chen"], "title": "An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection", "comment": "28 Pages, 12 Figures", "summary": "Underwater object detection (UOD) remains a critical challenge in computer\nvision due to underwater distortions which degrade low-level features and\ncompromise the reliability of even state-of-the-art detectors. While YOLO\nmodels have become the backbone of real-time object detection, little work has\nsystematically examined their robustness under these uniquely challenging\nconditions. This raises a critical question: Are YOLO models genuinely robust\nwhen operating under the chaotic and unpredictable conditions of underwater\nenvironments? In this study, we present one of the first comprehensive\nevaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated\nunderwater environments. Using a unified dataset of 10,000 annotated images\nfrom DUO and Roboflow100, we not only benchmark model robustness but also\nanalyze how distortions affect key low-level features such as texture, edges,\nand color. Our findings show that (1) YOLOv12 delivers the strongest overall\nperformance but is highly vulnerable to noise, and (2) noise disrupts edge and\ntexture features, explaining the poor detection performance in noisy images.\nClass imbalance is a persistent challenge in UOD. Experiments revealed that (3)\nimage counts and instance frequency primarily drive detection performance,\nwhile object appearance exerts only a secondary influence. Finally, we\nevaluated lightweight training-aware strategies: noise-aware sample injection,\nwhich improves robustness in both noisy and real-world conditions, and\nfine-tuning with advanced enhancement, which boosts accuracy in enhanced\ndomains but slightly lowers performance in original data, demonstrating strong\npotential for domain adaptation, respectively. Together, these insights provide\npractical guidance for building resilient and cost-efficient UOD systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86YOLOv8\u81f3YOLOv12\u5728\u516d\u79cd\u6a21\u62df\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0YOLOv12\u6574\u4f53\u6027\u80fd\u6700\u4f73\u4f46\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u566a\u58f0\u4f1a\u7834\u574f\u8fb9\u7f18\u548c\u7eb9\u7406\u7279\u5f81\uff0c\u5f71\u54cd\u68c0\u6d4b\u6548\u679c\u3002\u7c7b\u522b\u4e0d\u5e73\u8861\u662f\u4e3b\u8981\u6311\u6218\uff0c\u68c0\u6d4b\u6027\u80fd\u4e3b\u8981\u53d7\u56fe\u50cf\u6570\u91cf\u548c\u5b9e\u4f8b\u9891\u7387\u9a71\u52a8\u3002\u63d0\u51fa\u7684\u566a\u58f0\u611f\u77e5\u6837\u672c\u6ce8\u5165\u548c\u589e\u5f3a\u5fae\u8c03\u7b56\u7565\u53ef\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u7578\u53d8\u4f1a\u964d\u4f4e\u4f4e\u5c42\u7279\u5f81\u8d28\u91cf\uff0c\u5f71\u54cd\u73b0\u6709\u68c0\u6d4b\u5668\u7684\u53ef\u9760\u6027\uff0c\u800cYOLO\u7cfb\u5217\u6a21\u578b\u5728\u6b64\u7c7b\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u8bc4\u4f30\u5176\u5728\u6c34\u4e0b\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u5e76\u63a2\u7d22\u6709\u6548\u7684\u6539\u8fdb\u7b56\u7565\u3002", "method": "\u5728\u7edf\u4e00\u768410,000\u5f20\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\uff08DUO\u548cRoboflow100\uff09\u4e0a\uff0c\u5bf9YOLOv8-YOLOv12\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u5176\u5728\u516d\u79cd\u6a21\u62df\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u7578\u53d8\u5bf9\u7eb9\u7406\u3001\u8fb9\u7f18\u548c\u989c\u8272\u7b49\u4f4e\u5c42\u7279\u5f81\u7684\u5f71\u54cd\u3002\u540c\u65f6\u8bc4\u4f30\u566a\u58f0\u611f\u77e5\u6837\u672c\u6ce8\u5165\u548c\u57fa\u4e8e\u589e\u5f3a\u7684\u5fae\u8c03\u4e24\u79cd\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u7b56\u7565\u3002", "result": "1) YOLOv12\u6574\u4f53\u6027\u80fd\u6700\u5f3a\u4f46\u5bf9\u566a\u58f0\u654f\u611f\uff1b2) \u566a\u58f0\u663e\u8457\u7834\u574f\u8fb9\u7f18\u548c\u7eb9\u7406\u7279\u5f81\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\uff1b3) \u68c0\u6d4b\u6027\u80fd\u4e3b\u8981\u7531\u56fe\u50cf\u6570\u91cf\u548c\u5b9e\u4f8b\u9891\u7387\u51b3\u5b9a\uff0c\u7269\u4f53\u5916\u89c2\u5f71\u54cd\u8f83\u5c0f\uff1b4) \u566a\u58f0\u611f\u77e5\u6837\u672c\u6ce8\u5165\u63d0\u5347\u4e86\u5728\u566a\u58f0\u548c\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u589e\u5f3a\u5fae\u8c03\u63d0\u9ad8\u4e86\u589e\u5f3a\u57df\u7684\u7cbe\u5ea6\u4f46\u7565\u5fae\u964d\u4f4e\u539f\u59cb\u6570\u636e\u6027\u80fd\u3002", "conclusion": "YOLO\u6a21\u578b\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u5c24\u5176\u5bf9\u566a\u58f0\u654f\u611f\u3002\u901a\u8fc7\u5408\u7406\u7684\u6570\u636e\u7b56\u7565\u548c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u5176\u9c81\u68d2\u6027\u548c\u57df\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u6297\u5e72\u6270\u7684\u6c34\u4e0b\u76ee\u6807\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.17562", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17562", "abs": "https://arxiv.org/abs/2509.17562", "authors": ["Yuxuan Li", "Yicheng Zhang", "Wenhao Tang", "Yimian Dai", "Ming-Ming Cheng", "Xiang Li", "Jian Yang"], "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models", "comment": null, "summary": "Modern computer vision is converging on a closed loop in which perception,\nreasoning and generation mutually reinforce each other. However, this loop\nremains incomplete: the top-down influence of high-level reasoning on the\nfoundational learning of low-level perceptual features is not yet\nunderexplored. This paper addresses this gap by proposing a new paradigm for\npretraining foundation models in downstream domains. We introduce Visual\ninsTruction Pretraining (ViTP), a novel approach that directly leverages\nreasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)\nbackbone within a Vision-Language Model and pretrains it end-to-end using a\nrich corpus of visual instruction data curated from target downstream domains.\nViTP is powered by our proposed Visual Robustness Learning (VRL), which compels\nthe ViT to learn robust and domain-relevant features from a sparse set of\nvisual tokens. Extensive experiments on 16 challenging remote sensing and\nmedical imaging benchmarks demonstrate that ViTP establishes new\nstate-of-the-art performance across a diverse range of downstream tasks. The\ncode is available at github.com/zcablii/ViTP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u8303\u5f0f\u2014\u2014\u89c6\u89c9\u6307\u4ee4\u9884\u8bad\u7ec3\uff08ViTP\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u9636\u63a8\u7406\u4e0e\u4f4e\u5c42\u611f\u77e5\uff0c\u5229\u7528\u4e0b\u6e38\u9886\u57df\u7684\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u589e\u5f3a\u611f\u77e5\u80fd\u529b\uff0c\u5728\u9065\u611f\u548c\u533b\u5b66\u56fe\u50cf\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u7cfb\u7edf\u7684\u611f\u77e5\u3001\u63a8\u7406\u4e0e\u751f\u6210\u95ed\u73af\u4e2d\uff0c\u9ad8\u5c42\u63a8\u7406\u5bf9\u5e95\u5c42\u611f\u77e5\u7279\u5f81\u5b66\u4e60\u7684\u81ea\u4e0a\u800c\u4e0b\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u6307\u4ee4\u9884\u8bad\u7ec3\uff08ViTP\uff09\uff0c\u5c06Vision Transformer\u5d4c\u5165\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5e76\u4f7f\u7528\u6765\u81ea\u76ee\u6807\u4e0b\u6e38\u9886\u57df\u7684\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u8fdb\u884c\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\uff1b\u5f15\u5165\u89c6\u89c9\u9c81\u68d2\u6027\u5b66\u4e60\uff08VRL\uff09\uff0c\u4fc3\u4f7fViT\u4ece\u7a00\u758f\u89c6\u89c9token\u4e2d\u5b66\u4e60\u9c81\u68d2\u4e14\u9886\u57df\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u572816\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9065\u611f\u548c\u533b\u5b66\u56fe\u50cf\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cViTP\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "ViTP\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u9ad8\u9636\u63a8\u7406\u6307\u5bfc\u57fa\u7840\u611f\u77e5\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u9886\u57df\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17566", "abs": "https://arxiv.org/abs/2509.17566", "authors": ["Ding Shaodong", "Liu Ziyang", "Zhou Yijun", "Liu Tao"], "title": "MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data", "comment": "First-place solution of the classification track for MICCAI'2025\n  PDCADxFoundation Challenge", "summary": "The automatic diagnosis of Parkinson's disease is in high clinical demand due\nto its prevalence and the importance of targeted treatment. Current clinical\npractice often relies on diagnostic biomarkers in QSM and NM-MRI images.\nHowever, the lack of large, high-quality datasets makes training diagnostic\nmodels from scratch prone to overfitting. Adapting pre-trained 3D medical\nmodels is also challenging, as the diversity of medical imaging leads to\nmismatches in voxel spacing and modality between pre-training and fine-tuning\ndata. In this paper, we address these challenges by leveraging 2D vision\nfoundation models (VFMs). Specifically, we crop multiple key ROIs from NM and\nQSM images, process each ROI through separate branches to compress the ROI into\na token, and then combine these tokens into a unified patient representation\nfor classification. Within each branch, we use 2D VFMs to encode axial slices\nof the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary\nsegmentation head that steers the feature extraction toward specific brain\nnuclei. Additionally, we introduce multi-ROI supervised contrastive learning,\nwhich improves diagnostic performance by pulling together representations of\npatients from the same class while pushing away those from different classes.\nOur approach achieved first place in the MICCAI 2025 PDCADxFoundation\nchallenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled\nQSM and NM-MRI scans, outperforming the second-place method by 5.5%.These\nresults highlight the potential of 2D VFMs for clinical analysis of 3D MR\nimages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7684\u5e15\u91d1\u68ee\u75c5\u81ea\u52a8\u8bca\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u591aROI\u88c1\u526a\u3001\u5206\u652f\u7f16\u7801\u4e0e\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8686.0%\u7684\u51c6\u786e\u7387\uff0c\u4f4d\u5c45MICCAI 2025 PDCADxFoundation\u6311\u6218\u8d5b\u7b2c\u4e00\u3002", "motivation": "\u7531\u4e8e\u5e15\u91d1\u68ee\u75c5\u53d1\u75c5\u7387\u9ad8\u4e14\u9700\u7cbe\u51c6\u6cbb\u7597\uff0c\u4e34\u5e8a\u4e0a\u5bf9\u81ea\u52a8\u8bca\u65ad\u9700\u6c42\u5927\uff1b\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cfQSM\u548cNM-MRI\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u4ece\u5934\u8bad\u7ec3\u6613\u8fc7\u62df\u5408\uff0c\u800c\u9884\u8bad\u7ec33D\u533b\u5b66\u6a21\u578b\u53c8\u56e0\u6a21\u6001\u548c\u4f53\u7d20\u95f4\u8ddd\u5dee\u5f02\u96be\u4ee5\u8fc1\u79fb\u3002", "method": "\u4eceQSM\u548cNM-MRI\u56fe\u50cf\u4e2d\u88c1\u526a\u591a\u4e2a\u5173\u952eROI\uff0c\u6bcf\u4e2aROI\u901a\u8fc7\u72ec\u7acb\u5206\u652f\u5229\u75282D VFMs\u7f16\u7801\u8f74\u5411\u5207\u7247\uff0c\u5e76\u501f\u52a9\u8f85\u52a9\u5206\u5272\u5934\u5f15\u5bfc\u7279\u5f81\u63d0\u53d6\uff1b\u5404ROI\u538b\u7f29\u4e3atoken\u540e\u878d\u5408\u4e3a\u7edf\u4e00\u7684\u60a3\u8005\u8868\u5f81\u7528\u4e8e\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u591aROI\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7c7b\u5185\u805a\u7c7b\u548c\u7c7b\u95f4\u5206\u79bb\u3002", "result": "\u5728\u4ec5\u542b300\u4e2a\u6807\u6ce8\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\u8fbe\u523086.0%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u7b2c\u4e8c\u540d\u9ad8\u51fa5.5%\uff0c\u5728MICCAI 2025 PDCADxFoundation\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e862D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u57283D MRI\u4e34\u5e8a\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u533b\u5b66\u8bca\u65ad\u4efb\u52a1\u3002"}}
{"id": "2509.17581", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17581", "abs": "https://arxiv.org/abs/2509.17581", "authors": ["Florinel Alin Croitoru", "Vlad Hondru", "Radu Tudor Ionescu"], "title": "PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification", "comment": null, "summary": "We propose a novel benchmark for camera identification via Photo Response\nNon-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with\n120+ cameras, where the training and test photos are taken in different\nscenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel\nPRNU-based camera identification model that employs a hybrid architecture,\ncomprising a denoising autoencoder to estimate the PRNU signal and a\nconvolutional network that can perform 1:N verification of camera devices.\nInstead of using a conventional approach based on contrastive learning, our\nmethod takes the Hadamard product between reference and query PRNU signals as\ninput. This novel design leads to significantly better results compared with\nstate-of-the-art models based on denoising autoencoders and contrastive\nlearning. We release our dataset and code at:\nhttps://github.com/CroitoruAlin/PRNU-Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePRNU\u7684\u65b0\u578b\u76f8\u673a\u8bc6\u522b\u57fa\u51c6\u548c\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u67b6\u6784\u548cHadamard\u79ef\u8f93\u5165\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u76f8\u673a\u8bc6\u522b\u65b9\u6cd5\u5728\u8de8\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u8d34\u8fd1\u771f\u5b9e\u5e94\u7528\u7684\u2018in-the-wild\u2019\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b13K\u5f20\u56fe\u50cf\u3001\u6db5\u76d6120\u591a\u4e2a\u76f8\u673a\u7684\u65b0\u57fa\u51c6\uff1b\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u67b6\u6784\u6a21\u578b\uff0c\u7ed3\u5408\u53bb\u566a\u81ea\u7f16\u7801\u5668\u63d0\u53d6PRNU\u4fe1\u53f7\uff0c\u5e76\u4f7f\u7528\u5377\u79ef\u7f51\u7edc\u8fdb\u884c1:N\u9a8c\u8bc1\uff0c\u901a\u8fc7Hadamard\u79ef\u878d\u5408\u53c2\u8003\u4e0e\u67e5\u8be2PRNU\u4fe1\u53f7\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u76f8\u673a\u8bc6\u522b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u548c\u53bb\u566a\u81ea\u7f16\u7801\u5668\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86PRNU-based\u76f8\u673a\u8bc6\u522b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6027\u80fd\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2509.17588", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17588", "abs": "https://arxiv.org/abs/2509.17588", "authors": ["Jinyeong Kim", "Seil Kang", "Jiwoo Park", "Junhyeok Kim", "Seong Jae Hwang"], "title": "Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) answer visual questions by transferring\ninformation from images to text through a series of attention heads. While this\nimage-to-text information flow is central to visual question answering, its\nunderlying mechanism remains difficult to interpret due to the simultaneous\noperation of numerous attention heads. To address this challenge, we propose\nhead attribution, a technique inspired by component attribution methods, to\nidentify consistent patterns among attention heads that play a key role in\ninformation transfer. Using head attribution, we investigate how LVLMs rely on\nspecific attention heads to identify and answer questions about the main object\nin an image. Our analysis reveals that a distinct subset of attention heads\nfacilitates the image-to-text information flow. Remarkably, we find that the\nselection of these heads is governed by the semantic content of the input image\nrather than its visual appearance. We further examine the flow of information\nat the token level and discover that (1) text information first propagates to\nrole-related tokens and the final token before receiving image information, and\n(2) image information is embedded in both object-related and background tokens.\nOur work provides evidence that image-to-text information flow follows a\nstructured process, and that analysis at the attention-head level offers a\npromising direction toward understanding the mechanisms of LVLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5934\u5f52\u56e0\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u5934\u5728\u56fe\u50cf\u5230\u6587\u672c\u4fe1\u606f\u6d41\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u4f9d\u8d56\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u6765\u8bc6\u522b\u548c\u56de\u7b54\u5173\u4e8e\u56fe\u50cf\u4e3b\u8981\u5bf9\u8c61\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5927\u91cf\u6ce8\u610f\u529b\u5934\u7684\u540c\u65f6\u8fd0\u4f5c\uff0c\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u4ece\u56fe\u50cf\u5230\u6587\u672c\u7684\u4fe1\u606f\u6d41\u52a8\u673a\u5236\u96be\u4ee5\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc6\u522b\u5728\u6b64\u8fc7\u7a0b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u7684\u6ce8\u610f\u529b\u5934\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u5934\u5f52\u56e0\u6280\u672f\uff0c\u8be5\u6280\u672f\u53d7\u7ec4\u4ef6\u5f52\u56e0\u65b9\u6cd5\u542f\u53d1\uff0c\u7528\u4ee5\u8bc6\u522b\u5728\u4fe1\u606f\u4f20\u9012\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u7684\u6ce8\u610f\u529b\u5934\u7684\u4e00\u81f4\u6a21\u5f0f\uff0c\u5e76\u5728\u4ee4\u724c\u7ea7\u522b\u4e0a\u68c0\u67e5\u4fe1\u606f\u6d41\u52a8\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u7ec4\u72ec\u7279\u7684\u6ce8\u610f\u529b\u5934\u4fc3\u8fdb\u4e86\u56fe\u50cf\u5230\u6587\u672c\u7684\u4fe1\u606f\u6d41\uff0c\u4e14\u8fd9\u4e9b\u5934\u7684\u9009\u62e9\u7531\u8f93\u5165\u56fe\u50cf\u7684\u8bed\u4e49\u5185\u5bb9\u51b3\u5b9a\u800c\u975e\u5176\u89c6\u89c9\u5916\u89c2\uff1b\u540c\u65f6\u53d1\u73b0\u5728\u4ee4\u724c\u7ea7\u522b\u4e0a\uff0c\u6587\u672c\u4fe1\u606f\u5148\u4f20\u64ad\u5230\u4e0e\u89d2\u8272\u76f8\u5173\u7684\u4ee4\u724c\u548c\u6700\u540e\u4e00\u4e2a\u4ee4\u724c\uff0c\u7136\u540e\u624d\u63a5\u6536\u56fe\u50cf\u4fe1\u606f\uff0c\u800c\u56fe\u50cf\u4fe1\u606f\u5219\u5d4c\u5165\u5230\u4e86\u4e0e\u5bf9\u8c61\u76f8\u5173\u548c\u80cc\u666f\u7684\u4ee4\u724c\u4e2d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4ece\u56fe\u50cf\u5230\u6587\u672c\u7684\u4fe1\u606f\u6d41\u9075\u5faa\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u8fc7\u7a0b\uff0c\u5bf9\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u5206\u6790\u4e3a\u7406\u89e3\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2509.17593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17593", "abs": "https://arxiv.org/abs/2509.17593", "authors": ["Samet Hicsonmez", "Abd El Rahman Shabayek", "Arunkumar Rathinam", "Djamila Aouada"], "title": "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints", "comment": "Advanced Space Technologies in Robotics and Automation (ASTRA) 2025", "summary": "Object detection is essential in space applications targeting Space Domain\nAwareness and also applications involving relative navigation scenarios.\nCurrent deep learning models for Object Detection in space applications are\noften trained on synthetic data from simulators, however, the model performance\ndrops significantly on real-world data due to the domain gap. However, domain\nadaptive object detection is an overlooked problem in the community. In this\nwork, we first show the importance of domain adaptation and then explore\nSupervised Domain Adaptation (SDA) to reduce this gap using minimal labeled\nreal data. We build on a recent semi-supervised adaptation method and tailor it\nfor object detection. Our approach combines domain-invariant feature learning\nwith a CNN-based domain discriminator and invariant risk minimization using a\ndomain-independent regression head. To meet real-time deployment needs, we test\nour method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet\nbackbone and on the more advanced Fully Convolutional One-Stage object detector\n(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and\nSPARK. The results show up to 20-point improvements in average precision (AP)\nwith just 250 labeled real images.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u592a\u7a7a\u5e94\u7528\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u9886\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u548c\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\u7684\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5728\u4ec5\u4f7f\u7528250\u5f20\u6807\u6ce8\u771f\u5b9e\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u7cbe\u5ea6\uff08AP\uff09\u63d0\u5347\u4e86\u6700\u591a20\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u7531\u4e8e\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u9886\u57df\u81ea\u9002\u5e94\u95ee\u9898\u5728\u592a\u7a7a\u76ee\u6807\u68c0\u6d4b\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5173\u6ce8\u3002", "method": "\u57fa\u4e8e\u534a\u76d1\u7763\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u8fdb\u884c\u6539\u8fdb\uff0c\u7ed3\u5408CNN\u57df\u5224\u522b\u5668\u8fdb\u884c\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u57df\u72ec\u7acb\u56de\u5f52\u5934\u5b9e\u73b0\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\uff1b\u5728MobileNet-SSD\u548cResNet-FCOS\u4e24\u79cd\u8f7b\u91cf\u7ea7\u548c\u5148\u8fdb\u6a21\u578b\u4e0a\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u5728SPEED+\u548cSPARK\u4e24\u4e2a\u592a\u7a7a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u7528250\u5f20\u6807\u6ce8\u771f\u5b9e\u56fe\u50cf\uff0c\u5e73\u5747\u7cbe\u5ea6\uff08AP\uff09\u6700\u9ad8\u63d0\u534720\u70b9\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u9886\u57df\u5dee\u8ddd\u3002", "conclusion": "\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u80fd\u6709\u6548\u63d0\u5347\u592a\u7a7a\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u5907\u5b9e\u65f6\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2509.17598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17598", "abs": "https://arxiv.org/abs/2509.17598", "authors": ["Aiming Zhang", "Tianyuan Yu", "Liang Bai", "Jun Tang", "Yanming Guo", "Yirun Ruan", "Yun Zhou", "Zhihe Lu"], "title": "COLA: Context-aware Language-driven Test-time Adaptation", "comment": null, "summary": "Test-time adaptation (TTA) has gained increasing popularity due to its\nefficacy in addressing ``distribution shift'' issue while simultaneously\nprotecting data privacy.\n  However, most prior methods assume that a paired source domain model and\ntarget domain sharing the same label space coexist, heavily limiting their\napplicability.\n  In this paper, we investigate a more general source model capable of\nadaptation to multiple target domains without needing shared labels.\n  This is achieved by using a pre-trained vision-language model (VLM), \\egno,\nCLIP, that can recognize images through matching with class descriptions.\n  While the zero-shot performance of VLMs is impressive, they struggle to\neffectively capture the distinctive attributes of a target domain.\n  To that end, we propose a novel method -- Context-aware Language-driven TTA\n(COLA).\n  The proposed method incorporates a lightweight context-aware module that\nconsists of three key components: a task-aware adapter, a context-aware unit,\nand a residual connection unit for exploring task-specific knowledge,\ndomain-specific knowledge from the VLM and prior knowledge of the VLM,\nrespectively.\n  It is worth noting that the context-aware module can be seamlessly integrated\ninto a frozen VLM, ensuring both minimal effort and parameter efficiency.\n  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy\nto mitigate the adverse effects caused by class imbalance.\n  We demonstrate the effectiveness of our method not only in TTA scenarios but\nalso in class generalisation tasks.\n  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8bed\u8a00\u9a71\u52a8\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\uff08COLA\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\u548c\u7c7b\u522b\u5e73\u8861\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u65e0\u5171\u4eab\u6807\u7b7e\u60c5\u51b5\u4e0b\u5bf9\u591a\u4e2a\u76ee\u6807\u57df\u7684\u6709\u6548\u81ea\u9002\u5e94\uff0c\u5177\u6709\u53c2\u6570\u9ad8\u6548\u548c\u6613\u4e8e\u96c6\u6210\u7684\u4f18\u70b9\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6e90\u57df\u548c\u76ee\u6807\u57df\u5171\u4eab\u76f8\u540c\u6807\u7b7e\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u66f4\u4e00\u822c\u7684\u573a\u666f\uff1a\u4f7f\u7528\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u591a\u4e2a\u65e0\u6807\u7b7e\u5bf9\u5e94\u5173\u7cfb\u7684\u76ee\u6807\u57df\u3002", "method": "\u63d0\u51faContext-aware Language-driven TTA\uff08COLA\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\uff0c\u7531\u4efb\u52a1\u611f\u77e5\u9002\u914d\u5668\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u5355\u5143\u548c\u6b8b\u5dee\u8fde\u63a5\u5355\u5143\u7ec4\u6210\uff0c\u5e76\u7ed3\u5408\u7c7b\u522b\u5e73\u8861\u4f2a\u6807\u7b7e\uff08CBPL\uff09\u7b56\u7565\uff0c\u5728\u51bb\u7ed3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCOLA\u5728\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u548c\u7c7b\u522b\u6cdb\u5316\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u57df\u504f\u79fb\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "COLA\u80fd\u591f\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8de8\u57df\u3001\u8de8\u7c7b\u522b\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff0c\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3001\u53c2\u6570\u6548\u7387\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2509.17602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17602", "abs": "https://arxiv.org/abs/2509.17602", "authors": ["Giulio Martellucci", "Herve Goeau", "Pierre Bonnet", "Fabrice Vinatier", "Alexis Joly"], "title": "Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images", "comment": "13 pages, 4 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum, September 09 to 12, 2024, Madrid, Spain", "summary": "Quadrat images are essential for ecological studies, as they enable\nstandardized sampling, the assessment of plant biodiversity, long-term\nmonitoring, and large-scale field campaigns. These images typically cover an\narea of fifty centimetres or one square meter, and botanists carefully identify\nall the species present. Integrating AI could help specialists accelerate their\ninventories and expand the spatial coverage of ecological studies. To assess\nprogress in this area, the PlantCLEF 2025 challenge relies on a new test set of\n2,105 high-resolution multi-label images annotated by experts and covering\naround 400 species. It also provides a large training set of 1.4 million\nindividual plant images, along with vision transformer models pre-trained on\nthis data. The task is formulated as a (weakly labelled) multi-label\nclassification problem, where the goal is to predict all species present in a\nquadrat image using single-label training data. This paper provides a detailed\ndescription of the data, the evaluation methodology, the methods and models\nused by participants, and the results achieved.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PlantCLEF 2025\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u52a0\u901f\u690d\u7269\u591a\u6837\u6027\u8c03\u67e5\uff0c\u4f7f\u75282,105\u5f20\u4e13\u5bb6\u6807\u6ce8\u7684\u9ad8\u5206\u8fa8\u7387\u591a\u6807\u7b7e\u6837\u65b9\u56fe\u50cf\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u5e76\u63d0\u4f9b140\u4e07\u5f20\u5355\u6807\u7b7e\u8bad\u7ec3\u56fe\u50cf\u548c\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u4efb\u52a1\u4e3a\u57fa\u4e8e\u5355\u6807\u7b7e\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "motivation": "\u5229\u7528AI\u6280\u672f\u63d0\u5347\u751f\u6001\u5b66\u5bb6\u5728\u690d\u7269\u7269\u79cd\u8bc6\u522b\u4e2d\u7684\u6548\u7387\uff0c\u6269\u5927\u751f\u6001\u7814\u7a76\u7684\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u3002", "method": "\u91c7\u7528\u5f31\u6807\u6ce8\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u5355\u6807\u7b7e\u56fe\u50cf\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u7684\u89c6\u89c9Transformer\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u4e0e\u9884\u6d4b\u3002", "result": "\u63d0\u4f9b\u4e86\u65b0\u7684\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u8d44\u6e90\uff0c\u591a\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86\u6709\u6548\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u5728\u6837\u65b9\u56fe\u50cf\u4e2d\u81ea\u52a8\u8bc6\u522b\u591a\u79cd\u690d\u7269\u7269\u79cd\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4e3a\u690d\u7269\u56fe\u50cf\u8bc6\u522b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u548c\u8d44\u6e90\uff0c\u5c55\u793a\u4e86AI\u5728\u751f\u6001\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.17615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17615", "abs": "https://arxiv.org/abs/2509.17615", "authors": ["Lars Heckler-Kram", "Ashwin Vaidya", "Jan-Hendrik Neudeck", "Ulla Scheler", "Dick Ameln", "Samet Akcay", "Paula Ramos"], "title": "From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge", "comment": null, "summary": "Visual anomaly detection is a strongly application-driven field of research.\nConsequently, the connection between academia and industry is of paramount\nimportance. In this regard, we present the VAND 3.0 Challenge to showcase\ncurrent progress in anomaly detection across different practical settings\nwhilst addressing critical issues in the field. The challenge hosted two\ntracks, fostering the development of anomaly detection methods robust against\nreal-world distribution shifts (Category 1) and exploring the capabilities of\nVision Language Models within the few-shot regime (Category 2), respectively.\nThe participants' solutions reached significant improvements over previous\nbaselines by combining or adapting existing approaches and fusing them with\nnovel pipelines. While for both tracks the progress in large pre-trained vision\n(language) backbones played a pivotal role for the performance increase,\nscaling up anomaly detection methods more efficiently needs to be addressed by\nfuture research to meet real-time and computational constraints on-site.", "AI": {"tldr": "VAND 3.0 Challenge \u63a8\u52a8\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u5206\u5e03\u504f\u79fb\u9c81\u68d2\u6027\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u5e94\u7528\u3002", "motivation": "\u52a0\u5f3a\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u89e3\u51b3\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u548c\u5173\u952e\u6311\u6218\u3002", "method": "\u8bbe\u7acb\u4e24\u4e2a\u8d5b\u9053\uff1a\u4e00\u662f\u63d0\u5347\u65b9\u6cd5\u5bf9\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\uff1b\u4e8c\u662f\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6f5c\u529b\u3002", "result": "\u53c2\u8d5b\u65b9\u6848\u901a\u8fc7\u7ed3\u5408\u6216\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u5e76\u5f15\u5165\u65b0\u6d41\u7a0b\uff0c\u5728\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8d8a\u5148\u524d\u57fa\u7ebf\uff0c\u5927\u6a21\u578b\u9aa8\u5e72\u7f51\u7edc\u8d77\u5230\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u5c55\uff0c\u672a\u6765\u7814\u7a76\u4ecd\u9700\u66f4\u9ad8\u6548\u5730\u6269\u5c55\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u6ee1\u8db3\u73b0\u573a\u5b9e\u65f6\u6027\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u3002"}}
{"id": "2509.17620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17620", "abs": "https://arxiv.org/abs/2509.17620", "authors": ["Gregory Schroeder", "Mohamed Sabry", "Cristina Olaverri-Monreal"], "title": "Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method", "comment": null, "summary": "Estimating camera intrinsic parameters without prior scene knowledge is a\nfundamental challenge in computer vision. This capability is particularly\nimportant for applications such as autonomous driving and vehicle platooning,\nwhere precalibrated setups are impractical and real-time adaptability is\nnecessary. To advance the state-of-the-art, we present a set of equations based\non the calibrated trifocal tensor, enabling projective camera self-calibration\nfrom minimal image data. Our method, termed TrifocalCalib, significantly\nimproves accuracy and robustness compared to both recent learning-based and\nclassical approaches. Unlike many existing techniques, our approach requires no\ncalibration target, imposes no constraints on camera motion, and simultaneously\nestimates both focal length and principal point. Evaluations in both\nprocedurally generated synthetic environments and structured dataset-based\nscenarios demonstrate the effectiveness of our approach. To support\nreproducibility, we make the code publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u7126\u70b9\u5f20\u91cf\u7684\u76f8\u673a\u81ea\u6807\u5b9a\u65b9\u6cd5TrifocalCalib\uff0c\u65e0\u9700\u6807\u5b9a\u76ee\u6807\u6216\u7279\u5b9a\u8fd0\u52a8\u7ea6\u675f\uff0c\u53ef\u540c\u65f6\u4f30\u8ba1\u7126\u8ddd\u548c\u4e3b\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u7f3a\u4e4f\u5148\u9a8c\u573a\u666f\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u5c24\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u6027\u7684\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u6821\u51c6\u540e\u7684\u4e09\u7126\u70b9\u5f20\u91cf\u63a8\u5bfc\u51fa\u4e00\u7ec4\u65b9\u7a0b\uff0c\u5229\u7528\u6700\u5c11\u7684\u56fe\u50cf\u6570\u636e\u5b9e\u73b0\u6295\u5f71\u76f8\u673a\u7684\u81ea\u6807\u5b9a\uff0c\u65e0\u9700\u6807\u5b9a\u76ee\u6807\uff0c\u4e0d\u9650\u5236\u76f8\u673a\u8fd0\u52a8\u3002", "result": "\u5728\u5408\u6210\u73af\u5883\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u548c\u4f20\u7edf\u65b9\u6cd5\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u540c\u65f6\u4f30\u8ba1\u7126\u8ddd\u548c\u4e3b\u70b9\u3002", "conclusion": "TrifocalCalib\u4e3a\u65e0\u6807\u5b9a\u76ee\u6807\u7684\u76f8\u673a\u81ea\u6807\u5b9a\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u81ea\u6807\u5b9a\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.17622", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17622", "abs": "https://arxiv.org/abs/2509.17622", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale", "comment": "10 pages, 1 figure, CLEF 2023 Conference and Labs of the Evaluation\n  Forum, September 18 to 21, 2023, Thessaloniki, Greece", "summary": "The world is estimated to be home to over 300,000 species of vascular plants.\nIn the face of the ongoing biodiversity crisis, expanding our understanding of\nthese species is crucial for the advancement of human civilization,\nencompassing areas such as agriculture, construction, and pharmacopoeia.\nHowever, the labor-intensive process of plant identification undertaken by\nhuman experts poses a significant obstacle to the accumulation of new data and\nknowledge. Fortunately, recent advancements in automatic identification,\nparticularly through the application of deep learning techniques, have shown\npromising progress. Despite challenges posed by data-related issues such as a\nvast number of classes, imbalanced class distribution, erroneous\nidentifications, duplications, variable visual quality, and diverse visual\ncontents (such as photos or herbarium sheets), deep learning approaches have\nreached a level of maturity which gives us hope that in the near future we will\nhave an identification system capable of accurately identifying all plant\nspecies worldwide. The PlantCLEF2023 challenge aims to contribute to this\npursuit by addressing a multi-image (and metadata) classification problem\ninvolving an extensive set of classes (80,000 plant species). This paper\nprovides an overview of the challenge's resources and evaluations, summarizes\nthe methods and systems employed by participating research groups, and presents\nan analysis of key findings.", "AI": {"tldr": "PlantCLEF2023\u6311\u6218\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5b9e\u73b0\u5168\u74038\u4e07\u79cd\u690d\u7269\u7684\u591a\u56fe\u50cf\u5206\u7c7b\u8bc6\u522b\uff0c\u63a8\u52a8\u81ea\u52a8\u690d\u7269\u8bc6\u522b\u7684\u53d1\u5c55\u3002", "motivation": "\u7531\u4e8e\u690d\u7269\u79cd\u7c7b\u7e41\u591a\u4e14\u4f20\u7edf\u4eba\u5de5\u9274\u5b9a\u8017\u65f6\u8d39\u529b\uff0c\u4e9f\u9700\u53d1\u5c55\u81ea\u52a8\u8bc6\u522b\u6280\u672f\u4ee5\u52a0\u901f\u690d\u7269\u591a\u6837\u6027\u6570\u636e\u7684\u79ef\u7d2f\u548c\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7ec4\u7ec7PlantCLEF2023\u6311\u6218\u8d5b\uff0c\u63d0\u4f9b\u5927\u89c4\u6a21\u591a\u56fe\u50cf\u4e0e\u5143\u6570\u636e\u5206\u7c7b\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e0d\u540c\u7814\u7a76\u56e2\u961f\u63d0\u4ea4\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u3002", "result": "\u591a\u4e2a\u53c2\u8d5b\u56e2\u961f\u572880,000\u7c7b\u690d\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u9a8c\u8bc1\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5904\u7406\u590d\u6742\u3001\u5927\u89c4\u6a21\u690d\u7269\u8bc6\u522b\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5df2\u8d8b\u4e8e\u6210\u719f\uff0c\u6709\u671b\u5728\u672a\u6765\u5b9e\u73b0\u5168\u7403\u6240\u6709\u7ef4\u7ba1\u690d\u7269\u7269\u79cd\u7684\u51c6\u786e\u81ea\u52a8\u8bc6\u522b\u3002"}}
{"id": "2509.17627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17627", "abs": "https://arxiv.org/abs/2509.17627", "authors": ["Jinshu Chen", "Xinghui Li", "Xu Bai", "Tianxiang Ma", "Pengze Zhang", "Zhuowei Chen", "Gen Li", "Lijie Liu", "Songtao Zhao", "Bingchuan Li", "Qian He"], "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models", "comment": "Github Page: https://phantom-video.github.io/OmniInsert/", "summary": "Recent advances in video insertion based on diffusion models are impressive.\nHowever, existing methods rely on complex control signals but struggle with\nsubject consistency, limiting their practical applicability. In this paper, we\nfocus on the task of Mask-free Video Insertion and aim to resolve three key\nchallenges: data scarcity, subject-scene equilibrium, and insertion\nharmonization. To address the data scarcity, we propose a new data pipeline\nInsertPipe, constructing diverse cross-pair data automatically. Building upon\nour data pipeline, we develop OmniInsert, a novel unified framework for\nmask-free video insertion from both single and multiple subject references.\nSpecifically, to maintain subject-scene equilibrium, we introduce a simple yet\neffective Condition-Specific Feature Injection mechanism to distinctly inject\nmulti-source conditions and propose a novel Progressive Training strategy that\nenables the model to balance feature injection from subjects and source video.\nMeanwhile, we design the Subject-Focused Loss to improve the detailed\nappearance of the subjects. To further enhance insertion harmonization, we\npropose an Insertive Preference Optimization methodology to optimize the model\nby simulating human preferences, and incorporate a Context-Aware Rephraser\nmodule during reference to seamlessly integrate the subject into the original\nscenes. To address the lack of a benchmark for the field, we introduce\nInsertBench, a comprehensive benchmark comprising diverse scenes with\nmeticulously selected subjects. Evaluation on InsertBench indicates OmniInsert\noutperforms state-of-the-art closed-source commercial solutions. The code will\nbe released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmniInsert\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u63a9\u7801\u89c6\u9891\u63d2\u5165\uff0c\u901a\u8fc7\u65b0\u6784\u5efa\u7684\u6570\u636e\u7ba1\u9053InsertPipe\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u7279\u5b9a\u7279\u5f81\u6ce8\u5165\u3001\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u4e3b\u4f53\u805a\u7126\u635f\u5931\u6765\u4fdd\u6301\u4e3b\u4f53\u4e0e\u573a\u666f\u7684\u5e73\u8861\u5e76\u63d0\u5347\u63d2\u5165\u6548\u679c\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u63d2\u5165\u504f\u597d\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u91cd\u5199\u6a21\u5757\u4ee5\u589e\u5f3a\u878d\u5408\u6548\u679c\uff0c\u5e76\u6784\u5efa\u4e86\u8bc4\u6d4b\u57fa\u51c6InsertBench\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5546\u4e1a\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u63d2\u5165\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u63a7\u5236\u4fe1\u53f7\u4e14\u96be\u4ee5\u4fdd\u6301\u4e3b\u4f53\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u65e0\u63a9\u7801\u89c6\u9891\u63d2\u5165\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u4e3b\u4f53-\u573a\u666f\u5e73\u8861\u548c\u63d2\u5165\u878d\u5408\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faInsertPipe\u6570\u636e\u7ba1\u9053\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u8de8\u5bf9\u6570\u636e\uff1b\u8bbe\u8ba1OmniInsert\u7edf\u4e00\u6846\u67b6\uff0c\u5f15\u5165\u6761\u4ef6\u7279\u5b9a\u7279\u5f81\u6ce8\u5165\u673a\u5236\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u4ee5\u5e73\u8861\u591a\u6e90\u6761\u4ef6\u6ce8\u5165\uff1b\u91c7\u7528\u4e3b\u4f53\u805a\u7126\u635f\u5931\u4f18\u5316\u4e3b\u4f53\u7ec6\u8282\u8868\u73b0\uff1b\u901a\u8fc7\u63d2\u5165\u504f\u597d\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u91cd\u5199\u6a21\u5757\u63d0\u5347\u63d2\u5165\u548c\u8c10\u6027\u3002", "result": "\u5728\u65b0\u6784\u5efa\u7684\u57fa\u51c6InsertBench\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cOmniInsert\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "OmniInsert\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u63a9\u7801\u89c6\u9891\u63d2\u5165\u4e2d\u7684\u5173\u952e\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u4f53\u4e00\u81f4\u6027\u4e0e\u573a\u666f\u878d\u5408\u6548\u679c\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2509.17632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17632", "abs": "https://arxiv.org/abs/2509.17632", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2022: Image-based plant identification at global scale", "comment": "13 pages, 2 figures, CLEF 2022 Conference and Labs of the Evaluation\n  Forum, September 05 to 08, 2022, Bologna, Italy", "summary": "It is estimated that there are more than 300,000 species of vascular plants\nin the world. Increasing our knowledge of these species is of paramount\nimportance for the development of human civilization (agriculture,\nconstruction, pharmacopoeia, etc.), especially in the context of the\nbiodiversity crisis. However, the burden of systematic plant identification by\nhuman experts strongly penalizes the aggregation of new data and knowledge.\nSince then, automatic identification has made considerable progress in recent\nyears as highlighted during all previous editions of PlantCLEF. Deep learning\ntechniques now seem mature enough to address the ultimate but realistic problem\nof global identification of plant biodiversity in spite of many problems that\nthe data may present (a huge number of classes, very strongly unbalanced\nclasses, partially erroneous identifications, duplications, variable visual\nquality, diversity of visual contents such as photos or herbarium sheets, etc).\nThe PlantCLEF2022 challenge edition proposes to take a step in this direction\nby tackling a multi-image (and metadata) classification problem with a very\nlarge number of classes (80k plant species). This paper presents the resources\nand evaluations of the challenge, summarizes the approaches and systems\nemployed by the participating research groups, and provides an analysis of key\nfindings.", "AI": {"tldr": "PlantCLEF2022\u6311\u6218\u8d5b\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5b9e\u73b0\u5168\u7403\u690d\u7269\u751f\u7269\u591a\u6837\u6027\u7684\u591a\u56fe\u50cf\u4e0e\u5143\u6570\u636e\u5206\u7c7b\uff0c\u6db5\u76d68\u4e07\u79cd\u690d\u7269\u7269\u79cd\u3002", "motivation": "\u7531\u4e8e\u690d\u7269\u79cd\u7c7b\u7e41\u591a\u4e14\u9762\u4e34\u751f\u7269\u591a\u6837\u6027\u5371\u673a\uff0c\u4f20\u7edf\u7684\u4eba\u5de5\u8bc6\u522b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u690d\u7269\u5b66\u6570\u636e\u7684\u79ef\u7d2f\u548c\u77e5\u8bc6\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u7684\u690d\u7269\u8bc6\u522b\u6280\u672f\u3002", "method": "\u901a\u8fc7\u7ec4\u7ec7PlantCLEF2022\u6311\u6218\u8d5b\uff0c\u63d0\u4f9b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u9f13\u52b1\u7814\u7a76\u56e2\u961f\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u7b49\u6280\u672f\u8fdb\u884c\u591a\u56fe\u50cf\u548c\u5143\u6570\u636e\u7684\u690d\u7269\u5206\u7c7b\u3002", "result": "\u6311\u6218\u8d5b\u63a8\u52a8\u4e86\u690d\u7269\u81ea\u52a8\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u591a\u4e2a\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7cfb\u7edf\u548c\u65b9\u6cd5\uff0c\u5728\u5904\u7406\u5927\u91cf\u7c7b\u522b\u3001\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8d28\u91cf\u95ee\u9898\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u5e94\u5bf9\u5927\u89c4\u6a21\u690d\u7269\u7269\u79cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0cPlantCLEF2022\u4e3a\u63a8\u52a8\u5168\u7403\u690d\u7269\u751f\u7269\u591a\u6837\u6027\u81ea\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u91cd\u8981\u5e73\u53f0\u3002"}}
{"id": "2509.17638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17638", "abs": "https://arxiv.org/abs/2509.17638", "authors": ["Zilin Gao", "Qilong Wang", "Bingbing Zhang", "Qinghua Hu", "Peihua Li"], "title": "A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition", "comment": "27 pages, 13 figures, 7 tables", "summary": "Thanks to capability to alleviate the cost of large-scale annotation,\nfew-shot action recognition (FSAR) has attracted increased attention of\nresearchers in recent years. Existing FSAR approaches typically neglect the\nrole of individual motion pattern in comparison, and under-explore the feature\nstatistics for video dynamics. Thereby, they struggle to handle the challenging\ntemporal misalignment in video dynamics, particularly by using 2D backbones. To\novercome these limitations, this work proposes an adaptively aligned\nmulti-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the\nlatent video dynamics with a collection of powerful representation candidates\nand adaptively align them in an instance-guided manner. To this end, our\nA$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$\nmodule) for matching, and multi-scale second-order moment (M$^2$ block) for\nstrong representation. Specifically, M$^2$ block develops a collection of\nsemantic second-order descriptors at multiple spatio-temporal scales.\nFurthermore, A$^2$ module aims to adaptively select informative candidate\ndescriptors while considering the individual motion pattern. By such means, our\nA$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem\nby establishing an adaptive alignment protocol for strong representation.\nNotably, our proposed method generalizes well to various few-shot settings and\ndiverse metrics. The experiments are conducted on five widely used FSAR\nbenchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive\nperformance compared to state-of-the-arts, demonstrating its effectiveness and\ngeneralization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA\u00b2M\u00b2-Net\u7684\u81ea\u9002\u5e94\u5bf9\u9f50\u591a\u5c3a\u5ea6\u4e8c\u9636\u77e9\u7f51\u7edc\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u9f50\u548c\u591a\u5c3a\u5ea6\u4e8c\u9636\u77e9\u6a21\u5757\u6709\u6548\u5904\u7406\u89c6\u9891\u52a8\u6001\u4e2d\u7684\u65f6\u95f4\u9519\u4f4d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u5ffd\u89c6\u4e2a\u4f53\u8fd0\u52a8\u6a21\u5f0f\u7684\u6bd4\u8f83\u4f5c\u7528\uff0c\u5e76\u672a\u5145\u5206\u6316\u6398\u89c6\u9891\u52a8\u6001\u7684\u7279\u5f81\u7edf\u8ba1\u4fe1\u606f\uff0c\u96be\u4ee5\u5e94\u5bf9\u65f6\u95f4\u9519\u4f4d\u6311\u6218\uff0c\u5c24\u5176\u5728\u4f7f\u75282D\u4e3b\u5e72\u7f51\u7edc\u65f6\u8868\u73b0\u66f4\u5dee\u3002", "method": "\u8bbe\u8ba1\u4e86A\u00b2M\u00b2-Net\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u5bf9\u9f50\u6a21\u5757\uff08A\u00b2\u6a21\u5757\uff09\u548c\u591a\u5c3a\u5ea6\u4e8c\u9636\u77e9\u6a21\u5757\uff08M\u00b2\u6a21\u5757\uff09\u3002M\u00b2\u6a21\u5757\u5728\u591a\u4e2a\u65f6\u7a7a\u5c3a\u5ea6\u4e0a\u6784\u5efa\u8bed\u4e49\u4e8c\u9636\u63cf\u8ff0\u7b26\uff0cA\u00b2\u6a21\u5757\u6839\u636e\u5b9e\u4f8b\u5f15\u5bfc\u81ea\u9002\u5e94\u9009\u62e9\u6709\u4fe1\u606f\u91cf\u7684\u63cf\u8ff0\u7b26\uff0c\u5b9e\u73b0\u52a8\u6001\u5bf9\u9f50\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "A\u00b2M\u00b2-Net\u80fd\u6709\u6548\u5e94\u5bf9\u89c6\u9891\u65f6\u95f4\u9519\u4f4d\u95ee\u9898\uff0c\u5728\u591a\u79cd\u5c11\u6837\u672c\u8bbe\u7f6e\u548c\u5ea6\u91cf\u65b9\u5f0f\u4e0b\u5747\u8868\u73b0\u51fa\u5f3a\u7ade\u4e89\u529b\u548c\u826f\u597d\u6cdb\u5316\u6027\u3002"}}
{"id": "2509.17650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17650", "abs": "https://arxiv.org/abs/2509.17650", "authors": ["Soroush Mahdi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers", "comment": null, "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5728\u63a8\u7406\u65f6\u8fdb\u884ctoken\u5254\u9664\u7684\u7b56\u7565\uff0c\u6709\u6548\u9650\u5236\u4e86\u6d41\u5f0f\u89c6\u89c9Transformer\u4e2d\u7684KV\u5185\u5b58\u589e\u957f\uff0c\u5728\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\u51e0\u4e4e\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "motivation": "\u6d41\u5f0f\u89c6\u89c9Transformer\uff08\u5982StreamVGGT\uff09\u57283D\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u952e\u503c\uff08KV\uff09\u5185\u5b58\u968f\u5e8f\u5217\u589e\u957f\u800c\u65e0\u754c\u589e\u52a0\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bad\u7ec3-free\u7684\u63a8\u7406\u65f6token\u5254\u9664\u7b56\u7565\uff0c\u901a\u8fc7\u4e22\u5f03\u5197\u4f59token\u5e76\u4fdd\u7559\u4fe1\u606f\u91cf\u6700\u5927\u7684token\u6765\u5b9e\u73b0\u5185\u5b58\u4f7f\u7528\u4e0a\u9650\u7684\u63a7\u5236\u3002", "result": "\u57287-Scenes\u7b49\u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\uff0c\u5cf0\u503c\u5185\u5b58\u4ece18.63GB\u964d\u81f39.39GB\uff0c\u7cbe\u5ea6\u548c\u5b8c\u6574\u6027\u4ec5\u4e0b\u964d0.003\uff1b\u5728\u4e25\u683c\u5185\u5b58\u9884\u7b97\u4e0b\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u66f4\u5bc6\u96c6\u7684\u5e27\u91c7\u6837\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u6781\u4f4e\u5185\u5b58\u6d88\u8017\u4e0b\u63a5\u8fd1StreamVGGT\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u6d41\u5f0f\u63a8\u7406\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.17651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17651", "abs": "https://arxiv.org/abs/2509.17651", "authors": ["Filippo Botti", "Alex Ergasti", "Tomaso Fontanini", "Claudio Ferrari", "Massimo Bertozzi", "Andrea Prati"], "title": "SISMA: Semantic Face Image Synthesis with Mamba", "comment": null, "summary": "Diffusion Models have become very popular for Semantic Image Synthesis (SIS)\nof human faces. Nevertheless, their training and inference is computationally\nexpensive and their computational requirements are high due to the quadratic\ncomplexity of attention layers. In this paper, we propose a novel architecture\ncalled SISMA, based on the recently proposed Mamba. SISMA generates high\nquality samples by controlling their shape using a semantic mask at a reduced\ncomputational demand. We validated our approach through comprehensive\nexperiments with CelebAMask-HQ, revealing that our architecture not only\nachieves a better FID score yet also operates at three times the speed of\nstate-of-the-art architectures. This indicates that the proposed design is a\nviable, lightweight substitute to transformer-based models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMamba\u7684SISMA\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u4eba\u8138\u8bed\u4e49\u56fe\u50cf\u5408\u6210\uff0c\u5728CelebAMask-HQ\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18FID\u548c\u4e09\u500d\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u4eba\u8138\u8bed\u4e49\u56fe\u50cf\u5408\u6210\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u6548\u7387\u3002", "method": "\u91c7\u7528Mamba\u67b6\u6784\u8bbe\u8ba1\u65b0\u578bSISMA\u6a21\u578b\uff0c\u5229\u7528\u8bed\u4e49\u63a9\u7801\u63a7\u5236\u751f\u6210\u5f62\u72b6\uff0c\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5728CelebAMask-HQ\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSISMA\u4e0d\u4ec5FID\u5f97\u5206\u66f4\u597d\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6a21\u578b\u7684\u4e09\u500d\u3002", "conclusion": "SISMA\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u8f7b\u91cf\u5316\u7684Transformer\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9ad8\u6548\u8bed\u4e49\u56fe\u50cf\u5408\u6210\u3002"}}
{"id": "2509.17654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17654", "abs": "https://arxiv.org/abs/2509.17654", "authors": ["Sehyun Kim", "Hye Jun Lee", "Jiwoo Lee", "Taemin Lee"], "title": "Clothing agnostic Pre-inpainting Virtual Try-ON", "comment": null, "summary": "With the development of deep learning technology, virtual try-on technology\nhas become an important application value in the fields of e-commerce, fashion,\nand entertainment. The recently proposed Leffa has improved the texture\ndistortion problem of diffu-sion-based models, but there are limitations in\nthat the bottom detection inaccuracy and the existing clothing silhouette\nremain in the synthesis results. To solve this problem, this study proposes\nCaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has\nimproved the naturalness and consistency of whole-body clothing syn-thesis by\nintegrating multi-category masking based on Dress Code and skin inpainting\nbased on Stable Diffusion. In particular, a generate skin module was introduced\nto solve the skin restoration problem that occurs when long-sleeved images are\nconverted into short-sleeved or sleeveless ones, and high-quality restoration\nwas implemented consider-ing the human body posture and color. As a result,\nCaP-VTON recorded 92.5\\%, which is 15.4\\% better than Leffa in short-sleeved\nsynthesis accuracy, and showed the performance of consistently reproducing the\nstyle and shape of reference clothing in visual evaluation. These structures\nmaintain model-agnostic properties and are applicable to various\ndiffu-sion-based virtual inspection systems, and can contribute to applications\nthat require high-precision virtual wearing, such as e-commerce, custom\nstyling, and avatar creation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaP-VTON\u7684\u65b0\u578b\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c7b\u522b\u63a9\u7801\u548c\u57fa\u4e8eStable Diffusion\u7684\u76ae\u80a4\u4fee\u590d\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u7eb9\u7406\u53d8\u5f62\u3001\u4e0b\u88c5\u68c0\u6d4b\u4e0d\u51c6\u548c\u670d\u88c5\u8f6e\u5ed3\u4fdd\u7559\u65b9\u9762\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77ed\u8896\u5408\u6210\u7cbe\u5ea6\uff08\u6bd4Leffa\u9ad815.4%\uff09\u548c\u89c6\u89c9\u6548\u679c\u7684\u4e00\u81f4\u6027\u4e0e\u81ea\u7136\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff08\u5982Leffa\uff09\u5b58\u5728\u4e0b\u88c5\u68c0\u6d4b\u4e0d\u51c6\u3001\u670d\u88c5\u8f6e\u5ed3\u6b8b\u7559\u548c\u76ae\u80a4\u4fee\u590d\u4e0d\u4f73\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u5408\u6210\u7ed3\u679c\u7684\u81ea\u7136\u6027\u548c\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6574\u4f53\u8d28\u91cf\u3002", "method": "CaP-VTON\u7ed3\u5408\u4e86Dress Code\u7684\u591a\u7c7b\u522b\u63a9\u7801\u7b56\u7565\u548cStable Diffusion\u7684\u76ae\u80a4\u4fee\u590d\u673a\u5236\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u751f\u6210\u76ae\u80a4\u6a21\u5757\uff0c\u8003\u8651\u4eba\u4f53\u59ff\u6001\u548c\u80a4\u8272\u4fe1\u606f\uff0c\u4ee5\u5b9e\u73b0\u957f\u8896\u8f6c\u77ed\u8896\u6216\u65e0\u8896\u65f6\u7684\u9ad8\u8d28\u91cf\u76ae\u80a4\u6062\u590d\uff0c\u5e76\u589e\u5f3a\u6574\u4f53\u670d\u88c5\u5408\u6210\u7684\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u77ed\u8896\u5408\u6210\u4efb\u52a1\u4e2d\uff0cCaP-VTON\u8fbe\u5230\u4e8692.5%\u7684\u51c6\u786e\u7387\uff0c\u6bd4Leffa\u9ad8\u51fa15.4%\uff0c\u5e76\u5728\u89c6\u89c9\u8bc4\u4f30\u4e2d\u5c55\u73b0\u51fa\u5bf9\u53c2\u8003\u670d\u88c5\u98ce\u683c\u548c\u5f62\u72b6\u7684\u4e00\u81f4\u8fd8\u539f\u80fd\u529b\uff0c\u540c\u65f6\u5177\u5907\u6a21\u578b\u65e0\u5173\u6027\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u7c7b\u6269\u6563\u6a21\u578b\u9a71\u52a8\u7684\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\u3002", "conclusion": "CaP-VTON\u6709\u6548\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u4e2d\u670d\u88c5\u5408\u6210\u7684\u81ea\u7136\u6027\u4e0e\u7cbe\u786e\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u8863\u578b\u8f6c\u6362\u548c\u76ae\u80a4\u4fee\u590d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u901a\u7528\u6027\u548c\u5e94\u7528\u524d\u666f\uff0c\u9002\u7528\u4e8e\u7535\u5546\u3001\u4e2a\u6027\u5316\u9020\u578b\u548c\u865a\u62df\u5f62\u8c61\u521b\u5efa\u7b49\u9ad8\u7cbe\u5ea6\u865a\u62df\u7a7f\u6234\u573a\u666f\u3002"}}
{"id": "2509.17660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17660", "abs": "https://arxiv.org/abs/2509.17660", "authors": ["Yikun Ma", "Bo Li", "Ying Chen", "Zijie Yue", "Shuchang Xu", "Jingyao Li", "Lei Ma", "Liang Zhong", "Duowu Zou", "Leiming Xu", "Yunshi Zhong", "Xiaobo Li", "Weiqun Ding", "Minmin Zhang", "Dongli He", "Zhenghong Li", "Ye Chen", "Ye Zhao", "Jialong Zhuo", "Xiaofen Wu", "Lisha Yi", "Miaojing Shi", "Huihui Sun"], "title": "Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study", "comment": null, "summary": "The early detection of esophagogastric junction adenocarcinoma (EGJA) is\ncrucial for improving patient prognosis, yet its current diagnosis is highly\noperator-dependent. This paper aims to make the first attempt to develop an\nartificial intelligence (AI) foundation model-based method for both screening\nand staging diagnosis of EGJA using endoscopic images. In this cohort and\nlearning study, we conducted a multicentre study across seven Chinese hospitals\nbetween December 28, 2016 and December 30, 2024. It comprises 12,302 images\nfrom 1,546 patients; 8,249 of them were employed for model training, while the\nremaining were divided into the held-out (112 patients, 914 images), external\n(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test\nsets for evaluation. The proposed model employs DINOv2 (a vision foundation\nmodel) and ResNet50 (a convolutional neural network) to extract features of\nglobal appearance and local details of endoscopic images for EGJA staging\ndiagnosis. Our model demonstrates satisfactory performance for EGJA staging\ndiagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and\n0.8956, respectively. In contrast, among representative AI models, the best one\n(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test\nsets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on\nthe held-out test set. Moreover, with the assistance of our model, the overall\naccuracy for the trainee, competent, and expert endoscopists improves from\n0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our\nknowledge, our model is the first application of foundation models for EGJA\nstaging diagnosis and demonstrates great potential in both diagnostic accuracy\nand efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5c1d\u8bd5\u5229\u7528\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408DINOv2\u548cResNet50\uff0c\u5bf9\u98df\u7ba1\u80c3\u4ea4\u754c\u90e8\u817a\u764c\uff08EGJA\uff09\u8fdb\u884c\u5185\u955c\u56fe\u50cf\u7684\u7b5b\u67e5\u4e0e\u5206\u671f\u8bca\u65ad\uff0c\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548c\u533b\u751f\u4e13\u5bb6\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5404\u7ea7\u533b\u751f\u7684\u8bca\u65ad\u51c6\u786e\u7387\u3002", "motivation": "EGJA\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u8bca\u65ad\u9ad8\u5ea6\u4f9d\u8d56\u64cd\u4f5c\u8005\u6c34\u5e73\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u51c6\u786e\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u4e2d\u5fc3\u56de\u987e\u6027\u961f\u5217\u7814\u7a76\uff0c\u6536\u96c61,546\u540d\u60a3\u8005\u768412,302\u5f20\u5185\u955c\u56fe\u50cf\uff0c\u4f7f\u7528DINOv2\u548cResNet50\u63d0\u53d6\u56fe\u50cf\u5168\u5c40\u5916\u89c2\u4e0e\u5c40\u90e8\u7ec6\u8282\u7279\u5f81\uff0c\u6784\u5efaEGJA\u5206\u671f\u8bca\u65ad\u6a21\u578b\uff0c\u5e76\u5728\u72ec\u7acb\u7684\u5185\u90e8\u3001\u5916\u90e8\u53ca\u524d\u77bb\u6027\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u4e09\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a0.9256\u30010.8895\u548c0.8956\uff0c\u4f18\u4e8e\u6700\u4f73AI\u6a21\u578b\uff08ResNet50\uff09\u548c\u4e13\u5bb6\u533b\u751f\uff1b\u540c\u65f6\uff0c\u8f85\u52a9\u8bca\u65ad\u4f7f\u5404\u7ea7\u533b\u751f\u7684\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u4ece\u8bad\u7ec3\u751f\u5230\u4e13\u5bb6\u5206\u522b\u63d0\u5347\u81f30.8497\u30010.8521\u548c0.8696\u3002", "conclusion": "\u8be5\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684AI\u7cfb\u7edf\u662f\u9996\u4e2a\u7528\u4e8eEGJA\u5206\u671f\u8bca\u65ad\u7684\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u4e0e\u4e34\u5e8a\u8f85\u52a9\u6f5c\u529b\uff0c\u6709\u671b\u63d0\u9ad8\u8bca\u65ad\u4e00\u81f4\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.17664", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17664", "abs": "https://arxiv.org/abs/2509.17664", "authors": ["Pingyi Chen", "Yujing Lou", "Shen Cao", "Jinhui Guo", "Lubin Fan", "Yue Wu", "Lin Yang", "Lizhuang Ma", "Jieping Ye"], "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models", "comment": "Accepted by NeurIPS 2025", "summary": "While vision language models (VLMs) excel in 2D semantic visual\nunderstanding, their ability to quantitatively reason about 3D spatial\nrelationships remains under-explored, due to the deficiency of 2D images'\nspatial representation ability. In this paper, we analyze the problem hindering\nVLMs' spatial understanding abilities and propose SD-VLM, a novel framework\nthat significantly enhances fundamental spatial perception abilities of VLMs\nthrough two key contributions: (1) propose Massive Spatial Measuring and\nUnderstanding (MSMU) dataset with precise spatial annotations, and (2)\nintroduce a simple depth positional encoding method strengthening VLMs' spatial\nawareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA\npairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented\nsamples. We have trained SD-VLM, a strong generalist VLM which shows superior\nquantitative spatial measuring and understanding capability. SD-VLM not only\nachieves state-of-the-art performance on our proposed MSMU-Bench, but also\nshows spatial generalization abilities on other spatial understanding\nbenchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments\ndemonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and\n25.56% respectively on MSMU-Bench. Code and models are released at\nhttps://github.com/cpystan/SD-VLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSD-VLM\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u7a7a\u95f4\u6d4b\u91cf\u4e0e\u7406\u89e3\u6570\u636e\u96c6\uff08MSMU\uff09\u548c\u5f15\u5165\u6df1\u5ea6\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u57283D\u7a7a\u95f4\u5173\u7cfb\u4e2d\u7684\u5b9a\u91cf\u63a8\u7406\u80fd\u529b\u3002SD-VLM\u5728\u591a\u4e2a\u7a7a\u95f4\u7406\u89e3\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8aGPT-4o\u548cIntern-VL3-78B\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57282D\u8bed\u4e49\u7406\u89e3\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e2D\u56fe\u50cf\u5bf9\u7a7a\u95f4\u8868\u5f81\u80fd\u529b\u4e0d\u8db3\uff0c\u5176\u57283D\u7a7a\u95f4\u5173\u7cfb\u7684\u5b9a\u91cf\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002\u7f3a\u4e4f\u7cbe\u786e\u7684\u7a7a\u95f4\u6807\u6ce8\u6570\u636e\u96c6\u4e5f\u5236\u7ea6\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "1) \u6784\u5efa\u5305\u542b70\u4e07QA\u5bf9\u3001250\u4e07\u7269\u7406\u6570\u503c\u6807\u6ce8\u548c1\u4e07\u601d\u7ef4\u94fe\u589e\u5f3a\u6837\u672c\u7684\u5927\u89c4\u6a21\u7a7a\u95f4\u6d4b\u91cf\u4e0e\u7406\u89e3\uff08MSMU\uff09\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u6df1\u5ea6\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u589e\u5f3aVLM\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3SD-VLM\u6a21\u578b\u3002", "result": "SD-VLM\u5728\u63d0\u51fa\u7684MSMU-Bench\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u76f8\u6bd4GPT-4o\u548cIntern-VL3-78B\u5206\u522b\u63d0\u534726.91%\u548c25.56%\uff0c\u5e76\u5728Q-Spatial\u548cSpatialRGPT-Bench\u7b49\u5176\u4ed6\u7a7a\u95f4\u7406\u89e3\u57fa\u51c6\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SD-VLM\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf93D\u7a7a\u95f4\u5173\u7cfb\u7684\u5b9a\u91cf\u7406\u89e3\u4e0e\u6d4b\u91cf\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.17670", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17670", "abs": "https://arxiv.org/abs/2509.17670", "authors": ["Mariette Sch\u00f6nfeld", "Wannes Meert", "Hendrik Blockeel"], "title": "Tailored Transformation Invariance for Industrial Anomaly Detection", "comment": null, "summary": "Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision\nAnomaly Detection that has been receiving increasing amounts of attention due\nto its applicability to real-life scenarios. Recent research has focused on how\nto extract the most informative features, contrasting older kNN-based methods\nthat use only pretrained features. These recent methods are much more expensive\nto train however and could complicate real-life application. Careful study of\nrelated work with regards to transformation invariance leads to the idea that\npopular benchmarks require robustness to only minor translations. With this\nidea we then formulate LWinNN, a local window based approach that creates a\nmiddle ground between kNN based methods that have either complete or no\ntranslation invariance. Our experiments demonstrate that this small change\nincreases accuracy considerably, while simultaneously decreasing both train and\ntest time. This teaches us two things: first, the gap between kNN-based\napproaches and more complex state-of-the-art methodology can still be narrowed\nby effective usage of the limited data available. Second, our assumption of\nrequiring only limited translation invariance highlights potential areas of\ninterest for future work and the need for more spatially diverse benchmarks,\nfor which our method can hopefully serve as a new baseline. Our code can be\nfound at https://github.com/marietteschonfeld/LWinNN .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u7a97\u53e3\u7684kNN\u65b9\u6cd5LWinNN\uff0c\u7528\u4e8e\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709kNN\u65b9\u6cd5\u5728\u5e73\u79fb\u4e0d\u53d8\u6027\u4e0a\u5b58\u5728\u5b8c\u5168\u6216\u65e0\u7684\u6781\u7aef\u60c5\u51b5\uff0c\u4e14\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9700\u5bf9\u5fae\u5c0f\u5e73\u79fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6298\u4e2d\u65b9\u6848\u4ee5\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faLWinNN\uff0c\u901a\u8fc7\u5c40\u90e8\u7a97\u53e3\u673a\u5236\u5728\u7279\u5f81\u5339\u914d\u4e2d\u5f15\u5165\u6709\u9650\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7279\u5f81\u8fdb\u884c\u76f8\u4f3c\u6027\u68c0\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u95f4\uff0c\u9a8c\u8bc1\u4e86\u6709\u9650\u5e73\u79fb\u4e0d\u53d8\u6027\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1kNN\u65b9\u6cd5\u53ef\u7f29\u5c0f\u4e0e\u590d\u6742SOTA\u65b9\u6cd5\u7684\u5dee\u8ddd\uff0c\u4e14\u5f53\u524d\u57fa\u51c6\u7f3a\u4e4f\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u9700\u6784\u5efa\u66f4\u5177\u6311\u6218\u6027\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2509.17740", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17740", "abs": "https://arxiv.org/abs/2509.17740", "authors": ["Yiwen Jiang", "Deval Mehta", "Siyuan Yan", "Yaling Shen", "Zimu Wang", "Zongyuan Ge"], "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.", "AI": {"tldr": "\u63d0\u51faWISE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u5f15\u5bfc\u7684\u9010\u6b65\u89e3\u91ca\uff0c\u5c06\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u6982\u5ff5\u8868\u793a\u8f6c\u5316\u4e3a\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709MCoT\u65b9\u6cd5\u4f9d\u8d56\u5bcc\u542b\u63a8\u7406\u7684\u6807\u6ce8\u6570\u636e\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u7269\u4f53\u95f4\u63a8\u7406\uff0c\u5ffd\u89c6\u4e86\u5bf9\u56fe\u50cf\u5206\u7c7b\u81f3\u5173\u91cd\u8981\u7684\u7269\u4f53\u5185\u90e8\u7406\u89e3\uff08\u5373\u7ec6\u7c92\u5ea6\u6982\u5ff5\u7406\u89e3\uff09\u3002", "method": "\u63d0\u51faWISE\u65b9\u6cd5\uff0c\u5229\u7528\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u63d0\u53d6\u6982\u5ff5\u8868\u793a\uff0c\u5e76\u5728\u5f31\u76d1\u7763\u4e0b\u5c06\u5176\u91cd\u6784\u4e3a\u7b80\u6d01\u3001\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\uff0c\u4ece\u800c\u4e3a\u4efb\u610f\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u751f\u6210\u63a8\u7406\u94fe\u3002", "result": "\u5728\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u7684MCoT\u4f7f\u53ef\u89e3\u91ca\u6027\u63d0\u534737%\uff0c\u5e76\u80fd\u901a\u8fc7\u5fae\u8c03\u63d0\u9ad8MLLM\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "WISE\u6865\u63a5\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u751f\u6210\u5f0fMCoT\u63a8\u7406\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6cdb\u5316\u7684\u6846\u67b6\u6765\u589e\u5f3aMLLM\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.17686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17686", "abs": "https://arxiv.org/abs/2509.17686", "authors": ["Mohamad Mofeed Chaar", "Jamal Raiyn", "Galia Weidl"], "title": "Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation", "comment": "8 pages, 10 figures, VEHITS conference 2025", "summary": "Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it\nplays a key role in detecting and measuring objects in the vehicle's\nsurroundings. However, a significant challenge in this domain arises from\nmissing information in Depth images, where certain points are not measurable\ndue to gaps or inconsistencies in pixel data. Our research addresses two key\ntasks to overcome this challenge. First, we developed an algorithm using a\nmulti-layered training approach to generate Depth images from a single RGB\nimage. Second, we addressed the issue of missing information in Depth images by\napplying our algorithm to rectify these gaps, resulting in Depth images with\ncomplete and accurate data. We further tested our algorithm on the Cityscapes\ndataset and successfully resolved the missing information in its Depth images,\ndemonstrating the effectiveness of our approach in real-world urban\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u8bad\u7ec3\u7684\u7b97\u6cd5\uff0c\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u6df1\u5ea6\u56fe\uff0c\u5e76\u4fee\u590d\u6df1\u5ea6\u56fe\u4e2d\u7684\u7f3a\u5931\u4fe1\u606f\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u56fe\u50cf\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e38\u5b58\u5728\u56e0\u50cf\u7d20\u6570\u636e\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u4fe1\u606f\u7a7a\u7f3a\u95ee\u9898\uff0c\u5f71\u54cd\u73af\u5883\u611f\u77e5\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u5c42\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u6df1\u5ea6\u56fe\uff0c\u5e76\u5229\u7528\u8be5\u7b97\u6cd5\u4fee\u590d\u539f\u59cb\u6df1\u5ea6\u56fe\u4e2d\u7684\u7f3a\u5931\u533a\u57df\uff0c\u5b9e\u73b0\u5b8c\u6574\u4e14\u51c6\u786e\u7684\u6df1\u5ea6\u4fe1\u606f\u91cd\u5efa\u3002", "result": "\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u6210\u529f\u4fee\u590d\u4e86\u6df1\u5ea6\u56fe\u4e2d\u7684\u7f3a\u5931\u4fe1\u606f\uff0c\u751f\u6210\u4e86\u5b8c\u6574\u7684\u6df1\u5ea6\u56fe\u50cf\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u771f\u5b9e\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u6df1\u5ea6\u56fe\u50cf\u4e2d\u7684\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u5468\u56f4\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.17689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17689", "abs": "https://arxiv.org/abs/2509.17689", "authors": ["\u017diga Babnik", "Deepak Kumar Jain", "Peter Peer", "Vitomir \u0160truc"], "title": "FROQ: Observing Face Recognition Models for Efficient Quality Assessment", "comment": "Presented at the International Joint Conference on Biometrics (IJCB\n  2025)", "summary": "Face Recognition (FR) plays a crucial role in many critical (high-stakes)\napplications, where errors in the recognition process can lead to serious\nconsequences. Face Image Quality Assessment (FIQA) techniques enhance FR\nsystems by providing quality estimates of face samples, enabling the systems to\ndiscard samples that are unsuitable for reliable recognition or lead to\nlow-confidence recognition decisions. Most state-of-the-art FIQA techniques\nrely on extensive supervised training to achieve accurate quality estimation.\nIn contrast, unsupervised techniques eliminate the need for additional training\nbut tend to be slower and typically exhibit lower performance. In this paper,\nwe introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,\ntraining-free approach that leverages specific intermediate representations\nwithin a given FR model to estimate face-image quality, and combines the\nefficiency of supervised FIQA models with the training-free approach of\nunsupervised methods. A simple calibration step based on pseudo-quality labels\nallows FROQ to uncover specific representations, useful for quality assessment,\nin any modern FR model. To generate these pseudo-labels, we propose a novel\nunsupervised FIQA technique based on sample perturbations. Comprehensive\nexperiments with four state-of-the-art FR models and eight benchmark datasets\nshow that FROQ leads to highly competitive results compared to the\nstate-of-the-art, achieving both strong performance and efficient runtime,\nwithout requiring explicit training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFROQ\u7684\u534a\u76d1\u7763\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u4e2d\u95f4\u8868\u793a\u6765\u4f30\u8ba1\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\uff0c\u5728\u4e0d\u8fdb\u884c\u663e\u5f0f\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9ad8\u8d28\u91cf\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08FIQA\uff09\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u91cf\u76d1\u7763\u8bad\u7ec3\uff0c\u800c\u65e0\u76d1\u7763\u65b9\u6cd5\u867d\u65e0\u9700\u8bad\u7ec3\u4f46\u6027\u80fd\u8f83\u4f4e\u4e14\u901f\u5ea6\u6162\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u5177\u9ad8\u6548\u6027\u548c\u9ad8\u6027\u80fd\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\u3002", "method": "FROQ\u901a\u8fc7\u5229\u7528\u7ed9\u5b9a\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u4e2d\u95f4\u8868\u793a\u6765\u4f30\u8ba1\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4f2a\u8d28\u91cf\u6807\u7b7e\u7684\u7b80\u5355\u6821\u51c6\u6b65\u9aa4\uff1b\u4f2a\u6807\u7b7e\u7531\u4e00\u79cd\u57fa\u4e8e\u6837\u672c\u6270\u52a8\u7684\u65b0\u578b\u65e0\u76d1\u7763FIQA\u6280\u672f\u751f\u6210\uff0c\u4ece\u800c\u5728\u4efb\u4f55\u73b0\u4ee3\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u4e2d\u6316\u6398\u51fa\u53ef\u7528\u4e8e\u8d28\u91cf\u8bc4\u4f30\u7684\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u5148\u8fdb\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u548c\u516b\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFROQ\u5728\u6027\u80fd\u548c\u8fd0\u884c\u6548\u7387\u65b9\u9762\u5747\u5177\u6709\u5f88\u5f3a\u7684\u7ade\u4e89\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FROQ\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u534a\u76d1\u7763\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u5e7f\u6cdb\u9002\u7528\u4e8e\u73b0\u4ee3\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002"}}
{"id": "2509.17702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17702", "abs": "https://arxiv.org/abs/2509.17702", "authors": ["Patrick Schmidt", "Vasileios Belagiannis", "Lazaros Nalpantidis"], "title": "Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation", "comment": "Submitted to IEEE", "summary": "Autonomous robotic systems applied to new domains require an abundance of\nexpensive, pixel-level dense labels to train robust semantic segmentation\nmodels under full supervision. This study proposes a model-agnostic Depth Edge\nAlignment Loss to improve Weakly Supervised Semantic Segmentation models across\ndifferent datasets. The methodology generates pixel-level semantic labels from\nimage-level supervision, avoiding expensive annotation processes. While weak\nsupervision is widely explored in traditional computer vision, our approach\nadds supervision with pixel-level depth information, a modality commonly\navailable in robotic systems. We demonstrate how our approach improves\nsegmentation performance across datasets and models, but can also be combined\nwith other losses for even better performance, with improvements up to +5.439,\n+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /\nMS COCO validation, and the HOPE static onboarding split, respectively. Our\ncode will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6df1\u5ea6\u8fb9\u7f18\u5bf9\u9f50\u635f\u5931\uff08Depth Edge Alignment Loss\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5229\u7528\u56fe\u50cf\u7ea7\u76d1\u7763\u751f\u6210\u50cf\u7d20\u7ea7\u6807\u7b7e\uff0c\u5e76\u5f15\u5165\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u6df1\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u65b0\u5e94\u7528\u573a\u666f\u9700\u8981\u5927\u91cf\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u5bc6\u96c6\u6807\u6ce8\u8fdb\u884c\u5168\u76d1\u7763\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u6765\u51cf\u5c11\u5bf9\u5bc6\u96c6\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u6df1\u5ea6\u8fb9\u7f18\u5bf9\u9f50\u635f\u5931\uff08Depth Edge Alignment Loss\uff09\uff0c\u7ed3\u5408\u56fe\u50cf\u7ea7\u6807\u7b7e\u4e0e\u50cf\u7d20\u7ea7\u6df1\u5ea6\u4fe1\u606f\uff0c\u751f\u6210\u50cf\u7d20\u7ea7\u8bed\u4e49\u6807\u7b7e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4e14\u53ef\u4e0e\u5176\u4ed6\u635f\u5931\u51fd\u6570\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728PASCAL VOC\u3001MS COCO\u9a8c\u8bc1\u96c6\u548cHOPE\u9759\u6001\u4e0a\u8f66\u5206\u5272\u4e0a\uff0cmIoU\u5206\u522b\u63d0\u5347\u4e86+5.439\u3001+1.274\u548c+16.416\u4e2a\u767e\u5206\u70b9\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u589e\u5f3a\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff0c\u964d\u4f4e\u4e86\u5bf9\u5bc6\u96c6\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2509.17704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17704", "abs": "https://arxiv.org/abs/2509.17704", "authors": ["Bo Li", "Yunkuo Lei", "Tingting Bao", "Yaxian Wang", "Lingling Zhang", "Jun Liu"], "title": "Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion", "comment": "10 pages, 8 figures", "summary": "Multi-focus image fusion (MFIF) is a crucial technique in image processing,\nwith a key challenge being the generation of decision maps with precise\nboundaries. However, traditional methods based on heuristic rules and deep\nlearning methods with black-box mechanisms are difficult to generate\nhigh-quality decision maps. To overcome this challenge, we introduce\nneurodynamics-driven coupled neural P (CNP) systems, which are third-generation\nneural computation models inspired by spiking mechanisms, to enhance the\naccuracy of decision maps. Specifically, we first conduct an in-depth analysis\nof the model's neurodynamics to identify the constraints between the network\nparameters and the input signals. This solid analysis avoids abnormal\ncontinuous firing of neurons and ensures the model accurately distinguishes\nbetween focused and unfocused regions, generating high-quality decision maps\nfor MFIF. Based on this analysis, we propose a\n\\textbf{N}eurodynamics-\\textbf{D}riven \\textbf{CNP} \\textbf{F}usion model\n(\\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current\nideas of decision map generation, ND-CNPFuse distinguishes between focused and\nunfocused regions by mapping the source image into interpretable spike\nmatrices. By comparing the number of spikes, an accurate decision map can be\ngenerated directly without any post-processing. Extensive experimental results\nshow that ND-CNPFuse achieves new state-of-the-art performance on four\nclassical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code\nis available at https://github.com/MorvanLi/ND-CNPFuse.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u52a8\u529b\u5b66\u9a71\u52a8\u7684\u8026\u5408\u795e\u7ecfP\u7cfb\u7edf\uff08ND-CNPFuse\uff09\u7528\u4e8e\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8109\u51b2\u77e9\u9635\u751f\u6210\u9ad8\u8d28\u91cf\u51b3\u7b56\u56fe\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u751f\u6210\u8fb9\u754c\u7cbe\u786e\u7684\u9ad8\u8d28\u91cf\u51b3\u7b56\u56fe\uff0c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u7b2c\u4e09\u4ee3\u8868\u795e\u7ecf\u8ba1\u7b97\u6a21\u578b\u2014\u2014\u795e\u7ecf\u52a8\u529b\u5b66\u9a71\u52a8\u7684\u8026\u5408\u795e\u7ecfP\u7cfb\u7edf\uff0c\u5c06\u6e90\u56fe\u50cf\u6620\u5c04\u4e3a\u53ef\u89e3\u91ca\u7684\u8109\u51b2\u77e9\u9635\uff0c\u901a\u8fc7\u6bd4\u8f83\u8109\u51b2\u6570\u91cf\u76f4\u63a5\u751f\u6210\u51b3\u7b56\u56fe\uff0c\u65e0\u9700\u540e\u5904\u7406\u3002", "result": "\u5728Lytro\u3001MFFW\u3001MFI-WHU\u548cReal-MFF\u56db\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ND-CNPFuse\u80fd\u6709\u6548\u63d0\u5347\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u4e2d\u51b3\u7b56\u56fe\u7684\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u878d\u5408\u6548\u679c\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17707", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17707", "abs": "https://arxiv.org/abs/2509.17707", "authors": ["Emre G\u00fclsoylu", "Alhassan Abdelhalim", "Derya Kara Boztas", "Ole Grasse", "Carlos Jahn", "Simone Frintrop", "Janick Edinger"], "title": "Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review", "comment": "Submission to Transport Reviews. 36 pages, 2 figures, 4 tables", "summary": "The standardisation of Intermodal Loading Units (ILUs), such as containers,\nsemi-trailers and swap bodies, has revolutionised global trade yet their\nefficient and robust identification remains a critical bottleneck in\nhigh-throughput ports and terminals. This paper reviews 63 empirical studies\nthat propose computer vision (CV) based solutions. It covers the last 35 years\n(1990-2025), tracing the field's evolution from early digital image processing\n(DIP) and traditional machine learning (ML) to the current dominance of deep\nlearning (DL) techniques. While CV offers cost-effective alternatives for other\ntypes of identification techniques, its development is hindered by the lack of\npublicly available benchmarking datasets. This results in high variance for the\nreported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond\ndataset limitations, this review highlights the emerging challenges especially\nintroduced by the shift from character-based text recognition to scene-text\nspotting and the integration of mobile cameras (e.g. drones, sensor equipped\nground vehicles) for dynamic terminal monitoring. To advance the field, the\npaper calls for standardised terminology, open-access datasets, shared source\ncode, while outlining future research directions such as contextless text\nrecognition optimised for ISO6346 codes.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8fc7\u53bb35\u5e74\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u7684\u96c6\u88c5\u7bb1\u7b49\u591a\u5f0f\u8054\u8fd0\u88c5\u8f7d\u5355\u5143\uff08ILU\uff09\u8bc6\u522b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6307\u51fa\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5df2\u6210\u4e3b\u6d41\uff0c\u4f46\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u5927\uff0c\u672a\u6765\u9700\u63a8\u52a8\u6807\u51c6\u5316\u672f\u8bed\u3001\u5f00\u653e\u6570\u636e\u4e0e\u4ee3\u7801\uff0c\u5e76\u63a2\u7d22\u9762\u5411ISO6346\u7f16\u7801\u7684\u65e0\u4e0a\u4e0b\u6587\u6587\u672c\u8bc6\u522b\u7b49\u65b9\u5411\u3002", "motivation": "\u9ad8\u6548\u53ef\u9760\u7684ILU\u8bc6\u522b\u662f\u9ad8\u541e\u5410\u91cf\u6e2f\u53e3\u7684\u5173\u952e\uff0c\u4f46\u5f53\u524dCV\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u6807\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\u800c\u96be\u4ee5\u6bd4\u8f83\u548c\u63a8\u5e7f\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e861990\u81f32025\u5e74\u95f463\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u68b3\u7406\u4ece\u6570\u5b57\u56fe\u50cf\u5904\u7406\u3001\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5230\u6df1\u5ea6\u5b66\u4e60\u7684\u6280\u672f\u6f14\u8fdb\u8def\u5f84\u3002", "result": "CV\u5728ILU\u8bc6\u522b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7aef\u5230\u7aef\u51c6\u786e\u7387\u5dee\u5f02\u5de8\u5927\uff085%~96%\uff09\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u7f3a\u5931\u3001\u4ece\u5b57\u7b26\u8bc6\u522b\u8f6c\u5411\u573a\u666f\u6587\u672c\u68c0\u6d4b\u7684\u65b0\u6311\u6218\uff0c\u4ee5\u53ca\u79fb\u52a8\u6444\u50cf\u5934\u96c6\u6210\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u7684\u5173\u952e\u5728\u4e8e\u5efa\u7acb\u6807\u51c6\u672f\u8bed\u3001\u5f00\u653e\u6570\u636e\u96c6\u4e0e\u5171\u4eab\u4ee3\u7801\uff0c\u5e76\u805a\u7126\u5982\u65e0\u4e0a\u4e0b\u6587\u6587\u672c\u8bc6\u522b\u7b49\u9488\u5bf9\u6027\u7814\u7a76\u65b9\u5411\u4ee5\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.17712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17712", "abs": "https://arxiv.org/abs/2509.17712", "authors": ["Geonho Bang", "Minjae Seong", "Jisong Kim", "Geunju Baek", "Daye Oh", "Junhyung Kim", "Junho Koh", "Jun Won Choi"], "title": "RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion", "comment": "Accepted at ICCV 2025", "summary": "Radar-camera fusion methods have emerged as a cost-effective approach for 3D\nobject detection but still lag behind LiDAR-based methods in performance.\nRecent works have focused on employing temporal fusion and Knowledge\nDistillation (KD) strategies to overcome these limitations. However, existing\napproaches have not sufficiently accounted for uncertainties arising from\nobject motion or sensor-specific errors inherent in radar and camera\nmodalities. In this work, we propose RCTDistill, a novel cross-modal KD method\nbased on temporal fusion, comprising three key modules: Range-Azimuth Knowledge\nDistillation (RAKD), Temporal Knowledge Distillation (TKD), and\nRegion-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider\nthe inherent errors in the range and azimuth directions, enabling effective\nknowledge transfer from LiDAR features to refine inaccurate BEV\nrepresentations. TKD mitigates temporal misalignment caused by dynamic objects\nby aligning historical radar-camera BEV features with current LiDAR\nrepresentations. RDKD enhances feature discrimination by distilling relational\nknowledge from the teacher model, allowing the student to differentiate\nforeground and background features. RCTDistill achieves state-of-the-art\nradar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)\ndatasets, with the fastest inference speed of 26.2 FPS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5RCTDistill\uff0c\u5305\u542bRAKD\u3001TKD\u548cRDKD\u4e09\u4e2a\u6a21\u5757\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728nuScenes\u548cVoD\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe26.2 FPS\u3002", "motivation": "\u73b0\u6709\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u65b9\u6cd5\u5728\u5904\u7406\u4f20\u611f\u5668\u8bef\u5dee\u548c\u52a8\u6001\u7269\u4f53\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u6027\u80fd\u4ecd\u843d\u540e\u4e8eLiDAR\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRCTDistill\uff0c\u7ed3\u5408\u65f6\u95f4\u878d\u5408\u4e0e\u77e5\u8bc6\u84b8\u998f\uff0c\u8bbe\u8ba1\u4e86\u8003\u8651\u96f7\u8fbe\u65b9\u5411\u8bef\u5dee\u7684RAKD\u3001\u7f13\u89e3\u65f6\u5e8f\u9519\u4f4d\u7684TKD\uff0c\u4ee5\u53ca\u589e\u5f3a\u7279\u5f81\u533a\u5206\u80fd\u529b\u7684RDKD\u6a21\u5757\u3002", "result": "\u5728nuScenes\u548cView-of-Delft\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe\u523026.2 FPS\u3002", "conclusion": "RCTDistill\u6709\u6548\u89e3\u51b3\u4e86\u96f7\u8fbe\u548c\u76f8\u673a\u6a21\u6001\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u65f6\u5e8f\u9519\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002"}}
{"id": "2509.17726", "categories": ["cs.CV", "cs.LG", "I.4.0"], "pdf": "https://arxiv.org/pdf/2509.17726", "abs": "https://arxiv.org/abs/2509.17726", "authors": ["Javier Bisbal", "Patrick Winter", "Sebastian Jofre", "Aaron Ponce", "Sameer A. Ansari", "Ramez Abdalla", "Michael Markl", "Oliver Welin Odeback", "Sergio Uribe", "Cristian Tejos", "Julio Sotelo", "Susanne Schnell", "David Marlevi"], "title": "Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning", "comment": "16 pages, 6 figures", "summary": "Accurate anatomical labeling of intracranial arteries is essential for\ncerebrovascular diagnosis and hemodynamic analysis but remains time-consuming\nand subject to interoperator variability. We present a deep learning-based\nframework for automated artery labeling from 3D Time-of-Flight Magnetic\nResonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating\nuncertainty quantification to enhance interpretability and reliability. We\nevaluated three convolutional neural network architectures: (1) a UNet with\nresidual encoder blocks, reflecting commonly used baselines in vascular\nlabeling; (2) CS-Net, an attention-augmented UNet incorporating channel and\nspatial attention mechanisms for enhanced curvilinear structure recognition;\nand (3) nnUNet, a self-configuring framework that automates preprocessing,\ntraining, and architectural adaptation based on dataset characteristics. Among\nthese, nnUNet achieved the highest labeling performance (average Dice score:\n0.922; average surface distance: 0.387 mm), with improved robustness in\nanatomically complex vessels. To assess predictive confidence, we implemented\ntest-time augmentation (TTA) and introduced a novel coordinate-guided strategy\nto reduce interpolation errors during augmented inference. The resulting\nuncertainty maps reliably indicated regions of anatomical ambiguity,\npathological variation, or manual labeling inconsistency. We further validated\nclinical utility by comparing flow velocities derived from automated and manual\nlabels in co-registered 4D Flow MRI datasets, observing close agreement with no\nstatistically significant differences. Our framework offers a scalable,\naccurate, and uncertainty-aware solution for automated cerebrovascular\nlabeling, supporting downstream hemodynamic analysis and facilitating clinical\nintegration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece3D ToF-MRA\u5206\u5272\u4e2d\u81ea\u52a8\u6807\u6ce8\u9885\u5185\u52a8\u8109\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u5176\u4e2dnnUNet\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u4e34\u5e8a\u9a8c\u8bc1\u663e\u793a\u81ea\u52a8\u5316\u6807\u7b7e\u4e0e\u624b\u52a8\u6807\u7b7e\u5728\u8840\u6d41\u901f\u5ea6\u5206\u6790\u4e2d\u5177\u6709\u4e00\u81f4\u6027\u3002", "motivation": "\u51c6\u786e\u7684\u9885\u5185\u52a8\u8109\u89e3\u5256\u6807\u6ce8\u5bf9\u8111\u8840\u7ba1\u8bca\u65ad\u548c\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u5b58\u5728\u64cd\u4f5c\u8005\u95f4\u53d8\u5f02\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08UNet\u3001CS-Net\u3001nnUNet\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u4f7f\u7528\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u548c\u65b0\u578b\u5750\u6807\u5f15\u5bfc\u7b56\u7565\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u56fe\uff0c\u5e76\u572835\u4f8b\u6570\u636e\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "nnUNet\u8868\u73b0\u6700\u4f18\uff08\u5e73\u5747Dice\u5206\u65700.922\uff0c\u5e73\u5747\u8868\u9762\u8ddd\u79bb0.387 mm\uff09\uff0c\u5728\u590d\u6742\u89e3\u5256\u7ed3\u6784\u4e2d\u66f4\u5177\u9c81\u68d2\u6027\uff1b\u4e0d\u786e\u5b9a\u6027\u56fe\u80fd\u53ef\u9760\u6307\u793a\u89e3\u5256\u6a21\u7cca\u6216\u6807\u6ce8\u4e0d\u4e00\u81f4\u533a\u57df\uff1b\u81ea\u52a8\u5316\u4e0e\u624b\u52a8\u6807\u6ce8\u7684\u8840\u6d41\u901f\u5ea6\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u51c6\u786e\u4e14\u5177\u5907\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u81ea\u52a8\u5316\u8111\u8840\u7ba1\u6807\u6ce8\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4e34\u5e8a\u5e94\u7528\u548c\u4e0b\u6e38\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u3002"}}
{"id": "2509.17743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17743", "abs": "https://arxiv.org/abs/2509.17743", "authors": ["Chenglin Li", "Feng Han", "FengTao", "Ruilin Li", "Qianglong Chen", "Jingqi Tong", "Yin Zhang", "Jiaqi Wang"], "title": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA", "comment": null, "summary": "Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.", "AI": {"tldr": "\u63d0\u51faFS-VisPR\u6846\u67b6\uff0c\u7ed3\u5408\u5feb\u6162\u63a8\u7406\u673a\u5236\u4e0e\u53ef\u8c03\u53c2\u6570\u7684\u89c6\u89c9\u6a21\u5757\uff0c\u63d0\u5347\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u89c6\u89c9\u7a0b\u5e8f\u751f\u6210\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u95ed\u6e90\u6a21\u578b\u3001\u7f3a\u4e4f\u7cfb\u7edf\u6027\u63a8\u7406\uff0c\u4e14\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u5728\u89c6\u89c9\u7a0b\u5e8f\u751f\u6210\u4e0a\u7684\u80fd\u529b\u4e0e\u9002\u5e94\u6027\u3002", "method": "\u8bbe\u8ba1\u9ad8\u6548\u89c6\u89c9\u6a21\u5757\uff08\u5982\u5173\u952e\u7247\u6bb5\u68c0\u7d22\u3001\u5b57\u5e55\u68c0\u7d22\uff09\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u5feb\u6162\u63a8\u7406\u6570\u636e\u96c6\u4ee5\u5bf9\u9f50\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff08FS-LLM\uff09\uff0c\u5e76\u63d0\u51fa\u5feb\u6162\u53cc\u9636\u6bb5\u63a8\u7406\u6846\u67b6\uff1a\u7b80\u5355\u95ee\u9898\u76f4\u63a5\u56de\u7b54\uff0c\u590d\u6742\u95ee\u9898\u89e6\u53d1\u6162\u63a8\u7406\uff1b\u5f15\u5165\u6267\u884c\u5931\u8d25\u56de\u9000\u673a\u5236\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u901a\u8fc7\u53c2\u6570\u641c\u7d22\u4f18\u5316\u89c6\u89c9\u7a0b\u5e8f\u3002", "result": "FS-VisPR\u5728LVBench\u4e0a\u8fbe\u523050.4%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7GPT-4o\uff0c\u5728VideoMME\u4e0a\u6027\u80fd\u5339\u914dQwen2.5VL-72B\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002", "conclusion": "FS-VisPR\u901a\u8fc7\u81ea\u9002\u5e94\u5feb\u6162\u63a8\u7406\u673a\u5236\u548c\u7a0b\u5e8f\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4e86\u63a8\u7406\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a\u5f00\u6e90\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17747", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17747", "abs": "https://arxiv.org/abs/2509.17747", "authors": ["Sheng Huang", "Jiexuan Yan", "Beiyan Liu", "Bo Liu", "Richang Hong"], "title": "Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification", "comment": "accepted by IEEE Transactions on Image Processing", "summary": "Real-world datasets often exhibit class imbalance across multiple categories,\nmanifesting as long-tailed distributions and few-shot scenarios. This is\nespecially challenging in Class-Imbalanced Multi-Label Image Classification\n(CI-MLIC) tasks, where data imbalance and multi-object recognition present\nsignificant obstacles. To address these challenges, we propose a novel method\ntermed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which\nleverages multi-modal knowledge from vision-language pretrained (VLP) models to\nmitigate the class-imbalance problem in multi-label settings. Specifically,\nHP-DVAL employs dual-view alignment learning to transfer the powerful feature\nrepresentation capabilities from VLP models by extracting complementary\nfeatures for accurate image-text alignment. To better adapt VLP models for\nCI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes\nglobal and local prompts to learn task-specific and context-related prior\nknowledge. Additionally, we design a semantic consistency loss during prompt\ntuning to prevent learned prompts from deviating from general knowledge\nembedded in VLP models. The effectiveness of our approach is validated on two\nCI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results\ndemonstrate the superiority of our method over SOTA approaches, achieving mAP\nimprovements of 10.0\\% and 5.2\\% on the long-tailed multi-label image\nclassification task, and 6.8\\% and 2.9\\% on the multi-label few-shot image\nclassification task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHP-DVAL\u7684\u53cc\u89c6\u89d2\u5bf9\u9f50\u5b66\u4e60\u4e0e\u5206\u5c42\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u5728\u957f\u5c3e\u5206\u5e03\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7c7b\u522b\u4e0d\u5e73\u8861\u5728\u73b0\u5b9e\u6570\u636e\u96c6\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5c24\u5176\u5728\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u4e2d\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u591a\u6807\u7b7e\u3001\u957f\u5c3e\u5206\u5e03\u548c\u5c11\u6837\u672c\u6311\u6218\u3002", "method": "\u63d0\u51faHP-DVAL\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u89c6\u89d2\u5bf9\u9f50\u5b66\u4e60\u4ee5\u63d0\u53d6\u4e92\u8865\u7279\u5f81\u5b9e\u73b0\u7cbe\u51c6\u56fe\u6587\u5bf9\u9f50\uff0c\u5e76\u8bbe\u8ba1\u5206\u5c42\u63d0\u793a\u8c03\u4f18\u7b56\u7565\uff08\u5168\u5c40\u4e0e\u5c40\u90e8\u63d0\u793a\uff09\u6765\u83b7\u53d6\u4efb\u52a1\u7279\u5b9a\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u5148\u9a8c\u77e5\u8bc6\uff0c\u540c\u65f6\u5f15\u5165\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u9632\u6b62\u63d0\u793a\u504f\u79bb\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u77e5\u8bc6\u3002", "result": "\u5728MS-COCO\u548cVOC2007\u4e24\u4e2aCI-MLIC\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u957f\u5c3e\u591a\u6807\u7b7e\u5206\u7c7bmAP\u63d0\u534710.0%\u548c5.2%\uff0c\u591a\u6807\u7b7e\u5c11\u6837\u672c\u5206\u7c7b\u63d0\u53476.8%\u548c2.9%\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "HP-DVAL\u901a\u8fc7\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u5148\u9a8c\u77e5\u8bc6\u4e0e\u5206\u5c42\u63d0\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u957f\u5c3e\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u4f18\u52bf\u3002"}}
{"id": "2509.17762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17762", "abs": "https://arxiv.org/abs/2509.17762", "authors": ["Sitian Shen", "Georgi Pramatarov", "Yifu Tao", "Daniele De Martini"], "title": "Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene Reconstruction", "comment": null, "summary": "This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal\nlarge-scale scene reconstruction that fuses multiple sensing modalities in a\nper-gaussian compact, learnable embedding. While recent works focusing on\nlarge-scale scene reconstruction have incorporated LiDAR data to provide more\naccurate geometric constraints, we argue that LiDAR's rich physical properties\nremain underexplored. Similarly, semantic information has been used for object\nretrieval, but could provide valuable high-level context for scene\nreconstruction. Traditional approaches append these properties to Gaussians as\nseparate parameters, increasing memory usage and limiting information exchange\nacross modalities. Instead, our approach fuses all modalities -- image, LiDAR,\nand semantics -- into a compact, learnable embedding that implicitly encodes\noptical, physical, and semantic features in each Gaussian. We then train\nlightweight neural decoders to map these embeddings to Gaussian parameters,\nenabling the reconstruction of each sensing modality with lower memory overhead\nand improved scalability. We evaluate Neural-MMGS on the Oxford Spires and\nKITTI-360 datasets. On Oxford Spires, we achieve higher-quality\nreconstructions, while on KITTI-360, our method reaches competitive results\nwith less storage consumption compared with current approaches in LiDAR-based\nnovel-view synthesis.", "AI": {"tldr": "\u63d0\u51faNeural-MMGS\uff0c\u4e00\u79cd\u878d\u5408\u56fe\u50cf\u3001LiDAR\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u7d27\u51d1\u53ef\u5b66\u4e60\u5d4c\u5165\u795e\u7ecf3DGS\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u6a21\u6001\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u6316\u6398LiDAR\u7684\u7269\u7406\u7279\u6027\uff0c\u4e14\u8bed\u4e49\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\uff1b\u4f20\u7edf\u591a\u6a21\u6001\u878d\u5408\u65b9\u5f0f\u5185\u5b58\u5f00\u9500\u5927\u3001\u8de8\u6a21\u6001\u4ea4\u4e92\u53d7\u9650\u3002", "method": "\u5c06\u56fe\u50cf\u3001LiDAR\u548c\u8bed\u4e49\u6a21\u6001\u878d\u5408\u5230\u6bcf\u4e2a\u9ad8\u65af\u7684\u7d27\u51d1\u53ef\u5b66\u4e60\u5d4c\u5165\u4e2d\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u89e3\u7801\u5668\u6620\u5c04\u4e3a\u9ad8\u65af\u53c2\u6570\u3002", "result": "\u5728Oxford Spires\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u5728KITTI-360\u4e0a\u4ee5\u66f4\u4f4e\u5b58\u50a8\u6d88\u8017\u8fbe\u5230\u4e0e\u5f53\u524d\u65b9\u6cd5\u76f8\u5f53\u7684LiDAR\u65b0\u89c6\u89d2\u5408\u6210\u6027\u80fd\u3002", "conclusion": "Neural-MMGS\u901a\u8fc7\u7d27\u51d1\u5d4c\u5165\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\uff0c\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u8d28\u91cf\u3002"}}
{"id": "2509.17769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17769", "abs": "https://arxiv.org/abs/2509.17769", "authors": ["Yang Li", "Xinyi Zeng", "Zhe Xue", "Pinxian Zeng", "Zikai Zhang", "Yan Wang"], "title": "Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics", "comment": null, "summary": "As the third generation of neural networks, spiking neural networks (SNNs)\nhave recently gained widespread attention for their biological plausibility,\nenergy efficiency, and effectiveness in processing neuromorphic datasets. To\nbetter emulate biological neurons, various models such as Integrate-and-Fire\n(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.\nHowever, these neuron models overlook the refractory period, a fundamental\ncharacteristic of biological neurons. Research on excitable neurons reveal that\nafter firing, neurons enter a refractory period during which they are\ntemporarily unresponsive to subsequent stimuli. This mechanism is critical for\npreventing over-excitation and mitigating interference from aberrant signals.\nTherefore, we propose a simple yet effective method to incorporate the\nrefractory period into spiking LIF neurons through spike-triggered threshold\ndynamics, termed RPLIF. Our method ensures that each spike accurately encodes\nneural information, effectively preventing neuron over-excitation under\ncontinuous inputs and interference from anomalous inputs. Incorporating the\nrefractory period into LIF neurons is seamless and computationally efficient,\nenhancing robustness and efficiency while yielding better performance with\nnegligible overhead. To the best of our knowledge, RPLIF achieves\nstate-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)\nwith fewer timesteps and demonstrates superior performance on DVS128\nGesture(97.22%) at low latency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c16\u5cf0\u89e6\u53d1\u9608\u503c\u52a8\u6001\u7684LIF\u795e\u7ecf\u5143\u6539\u8fdb\u65b9\u6cd5RPLIF\uff0c\u5f15\u5165\u751f\u7269\u5b66\u4e2d\u7684\u4e0d\u5e94\u671f\u673a\u5236\uff0c\u63d0\u5347SNN\u7684\u9c81\u68d2\u6027\u3001\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLIF\u795e\u7ecf\u5143\u6a21\u578b\u5ffd\u7565\u4e86\u751f\u7269\u795e\u7ecf\u5143\u7684\u5173\u952e\u7279\u6027\u2014\u2014\u4e0d\u5e94\u671f\uff0c\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5174\u594b\u548c\u5f02\u5e38\u4fe1\u53f7\u5e72\u6270\uff0c\u5f71\u54cdSNN\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5728LIF\u795e\u7ecf\u5143\u4e2d\u5f15\u5165\u5c16\u5cf0\u89e6\u53d1\u7684\u9608\u503c\u52a8\u6001\u673a\u5236\uff0c\u6a21\u62df\u795e\u7ecf\u5143\u7684\u4e0d\u5e94\u671f\uff0c\u4f7f\u795e\u7ecf\u5143\u5728\u53d1\u653e\u540e\u6682\u65f6\u4e0d\u54cd\u5e94\u8f93\u5165\uff0c\u4ece\u800c\u6291\u5236\u8fc7\u6fc0\u53d1\u548c\u5f02\u5e38\u4fe1\u53f7\u5e72\u6270\u3002", "result": "RPLIF\u5728Cifar10-DVS\uff0882.40%\uff09\u3001N-Caltech101\uff0883.35%\uff09\u548cDVS128 Gesture\uff0897.22%\uff09\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4e14\u5177\u6709\u66f4\u4f4e\u5ef6\u8fdf\u548c\u66f4\u5c11\u65f6\u95f4\u6b65\u3002", "conclusion": "\u5f15\u5165\u4e0d\u5e94\u671f\u673a\u5236\u7684RPLIF\u663e\u8457\u63d0\u5347\u4e86SNN\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u9ad8\u6548\uff0c\u4e3a\u6784\u5efa\u66f4\u751f\u7269\u53ef\u89e3\u91ca\u3001\u9ad8\u6548\u7684\u8109\u51b2\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2509.17773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17773", "abs": "https://arxiv.org/abs/2509.17773", "authors": ["Guanjie Wang", "Zehua Ma", "Han Fang", "Weiming Zhang"], "title": "I2VWM: Robust Watermarking for Image to Video Generation", "comment": "10 pages", "summary": "The rapid progress of image-guided video generation (I2V) has raised concerns\nabout its potential misuse in misinformation and fraud, underscoring the urgent\nneed for effective digital watermarking. While existing watermarking methods\ndemonstrate robustness within a single modality, they fail to trace source\nimages in I2V settings. To address this gap, we introduce the concept of Robust\nDiffusion Distance, which measures the temporal persistence of watermark\nsignals in generated videos. Building on this, we propose I2VWM, a cross-modal\nwatermarking framework designed to enhance watermark robustness across time.\nI2VWM leverages a video-simulation noise layer during training and employs an\noptical-flow-based alignment module during inference. Experiments on both\nopen-source and commercial I2V models demonstrate that I2VWM significantly\nimproves robustness while maintaining imperceptibility, establishing a new\nparadigm for cross-modal watermarking in the era of generative video.\n\\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code\nReleased.}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff08I2V\uff09\u7684\u8de8\u6a21\u6001\u6c34\u5370\u6846\u67b6I2VWM\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u9c81\u68d2\u6269\u6563\u8ddd\u79bb\u201d\u6982\u5ff5\uff0c\u7ed3\u5408\u89c6\u9891\u6a21\u62df\u566a\u58f0\u5c42\u548c\u5149\u6d41\u5bf9\u9f50\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u5728\u65f6\u5e8f\u4e0a\u7684\u9c81\u68d2\u6027\u4e0e\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5728\u5355\u4e00\u6a21\u6001\u5185\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728I2V\u573a\u666f\u4e2d\u65e0\u6cd5\u8ffd\u6eaf\u6e90\u56fe\u50cf\uff0c\u5b58\u5728\u88ab\u6ee5\u7528\u7684\u98ce\u9669\uff0c\u4e9f\u9700\u6709\u6548\u7684\u8de8\u6a21\u6001\u6c34\u5370\u6280\u672f\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u6269\u6563\u8ddd\u79bb\u6765\u8861\u91cf\u6c34\u5370\u4fe1\u53f7\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u6301\u4e45\u6027\uff1b\u8bbe\u8ba1I2VWM\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u89c6\u9891\u6a21\u62df\u566a\u58f0\u5c42\uff0c\u63a8\u7406\u65f6\u91c7\u7528\u57fa\u4e8e\u5149\u6d41\u7684\u5bf9\u9f50\u6a21\u5757\u4ee5\u589e\u5f3a\u6c34\u5370\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u548c\u5546\u4e1aI2V\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cI2VWM\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u5370\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u4e0e\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "conclusion": "I2VWM\u4e3a\u751f\u6210\u5f0f\u89c6\u9891\u65f6\u4ee3\u7684\u8de8\u6a21\u6001\u6c34\u5370\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u5e94\u5bf9I2V\u6280\u672f\u6f5c\u5728\u7684\u6ee5\u7528\u95ee\u9898\u3002"}}
{"id": "2509.17786", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17786", "abs": "https://arxiv.org/abs/2509.17786", "authors": ["Aniello Panariello", "Daniel Marczak", "Simone Magistri", "Angelo Porrello", "Bart\u0142omiej Twardowski", "Andrew D. Bagdanov", "Simone Calderara", "Joost van de Weijer"], "title": "Accurate and Efficient Low-Rank Model Merging in Core Space", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025), San Diego, USA", "summary": "In this paper, we address the challenges associated with merging low-rank\nadaptations of large neural networks. With the rise of parameter-efficient\nadaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning\nhas become more accessible. While fine-tuning models with LoRA is highly\nefficient, existing merging methods often sacrifice this efficiency by merging\nfully-sized weight matrices. We propose the Core Space merging framework, which\nenables the merging of LoRA-adapted models within a common alignment basis,\nthereby preserving the efficiency of low-rank adaptation while substantially\nimproving accuracy across tasks. We further provide a formal proof that\nprojection into Core Space ensures no loss of information and provide a\ncomplexity analysis showing the efficiency gains. Extensive empirical results\ndemonstrate that Core Space significantly improves existing merging techniques\nand achieves state-of-the-art results on both vision and language tasks while\nutilizing a fraction of the computational resources. Codebase is available at\nhttps://github.com/apanariello4/core-space-merging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCore Space\u7684\u5408\u5e76\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4fdd\u6301\u4f4e\u79e9\u9002\u5e94\u6548\u7387\u7684\u540c\u65f6\uff0c\u6709\u6548\u5408\u5e76LoRA\u5fae\u8c03\u7684\u5927\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u8de8\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684LoRA\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u5408\u5e76\u5168\u5c3a\u5bf8\u6743\u91cd\u77e9\u9635\u727a\u7272\u4e86\u4f4e\u79e9\u9002\u5e94\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6548\u7387\u53c8\u80fd\u63d0\u5347\u6027\u80fd\u7684\u5408\u5e76\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCore Space\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5171\u540c\u5bf9\u9f50\u57fa\u4e2d\u5408\u5e76LoRA\u9002\u914d\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u6295\u5f71\u5230Core Space\u4e0d\u4f1a\u635f\u5931\u4fe1\u606f\uff0c\u540c\u65f6\u8fdb\u884c\u590d\u6742\u5ea6\u5206\u6790\u4ee5\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCore Space\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5408\u5e76\u6280\u672f\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e14\u4ec5\u4f7f\u7528\u5c11\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "Core Space\u80fd\u591f\u5728\u4e0d\u727a\u7272\u4f4e\u79e9\u9002\u5e94\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u5408\u5e76LoRA\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2509.17789", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17789", "abs": "https://arxiv.org/abs/2509.17789", "authors": ["Guoxi Huang", "Haoran Wang", "Zipeng Qi", "Wenjun Lu", "David Bull", "Nantheera Anantrasirichai"], "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes", "comment": null, "summary": "Underwater image degradation poses significant challenges for 3D\nreconstruction, where simplified physical models often fail in complex scenes.\nWe propose \\textbf{R-Splatting}, a unified framework that bridges underwater\nimage restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both\nrendering quality and geometric fidelity. Our method integrates multiple\nenhanced views produced by diverse UIR models into a single reconstruction\npipeline. During inference, a lightweight illumination generator samples latent\ncodes to support diverse yet coherent renderings, while a contrastive loss\nensures disentangled and stable illumination representations. Furthermore, we\npropose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models\nopacity as a stochastic function to regularize training. This suppresses abrupt\ngradient responses triggered by illumination variation and mitigates\noverfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF\nand our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong\nbaselines in both rendering quality and geometric accuracy.", "AI": {"tldr": "\u63d0\u51faR-Splatting\u6846\u67b6\uff0c\u7ed3\u5408\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u4e0e3D\u9ad8\u65af\u70b9\u9635\uff0c\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u4e0e\u51e0\u4f55\u7cbe\u5ea6\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u9000\u5316\u4e25\u91cd\uff0c\u4f20\u7edf\u7269\u7406\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd3D\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u5c06\u591a\u79cd\u589e\u5f3a\u89c6\u56fe\u6574\u5408\u8fdb3DGS\u6d41\u7a0b\uff0c\u5f15\u5165\u8f7b\u91cf\u5149\u7167\u751f\u6210\u5668\u548c\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4e0d\u900f\u660e\u5ea6\u4f18\u5316\uff08UAOO\uff09\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728Seathru-NeRF\u548c\u65b0\u6784\u5efa\u7684BlueCoral3D\u6570\u636e\u96c6\u4e0a\uff0cR-Splatting\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u51e0\u4f55\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "R-Splatting\u6709\u6548\u8054\u5408\u4e86\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u4e0e3D\u91cd\u5efa\uff0c\u63d0\u5347\u4e86\u590d\u6742\u6c34\u4e0b\u573a\u666f\u7684\u89c6\u89c9\u4e0e\u51e0\u4f55\u8868\u73b0\u3002"}}
{"id": "2509.17792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17792", "abs": "https://arxiv.org/abs/2509.17792", "authors": ["S M A Sharif", "Abdur Rehman", "Fayaz Ali Dharejo", "Radu Timofte", "Rizwan Ali Naqvi"], "title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding", "comment": null, "summary": "Real-world images often suffer from spatially diverse degradations such as\nhaze, rain, snow, and low-light, significantly impacting visual quality and\ndownstream vision tasks. Existing all-in-one restoration (AIR) approaches\neither depend on external text prompts or embed hand-crafted architectural\npriors (e.g., frequency heuristics); both impose discrete, brittle assumptions\nthat weaken generalization to unseen or mixed degradations. To address this\nlimitation, we propose to reframe AIR as learned latent prior inference, where\ndegradation-aware representations are automatically inferred from the input\nwithout explicit task cues. Based on latent priors, we formulate AIR as a\nstructured reasoning paradigm: (1) which features to route (adaptive feature\nselection), (2) where to restore (spatial localization), and (3) what to\nrestore (degradation semantics). We design a lightweight decoding module that\nefficiently leverages these latent encoded cues for spatially-adaptive\nrestoration. Extensive experiments across six common degradation tasks, five\ncompound settings, and previously unseen degradations demonstrate that our\nmethod outperforms state-of-the-art (SOTA) approaches, achieving an average\nPSNR improvement of 1.68 dB while being three times more efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6f5c\u5728\u5148\u9a8c\u63a8\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u771f\u5b9e\u56fe\u50cf\u4e2d\u591a\u79cd\u9000\u5316\u95ee\u9898\u7684\u81ea\u9002\u5e94\u6062\u590d\uff0c\u65e0\u9700\u5916\u90e8\u63d0\u793a\u6216\u624b\u5de5\u8bbe\u8ba1\u5148\u9a8c\uff0c\u5728\u516d\u79cd\u5e38\u89c1\u548c\u590d\u5408\u9000\u5316\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5168\u5408\u4e00\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6587\u672c\u63d0\u793a\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u67b6\u6784\u5148\u9a8c\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u6216\u6df7\u5408\u9000\u5316\u573a\u666f\u3002", "method": "\u5c06\u5168\u5408\u4e00\u6062\u590d\u91cd\u6784\u4e3a\u6f5c\u5728\u5148\u9a8c\u63a8\u65ad\uff0c\u901a\u8fc7\u8f93\u5165\u81ea\u52a8\u63a8\u65ad\u9000\u5316\u611f\u77e5\u8868\u793a\uff0c\u5e76\u57fa\u4e8e\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u9000\u5316\u8bed\u4e49\u6062\u590d\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u5316\u89e3\u7801\u6a21\u5757\u5b9e\u73b0\u7a7a\u95f4\u81ea\u9002\u5e94\u6062\u590d\u3002", "result": "\u5728\u516d\u4e2a\u5e38\u89c1\u9000\u5316\u4efb\u52a1\u3001\u4e94\u79cd\u590d\u5408\u8bbe\u7f6e\u53ca\u672a\u89c1\u9000\u5316\u4e0a\u663e\u8457\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u5e73\u5747PSNR\u63d0\u53471.68 dB\uff0c\u6548\u7387\u63d0\u9ad8\u4e09\u500d\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u6f5c\u5728\u5148\u9a8c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6548\u7684\u7a7a\u95f4\u81ea\u9002\u5e94\u56fe\u50cf\u6062\u590d\uff0c\u9002\u7528\u4e8e\u590d\u6742\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2509.17802", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17802", "abs": "https://arxiv.org/abs/2509.17802", "authors": ["Qi'ao Xu", "Pengfei Wang", "Bo Zhong", "Tianwen Qian", "Xiaoling Wang", "Ye Wang", "Hong Yu"], "title": "TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification", "comment": "12 pages, 4 figures", "summary": "Medical time series (MedTS) classification is pivotal for intelligent\nhealthcare, yet its efficacy is severely limited by poor cross-subject\ngeneration due to the profound cross-individual heterogeneity. Despite advances\nin architectural innovations and transfer learning techniques, current methods\nremain constrained by modality-specific inductive biases that limit their\nability to learn universally invariant representations. To overcome this, we\npropose TS-P$^2$CL, a novel plug-and-play framework that leverages the\nuniversal pattern recognition capabilities of pre-trained vision models. We\nintroduce a vision-guided paradigm that transforms 1D physiological signals\ninto 2D pseudo-images, establishing a bridge to the visual domain. This\ntransformation enables implicit access to rich semantic priors learned from\nnatural images. Within this unified space, we employ a dual-contrastive\nlearning strategy: intra-modal consistency enforces temporal coherence, while\ncross-modal alignment aligns time-series dynamics with visual semantics,\nthereby mitigating individual-specific biases and learning robust,\ndomain-invariant features. Extensive experiments on six MedTS datasets\ndemonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both\nsubject-dependent and subject-independent settings.", "AI": {"tldr": "\u63d0\u51faTS-P$^2$CL\u6846\u67b6\uff0c\u901a\u8fc7\u5c061D\u751f\u7406\u4fe1\u53f7\u8f6c\u6362\u4e3a2D\u4f2a\u56fe\u50cf\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff0c\u7ed3\u5408\u53cc\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u8de8\u4e2a\u4f53\u9c81\u68d2\u7684\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u3002", "motivation": "\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u53d7\u9650\u4e8e\u4e2a\u4f53\u95f4\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u8de8\u88ab\u8bd5\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u6a21\u6001\u7279\u5b9a\u5f52\u7eb3\u504f\u89c1\u96be\u4ee5\u5b66\u4e60\u901a\u7528\u4e0d\u53d8\u8868\u5f81\u3002", "method": "\u63d0\u51faTS-P$^2$CL\uff0c\u5c061D\u751f\u7406\u4fe1\u53f7\u8f6c\u5316\u4e3a2D\u4f2a\u56fe\u50cf\uff0c\u5f15\u5165\u89c6\u89c9\u5f15\u5bfc\u8303\u5f0f\uff0c\u5e76\u91c7\u7528\u53cc\u5bf9\u6bd4\u5b66\u4e60\uff08\u6a21\u5185\u4e00\u81f4\u6027\u4e0e\u8de8\u6a21\u6001\u5bf9\u9f50\uff09\u6765\u5b66\u4e60\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5728\u516d\u4e2a\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cTS-P$^2$CL\u5728\u88ab\u8bd5\u4f9d\u8d56\u548c\u72ec\u7acb\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u5341\u56db\u79cd\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TS-P$^2$CL\u901a\u8fc7\u8de8\u6a21\u6001\u8fc1\u79fb\u548c\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u514b\u670d\u4e2a\u4f53\u5dee\u5f02\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.17805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17805", "abs": "https://arxiv.org/abs/2509.17805", "authors": ["Dong Chen", "Huili Peng", "Yong Hu", "Kenneth MC. Cheung"], "title": "Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric Assessment of 2D Projections", "comment": null, "summary": "Objective: To systematically quantify the effect of the camera view (frontal\nvs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D\nmotion capture ground truth. Methods: Gait data from 18 subjects were recorded\nsimultaneously using frontal, lateral and 3D motion capture systems. Pose\nestimation used YOLOv8. Four metrics were assessed to evaluate agreement:\nDynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation\n(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution\ndifferences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank\ntests (significance: $p < 0.05$) and Cliff's delta ($\\delta$) were used to\nmeasure statistical differences and effect sizes. Results: Lateral views\nsignificantly outperformed frontal views for sagittal plane kinematics: step\nlength (DTW: $53.08 \\pm 24.50$ vs. $69.87 \\pm 25.36$, $p = 0.005$) and knee\nrotation (DTW: $106.46 \\pm 38.57$ vs. $155.41 \\pm 41.77$, $p = 0.004$). Frontal\nviews were superior for symmetry parameters: trunk rotation (KLD: $0.09 \\pm\n0.06$ vs. $0.30 \\pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:\n$105.77 \\pm 29.72$ vs. $75.20 \\pm 20.38$, $p = 0.003$). Effect sizes were\nmedium-to-large ($\\delta: 0.34$--$0.76$). Conclusion: Camera view critically\nimpacts gait parameter accuracy. Lateral views are optimal for sagittal\nkinematics; frontal views excel for trunk symmetry. Significance: This first\nsystematic evidence enables data-driven camera deployment in 2D gait analysis,\nenhancing clinical utility. Future implementations should leverage both views\nvia disease-oriented setups.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6b63\u9762\u548c\u4fa7\u9762\u89c6\u89d2\u5bf9\u57fa\u4e8e2D\u65e0\u6807\u8bb0\u6b65\u6001\u5206\u6790\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4fa7\u9762\u89c6\u89d2\u5728\u77e2\u72b6\u9762\u8fd0\u52a8\u5b66\u53c2\u6570\uff08\u5982\u6b65\u957f\u3001\u819d\u5173\u8282\u65cb\u8f6c\uff09\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u6b63\u9762\u89c6\u89d2\u5728\u8eaf\u5e72\u5bf9\u79f0\u6027\u53c2\u6570\u4e0a\u66f4\u5177\u4f18\u52bf\uff0c\u5efa\u8bae\u4e34\u5e8a\u5e94\u7528\u4e2d\u6839\u636e\u75be\u75c5\u7279\u5f81\u7ed3\u5408\u53cc\u89c6\u89d2\u8fdb\u884c\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u4e3a\u4e86\u660e\u786e\u4e0d\u540c\u6444\u50cf\u673a\u89c6\u89d2\uff08\u6b63\u9762 vs. \u4fa7\u9762\uff09\u5bf92D\u65e0\u6807\u8bb0\u6b65\u6001\u5206\u6790\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u6444\u50cf\u5934\u7684\u90e8\u7f72\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002", "method": "\u4f7f\u7528YOLOv8\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\uff0c\u540c\u6b65\u91c7\u96c618\u540d\u53d7\u8bd5\u8005\u7684\u6b63\u9762\u3001\u4fa7\u9762\u53ca3D\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff0c\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u3001\u6700\u5927\u4e92\u76f8\u5173\uff08MCC\uff09\u3001KL\u6563\u5ea6\uff08KLD\uff09\u548c\u4fe1\u606f\u71b5\uff08IE\uff09\u56db\u79cd\u6307\u6807\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7Wilcoxon\u7b26\u53f7\u79e9\u68c0\u9a8c\u548cCliff's delta\u5206\u6790\u7edf\u8ba1\u5dee\u5f02\u4e0e\u6548\u5e94\u91cf\u3002", "result": "\u4fa7\u9762\u89c6\u89d2\u5728\u6b65\u957f\uff08DTW: p=0.005\uff09\u548c\u819d\u5173\u8282\u65cb\u8f6c\uff08DTW: p=0.004\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u6b63\u9762\u89c6\u89d2\uff1b\u6b63\u9762\u89c6\u89d2\u5728\u8eaf\u5e72\u65cb\u8f6c\uff08KLD: p<0.001\uff09\u548c\u8155-\u9acb\u4e2d\u70b9\u8ddd\u79bb\uff08MCC: p=0.003\uff09\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u6548\u5e94\u91cf\u4e3a\u4e2d\u5230\u5927\uff08\u03b4: 0.34\u20130.76\uff09\u3002", "conclusion": "\u6444\u50cf\u673a\u89c6\u89d2\u663e\u8457\u5f71\u54cd\u6b65\u6001\u53c2\u6570\u7684\u51c6\u786e\u6027\uff0c\u4fa7\u9762\u9002\u5408\u77e2\u72b6\u9762\u8fd0\u52a8\u5206\u6790\uff0c\u6b63\u9762\u66f4\u9002\u5408\u5bf9\u79f0\u6027\u8bc4\u4f30\uff0c\u672a\u6765\u5e94\u6839\u636e\u75be\u75c5\u7279\u70b9\u8bbe\u8ba1\u591a\u89c6\u89d2\u878d\u5408\u7684\u91c7\u96c6\u65b9\u6848\u3002"}}
{"id": "2509.17943", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17943", "abs": "https://arxiv.org/abs/2509.17943", "authors": ["Romain Thoreau", "Jessie Levillain", "Dawa Derksen"], "title": "Can multimodal representation learning by alignment preserve modality-specific information?", "comment": "Accepted as a workshop paper at MACLEAN - ECML/PKDD 2025", "summary": "Combining multimodal data is a key issue in a wide range of machine learning\ntasks, including many remote sensing problems. In Earth observation, early\nmultimodal data fusion methods were based on specific neural network\narchitectures and supervised learning. Ever since, the scarcity of labeled data\nhas motivated self-supervised learning techniques. State-of-the-art multimodal\nrepresentation learning techniques leverage the spatial alignment between\nsatellite data from different modalities acquired over the same geographic area\nin order to foster a semantic alignment in the latent space. In this paper, we\ninvestigate how this methods can preserve task-relevant information that is not\nshared across modalities. First, we show, under simplifying assumptions, when\nalignment strategies fundamentally lead to an information loss. Then, we\nsupport our theoretical insight through numerical experiments in more realistic\nsettings. With those theoretical and empirical evidences, we hope to support\nnew developments in contrastive learning for the combination of multimodal\nsatellite data. Our code and data is publicly available at\nhttps://github.com/Romain3Ch216/alg_maclean_25.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u536b\u661f\u6570\u636e\u878d\u5408\u4e2d\u5bf9\u9f50\u7b56\u7565\u5bfc\u81f4\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u7559\u6a21\u6001\u7279\u6709\u4fe1\u606f\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u5728\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u878d\u5408\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "motivation": "\u7531\u4e8e\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u878d\u5408\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u5bf9\u9f50\u6765\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u53ef\u80fd\u4e22\u5931\u6a21\u6001\u7279\u6709\u7684\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u7b80\u5316\u5047\u8bbe\u4e0b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63a2\u8ba8\u5bf9\u9f50\u7b56\u7565\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u7684\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u66f4\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u53d1\u73b0\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u5bf9\u9f50\u7b56\u7565\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f1a\u6839\u672c\u6027\u5730\u5bfc\u81f4\u4efb\u52a1\u76f8\u5173\u975e\u5171\u4eab\u4fe1\u606f\u7684\u635f\u5931\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u80fd\u66f4\u597d\u4fdd\u7559\u6a21\u6001\u7279\u6709\u4fe1\u606f\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2509.17816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17816", "abs": "https://arxiv.org/abs/2509.17816", "authors": ["Brown Ebouky", "Ajad Chhatkuli", "Cristiano Malossi", "Christoph Studer", "Roy Assaf", "Andrea Bartezzaghi"], "title": "Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training", "comment": "24 pages, 5 figures", "summary": "Self-supervised learning (SSL) has emerged as a central paradigm for training\nfoundation models by leveraging large-scale unlabeled datasets, often producing\nrepresentations with strong generalization capabilities. These models are\ntypically pre-trained on general-purpose datasets such as ImageNet and\nsubsequently adapted to various downstream tasks through finetuning. While\nrecent advances have explored parameter-efficient strategies for adapting\npre-trained models, extending SSL pre-training itself to new domains -\nparticularly under limited data regimes and for dense prediction tasks -\nremains underexplored. In this work, we address the problem of adapting vision\nfoundation models to new domains in an unsupervised and data-efficient manner,\nspecifically targeting downstream semantic segmentation. We propose GLARE\n(Global Local and Regional Enforcement), a novel continual self-supervised\npre-training task designed to enhance downstream segmentation performance.\nGLARE introduces patch-level augmentations to encourage local consistency and\nincorporates a regional consistency constraint that leverages spatial semantics\nin the data. For efficient continual pre-training, we initialize Vision\nTransformers (ViTs) with weights from existing SSL models and update only\nlightweight adapter modules - specifically UniAdapter - while keeping the rest\nof the backbone frozen. Experiments across multiple semantic segmentation\nbenchmarks on different domains demonstrate that GLARE consistently improves\ndownstream performance with minimal computational and parameter overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGLARE\u7684\u6301\u7eed\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u533a\u57df\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u6709\u6548\u63d0\u5347\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u65b0\u9886\u57df\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u5728\u901a\u7528\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u96be\u4ee5\u9ad8\u6548\u9002\u5e94\u65b0\u9886\u57df\u5c24\u5176\u662f\u6570\u636e\u7a00\u7f3a\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u4e9f\u9700\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u4e14\u65e0\u76d1\u7763\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86GLARE\u6846\u67b6\uff0c\u5f15\u5165patch\u7ea7\u589e\u5f3a\u4fdd\u8bc1\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u7ed3\u5408\u5229\u7528\u7a7a\u95f4\u8bed\u4e49\u7684\u533a\u57df\u4e00\u81f4\u6027\u7ea6\u675f\uff1b\u91c7\u7528\u5df2\u6709\u7684SSL\u6a21\u578b\u521d\u59cb\u5316ViT\uff0c\u5e76\u4ec5\u66f4\u65b0\u8f7b\u91cf\u9002\u914d\u6a21\u5757\uff08UniAdapter\uff09\uff0c\u4fdd\u6301\u4e3b\u5e72\u7f51\u7edc\u51bb\u7ed3\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cGLARE\u5728\u6781\u4f4e\u8ba1\u7b97\u548c\u53c2\u6570\u5f00\u9500\u4e0b\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "GLARE\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2509.17818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17818", "abs": "https://arxiv.org/abs/2509.17818", "authors": ["Yiyang Chen", "Xuanhua He", "Xiujun Ma", "Yue Ma"], "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment", "comment": "The project page is at https://yychen233.github.io/ContextFlow-page", "summary": "Training-free video object editing aims to achieve precise object-level\nmanipulation, including object insertion, swapping, and deletion. However, it\nfaces significant challenges in maintaining fidelity and temporal consistency.\nExisting methods, often designed for U-Net architectures, suffer from two\nprimary limitations: inaccurate inversion due to first-order solvers, and\ncontextual conflicts caused by crude \"hard\" feature replacement. These issues\nare more challenging in Diffusion Transformers (DiTs), where the unsuitability\nof prior layer-selection heuristics makes effective guidance challenging. To\naddress these limitations, we introduce ContextFlow, a novel training-free\nframework for DiT-based video object editing. In detail, we first employ a\nhigh-order Rectified Flow solver to establish a robust editing foundation. The\ncore of our framework is Adaptive Context Enrichment (for specifying what to\nedit), a mechanism that addresses contextual conflicts. Instead of replacing\nfeatures, it enriches the self-attention context by concatenating Key-Value\npairs from parallel reconstruction and editing paths, empowering the model to\ndynamically fuse information. Additionally, to determine where to apply this\nenrichment (for specifying where to edit), we propose a systematic, data-driven\nanalysis to identify task-specific vital layers. Based on a novel Guidance\nResponsiveness Metric, our method pinpoints the most influential DiT blocks for\ndifferent tasks (e.g., insertion, swapping), enabling targeted and highly\neffective guidance. Extensive experiments show that ContextFlow significantly\noutperforms existing training-free methods and even surpasses several\nstate-of-the-art training-based approaches, delivering temporally coherent,\nhigh-fidelity results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContextFlow\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5bf9\u8c61\u7f16\u8f91\u6846\u67b6\uff0c\u57fa\u4e8e\u6269\u6563Transformer\uff08DiT\uff09\uff0c\u901a\u8fc7\u9ad8\u9636Rectified Flow\u6c42\u89e3\u5668\u548c\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u589e\u5f3a\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5728U-Net\u67b6\u6784\u4e0b\u5b58\u5728\u53cd\u6f14\u4e0d\u51c6\u786e\u548c\u4e0a\u4e0b\u6587\u51b2\u7a81\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u9002\u7528\u4e8eDiT\u67b6\u6784\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u5c42\u9009\u62e9\u6307\u5bfc\u7b56\u7565\u3002", "method": "\u91c7\u7528\u9ad8\u9636Rectified Flow\u6c42\u89e3\u5668\u63d0\u5347\u53cd\u6f14\u8d28\u91cf\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u589e\u5f3a\u673a\u5236\uff0c\u901a\u8fc7\u5e76\u884c\u8def\u5f84\u62fc\u63a5Key-Value\u5bf9\u6765\u4e30\u5bcc\u81ea\u6ce8\u610f\u529b\u4e0a\u4e0b\u6587\uff0c\u907f\u514d\u786c\u6027\u7279\u5f81\u66ff\u6362\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u5f15\u5bfc\u54cd\u5e94\u5ea6\u91cf\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u7684\u5173\u952eDiT\u5c42\u8fdb\u884c\u7cbe\u51c6\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cContextFlow\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u8d85\u8d8a\u591a\u4e2a\u9700\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5bf9\u8c61\u63d2\u5165\u3001\u4ea4\u6362\u548c\u5220\u9664\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u4e14\u65f6\u5e8f\u4e00\u81f4\u7684\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "ContextFlow\u4e3a\u57fa\u4e8eDiT\u7684\u65e0\u9700\u8bad\u7ec3\u89c6\u9891\u5bf9\u8c61\u7f16\u8f91\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u51b2\u7a81\u4e0e\u5f15\u5bfc\u65e0\u6548\u95ee\u9898\uff0c\u5177\u5907\u8f83\u5f3a\u7684\u901a\u7528\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.17847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17847", "abs": "https://arxiv.org/abs/2509.17847", "authors": ["Saghir Alfasly", "Wataru Uegami", "MD Enamul Hoq", "Ghazal Alabtah", "H. R. Tizhoosh"], "title": "Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology", "comment": "NeurIPS 2025", "summary": "Synthetic data generation in histopathology faces unique challenges:\npreserving tissue heterogeneity, capturing subtle morphological features, and\nscaling to unannotated datasets. We present a latent diffusion model that\ngenerates realistic heterogeneous histopathology images through a novel\ndual-conditioning approach combining semantic segmentation maps with\ntissue-specific visual crops. Unlike existing methods that rely on text prompts\nor abstract visual embeddings, our approach preserves critical morphological\ndetails by directly incorporating raw tissue crops from corresponding semantic\nregions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches\nensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we\nintroduce a self-supervised extension that clusters whole-slide images into 100\ntissue types using foundation model embeddings, automatically generating\npseudo-semantic maps for training. Our method synthesizes high-fidelity images\nwith precise region-wise annotations, achieving superior performance on\ndownstream segmentation tasks. When evaluated on annotated datasets, models\ntrained on our synthetic data show competitive performance to those trained on\nreal data, demonstrating the utility of controlled heterogeneous tissue\ngeneration. In quantitative evaluation, prompt-guided synthesis reduces Frechet\nDistance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower\nFD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on\nsynthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within\n1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA\nwhole-slide images without manual annotations, our framework offers a practical\nsolution for an urgent need for generating diverse, annotated histopathology\ndata, addressing a critical bottleneck in computational pathology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u53cc\u6761\u4ef6\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u5408\u6210\u5177\u6709\u7ec4\u7ec7\u5f02\u8d28\u6027\u548c\u7cbe\u7ec6\u5f62\u6001\u7279\u5f81\u7684\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u56fe\u4e0e\u7ec4\u7ec7\u7279\u5f02\u6027\u89c6\u89c9\u5757\uff0c\u5728\u6807\u6ce8\u548c\u65e0\u6807\u6ce8\u6570\u636e\u4e0a\u5747\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u75c5\u7406\u5b66\u56fe\u50cf\u5408\u6210\u9762\u4e34\u4fdd\u6301\u7ec4\u7ec7\u5f02\u8d28\u6027\u3001\u6355\u6349\u7ec6\u5fae\u5f62\u6001\u7279\u5f81\u548c\u6269\u5c55\u5230\u65e0\u6807\u6ce8\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u7559\u5173\u952e\u5f62\u6001\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53cc\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u56fe\u4e0e\u6765\u81ea\u5bf9\u5e94\u533a\u57df\u7684\u5b9e\u9645\u7ec4\u7ec7\u56fe\u50cf\u5757\u8fdb\u884c\u751f\u6210\uff1b\u5bf9\u4e8e\u6807\u6ce8\u6570\u636e\u4f7f\u7528\u542b20-80%\u7ec4\u7ec7\u5f02\u8d28\u6027\u7684\u8865\u4e01\uff0c\u5bf9\u4e8e\u65e0\u6807\u6ce8\u6570\u636e\uff08\u5982TCGA\uff09\u5219\u5229\u7528\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u8fdb\u884c\u81ea\u76d1\u7763\u805a\u7c7b\uff0c\u751f\u6210\u4f2a\u8bed\u4e49\u56fe\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728Camelyon16\u548cPanda\u6570\u636e\u96c6\u4e0a\uff0c\u751f\u6210\u56fe\u50cf\u4f7f\u4e0b\u6e38DeepLabv3+\u6a21\u578b\u5206\u522b\u8fbe\u52300.71\u548c0.95\u7684IoU\uff0c\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7ed3\u679c\uff1bFrechet\u8ddd\u79bb\u6700\u591a\u964d\u4f4e6\u500d\uff08Camelyon16\u4ece430.1\u964d\u81f372.0\uff09\uff1b\u5e76\u572811,765\u5f20TCGA\u5168\u5207\u7247\u56fe\u50cf\u4e0a\u6210\u529f\u6269\u5c55\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5e26\u7cbe\u786e\u533a\u57df\u6807\u6ce8\u7684\u5f02\u8d28\u6027\u75c5\u7406\u56fe\u50cf\uff0c\u652f\u6301\u5728\u65e0\u6807\u6ce8\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u7684\u6269\u5c55\uff0c\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.17864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17864", "abs": "https://arxiv.org/abs/2509.17864", "authors": ["Shi Chen", "Erik Sandstr\u00f6m", "Sandro Lombardi", "Siyuan Li", "Martin R. Oswald"], "title": "ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos", "comment": null, "summary": "Achieving truly practical dynamic 3D reconstruction requires online\noperation, global pose and map consistency, detailed appearance modeling, and\nthe flexibility to handle both RGB and RGB-D inputs. However, existing SLAM\nmethods typically merely remove the dynamic parts or require RGB-D input, while\noffline methods are not scalable to long video sequences, and current\ntransformer-based feedforward methods lack global consistency and appearance\ndetails. To this end, we achieve online dynamic scene reconstruction by\ndisentangling the static and dynamic parts within a SLAM system. The poses are\ntracked robustly with a novel motion masking strategy, and dynamic parts are\nreconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.\nOur method yields novel view renderings competitive to offline methods and\nachieves on-par tracking with state-of-the-art dynamic SLAM methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728SLAM\u7cfb\u7edf\u4e2d\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u90e8\u5206\uff0c\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u548c\u9ad8\u8d28\u91cf\u5916\u89c2\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709SLAM\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u52a8\u6001\u90e8\u5206\u6216\u4f9d\u8d56RGB-D\u8f93\u5165\uff0c\u79bb\u7ebf\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u957f\u5e8f\u5217\uff0c\u800c\u57fa\u4e8eTransformer\u7684\u524d\u9988\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u652f\u6301\u5728\u7ebf\u64cd\u4f5c\u3001\u5177\u5907\u5168\u5c40\u4e00\u81f4\u6027\u4e14\u80fd\u5904\u7406RGB/RGB-D\u8f93\u5165\u7684\u52a8\u6001\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u5728SLAM\u6846\u67b6\u5185\u89e3\u8026\u9759\u6001\u4e0e\u52a8\u6001\u6210\u5206\uff1b\u91c7\u7528\u65b0\u7684\u8fd0\u52a8\u63a9\u7801\u7b56\u7565\u8fdb\u884c\u9c81\u68d2\u59ff\u6001\u8ddf\u8e2a\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u5f0f\u9002\u5e94\u7684Motion Scaffolds\u56fe\u6765\u91cd\u5efa\u52a8\u6001\u90e8\u5206\u3002", "result": "\u5728\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u6027\u80fd\u63a5\u8fd1\u79bb\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u52a8\u6001SLAM\u59ff\u6001\u8ddf\u8e2a\u4e0a\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u517c\u987e\u6548\u7387\u3001\u4e00\u81f4\u6027\u4e0e\u7ec6\u8282\u7684\u5728\u7ebf\u52a8\u60013D\u91cd\u5efa\uff0c\u652f\u6301RGB\u548cRGB-D\u8f93\u5165\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u4f4d\u59ff\u7cbe\u5ea6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.17888", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17888", "abs": "https://arxiv.org/abs/2509.17888", "authors": ["Divya Mereddy", "Marcos Quinones-Grueiro", "Ashwin T S", "Eduardo Davalos", "Gautam Biswas", "Kent Etherton", "Tyler Davis", "Katelyn Kay", "Jill Lear", "Benjamin Goldberg"], "title": "Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training", "comment": null, "summary": "This study examines how Critical Care Air Transport Team (CCATT) members are\ntrained using mixed-reality simulations that replicate the high-pressure\nconditions of aeromedical evacuation. Each team - a physician, nurse, and\nrespiratory therapist - must stabilize severely injured soldiers by managing\nventilators, IV pumps, and suction devices during flight. Proficient\nperformance requires clinical expertise and cognitive skills, such as\nsituational awareness, rapid decision-making, effective communication, and\ncoordinated task management, all of which must be maintained under stress.\nRecent advances in simulation and multimodal data analytics enable more\nobjective and comprehensive performance evaluation. In contrast, traditional\ninstructor-led assessments are subjective and may overlook critical events,\nthereby limiting generalizability and consistency. However, AI-based automated\nand more objective evaluation metrics still demand human input to train the AI\nalgorithms to assess complex team dynamics in the presence of environmental\nnoise and the need for accurate re-identification in multi-person tracking. To\naddress these challenges, we introduce a systematic, data-driven assessment\nframework that combines Cognitive Task Analysis (CTA) with Multimodal Learning\nAnalytics (MMLA). We have developed a domain-specific CTA model for CCATT\ntraining and a vision-based action recognition pipeline using a fine-tuned\nHuman-Object Interaction model, the Cascade Disentangling Network (CDN), to\ndetect and track trainee-equipment interactions over time. These interactions\nautomatically yield performance indicators (e.g., reaction time, task\nduration), which are mapped onto a hierarchical CTA model tailored to CCATT\noperations, enabling interpretable, domain-relevant performance evaluations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ba4\u77e5\u4efb\u52a1\u5206\u6790\uff08CTA\uff09\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\u5206\u6790\uff08MMLA\uff09\u7684\u6570\u636e\u9a71\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6df7\u5408\u73b0\u5b9e\u6a21\u62df\u4e2d\u5ba2\u89c2\u8bc4\u4f30\u91cd\u75c7\u76d1\u62a4\u7a7a\u8fd0\u56e2\u961f\uff08CCATT\uff09\u7684\u8bad\u7ec3\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u6559\u5b98\u4e3b\u5bfc\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u89c2\u6027\u5f3a\uff0c\u6613\u9057\u6f0f\u5173\u952e\u4e8b\u4ef6\uff0c\u7f3a\u4e4f\u4e00\u81f4\u6027\u548c\u53ef\u63a8\u5e7f\u6027\uff0c\u4e14\u73b0\u6709AI\u8bc4\u4f30\u65b9\u6cd5\u4ecd\u9700\u4eba\u5de5\u8f93\u5165\u6765\u5e94\u5bf9\u590d\u6742\u56e2\u961f\u52a8\u6001\u548c\u73af\u5883\u566a\u58f0\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u624b\u6bb5\u3002", "method": "\u5f00\u53d1\u4e86\u9488\u5bf9CCATT\u8bad\u7ec3\u7684\u9886\u57df\u7279\u5b9a\u8ba4\u77e5\u4efb\u52a1\u5206\u6790\uff08CTA\uff09\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u57fa\u4e8e\u89c6\u89c9\u7684\u4eba-\u7269\u4ea4\u4e92\u8bc6\u522b\u6d41\u7a0b\uff0c\u91c7\u7528\u5fae\u8c03\u7684\u7ea7\u8054\u89e3\u8026\u7f51\u7edc\uff08CDN\uff09\u68c0\u6d4b\u548c\u8ffd\u8e2a\u5b66\u5458\u4e0e\u8bbe\u5907\u7684\u4ea4\u4e92\uff1b\u5c06\u63d0\u53d6\u7684\u4ea4\u4e92\u884c\u4e3a\u8f6c\u5316\u4e3a\u53cd\u5e94\u65f6\u95f4\u3001\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u7b49\u7ee9\u6548\u6307\u6807\uff0c\u5e76\u6620\u5c04\u5230\u5206\u5c42CTA\u6a21\u578b\u4e2d\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9CCATT\u56e2\u961f\u5728\u9ad8\u538b\u529b\u7a7a\u4e2d\u533b\u7597\u540e\u9001\u6a21\u62df\u4e2d\u7684\u884c\u4e3a\u81ea\u52a8\u8bc6\u522b\u4e0e\u91cf\u5316\u8bc4\u4f30\uff0c\u751f\u6210\u4e86\u4e0e\u9886\u57df\u76f8\u5173\u7684\u3001\u53ef\u89e3\u91ca\u7684\u7ee9\u6548\u6307\u6807\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u5ba2\u89c2\u6027\u548c\u5168\u9762\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CTA\u4e0eMMLA\u7ed3\u5408\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u590d\u6742\u533b\u7597\u56e2\u961f\u5728\u6df7\u5408\u73b0\u5b9e\u6a21\u62df\u4e2d\u7684\u81ea\u52a8\u5316\u3001\u6570\u636e\u9a71\u52a8\u6027\u80fd\u8bc4\u4f30\uff0c\u4e3a\u63d0\u5347\u56e2\u961f\u8bad\u7ec3\u8d28\u91cf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2509.17901", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17901", "abs": "https://arxiv.org/abs/2509.17901", "authors": ["Geewook Kim", "Minjoon Seo"], "title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?", "comment": "5 pages, 2 figures, under review. Project page:\n  https://github.com/naver-ai/LLaVA-AV-SSM", "summary": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u97f3\u9891\u5728\u73b0\u4ee3\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video-LLMs\uff09\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528\uff0c\u53d1\u73b0\u5c3d\u7ba1\u8bb8\u591a\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u7565\u4e86\u97f3\u9891\uff0c\u4f46\u5728\u97f3\u9891\u654f\u611f\u7684\u4efb\u52a1\u4e2d\u97f3\u9891\u81f3\u5173\u91cd\u8981\u3002\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u7ed3\u5408\u97f3\u9891\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u538b\u7f29\u6a21\u5757\u7684\u6a21\u578b\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u7684\u8bc4\u6d4b\u96c6AVQA-Hard\u548cMusic-AVQA-Hard\uff0c\u4ee5\u63a8\u52a8\u66f4\u771f\u5b9e\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u591a\u6570\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u5ffd\u7565\u97f3\u9891\uff0c\u5bfc\u81f4\u5bf9\u6a21\u578b\u771f\u5b9e\u591a\u6a21\u6001\u80fd\u529b\u7684\u8bef\u5224\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u97f3\u9891\u5728\u5b9e\u9645\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8eLLaVA-OneVision\u67b6\u6784\uff0c\u96c6\u6210Whisper\u7b49\u97f3\u9891\u7f16\u7801\u5668\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u97f3\u9891\u4ee4\u724c\u538b\u7f29\uff0c\u4ee5\u89e3\u51b3\u97f3\u9891token\u81a8\u80c0\u95ee\u9898\uff1b\u5728\u6807\u51c6\u53ca\u65b0\u6784\u5efa\u7684\u97f3\u9891\u654f\u611f\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5728\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e0a\u97f3\u9891\u5e26\u6765\u7684\u589e\u76ca\u6709\u9650\uff0c\u4f46\u5728\u4f5c\u8005\u6784\u5efa\u7684\u97f3\u9891\u654f\u611f\u5b50\u96c6\uff08\u5982AVQA-Hard\u548cMusic-AVQA-Hard\uff09\u4e0a\u97f3\u9891\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8bc1\u660e\u97f3\u9891\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u51b3\u5b9a\u6027\u4f5c\u7528\u3002", "conclusion": "\u5f53\u524dVideo-LLM\u8bc4\u4f30\u4e25\u91cd\u4f4e\u4f30\u4e86\u97f3\u9891\u7684\u4f5c\u7528\uff0c\u9700\u5efa\u7acb\u66f4\u5177\u6311\u6218\u6027\u7684\u97f3\u9891-\u89c6\u89c9\u4efb\u52a1\u6765\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u80fd\u529b\uff1b\u672c\u6587\u63d0\u4f9b\u7684\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u97f3\u89c6\u9891\u5927\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2509.17925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17925", "abs": "https://arxiv.org/abs/2509.17925", "authors": ["Yuanhan Wang", "Yifei Chen", "Shuo Jiang", "Wenjing Yu", "Mingxuan Liu", "Beining Wu", "Jinying Zong", "Feiwei Qin", "Changmiao Wang", "Qiyuan Tian"], "title": "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI", "comment": "11 pages, 6 figures", "summary": "Reliable brain tumor segmentation in MRI is indispensable for treatment\nplanning and outcome monitoring, yet models trained on curated benchmarks often\nfail under domain shifts arising from scanner and protocol variability as well\nas population heterogeneity. Such gaps are especially severe in low-resource\nand pediatric cohorts, where conventional test-time or source-free adaptation\nstrategies often suffer from instability and structural inconsistency. We\npropose SmaRT, a style-modulated robust test-time adaptation framework that\nenables source-free cross-domain generalization. SmaRT integrates style-aware\naugmentation to mitigate appearance discrepancies, a dual-branch momentum\nstrategy for stable pseudo-label refinement, and structural priors enforcing\nconsistency, integrity, and connectivity. This synergy ensures both adaptation\nstability and anatomical fidelity under extreme domain shifts. Extensive\nevaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT\nconsistently outperforms state-of-the-art methods, with notable gains in Dice\naccuracy and boundary precision. Overall, SmaRT bridges the gap between\nalgorithmic advances and equitable clinical applicability, supporting robust\ndeployment of MRI-based neuro-oncology tools in diverse clinical environments.\nOur source code is available at https://github.com/baiyou1234/SmaRT.", "AI": {"tldr": "SmaRT\u662f\u4e00\u79cd\u98ce\u683c\u8c03\u5236\u7684\u9c81\u68d2\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u65e0\u6e90\u8de8\u57df\u6cdb\u5316\uff0c\u63d0\u5347\u8111\u80bf\u7624MRI\u5206\u5272\u5728\u4e0d\u540c\u57df\u95f4\u7684\u7a33\u5b9a\u6027\u548c\u89e3\u5256\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9762\u5bf9\u626b\u63cf\u4eea\u3001\u534f\u8bae\u5dee\u5f02\u53ca\u4eba\u7fa4\u5f02\u8d28\u6027\u5e26\u6765\u7684\u57df\u504f\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u6709\u9650\u548c\u513f\u79d1\u7fa4\u4f53\u4e2d\uff0c\u4f20\u7edf\u81ea\u9002\u5e94\u65b9\u6cd5\u5e38\u4e0d\u7a33\u5b9a\u4e14\u7ed3\u6784\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faSmaRT\u6846\u67b6\uff0c\u7ed3\u5408\u98ce\u683c\u611f\u77e5\u589e\u5f3a\u7f13\u89e3\u5916\u89c2\u5dee\u5f02\uff0c\u91c7\u7528\u53cc\u5206\u652f\u52a8\u91cf\u7b56\u7565\u7a33\u5b9a\u4f2a\u6807\u7b7e\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u7ed3\u6784\u5148\u9a8c\u4fdd\u8bc1\u4e00\u81f4\u6027\u3001\u5b8c\u6574\u6027\u548c\u8fde\u901a\u6027\u3002", "result": "\u5728\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u548c\u513f\u79d1\u80f6\u8d28\u7624\u6570\u636e\u96c6\u4e0a\uff0cSmaRT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cDice\u51c6\u786e\u7387\u548c\u8fb9\u754c\u7cbe\u5ea6\u5747\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "SmaRT\u6709\u6548\u5f25\u5408\u4e86\u7b97\u6cd5\u8fdb\u6b65\u4e0e\u4e34\u5e8a\u516c\u5e73\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u652f\u6301MRI\u795e\u7ecf\u80bf\u7624\u5b66\u5de5\u5177\u5728\u591a\u6837\u5316\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u7a33\u5065\u90e8\u7f72\u3002"}}
{"id": "2509.17931", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.17931", "abs": "https://arxiv.org/abs/2509.17931", "authors": ["Zhuo Xiao", "Fugen Zhou", "Jingjing Wang", "Chongyu He", "Bo Liu", "Haitao Sun", "Zhe Ji", "Yuliang Jiang", "Junjie Wang", "Qiuwen Wu"], "title": "Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on Tip-handle Detection and Matching", "comment": null, "summary": "Accurate multi-needle localization in intraoperative CT images is crucial for\noptimizing seed placement in pelvic seed implant brachytherapy. However, this\ntask is challenging due to poor image contrast and needle adhesion. This paper\npresents a novel approach that reframes needle localization as a tip-handle\ndetection and matching problem to overcome these difficulties. An anchor-free\nnetwork, based on HRNet, is proposed to extract multi-scale features and\naccurately detect needle tips and handles by predicting their centers and\norientations using decoupled branches for heatmap regression and polar angle\nprediction. To associate detected tips and handles into individual needles, a\ngreedy matching and merging (GMM) method designed to solve the unbalanced\nassignment problem with constraints (UAP-C) is presented. The GMM method\niteratively selects the most probable tip-handle pairs and merges them based on\na distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100\npatients, the proposed method demonstrates superior performance, achieving\nhigher precision and F1 score compared to a segmentation-based method utilizing\nthe nnUNet model,thereby offering a more robust and accurate solution for\nneedle localization in complex clinical scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u951a\u70b9\u81ea\u7531\u7f51\u7edc\u548c\u8d2a\u5fc3\u5339\u914d\u878d\u5408\u65b9\u6cd5\u7684\u65b0\u578b\u591a\u9488\u5b9a\u4f4d\u6846\u67b6\uff0c\u5c06\u9488\u5b9a\u4f4d\u95ee\u9898\u8f6c\u5316\u4e3a\u9488\u5c16\u4e0e\u9488\u67c4\u68c0\u6d4b\u4e0e\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672f\u4e2dCT\u56fe\u50cf\u4e0b\u76c6\u8154\u79cd\u690d\u7c92\u5b50\u6cbb\u7597\u4e2d\u7684\u9488\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u672f\u4e2dCT\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u4f4e\u4e14\u5b58\u5728\u9488\u5177\u7c98\u8fde\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5b9e\u73b0\u591a\u9488\u5b9a\u4f4d\uff0c\u5f71\u54cd\u79cd\u5b50\u690d\u5165\u7684\u7cbe\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eHRNet\u7684\u65e0\u951a\u70b9\u7f51\u7edc\uff0c\u901a\u8fc7\u89e3\u8026\u5206\u652f\u8fdb\u884c\u70ed\u56fe\u56de\u5f52\u548c\u6781\u89d2\u9884\u6d4b\uff0c\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u4ee5\u68c0\u6d4b\u9488\u5c16\u548c\u9488\u67c4\uff1b\u8bbe\u8ba1\u8d2a\u5fc3\u5339\u914d\u4e0e\u878d\u5408\uff08GMM\uff09\u65b9\u6cd5\u89e3\u51b3\u5e26\u7ea6\u675f\u7684\u4e0d\u5e73\u8861\u5206\u914d\u95ee\u9898\uff08UAP-C\uff09\uff0c\u901a\u8fc7\u8ddd\u79bb\u5ea6\u91cf\u8fed\u4ee3\u5339\u914d\u5e76\u5408\u5e76\u9488\u5c16-\u9488\u67c4\u5bf9\u4ee5\u91cd\u5efa3D\u9488\u9053\u3002", "result": "\u5728100\u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8ennUNet\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u591a\u9488\u4f4d\u7f6e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u9488\u5b9a\u4f4d\u8f6c\u5316\u4e3a\u68c0\u6d4b\u4e0e\u5339\u914d\u95ee\u9898\uff0c\u7ed3\u5408\u9ad8\u6027\u80fd\u7f51\u7edc\u7ed3\u6784\u4e0e\u4f18\u5316\u5339\u914d\u7b56\u7565\uff0c\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u3001\u66f4\u7a33\u5065\u7684\u591a\u9488\u5b9a\u4f4d\uff0c\u5177\u6709\u826f\u597d\u7684\u4e34\u5e8a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17951", "categories": ["cs.CV", "I.5.4"], "pdf": "https://arxiv.org/pdf/2509.17951", "abs": "https://arxiv.org/abs/2509.17951", "authors": ["Kai Li", "Xingxing Weng", "Yupeng Deng", "Yu Meng", "Chao Pang", "Gui-Song Xia", "Xiangyu Zhao"], "title": "DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels", "comment": "17 Pages", "summary": "Extracting polygonal roofs and footprints from remote sensing images is\ncritical for large-scale urban analysis. Most existing methods rely on\nsegmentation-based models that assume clear semantic boundaries of roofs, but\nthese approaches struggle in off- nadir images, where the roof and footprint\nare significantly displaced, and facade pixels are fused with the roof\nboundary. With the increasing availability of open vector map annotations,\ne.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation\nhas become viable because remote sensing images are georeferenced once\ncaptured. However, these historical labels commonly suffer from significant\npositional discrepancies with new images and only have one annotation (roof or\nfootprint), which fails to describe the correct structures of a building. To\naddress these discrepancies, we first introduce a concept of an alignment\ntoken, which encodes the correction vector to guide the label correction. Based\non this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel\nmodel designed to align dislocated historical labels with roofs and footprints.\nSpecifically, DragOSM formulates the label alignment as an interactive\ndenoising process, modeling the positional discrepancy as a Gaussian\ndistribution. During training, it learns to correct these errors by simulating\nmisalignment with random Gaussian perturbations; during inference, it\niteratively refines the positions of input labels. To validate our method, we\nfurther present a new dataset, Repairing Buildings in OSM (ReBO), comprising\n179,265 buildings with both OpenStreetMap and manually corrected annotations\nacross 5,473 images from 41 cities. Experimental results on ReBO demonstrate\nthe effectiveness of DragOSM. Code, dataset, and trained models are publicly\navailable at https://github.com/likaiucas/DragOSM.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDragOSM\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7\u4ea4\u4e92\u5f0f\u53bb\u566a\u8fc7\u7a0b\u5bf9\u9f50\u503e\u659c\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u5386\u53f2OpenStreetMap\u5efa\u7b51\u6807\u7b7e\uff08\u5c4b\u9876\u6216\u8f6e\u5ed3\uff09\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u4f4d\u7f6e\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b179,265\u4e2a\u5efa\u7b51\u7269\u7684ReBO\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u5272\u7684\u65b9\u6cd5\u5728\u5904\u7406\u503e\u659c\u89c6\u89d2\u9065\u611f\u56fe\u50cf\u65f6\u96be\u4ee5\u51c6\u786e\u63d0\u53d6\u5c4b\u9876\u548c\u5efa\u7b51\u8f6e\u5ed3\uff0c\u4e14\u5386\u53f2\u77e2\u91cf\u5730\u56fe\u6807\u7b7e\u5b58\u5728\u663e\u8457\u4f4d\u7f6e\u504f\u5dee\uff0c\u4ec5\u63d0\u4f9b\u5355\u4e00\u6807\u6ce8\uff08\u5c4b\u9876\u6216\u8f6e\u5ed3\uff09\uff0c\u65e0\u6cd5\u5b8c\u6574\u63cf\u8ff0\u5efa\u7b51\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u201c\u5bf9\u9f50\u6807\u8bb0\u201d\u6982\u5ff5\uff0c\u5c06\u4f4d\u7f6e\u504f\u5dee\u5efa\u6a21\u4e3a\u9ad8\u65af\u5206\u5e03\uff0c\u901a\u8fc7\u6a21\u62df\u968f\u673a\u9ad8\u65af\u6270\u52a8\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u8fed\u4ee3\u4f18\u5316\u8f93\u5165\u6807\u7b7e\u7684\u4f4d\u7f6e\uff0c\u5b9e\u73b0\u5386\u53f2\u6807\u7b7e\u4e0e\u5b9e\u9645\u5c4b\u9876\u53ca\u8f6e\u5ed3\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u65b0\u6784\u5efa\u7684ReBO\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cDragOSM\u80fd\u6709\u6548\u6821\u6b63OpenStreetMap\u6807\u7b7e\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u63d0\u5347\u503e\u659c\u56fe\u50cf\u4e2d\u5efa\u7b51\u5c4b\u9876\u4e0e\u8f6e\u5ed3\u7684\u63d0\u53d6\u7cbe\u5ea6\u3002", "conclusion": "DragOSM\u901a\u8fc7\u5b66\u4e60\u7ea0\u6b63\u5386\u53f2\u77e2\u91cf\u6807\u7b7e\u7684\u4f4d\u7f6e\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u503e\u659c\u89c6\u89d2\u4e0b\u5bf9\u5efa\u7b51\u7ed3\u6784\u7684\u7cbe\u786e\u91cd\u5efa\uff0c\u63a8\u52a8\u4e86\u5927\u89c4\u6a21\u57ce\u5e02\u5206\u6790\u4e2d\u9065\u611f\u56fe\u50cf\u4e0e\u5f00\u653e\u5730\u56fe\u6570\u636e\u7684\u878d\u5408\u5e94\u7528\u3002"}}
{"id": "2509.17955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17955", "abs": "https://arxiv.org/abs/2509.17955", "authors": ["Fan Xu", "Hao Wu", "Nan Wang", "Lilan Peng", "Kun Wang", "Wei Gong", "Xibin Zhao"], "title": "Breaking the Discretization Barrier of Continuous Physics Simulation Learning", "comment": null, "summary": "The modeling of complicated time-evolving physical dynamics from partial\nobservations is a long-standing challenge. Particularly, observations can be\nsparsely distributed in a seemingly random or unstructured manner, making it\ndifficult to capture highly nonlinear features in a variety of scientific and\nengineering problems. However, existing data-driven approaches are often\nconstrained by fixed spatial and temporal discretization. While some\nresearchers attempt to achieve spatio-temporal continuity by designing novel\nstrategies, they either overly rely on traditional numerical methods or fail to\ntruly overcome the limitations imposed by discretization. To address these, we\npropose CoPS, a purely data-driven methods, to effectively model continuous\nphysics simulation from partial observations. Specifically, we employ\nmultiplicative filter network to fuse and encode spatial information with the\ncorresponding observations. Then we customize geometric grids and use\nmessage-passing mechanism to map features from original spatial domain to the\ncustomized grids. Subsequently, CoPS models continuous-time dynamics by\ndesigning multi-scale graph ODEs, while introducing a Markov-based neural\nauto-correction module to assist and constrain the continuous extrapolations.\nComprehensive experiments demonstrate that CoPS advances the state-of-the-art\nmethods in space-time continuous modeling across various scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoPS\u7684\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u90e8\u5206\u89c2\u6d4b\u4e2d\u6709\u6548\u5efa\u6a21\u8fde\u7eed\u7269\u7406\u7cfb\u7edf\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u65f6\u7a7a\u79bb\u6563\u5316\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u7531\u4e8e\u89c2\u6d4b\u6570\u636e\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u53ef\u80fd\u7a00\u758f\u4e14\u4e0d\u89c4\u5219\u5206\u5e03\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u975e\u7ebf\u6027\u7269\u7406\u52a8\u6001\uff0c\u4e14\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u65f6\u7a7a\u79bb\u6563\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u771f\u6b63\u8fde\u7eed\u5efa\u6a21\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e58\u6cd5\u6ee4\u6ce2\u7f51\u7edc\u878d\u5408\u7f16\u7801\u7a7a\u95f4\u4fe1\u606f\u4e0e\u89c2\u6d4b\u6570\u636e\uff0c\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u51e0\u4f55\u7f51\u683c\u5e76\u5229\u7528\u6d88\u606f\u4f20\u9012\u673a\u5236\u6620\u5c04\u7279\u5f81\uff1b\u901a\u8fc7\u6784\u5efa\u591a\u5c3a\u5ea6\u56fe\u5e38\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u7684\u795e\u7ecf\u81ea\u6821\u6b63\u6a21\u5757\u8f85\u52a9\u548c\u7ea6\u675f\u5916\u63a8\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoPS\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u65f6\u7a7a\u8fde\u7eed\u5efa\u6a21\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u590d\u6742\u7269\u7406\u52a8\u6001\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CoPS\u662f\u4e00\u79cd\u6709\u6548\u7684\u8fde\u7eed\u7269\u7406\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u5728\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7a81\u7834\u4f20\u7edf\u79bb\u6563\u5316\u9650\u5236\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u65f6\u7a7a\u8fde\u7eed\u9884\u6d4b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.17968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17968", "abs": "https://arxiv.org/abs/2509.17968", "authors": ["Qizhen Lan", "Jung Im Choi", "Qing Tian"], "title": "Visual Detector Compression via Location-Aware Discriminant Analysis", "comment": null, "summary": "Deep neural networks are powerful, yet their high complexity greatly limits\ntheir potential to be deployed on billions of resource-constrained edge\ndevices. Pruning is a crucial network compression technique, yet most existing\nmethods focus on classification models, with limited attention to detection.\nEven among those addressing detection, there is a lack of utilization of\nessential localization information. Also, many pruning methods passively rely\non pre-trained models, in which useful and useless components are intertwined,\nmaking it difficult to remove the latter without harming the former at the\nneuron/filter level. To address the above issues, in this paper, we propose a\nproactive detection-discriminants-based network compression approach for deep\nvisual detectors, which alternates between two steps: (1) maximizing and\ncompressing detection-related discriminants and aligning them with a subset of\nneurons/filters immediately before the detection head, and (2) tracing the\ndetection-related discriminating power across the layers and discarding\nfeatures of lower importance. Object location information is exploited in both\nsteps. Extensive experiments, employing four advanced detection models and four\nstate-of-the-art competing methods on the KITTI and COCO datasets, highlight\nthe superiority of our approach. Remarkably, our compressed models can even\nbeat the original base models with a substantial reduction in complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68c0\u6d4b\u5224\u522b\u6027\u7684\u4e3b\u52a8\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\uff0c\u5229\u7528\u76ee\u6807\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u591a\u5173\u6ce8\u5206\u7c7b\u6a21\u578b\uff0c\u5bf9\u68c0\u6d4b\u6a21\u578b\u7684\u5173\u6ce8\u8f83\u5c11\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5b9a\u4f4d\u4fe1\u606f\u7684\u5229\u7528\uff1b\u540c\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u96be\u4ee5\u6709\u6548\u533a\u5206\u6709\u7528\u548c\u65e0\u7528\u7ec4\u4ef6\u3002", "method": "\u63d0\u51fa\u4ea4\u66ff\u8fdb\u884c\u7684\u4e24\u6b65\u6cd5\uff1a(1) \u6700\u5927\u5316\u5e76\u538b\u7f29\u4e0e\u68c0\u6d4b\u76f8\u5173\u7684\u5224\u522b\u6027\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u5bf9\u9f50\u5230\u68c0\u6d4b\u5934\u524d\u7684\u795e\u7ecf\u5143/\u6ee4\u6ce2\u5668\u5b50\u96c6\uff1b(2) \u8ffd\u8e2a\u5404\u5c42\u4e2d\u68c0\u6d4b\u76f8\u5173\u7684\u5224\u522b\u80fd\u529b\uff0c\u4e22\u5f03\u91cd\u8981\u6027\u8f83\u4f4e\u7684\u7279\u5f81\u3002\u6574\u4e2a\u8fc7\u7a0b\u5145\u5206\u5229\u7528\u76ee\u6807\u4f4d\u7f6e\u4fe1\u606f\u3002", "result": "\u5728KITTI\u548cCOCO\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u56db\u79cd\u5148\u8fdb\u68c0\u6d4b\u6a21\u578b\u548c\u56db\u79cd\u524d\u6cbf\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u538b\u7f29\u540e\u7684\u6a21\u578b\u5728\u590d\u6742\u5ea6\u5927\u5e45\u964d\u4f4e\u7684\u540c\u65f6\u6027\u80fd\u751a\u81f3\u8d85\u8fc7\u539f\u59cb\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u68c0\u6d4b\u6a21\u578b\u526a\u679d\u4e2d\u7f3a\u4e4f\u5b9a\u4f4d\u4fe1\u606f\u5229\u7528\u548c\u7ec4\u4ef6\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u590d\u6742\u5ea6\u7684\u89c6\u89c9\u68c0\u6d4b\u6a21\u578b\u538b\u7f29\u3002"}}
{"id": "2509.17993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.17993", "abs": "https://arxiv.org/abs/2509.17993", "authors": ["Haoxin Yang", "Bangzhen Liu", "Xuemiao Xu", "Cheng Xu", "Yuyang Yu", "Zikai Huang", "Yi Wang", "Shengfeng He"], "title": "StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models", "comment": "Accepted by NeurIPS 2025", "summary": "The advancement of diffusion models has enhanced the realism of AI-generated\ncontent but also raised concerns about misuse, necessitating robust copyright\nprotection and tampering localization. Although recent methods have made\nprogress toward unified solutions, their reliance on post hoc processing\nintroduces considerable application inconvenience and compromises forensic\nreliability. We propose StableGuard, a novel framework that seamlessly\nintegrates a binary watermark into the diffusion generation process, ensuring\ncopyright protection and tampering localization in Latent Diffusion Models\nthrough an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)\nby equipping a pretrained Variational Autoencoder (VAE) with a lightweight\nlatent residual-based adapter, enabling the generation of paired watermarked\nand watermark-free images. These pairs, fused via random masks, create a\ndiverse dataset for training a tampering-agnostic forensic network. To further\nenhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic\nNetwork (MoE-GFN) that dynamically integrates holistic watermark patterns,\nlocal tampering traces, and frequency-domain cues for precise watermark\nverification and tampered region detection. The MPW-VAE and MoE-GFN are jointly\noptimized in a self-supervised, end-to-end manner, fostering a reciprocal\ntraining between watermark embedding and forensic accuracy. Extensive\nexperiments demonstrate that StableGuard consistently outperforms\nstate-of-the-art methods in image fidelity, watermark verification, and\ntampering localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStableGuard\uff0c\u4e00\u79cd\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u4e8c\u503c\u6c34\u5370\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5b9e\u73b0\u7248\u6743\u4fdd\u62a4\u4e0e\u7be1\u6539\u5b9a\u4f4d\u7684\u7edf\u4e00\uff0c\u65e0\u9700\u540e\u5904\u7406\uff0c\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u4e0e\u6cd5\u533b\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u540e\u5904\u7406\u8fdb\u884c\u7248\u6743\u4fdd\u62a4\u548c\u7be1\u6539\u5b9a\u4f4d\uff0c\u5b58\u5728\u5e94\u7528\u4e0d\u4fbf\u548c\u6cd5\u533b\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1Multiplexing Watermark VAE\uff08MPW-VAE\uff09\u5728\u6f5c\u7a7a\u95f4\u5d4c\u5165\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u9002\u914d\u5668\u751f\u6210\u5e26\u6c34\u5370\u4e0e\u65e0\u6c34\u5370\u56fe\u50cf\u5bf9\uff0c\u5e76\u878d\u5408\u968f\u673a\u63a9\u7801\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\uff1b\u63d0\u51faMixture-of-Experts Guided Forensic Network\uff08MoE-GFN\uff09\u52a8\u6001\u878d\u5408\u5168\u5c40\u6c34\u5370\u3001\u5c40\u90e8\u7be1\u6539\u75d5\u8ff9\u548c\u9891\u57df\u7ebf\u7d22\uff0c\u5b9e\u73b0\u7cbe\u786e\u9a8c\u8bc1\u4e0e\u68c0\u6d4b\uff1b\u4e24\u8005\u8054\u5408\u81ea\u76d1\u7763\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStableGuard\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u6c34\u5370\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u7be1\u6539\u533a\u57df\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "StableGuard\u901a\u8fc7\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316\u6c34\u5370\u5d4c\u5165\u4e0e\u53d6\u8bc1\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u53ef\u9760\u6027\u7684\u7248\u6743\u4fdd\u62a4\u4e0e\u7be1\u6539\u5b9a\u4f4d\u4e00\u4f53\u5316\u65b9\u6848\u3002"}}
{"id": "2509.18015", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18015", "abs": "https://arxiv.org/abs/2509.18015", "authors": ["Advait Gosai", "Arun Kavishwar", "Stephanie L. McNamara", "Soujanya Samineni", "Renato Umeton", "Alexander Chowdhury", "William Lotter"], "title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs", "comment": null, "summary": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86GPT-5\u3001GPT-4\u548cMedGemma\u5728\u80f8\u90e8X\u5149\u7247\u75c5\u7406\u5b9a\u4f4d\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1GPT-5\u8868\u73b0\u6700\u4f73\uff0849.7%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u4ecd\u4f4e\u4e8e\u4e13\u7528\u6a21\u578b\u548c\u653e\u5c04\u79d1\u533b\u751f\uff0c\u663e\u793a\u51fa\u5f53\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u75c5\u7406\u7684\u80fd\u529b\uff0c\u4e0d\u4ec5\u5177\u6709\u4e34\u5e8a\u548c\u6559\u80b2\u610f\u4e49\uff0c\u8fd8\u80fd\u63ed\u793a\u6a21\u578b\u5bf9\u89e3\u5256\u7ed3\u6784\u548c\u75be\u75c5\u7684\u7a7a\u95f4\u7406\u89e3\u6c34\u5e73\u3002", "method": "\u91c7\u7528\u8986\u76d6\u7a7a\u95f4\u7f51\u683c\u5e76\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa\u5750\u6807\u9884\u6d4b\u7684\u63d0\u793a\u6d41\u7a0b\uff0c\u7cfb\u7edf\u8bc4\u4f30GPT-5\u3001GPT-4\u548cMedGemma\u5728CheXlocalize\u6570\u636e\u96c6\u4e0a\u5bf9\u4e5d\u79cd\u75c5\u7406\u7684\u5b9a\u4f4d\u80fd\u529b\u3002", "result": "GPT-5\u5e73\u5747\u5b9a\u4f4d\u51c6\u786e\u7387\u4e3a49.7%\uff0cGPT-4\u4e3a39.1%\uff0cMedGemma\u4e3a17.7%\uff0c\u5747\u4f4e\u4e8e\u4e13\u7528CNN\u6a21\u578b\uff0859.9%\uff09\u548c\u653e\u5c04\u79d1\u533b\u751f\uff0880.1%\uff09\uff1bGPT-5\u9884\u6d4b\u591a\u4f4d\u4e8e\u89e3\u5256\u5b66\u5408\u7406\u533a\u57df\u4f46\u4e0d\u591f\u7cbe\u786e\uff0cGPT-4\u5bf9\u4f4d\u7f6e\u56fa\u5b9a\u7684\u75c5\u53d8\u8868\u73b0\u8f83\u597d\uff0cMedGemma\u6574\u4f53\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u5f53\u524d\u901a\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u75c5\u7406\u5b9a\u4f4d\u65b9\u9762\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u6027\u80fd\u4ecd\u6709\u9650\uff0c\u9700\u7ed3\u5408\u4efb\u52a1\u4e13\u7528\u5de5\u5177\u4ee5\u5b9e\u73b0\u53ef\u9760\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2509.18041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18041", "abs": "https://arxiv.org/abs/2509.18041", "authors": ["Sahil Shah", "S P Sharan", "Harsh Goel", "Minkyu Choi", "Mustafa Munir", "Manvik Pasula", "Radu Marculescu", "Sandeep Chinchali"], "title": "NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning", "comment": null, "summary": "Long-Form Video Question Answering (LVQA) poses challenges beyond traditional\nvisual question answering (VQA), which is often limited to static images or\nshort video clips. While current vision-language models (VLMs) perform well in\nthose settings, they struggle with complex queries in LVQA over long videos\ninvolving multi-step temporal reasoning and causality. Vanilla approaches,\nwhich sample frames uniformly and feed them to a VLM with the question, incur\nsignificant token overhead, forcing severe downsampling. As a result, the model\noften misses fine-grained visual structure, subtle event transitions, or key\ntemporal cues, ultimately leading to incorrect answers. To address these\nlimitations, recent works have explored query-adaptive frame sampling,\nhierarchical keyframe selection, and agent-based iterative querying. However,\nthese methods remain fundamentally heuristic: they lack explicit temporal\nrepresentations and cannot enforce or verify logical event relationships. As a\nresult, there are no formal guarantees that the sampled context actually\nencodes the compositional or causal logic demanded by the question. To address\nthese foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play\nneuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language\nquestion into a formal temporal logic expression, constructs a video automaton\nfrom frame-level semantic propositions, and applies model checking to\nrigorously identify video segments satisfying the question's logical\nrequirements. Only these logic-verified segments are submitted to the VLM, thus\nimproving interpretability, reducing hallucinations, and enabling compositional\nreasoning without modifying or fine-tuning the model. Experiments on\nLongVideoBench and CinePile show NeuS-QA improves performance by over 10%,\nespecially on questions involving event ordering, causality, and multi-step\ncompositional reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NeuS-QA\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u95ee\u7b54\uff08LVQA\uff09\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u65f6\u5e8f\u903b\u8f91\uff0c\u6784\u5efa\u89c6\u9891\u81ea\u52a8\u673a\u5e76\u5e94\u7528\u6a21\u578b\u9a8c\u8bc1\u6765\u7cbe\u786e\u8bc6\u522b\u6ee1\u8db3\u95ee\u9898\u903b\u8f91\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e8b\u4ef6\u987a\u5e8f\u3001\u56e0\u679c\u5173\u7cfb\u548c\u591a\u6b65\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u95ee\u7b54\u65f6\u56e0\u5747\u5300\u91c7\u6837\u5e27\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u65f6\u5e8f\u903b\u8f91\u548c\u56e0\u679c\u5173\u7cfb\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u96be\u4ee5\u4fdd\u8bc1\u91c7\u6837\u4e0a\u4e0b\u6587\u7b26\u5408\u95ee\u9898\u7684\u590d\u5408\u903b\u8f91\u9700\u6c42\u3002", "method": "NeuS-QA\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3a\u5f62\u5f0f\u5316\u65f6\u5e8f\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u57fa\u4e8e\u5e27\u7ea7\u8bed\u4e49\u547d\u9898\u6784\u5efa\u89c6\u9891\u81ea\u52a8\u673a\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u9a8c\u8bc1\u6280\u672f\u8bc6\u522b\u6ee1\u8db3\u903b\u8f91\u6761\u4ef6\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u4ec5\u5c06\u8fd9\u4e9b\u7ecf\u8fc7\u903b\u8f91\u9a8c\u8bc1\u7684\u7247\u6bb5\u8f93\u5165VLM\u8fdb\u884c\u56de\u7b54\u3002", "result": "\u5728LongVideoBench\u548cCinePile\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cNeuS-QA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u8d85\u8fc710%\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u4e8b\u4ef6\u987a\u5e8f\u3001\u56e0\u679c\u6027\u548c\u591a\u6b65\u7ec4\u5408\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "NeuS-QA\u901a\u8fc7\u5f15\u5165\u5f62\u5f0f\u5316\u65f6\u5e8f\u903b\u8f91\u4e0e\u6a21\u578b\u9a8c\u8bc1\u673a\u5236\uff0c\u5728\u4e0d\u4fee\u6539\u6216\u5fae\u8c03VLM\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u957f\u89c6\u9891\u95ee\u7b54\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u65f6\u5e8f\u5efa\u6a21\u548c\u903b\u8f91\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2509.18056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18056", "abs": "https://arxiv.org/abs/2509.18056", "authors": ["Yunheng Li", "Jing Cheng", "Shaoyong Jia", "Hangyi Kuang", "Shaohui Jiao", "Qibin Hou", "Ming-Ming Cheng"], "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs", "comment": "Accepted at NeurIPS 2025", "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TempSamp-R1\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u7684\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u7b56\u7565\u76d1\u7763\u548c\u975e\u7ebf\u6027\u4f18\u52bf\u8ba1\u7b97\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u5bf9\u9f50\u80fd\u529b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eon-policy\u91c7\u6837\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u5927\u65f6\u5e8f\u641c\u7d22\u7a7a\u95f4\u4e2d\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u627e\u5230\u51c6\u786e\u7684\u65f6\u95f4\u8fb9\u754c\uff0c\u9650\u5236\u4e86\u5176\u5728\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "TempSamp-R1\u5229\u7528\u771f\u5b9e\u6807\u6ce8\u4f5c\u4e3a\u79bb\u7b56\u7565\u76d1\u7763\u4fe1\u53f7\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u65f6\u5e8f\u6307\u5bfc\uff0c\u5e76\u91c7\u7528\u975e\u7ebf\u6027\u8f6f\u4f18\u52bf\u8ba1\u7b97\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u5956\u52b1\u53cd\u9988\uff1b\u7ed3\u5408\u6df7\u5408Chain-of-Thought\u8bad\u7ec3\u8303\u5f0f\uff0c\u7edf\u4e00\u4f18\u5316\u652f\u6301CoT\u4e0e\u975eCoT\u63a8\u7406\u7684\u6a21\u578b\u3002", "result": "TempSamp-R1\u5728Charades-STA\u3001ActivityNet Captions\u548cQVHighlights\u4e09\u4e2a\u57fa\u51c6\u4e0a\u5747\u8d85\u8d8aGRPO\u57fa\u7ebf\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TempSamp-R1\u6709\u6548\u89e3\u51b3\u4e86on-policy\u65b9\u6cd5\u5728\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u79bb\u7b56\u7565\u76d1\u7763\u548c\u5956\u52b1\u91cd\u5851\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u65f6\u5e8f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18081", "abs": "https://arxiv.org/abs/2509.18081", "authors": ["Md. Mahmudul Hasan", "Ahmed Nesar Tahsin Choudhury", "Mahmudul Hasan", "Md. Mosaddek Khan"], "title": "GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer", "comment": "7 pages. Accepted at the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP) System Demonstrations. Equal\n  Contribution: Md. Mahmudul Hasan and Ahmed Nesar Tahsin Choudhury", "summary": "Despite Bengali being the sixth most spoken language in the world,\nhandwritten text recognition (HTR) systems for Bengali remain severely\nunderdeveloped. The complexity of Bengali script--featuring conjuncts,\ndiacritics, and highly variable handwriting styles--combined with a scarcity of\nannotated datasets makes this task particularly challenging. We present\nGraDeT-HTR, a resource-efficient Bengali handwritten text recognition system\nbased on a Grapheme-aware Decoder-only Transformer architecture. To address the\nunique challenges of Bengali script, we augment the performance of a\ndecoder-only transformer by integrating a grapheme-based tokenizer and\ndemonstrate that it significantly improves recognition accuracy compared to\nconventional subword tokenizers. Our model is pretrained on large-scale\nsynthetic data and fine-tuned on real human-annotated samples, achieving\nstate-of-the-art performance on multiple benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7d20\u611f\u77e5\u7684\u89e3\u7801\u5668-only Transformer\u67b6\u6784\uff08GraDeT-HTR\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u8bc6\u522b\u6027\u80fd\uff0c\u7ed3\u5408\u56fe\u7d20\u5206\u8bcd\u5668\u5e76\u5728\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bc6\u522b\u6548\u679c\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u662f\u4e16\u754c\u7b2c\u516d\u5927\u8bed\u8a00\uff0c\u4f46\u5176\u624b\u5199\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\u53d1\u5c55\u6ede\u540e\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6587\u5b57\u590d\u6742\u6027\uff08\u5982\u8fde\u4f53\u5b57\u3001\u53d8\u97f3\u7b26\u53f7\uff09\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u91c7\u7528\u56fe\u7d20\u611f\u77e5\u7684\u89e3\u7801\u5668-only Transformer\u67b6\u6784\uff0c\u5f15\u5165\u57fa\u4e8e\u56fe\u7d20\u7684\u5206\u8bcd\u5668\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u5728\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u4e0a\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5b50\u8bcd\u5206\u8bcd\u5668\u3002", "conclusion": "GraDeT-HTR\u901a\u8fc7\u7ed3\u5408\u56fe\u7d20\u5206\u8bcd\u548c\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8d44\u6e90\u6709\u9650\u6761\u4ef6\u4e0b\u7684\u5b5f\u52a0\u62c9\u8bed\u624b\u5199\u6587\u672c\u8bc6\u522b\u51c6\u786e\u7387\u3002"}}
{"id": "2509.18090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18090", "abs": "https://arxiv.org/abs/2509.18090", "authors": ["Jiahe Li", "Jiawei Zhang", "Youmin Zhang", "Xiao Bai", "Jin Zheng", "Xiaohan Yu", "Lin Gu"], "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction", "comment": "Accepted at NeurIPS 2025 (Spotlight). Project page:\n  https://fictionarry.github.io/GeoSVR-project/", "summary": "Reconstructing accurate surfaces with radiance fields has achieved remarkable\nprogress in recent years. However, prevailing approaches, primarily based on\nGaussian Splatting, are increasingly constrained by representational\nbottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based\nframework that explores and extends the under-investigated potential of sparse\nvoxels for achieving accurate, detailed, and complete surface reconstruction.\nAs strengths, sparse voxels support preserving the coverage completeness and\ngeometric clarity, while corresponding challenges also arise from absent scene\nconstraints and locality in surface refinement. To ensure correct scene\nconvergence, we first propose a Voxel-Uncertainty Depth Constraint that\nmaximizes the effect of monocular depth cues while presenting a voxel-oriented\nuncertainty to avoid quality degradation, enabling effective and robust scene\nconstraints yet preserving highly accurate geometries. Subsequently, Sparse\nVoxel Surface Regularization is designed to enhance geometric consistency for\ntiny voxels and facilitate the voxel-based formation of sharp and accurate\nsurfaces. Extensive experiments demonstrate our superior performance compared\nto existing methods across diverse challenging scenarios, excelling in\ngeometric accuracy, detail preservation, and reconstruction completeness while\nmaintaining high efficiency. Code is available at\nhttps://github.com/Fictionarry/GeoSVR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GeoSVR\uff0c\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4f53\u7d20\u7684\u663e\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u51c6\u786e\u3001\u7ec6\u81f4\u4e14\u5b8c\u6574\u7684\u8868\u9762\u91cd\u5efa\uff0c\u76f8\u8f83\u4e8e\u57fa\u4e8e\u9ad8\u65af\u70b9\u9635\u7684\u65b9\u6cd5\uff0c\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u7ec6\u8282\u4fdd\u6301\u548c\u91cd\u5efa\u5b8c\u6574\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9ad8\u65af\u70b9\u9635\u7684\u8f90\u5c04\u573a\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u53d7\u9650\u4e8e\u8868\u793a\u80fd\u529b\uff0c\u96be\u4ee5\u517c\u987e\u51e0\u4f55\u6e05\u6670\u5ea6\u4e0e\u573a\u666f\u5b8c\u6574\u6027\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8868\u793a\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u7684\u4f53\u7d20\u5316\u6846\u67b6GeoSVR\uff0c\u5f15\u5165\u4f53\u7d20\u4e0d\u786e\u5b9a\u6027\u6df1\u5ea6\u7ea6\u675f\u4ee5\u589e\u5f3a\u5355\u76ee\u6df1\u5ea6\u7ebf\u7d22\u5e76\u907f\u514d\u8d28\u91cf\u4e0b\u964d\uff0c\u540c\u65f6\u8bbe\u8ba1\u7a00\u758f\u4f53\u7d20\u8868\u9762\u6b63\u5219\u5316\u65b9\u6cd5\u63d0\u5347\u5c0f\u4f53\u7d20\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4fc3\u8fdb\u9510\u5229\u7cbe\u786e\u8868\u9762\u7684\u5f62\u6210\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\u5b9e\u9a8c\u8868\u660e\uff0cGeoSVR\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u91cd\u5efa\u5b8c\u6574\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u3002", "conclusion": "GeoSVR\u901a\u8fc7\u663e\u5f0f\u7a00\u758f\u4f53\u7d20\u5efa\u6a21\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u8d28\u91cf\u8868\u9762\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18092", "abs": "https://arxiv.org/abs/2509.18092", "authors": ["Guocheng Gordon Qian", "Daniil Ostashev", "Egor Nemchinov", "Avihay Assouline", "Sergey Tulyakov", "Kuan-Chieh Jackson Wang", "Kfir Aberman"], "title": "ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation", "comment": "Accepted to SIGGRAPH Asia 2025, webpage:\n  https://snap-research.github.io/composeme/", "summary": "Generating high-fidelity images of humans with fine-grained control over\nattributes such as hairstyle and clothing remains a core challenge in\npersonalized text-to-image synthesis. While prior methods emphasize identity\npreservation from a reference image, they lack modularity and fail to provide\ndisentangled control over specific visual attributes. We introduce a new\nparadigm for attribute-specific image prompting, in which distinct sets of\nreference images are used to guide the generation of individual aspects of\nhuman appearance, such as hair, clothing, and identity. Our method encodes\nthese inputs into attribute-specific tokens, which are injected into a\npre-trained text-to-image diffusion model. This enables compositional and\ndisentangled control over multiple visual factors, even across multiple people\nwithin a single image. To promote natural composition and robust\ndisentanglement, we curate a cross-reference training dataset featuring\nsubjects in diverse poses and expressions, and propose a multi-attribute\ncross-reference training strategy that encourages the model to generate\nfaithful outputs from misaligned attribute inputs while adhering to both\nidentity and textual conditioning. Extensive experiments show that our method\nachieves state-of-the-art performance in accurately following both visual and\ntextual prompts. Our framework paves the way for more configurable human image\nsynthesis by combining visual prompting with text-driven generation. Webpage is\navailable at: https://snap-research.github.io/composeme/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u7279\u5b9a\u56fe\u50cf\u63d0\u793a\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u53c2\u8003\u56fe\u50cf\u96c6\u5206\u522b\u63a7\u5236\u4eba\u7269\u7684\u53d1\u578b\u3001\u670d\u88c5\u548c\u8eab\u4efd\u7b49\u7279\u5f81\uff0c\u5728\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u89e3\u8026\u4e14\u53ef\u7ec4\u5408\u7684\u751f\u6210\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u867d\u80fd\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u89c6\u89c9\u5c5e\u6027\uff08\u5982\u53d1\u578b\u3001\u670d\u88c5\uff09\u7684\u6a21\u5757\u5316\u548c\u89e3\u8026\u63a7\u5236\u3002", "method": "\u5c06\u591a\u4e2a\u53c2\u8003\u56fe\u50cf\u7f16\u7801\u4e3a\u5c5e\u6027\u7279\u5b9a\u7684token\uff0c\u5e76\u6ce8\u5165\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff1b\u63d0\u51fa\u591a\u5c5e\u6027\u4ea4\u53c9\u53c2\u8003\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u5305\u542b\u591a\u6837\u59ff\u6001\u548c\u8868\u60c5\u7684\u4ea4\u53c9\u53c2\u8003\u6570\u636e\u96c6\u4ee5\u589e\u5f3a\u89e3\u8026\u4e0e\u81ea\u7136\u7ec4\u5408\u80fd\u529b\u3002", "result": "\u5728\u9075\u5faa\u89c6\u89c9\u548c\u6587\u672c\u63d0\u793a\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u591a\u4eba\u573a\u666f\u4e0b\u7684\u7ec6\u7c92\u5ea6\u5c5e\u6027\u63a7\u5236\uff0c\u5373\u4f7f\u8f93\u5165\u5c5e\u6027\u4e0d\u5bf9\u9f50\u4e5f\u80fd\u751f\u6210\u5fe0\u5b9e\u4e14\u81ea\u7136\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u63d0\u793a\u4e0e\u6587\u672c\u751f\u6210\uff0c\u4e3a\u53ef\u914d\u7f6e\u7684\u4eba\u7c7b\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.18094", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18094", "abs": "https://arxiv.org/abs/2509.18094", "authors": ["Ye Liu", "Zongyang Ma", "Junfu Pu", "Zhongang Qi", "Yang Wu", "Ying Shan", "Chang Wen Chen"], "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning", "comment": "NeurIPS 2025 Camera Ready. Project Page:\n  https://polyu-chenlab.github.io/unipixel/", "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniPixel\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u591f\u7075\u6d3b\u7406\u89e3\u89c6\u89c9\u63d0\u793a\u5e76\u751f\u6210\u57fa\u4e8e\u63a9\u7801\u7684\u54cd\u5e94\uff0c\u5b9e\u73b0\u4e86\u50cf\u7d20\u7ea7\u611f\u77e5\u4e0e\u901a\u7528\u89c6\u89c9\u7406\u89e3\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u50cf\u7d20\u7ea7\u7406\u89e3\uff08\u5982\u89c6\u89c9\u4fe1\u53f7\u4e0e\u8bed\u8a00\u8bed\u4e49\u7684\u50cf\u7d20\u5bf9\u9f50\uff09\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4e14\u65e0\u6cd5\u5c06\u6307\u4ee3\u3001\u5206\u5272\u7b49\u4efb\u52a1\u6574\u5408\u5230\u89c6\u89c9\u63a8\u7406\u4e2d\u3002", "method": "\u63d0\u51faUniPixel\u6a21\u578b\uff0c\u901a\u8fc7\u5904\u7406\u89c6\u89c9\u63d0\u793a\u751f\u6210\u6240\u9700\u63a9\u7801\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u57fa\u4e8e\u8fd9\u4e9b\u4e2d\u95f4\u6307\u9488\u8fdb\u884c\u540e\u7eed\u63a8\u7406\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u7ec6\u7c92\u5ea6\u7406\u89e3\u4e0e\u591a\u4efb\u52a1\u878d\u5408\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6db5\u76d6\u56fe\u50cf/\u89c6\u9891\u4e2d\u7684\u50cf\u7d20\u7ea7\u6307\u4ee3\u3001\u5206\u5272\u53ca\u5bf9\u8c61\u7406\u89e3\u4efb\u52a1\uff1b\u8bbe\u8ba1\u4e86\u65b0\u7684PixelQA\u4efb\u52a1\u9a8c\u8bc1\u6a21\u578b\u7075\u6d3b\u6027\u3002", "conclusion": "UniPixel\u6210\u529f\u878d\u5408\u4e86\u50cf\u7d20\u7ea7\u611f\u77e5\u4e0e\u901a\u7528\u89c6\u89c9\u7406\u89e3\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a8\u7406\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u591a\u6a21\u6001\u7406\u89e3\u6f5c\u529b\u3002"}}
{"id": "2509.18096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18096", "abs": "https://arxiv.org/abs/2509.18096", "authors": ["Chaehyun Kim", "Heeseong Shin", "Eunbeen Hong", "Heeji Yoon", "Anurag Arnab", "Paul Hongsuck Seo", "Sunghwan Hong", "Seungryong Kim"], "title": "Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers", "comment": "NeurIPS 2025. Project page: https://cvlab-kaist.github.io/Seg4Diff/", "summary": "Text-to-image diffusion models excel at translating language prompts into\nphotorealistic images by implicitly grounding textual concepts through their\ncross-modal attention mechanisms. Recent multi-modal diffusion transformers\nextend this by introducing joint self-attention over concatenated image and\ntext tokens, enabling richer and more scalable cross-modal alignment. However,\na detailed understanding of how and where these attention maps contribute to\nimage generation remains limited. In this paper, we introduce Seg4Diff\n(Segmentation for Diffusion), a systematic framework for analyzing the\nattention structures of MM-DiT, with a focus on how specific layers propagate\nsemantic information from text to image. Through comprehensive analysis, we\nidentify a semantic grounding expert layer, a specific MM-DiT block that\nconsistently aligns text tokens with spatially coherent image regions,\nnaturally producing high-quality semantic segmentation masks. We further\ndemonstrate that applying a lightweight fine-tuning scheme with mask-annotated\nimage data enhances the semantic grouping capabilities of these layers and\nthereby improves both segmentation performance and generated image fidelity.\nOur findings demonstrate that semantic grouping is an emergent property of\ndiffusion transformers and can be selectively amplified to advance both\nsegmentation and generation performance, paving the way for unified models that\nbridge visual perception and generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Seg4Diff\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u591a\u6a21\u6001\u6269\u6563Transformer\uff08MM-DiT\uff09\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53d1\u73b0\u7279\u5b9a\u5c42\u80fd\u81ea\u7136\u5b9e\u73b0\u6587\u672c\u5230\u56fe\u50cf\u7684\u8bed\u4e49\u5bf9\u9f50\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u5206\u5272\u63a9\u7801\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u5fae\u8c03\u63d0\u5347\u5206\u5272\u4e0e\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6ce8\u610f\u529b\u673a\u5236\u5982\u4f55\u4fc3\u8fdb\u8bed\u4e49\u5bf9\u9f50\u5c1a\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u51faSeg4Diff\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u548c\u5206\u6790MM-DiT\u4e2d\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u56fe\uff0c\u8bc6\u522b\u8d1f\u8d23\u8bed\u4e49\u63a5\u5730\u7684\u5173\u952e\u7f51\u7edc\u5c42\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u63a9\u7801\u6807\u6ce8\u6570\u636e\u7684\u8f7b\u91cf\u5fae\u8c03\u7b56\u7565\u4ee5\u589e\u5f3a\u8bed\u4e49\u5206\u7ec4\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u4e00\u4e2a\u2018\u8bed\u4e49\u63a5\u5730\u4e13\u5bb6\u5c42\u2019\u80fd\u7a33\u5b9a\u5730\u5c06\u6587\u672ctoken\u4e0e\u56fe\u50cf\u7a7a\u95f4\u533a\u57df\u5bf9\u9f50\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u8bed\u4e49\u5206\u5272\u63a9\u7801\uff1b\u5fae\u8c03\u540e\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u548c\u56fe\u50cf\u751f\u6210\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8bed\u4e49\u5206\u7ec4\u662f\u6269\u6563Transformer\u7684\u6d8c\u73b0\u7279\u6027\uff0c\u53ef\u901a\u8fc7\u9488\u5bf9\u6027\u4f18\u5316\u52a0\u4ee5\u653e\u5927\uff0c\u4e3a\u8fde\u63a5\u89c6\u89c9\u611f\u77e5\u4e0e\u751f\u6210\u7684\u7edf\u4e00\u6a21\u578b\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
